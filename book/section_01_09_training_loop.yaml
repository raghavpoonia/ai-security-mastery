# section_01_09_training_loop.yaml

---
document_info:
  chapter: "01"
  section: "09"
  title: "Training Loop and Convergence"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-30"
  estimated_pages: 6
  tags: ["training-loop", "convergence", "gradient-descent", "batch-sgd-minibatch", "learning-rate", "stopping-criteria"]

# ============================================================================
# SECTION 1.09: TRAINING LOOP AND CONVERGENCE
# ============================================================================

section_01_09_training_loop:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    You've implemented logistic regression and gradient descent, but critical questions 
    remain: When do you stop training? How do you know if training is working? What if 
    loss isn't decreasing? Why is training taking forever?
    
    The training loop is where theory meets practice. It's the heartbeat of machine 
    learning: iterate, compute gradients, update parameters, check progress, repeat. 
    Understanding how to monitor training, diagnose problems, and know when to stop is 
    essential for building reliable models.
    
    This section covers the complete training workflow: batch vs stochastic vs mini-batch 
    gradient descent, learning rate schedules, convergence criteria, early stopping, and 
    debugging techniques. You'll learn to recognize training failures (not converging, 
    overfitting, getting stuck) and fix them.
    
    For security engineers, proper training is critical because:
    1. Poorly trained models have high false positive/negative rates
    2. Training failures can be caused by adversarial data poisoning
    3. Convergence monitoring detects data distribution shifts
    4. Production models need stable, reliable training pipelines
  
  why_this_matters: |
    Security context:
    - Production detectors retrain regularly (daily/weekly) as attacks evolve
    - Training failures = blind spots in detection coverage
    - Slow convergence = delayed response to new attack patterns
    - Overfitting = detector memorizes training attacks, misses variants
    - Convergence monitoring = early warning for data poisoning
    
    Real scenarios:
    - Spam filter retrains nightly on new spam samples
    - Fraud detector retrains when fraud patterns change
    - Malware detector retrains as new malware families emerge
    - IDS retrains when network topology changes
    
    Need robust training pipeline that:
    - Converges reliably despite noisy data
    - Stops at right time (not too early, not too late)
    - Detects and handles adversarial training data
    - Scales to millions of samples
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Gradient Descent Variants
  # --------------------------------------------------------------------------
  
  gradient_descent_variants:
    
    batch_gradient_descent:
      
      description: "Use ALL training samples to compute single gradient update"
      
      algorithm: |
        For each iteration:
          1. Compute predictions for ALL samples: ŷ = model(X_all)
          2. Compute loss on ALL samples: J = loss(y_all, ŷ)
          3. Compute gradient using ALL samples: ∇J = gradient(X_all, y_all, ŷ)
          4. Update parameters: θ = θ - α∇J
      
      pros:
        - "Stable convergence (smooth gradient)"
        - "Guaranteed to decrease loss each iteration (with small learning rate)"
        - "Optimal for convex problems (finds global minimum)"
      
      cons:
        - "Slow for large datasets (compute gradient on millions of samples)"
        - "Memory intensive (load all data at once)"
        - "Can get stuck in local minima (for non-convex problems)"
      
      when_to_use: "Small datasets (<10K samples), convex problems"
      
      numpy_implementation: |
        def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):
            """Batch GD: Use all samples per iteration"""
            m, n = X.shape
            weights = np.zeros(n)
            bias = 0
            losses = []
            
            for iteration in range(n_iterations):
                # Compute predictions for ALL samples
                z = np.dot(X, weights) + bias
                y_pred = sigmoid(z)
                
                # Compute loss on ALL samples
                loss = binary_cross_entropy(y, y_pred)
                losses.append(loss)
                
                # Compute gradient using ALL samples
                dz = y_pred - y
                dw = (1/m) * np.dot(X.T, dz)
                db = (1/m) * np.sum(dz)
                
                # Update
                weights -= learning_rate * dw
                bias -= learning_rate * db
            
            return weights, bias, losses
    
    stochastic_gradient_descent:
      
      description: "Use ONE sample at a time to compute gradient update"
      
      algorithm: |
        For each epoch:
          Shuffle training data
          For each sample (xi, yi):
            1. Compute prediction for THIS sample: ŷi = model(xi)
            2. Compute loss on THIS sample: Ji = loss(yi, ŷi)
            3. Compute gradient using THIS sample: ∇Ji
            4. Update parameters: θ = θ - α∇Ji
      
      pros:
        - "Fast updates (don't wait for full dataset)"
        - "Memory efficient (process one sample at a time)"
        - "Can escape local minima (noise helps exploration)"
        - "Works online (update as data arrives)"
      
      cons:
        - "Noisy gradients (erratic convergence path)"
        - "Doesn't fully utilize vectorization"
        - "Loss oscillates (doesn't smoothly decrease)"
        - "May never fully converge (keeps bouncing around minimum)"
      
      when_to_use: "Very large datasets, online learning, escaping local minima"
      
      numpy_implementation: |
        def stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=10):
            """SGD: Use one sample per iteration"""
            m, n = X.shape
            weights = np.zeros(n)
            bias = 0
            losses = []
            
            for epoch in range(n_epochs):
                # Shuffle data each epoch
                indices = np.random.permutation(m)
                
                for i in indices:
                    # Get single sample
                    xi = X[i:i+1]
                    yi = y[i:i+1]
                    
                    # Compute prediction for THIS sample
                    z = np.dot(xi, weights) + bias
                    y_pred = sigmoid(z)
                    
                    # Compute gradient using THIS sample
                    dz = y_pred - yi
                    dw = np.dot(xi.T, dz).flatten()
                    db = np.sum(dz)
                    
                    # Update
                    weights -= learning_rate * dw
                    bias -= learning_rate * db
                
                # Compute loss after each epoch (for monitoring)
                z_all = np.dot(X, weights) + bias
                y_pred_all = sigmoid(z_all)
                loss = binary_cross_entropy(y, y_pred_all)
                losses.append(loss)
            
            return weights, bias, losses
    
    mini_batch_gradient_descent:
      
      description: "Use small batch of samples (e.g., 32, 64, 128) per iteration"
      
      algorithm: |
        For each epoch:
          Shuffle training data
          Split data into mini-batches of size B
          For each mini-batch (X_batch, y_batch):
            1. Compute predictions for batch: ŷ_batch = model(X_batch)
            2. Compute loss on batch: J_batch = loss(y_batch, ŷ_batch)
            3. Compute gradient using batch: ∇J_batch
            4. Update parameters: θ = θ - α∇J_batch
      
      pros:
        - "Balance speed and stability"
        - "Efficient vectorization (GPU parallelism)"
        - "Smoother convergence than SGD"
        - "Works with large datasets"
      
      cons:
        - "Requires batch size tuning"
        - "More hyperparameters to set"
      
      when_to_use: "Most practical scenarios (default choice)"
      
      typical_batch_sizes:
        - "Small: 32 (faster updates, more noise)"
        - "Medium: 64, 128 (good balance)"
        - "Large: 256, 512 (slower updates, more stable)"
      
      numpy_implementation: |
        def mini_batch_gradient_descent(X, y, batch_size=32, learning_rate=0.01, n_epochs=10):
            """Mini-batch GD: Use small batches"""
            m, n = X.shape
            weights = np.zeros(n)
            bias = 0
            losses = []
            
            for epoch in range(n_epochs):
                # Shuffle data
                indices = np.random.permutation(m)
                X_shuffled = X[indices]
                y_shuffled = y[indices]
                
                # Process mini-batches
                for i in range(0, m, batch_size):
                    # Get mini-batch
                    X_batch = X_shuffled[i:i+batch_size]
                    y_batch = y_shuffled[i:i+batch_size]
                    batch_m = len(X_batch)
                    
                    # Compute predictions for batch
                    z = np.dot(X_batch, weights) + bias
                    y_pred = sigmoid(z)
                    
                    # Compute gradient using batch
                    dz = y_pred - y_batch
                    dw = (1/batch_m) * np.dot(X_batch.T, dz)
                    db = (1/batch_m) * np.sum(dz)
                    
                    # Update
                    weights -= learning_rate * dw
                    bias -= learning_rate * db
                
                # Compute loss after each epoch
                z_all = np.dot(X, weights) + bias
                y_pred_all = sigmoid(z_all)
                loss = binary_cross_entropy(y, y_pred_all)
                losses.append(loss)
            
            return weights, bias, losses
    
    comparison_summary:
      
      table: |
        | Method        | Samples/Update | Speed | Stability | Memory | Best For           |
        |---------------|----------------|-------|-----------|--------|--------------------|
        | Batch GD      | All (m)        | Slow  | High      | High   | Small datasets     |
        | Stochastic GD | 1              | Fast  | Low       | Low    | Large/online       |
        | Mini-batch GD | 32-256         | Fast  | Medium    | Medium | Most cases (default)|
      
      practical_recommendation: |
        Default: Use mini-batch gradient descent with batch size 32-128
        
        Reasons:
        - Efficient GPU utilization
        - Good convergence properties
        - Scales to large datasets
        - Industry standard (TensorFlow, PyTorch default)
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Learning Rate and Schedules
  # --------------------------------------------------------------------------
  
  learning_rate_strategies:
    
    fixed_learning_rate:
      
      approach: "Use same learning rate throughout training"
      
      example: "α = 0.01 for all iterations"
      
      pros:
        - "Simple, no tuning needed"
        - "Predictable behavior"
      
      cons:
        - "May converge slowly (if α too small)"
        - "May oscillate/diverge (if α too large)"
        - "Suboptimal (can't adjust based on progress)"
      
      tuning_strategy: |
        Try sequence: [1.0, 0.1, 0.01, 0.001, 0.0001]
        Pick α where loss decreases steadily
        
        Signs α is too large:
        - Loss increases
        - Loss oscillates wildly
        - Training unstable
        
        Signs α is too small:
        - Loss decreases very slowly
        - Training takes forever
        - Gets stuck in plateau
    
    learning_rate_decay:
      
      motivation: |
        Early training: Need large steps to make progress
        Late training: Need small steps to fine-tune
        
        Solution: Decrease learning rate over time
      
      time_based_decay:
        formula: "α(t) = α₀ / (1 + decay_rate × t)"
        
        example: |
          α₀ = 0.1 (initial learning rate)
          decay_rate = 0.01
          
          Iteration 0: α = 0.1 / (1 + 0.01×0) = 0.1
          Iteration 100: α = 0.1 / (1 + 0.01×100) = 0.05
          Iteration 1000: α = 0.1 / (1 + 0.01×1000) = 0.009
        
        numpy_implementation: |
          def time_based_decay(initial_lr, decay_rate, iteration):
              return initial_lr / (1 + decay_rate * iteration)
      
      step_decay:
        formula: "α(t) = α₀ × drop_factor^floor(t / epochs_drop)"
        
        example: |
          Drop learning rate by factor of 0.5 every 100 epochs
          
          Epochs 0-99: α = 0.1
          Epochs 100-199: α = 0.05
          Epochs 200-299: α = 0.025
        
        numpy_implementation: |
          def step_decay(initial_lr, drop_factor, epochs_drop, epoch):
              return initial_lr * (drop_factor ** np.floor(epoch / epochs_drop))
      
      exponential_decay:
        formula: "α(t) = α₀ × e^(-decay_rate × t)"
        
        numpy_implementation: |
          def exponential_decay(initial_lr, decay_rate, iteration):
              return initial_lr * np.exp(-decay_rate * iteration)
    
    adaptive_learning_rates:
      
      motivation: |
        Different parameters may need different learning rates
        Adjust learning rate per parameter based on gradient history
      
      adagrad:
        concept: "Larger updates for infrequent features, smaller for frequent"
        formula: "θ = θ - (α / √(Σ g²)) × g"
        when_to_use: "Sparse data (NLP, recommendation systems)"
      
      rmsprop:
        concept: "Moving average of squared gradients"
        formula: "v = β×v + (1-β)×g², θ = θ - (α / √v) × g"
        benefit: "Fixes Adagrad's aggressive learning rate reduction"
      
      adam:
        concept: "Combines momentum + adaptive learning rates"
        most_popular: "Default optimizer for deep learning"
        formula: |
          m = β₁×m + (1-β₁)×g  (momentum)
          v = β₂×v + (1-β₂)×g²  (RMSprop)
          θ = θ - α × m/√v
        
        typical_params: "α=0.001, β₁=0.9, β₂=0.999"
      
      practical_note: |
        For simple models (logistic regression): Fixed or decaying learning rate sufficient
        For deep networks: Use Adam (covered in Chapter 2)
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Convergence Criteria and Early Stopping
  # --------------------------------------------------------------------------
  
  convergence_and_stopping:
    
    when_to_stop_training:
      
      fixed_iterations:
        approach: "Stop after N iterations/epochs"
        
        pros:
          - "Simple, deterministic"
          - "Predictable training time"
        
        cons:
          - "May stop too early (undertraining)"
          - "May train too long (overtraining)"
          - "Doesn't adapt to problem difficulty"
        
        when_to_use: "Prototyping, benchmarking"
      
      loss_convergence:
        approach: "Stop when loss change becomes very small"
        
        criterion: "|J(t) - J(t-1)| < threshold"
        
        typical_threshold: "1e-6 to 1e-4"
        
        numpy_implementation: |
          def check_convergence(losses, threshold=1e-4, patience=10):
              """
              Check if loss has converged
              
              Args:
                  losses: List of loss values
                  threshold: Minimum loss change to continue
                  patience: Number of iterations to check
              
              Returns:
                  True if converged
              """
              if len(losses) < patience + 1:
                  return False
              
              recent_losses = losses[-patience-1:]
              loss_changes = np.abs(np.diff(recent_losses))
              
              # Converged if all recent changes < threshold
              return np.all(loss_changes < threshold)
        
        issue: |
          Can stop prematurely if loss plateaus temporarily
          Solution: Check over multiple iterations (patience parameter)
      
      gradient_magnitude:
        approach: "Stop when gradient becomes very small"
        
        criterion: "||∇J|| < threshold"
        
        intuition: |
          At minimum: gradient = 0
          Near minimum: gradient ≈ 0
          
          Small gradient = near minimum = converged
        
        numpy_implementation: |
          def check_gradient_convergence(gradient, threshold=1e-6):
              """Check if gradient magnitude is small enough"""
              grad_norm = np.linalg.norm(gradient)
              return grad_norm < threshold
      
      validation_based_stopping:
        approach: "Stop when validation loss stops improving (early stopping)"
        
        algorithm: |
          Best practice for preventing overfitting
          
          1. Split data: train + validation
          2. Train on train set
          3. After each epoch, evaluate on validation set
          4. Track best validation loss
          5. If validation loss doesn't improve for N epochs (patience): STOP
          6. Restore weights from best validation epoch
        
        numpy_implementation: |
          def train_with_early_stopping(X_train, y_train, X_val, y_val, 
                                       patience=10, max_epochs=1000):
              """
              Train with early stopping based on validation loss
              """
              weights = np.zeros(X_train.shape[1])
              bias = 0
              
              best_val_loss = float('inf')
              best_weights = weights.copy()
              best_bias = bias
              epochs_without_improvement = 0
              
              for epoch in range(max_epochs):
                  # Train one epoch
                  weights, bias = train_one_epoch(X_train, y_train, weights, bias)
                  
                  # Evaluate on validation set
                  val_loss = compute_loss(X_val, y_val, weights, bias)
                  
                  # Check if improved
                  if val_loss < best_val_loss:
                      best_val_loss = val_loss
                      best_weights = weights.copy()
                      best_bias = bias
                      epochs_without_improvement = 0
                  else:
                      epochs_without_improvement += 1
                  
                  # Early stopping check
                  if epochs_without_improvement >= patience:
                      print(f"Early stopping at epoch {epoch}")
                      print(f"Best validation loss: {best_val_loss:.4f}")
                      break
              
              # Restore best weights
              return best_weights, best_bias
        
        benefit: |
          Prevents overfitting automatically
          Finds optimal stopping point
          No need to guess number of epochs
        
        typical_patience: "10-50 epochs (depends on problem)"
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Monitoring and Debugging Training
  # --------------------------------------------------------------------------
  
  monitoring_and_debugging:
    
    what_to_monitor:
      
      training_loss:
        what: "Loss on training data"
        
        should_see: "Steady decrease over time"
        
        problems:
          not_decreasing:
            causes:
              - "Learning rate too small"
              - "Model too simple (underfitting)"
              - "Gradients computed incorrectly"
            
            fixes:
              - "Increase learning rate"
              - "Add features or model complexity"
              - "Verify gradient computation"
          
          oscillating:
            causes:
              - "Learning rate too large"
              - "Batch size too small"
            
            fixes:
              - "Decrease learning rate"
              - "Increase batch size"
              - "Use learning rate decay"
          
          increasing:
            causes:
              - "Learning rate way too large (divergence)"
              - "Numerical instability"
            
            fixes:
              - "Reduce learning rate by 10x or more"
              - "Check for NaN/inf in computations"
              - "Add gradient clipping"
      
      validation_loss:
        what: "Loss on held-out validation data"
        
        should_see: "Decrease initially, then plateau or increase"
        
        patterns:
          val_much_higher_than_train:
            interpretation: "Overfitting"
            action: "Regularization, early stopping, more data"
          
          both_high:
            interpretation: "Underfitting"
            action: "More features, more complex model, train longer"
          
          val_lower_than_train:
            interpretation: "Data leakage or validation set too easy"
            action: "Check data split, verify no leakage"
      
      learning_curves:
        what: "Plot training and validation loss over time"
        
        visualization: |
          import matplotlib.pyplot as plt
          
          def plot_learning_curves(train_losses, val_losses):
              plt.figure(figsize=(10, 6))
              plt.plot(train_losses, label='Training Loss')
              plt.plot(val_losses, label='Validation Loss')
              plt.xlabel('Epoch')
              plt.ylabel('Loss')
              plt.legend()
              plt.title('Learning Curves')
              plt.grid(True)
              plt.show()
        
        interpretation_guide:
          both_decreasing_converging:
            status: "✓ Good - model learning, not overfitting"
          
          train_decreasing_val_increasing:
            status: "✗ Overfitting - stop training or regularize"
          
          both_high_flat:
            status: "✗ Underfitting - need better model"
          
          erratic_oscillating:
            status: "✗ Training unstable - reduce learning rate"
      
      gradient_norms:
        what: "Magnitude of gradients during training"
        
        computation: "||∇J|| = √(Σ gradients²)"
        
        problems:
          exploding_gradients:
            symptom: "Gradient norm grows exponentially"
            consequence: "NaN/inf values, training fails"
            fix: "Gradient clipping (cap maximum norm)"
          
          vanishing_gradients:
            symptom: "Gradient norm approaches zero"
            consequence: "Training stops making progress"
            fix: "Better initialization, different activation"
        
        numpy_implementation: |
          def compute_gradient_norm(gradient):
              """Compute L2 norm of gradient"""
              return np.linalg.norm(gradient)
          
          def clip_gradient(gradient, max_norm=5.0):
              """Clip gradient if norm exceeds max_norm"""
              norm = np.linalg.norm(gradient)
              if norm > max_norm:
                  gradient = gradient * (max_norm / norm)
              return gradient
    
    debugging_checklist:
      
      training_not_starting:
        checks:
          - "Loss is NaN or inf?"
          - "Learning rate reasonable? (try 0.01)"
          - "Features scaled? (mean=0, std=1)"
          - "Gradients being computed? (not all zeros)"
          - "Labels correct format? (0/1 for binary)"
      
      training_too_slow:
        checks:
          - "Learning rate too small? (increase by 10x)"
          - "Batch size too small? (increase to 32-128)"
          - "Convergence criteria too strict?"
          - "Model initialized properly?"
      
      poor_generalization:
        checks:
          - "Overfitting? (train loss << val loss)"
          - "Need regularization? (L2 penalty)"
          - "Need more data?"
          - "Data leakage? (test data in training)"
      
      unstable_training:
        checks:
          - "Learning rate too large? (reduce by 10x)"
          - "Need gradient clipping?"
          - "Batch size too small? (increase)"
          - "Numerical instability? (check for NaN/inf)"
  
  # --------------------------------------------------------------------------
  # Practical Implementation: Complete Training Pipeline
  # --------------------------------------------------------------------------
  
  complete_training_pipeline: |
    import numpy as np
    
    class TrainingPipeline:
        """
        Complete training pipeline with monitoring and early stopping
        """
        
        def __init__(self, learning_rate=0.01, batch_size=32, max_epochs=1000,
                     early_stopping_patience=20, verbose=True):
            self.learning_rate = learning_rate
            self.batch_size = batch_size
            self.max_epochs = max_epochs
            self.patience = early_stopping_patience
            self.verbose = verbose
            
            # Training history
            self.train_losses = []
            self.val_losses = []
            self.gradient_norms = []
        
        def train(self, X_train, y_train, X_val, y_val):
            """
            Train model with mini-batch GD and early stopping
            """
            m, n = X_train.shape
            weights = np.random.randn(n) * 0.01  # Small random initialization
            bias = 0
            
            best_val_loss = float('inf')
            best_weights = weights.copy()
            best_bias = bias
            epochs_no_improve = 0
            
            for epoch in range(self.max_epochs):
                # Mini-batch training
                epoch_train_loss = self._train_epoch(X_train, y_train, weights, bias)
                
                # Validation evaluation
                val_loss = self._compute_loss(X_val, y_val, weights, bias)
                
                # Store history
                self.train_losses.append(epoch_train_loss)
                self.val_losses.append(val_loss)
                
                # Early stopping check
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_weights = weights.copy()
                    best_bias = bias
                    epochs_no_improve = 0
                else:
                    epochs_no_improve += 1
                
                # Logging
                if self.verbose and epoch % 10 == 0:
                    print(f"Epoch {epoch}: "
                          f"Train Loss = {epoch_train_loss:.4f}, "
                          f"Val Loss = {val_loss:.4f}, "
                          f"No Improve = {epochs_no_improve}")
                
                # Early stopping
                if epochs_no_improve >= self.patience:
                    if self.verbose:
                        print(f"\nEarly stopping at epoch {epoch}")
                        print(f"Best validation loss: {best_val_loss:.4f}")
                    break
            
            # Restore best weights
            return best_weights, best_bias
        
        def _train_epoch(self, X, y, weights, bias):
            """Train for one epoch using mini-batch GD"""
            m = len(y)
            indices = np.random.permutation(m)
            
            epoch_losses = []
            
            for i in range(0, m, self.batch_size):
                # Get mini-batch
                batch_idx = indices[i:i+self.batch_size]
                X_batch = X[batch_idx]
                y_batch = y[batch_idx]
                
                # Forward pass
                z = np.dot(X_batch, weights) + bias
                y_pred = self._sigmoid(z)
                
                # Compute loss
                loss = self._compute_loss_from_pred(y_batch, y_pred)
                epoch_losses.append(loss)
                
                # Backward pass
                dz = y_pred - y_batch
                dw = (1/len(y_batch)) * np.dot(X_batch.T, dz)
                db = (1/len(y_batch)) * np.sum(dz)
                
                # Track gradient norm
                grad_norm = np.linalg.norm(dw)
                self.gradient_norms.append(grad_norm)
                
                # Update
                weights -= self.learning_rate * dw
                bias -= self.learning_rate * db
            
            return np.mean(epoch_losses)
        
        def _sigmoid(self, z):
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        
        def _compute_loss(self, X, y, weights, bias):
            z = np.dot(X, weights) + bias
            y_pred = self._sigmoid(z)
            return self._compute_loss_from_pred(y, y_pred)
        
        def _compute_loss_from_pred(self, y_true, y_pred):
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            return -np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))
        
        def plot_training_curves(self):
            """Visualize training progress"""
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(1, 2, figsize=(15, 5))
            
            # Loss curves
            axes[0].plot(self.train_losses, label='Training Loss')
            axes[0].plot(self.val_losses, label='Validation Loss')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Loss')
            axes[0].set_title('Learning Curves')
            axes[0].legend()
            axes[0].grid(True)
            
            # Gradient norms
            axes[1].plot(self.gradient_norms)
            axes[1].set_xlabel('Iteration')
            axes[1].set_ylabel('Gradient Norm')
            axes[1].set_title('Gradient Magnitude')
            axes[1].grid(True)
            
            plt.tight_layout()
            plt.show()
    
    # Usage example
    # Generate data, split train/val/test
    # ...
    
    # Train with pipeline
    pipeline = TrainingPipeline(
        learning_rate=0.01,
        batch_size=64,
        max_epochs=1000,
        early_stopping_patience=20
    )
    
    weights, bias = pipeline.train(X_train, y_train, X_val, y_val)
    
    # Visualize
    pipeline.plot_training_curves()
  
  # --------------------------------------------------------------------------
  # Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    detecting_data_poisoning:
      
      symptom: "Training loss suddenly increases or fails to converge"
      
      explanation: |
        Adversary injects poisoned samples into training data
        Poisoned samples have contradictory labels or extreme feature values
        Model struggles to fit data → training instability
      
      detection_strategy: |
        Monitor training metrics:
        - Sudden loss increase (poisoned batch encountered)
        - Gradient explosion (extreme feature values)
        - Validation loss diverges from training (poisoned samples don't generalize)
      
      numpy_implementation: |
        def detect_training_anomaly(losses, window=10, threshold=0.1):
            """
            Detect sudden loss increases (potential poisoning)
            
            Returns True if recent loss increase > threshold
            """
            if len(losses) < window + 1:
                return False
            
            recent_losses = losses[-window:]
            loss_increase = recent_losses[-1] - min(recent_losses[:-1])
            
            return loss_increase > threshold
    
    convergence_monitoring_for_drift:
      
      use_case: "Production detectors retrained periodically"
      
      concern: |
        Attack patterns evolve over time
        If training doesn't converge or takes much longer → distribution shift
      
      monitoring: |
        Track convergence metrics over retraining cycles:
        - Number of epochs to converge
        - Final training loss
        - Final validation loss
        - Gradient norms
        
        Alert if:
        - Convergence time doubles
        - Final loss significantly higher
        - Validation gap increases
      
      action: |
        Investigate training data:
        - New attack variants?
        - Data quality issues?
        - Adversarial poisoning?
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Three GD variants: Batch (all samples), Stochastic (one sample), Mini-batch (default)"
      - "Learning rate controls convergence: too large diverges, too small is slow"
      - "Early stopping prevents overfitting: stop when validation loss stops improving"
      - "Monitor both training and validation loss to diagnose problems"
      - "Gradient norms reveal training stability (exploding/vanishing)"
    
    practical_skills:
      - "Implement mini-batch gradient descent with early stopping"
      - "Choose appropriate batch size (32-128) and learning rate (0.001-0.1)"
      - "Plot learning curves to diagnose training issues"
      - "Detect overfitting (train loss << val loss)"
      - "Debug convergence failures (check gradients, learning rate, scaling)"
    
    security_mindset:
      - "Training failures may indicate data poisoning attacks"
      - "Monitor convergence metrics to detect distribution shifts"
      - "Sudden loss increases during training = suspicious batches"
      - "Production retraining must be robust to adversarial data"
      - "Convergence monitoring = early warning system for attacks"
    
    remember_this:
      - "Training is iterative process: predict, compute loss, update, repeat"
      - "Stop when validation loss stops improving (early stopping)"
      - "Monitor training - don't just wait and hope it works"
    
    next_steps:
      - "Next section: Loss functions deep dive (when to use which loss)"
      - "Connect to evaluation: Section 11 covers metrics (accuracy, precision, recall)"
      - "You now understand the complete training workflow!"

---
