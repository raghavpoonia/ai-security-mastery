# section_03_18_t5_architecture.yaml

---
document_info:
  title: "T5: Text-to-Text Transfer Transformer"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 18
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into T5: encoder-decoder architecture, text-to-text framework, span corruption pre-training, multitask learning, and security implications"
  estimated_pages: 6
  tags:
    - t5
    - encoder-decoder
    - text-to-text
    - span-corruption
    - multitask-learning
    - unified-interface

section_overview:
  title: "T5: Text-to-Text Transfer Transformer"
  number: "3.18"
  
  purpose: |
    T5 (Text-to-Text Transfer Transformer) by Google (Raffel et al., 2020) unifies all NLP 
    tasks as text-to-text transformations. Every task - translation, summarization, question 
    answering, classification - is framed as generating text output from text input. This 
    unified interface uses the full encoder-decoder transformer, combining BERT's bidirectional 
    understanding with GPT's generation capabilities.
    
    T5 pre-trains on span corruption: mask contiguous spans of tokens and train the decoder 
    to regenerate them. Unlike BERT's single token masking, span corruption requires generating 
    multiple tokens sequentially, better matching downstream generation tasks. T5 also pioneers 
    multitask pre-training - training on many tasks simultaneously with task-specific prefixes.
    
    For security engineers: T5's text-to-text framework means all tasks use the same attack 
    surface - encoder input manipulation and decoder output generation. The encoder-decoder 
    design has more components to attack than BERT or GPT alone. Multitask training can create 
    task interference where poisoning one task affects others. Understanding T5 reveals how 
    architectural choices affect security across diverse NLP applications.
  
  learning_objectives:
    conceptual:
      - "Understand text-to-text framework: unified interface for all tasks"
      - "Grasp span corruption: mask and regenerate contiguous spans"
      - "Learn encoder-decoder advantages: understanding + generation"
      - "See multitask pre-training: learning many tasks together"
      - "Compare T5 vs BERT vs GPT: architectural trade-offs"
    
    practical:
      - "Implement T5 encoder-decoder architecture"
      - "Build span corruption pre-training"
      - "Create task-specific prefixes for multitask learning"
      - "Frame different NLP tasks as text-to-text"
      - "Fine-tune T5 for various applications"
    
    security_focused:
      - "Exploit encoder-decoder attack surface"
      - "Analyze multitask interference vulnerabilities"
      - "Attack span corruption pre-training"
      - "Manipulate task prefix routing"
      - "Audit T5 models across multiple tasks"
  
  prerequisites:
    knowledge:
      - "Section 3.14: Complete transformer (encoder-decoder)"
      - "Section 3.16: BERT (encoder-only)"
      - "Section 3.17: GPT (decoder-only)"
      - "Understanding of various NLP tasks"
    
    skills:
      - "Encoder-decoder implementation"
      - "Multiple pre-training objectives"
      - "Task framing and formatting"
      - "Multitask learning concepts"
  
  key_transitions:
    from_section_3_17: |
      Section 3.17 covered GPT - decoder-only for generation. Now we cover T5 - full 
      encoder-decoder that combines understanding and generation, unifying all NLP tasks 
      under a single text-to-text interface.
    
    to_next_section: |
      Section 3.19 will compare attention mechanisms and architectural variants, synthesizing 
      the different transformer designs (BERT, GPT, T5) and their trade-offs for different 
      applications.

topics:
  - topic_number: 1
    title: "Text-to-Text Framework: Unified Interface"
    
    overview: |
      T5's key innovation is treating every NLP task as text-to-text transformation. Input 
      is always text, output is always text. Classification becomes generating the class name, 
      question answering generates the answer, translation generates the target language. 
      This unified interface allows using the same model architecture and training procedure 
      for all tasks.
    
    content:
      text_to_text_paradigm:
        core_idea: |
          All tasks have the same format:
          
          Input: Text string
          Output: Text string
          
          No task-specific architectures needed!
        
        examples:
          translation:
            task: "Translate English to German"
            input: "translate English to German: That is good."
            output: "Das ist gut."
          
          summarization:
            task: "Summarize text"
            input: "summarize: [long article text]"
            output: "[summary text]"
          
          question_answering:
            task: "Answer question from context"
            input: "question: What is the capital? context: Paris is the capital of France."
            output: "Paris"
          
          classification:
            task: "Classify sentiment"
            input: "cola sentence: The course is jumping well."
            output: "acceptable"
          
          named_entity_recognition:
            task: "Extract entities"
            input: "ner: John lives in Paris"
            output: "John: PERSON, Paris: LOCATION"
        
        task_prefix: |
          Task specified by prefix in input:
          "translate English to German: ..."
          "summarize: ..."
          "question: ... context: ..."
          
          → Model learns task routing from prefix
      
      advantages_of_unified_interface:
        single_architecture: |
          Same encoder-decoder for all tasks
          → No task-specific heads
          → Simpler implementation
        
        transfer_learning: |
          Pre-train on many tasks
          → Knowledge transfers across tasks
          → Better generalization
        
        multitask_training: |
          Train on all tasks simultaneously
          → Shared representations
          → Efficient learning
        
        flexible_generation: |
          Can generate variable-length outputs
          → Handles open-ended tasks
          → Not limited to fixed classes
      
      comparison_to_other_frameworks:
        bert_style: |
          Task-specific heads on [CLS]:
          - Classification: [CLS] → Linear → Softmax
          - NER: Each token → Linear → Softmax
          - QA: Start/end position classifiers
          
          Different architecture per task!
        
        gpt_style: |
          Prompt engineering:
          - Specify task in prompt
          - Model generates completion
          
          Flexible but less structured
        
        t5_style: |
          Text-to-text:
          - Task prefix + input text
          - Generate output text
          
          Unified + structured
    
    implementation:
      t5_architecture:
        language: python
        code: |
          import numpy as np
          
          class T5Model:
              """
              T5 encoder-decoder transformer.
              
              Architecture:
              - Encoder stack (bidirectional self-attention)
              - Decoder stack (causal self-attention + cross-attention)
              - Text-to-text interface for all tasks
              """
              
              def __init__(self,
                          vocab_size: int,
                          max_len: int = 512,
                          num_layers: int = 12,
                          d_model: int = 768,
                          num_heads: int = 12,
                          d_ff: int = 3072,
                          dropout: float = 0.1):
                  """
                  Args:
                      vocab_size: Vocabulary size (shared encoder/decoder)
                      max_len: Maximum sequence length
                      num_layers: Number of encoder/decoder layers
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  # Import from previous sections
                  from section_3_14_complete_transformer import Transformer
                  
                  # Full encoder-decoder transformer
                  self.transformer = Transformer(
                      src_vocab_size=vocab_size,
                      tgt_vocab_size=vocab_size,  # Shared vocabulary
                      max_len=max_len,
                      num_layers=num_layers,
                      d_model=d_model,
                      num_heads=num_heads,
                      d_ff=d_ff,
                      dropout=dropout
                  )
                  
                  self.vocab_size = vocab_size
              
              def forward(self,
                         input_ids: np.ndarray,
                         target_ids: np.ndarray,
                         training: bool = False) -> np.ndarray:
                  """
                  Forward pass through T5.
                  
                  Args:
                      input_ids: Input token IDs (batch, input_len)
                      target_ids: Target token IDs (batch, target_len)
                      training: Whether in training mode
                  
                  Returns:
                      logits: Output logits (batch, target_len, vocab_size)
                  """
                  logits = self.transformer(input_ids, target_ids, training=training)
                  return logits
              
              def __call__(self, input_ids: np.ndarray, target_ids: np.ndarray,
                          training: bool = False) -> np.ndarray:
                  """Alias for forward."""
                  return self.forward(input_ids, target_ids, training)
          
          
          # Example usage
          print("=== T5 Text-to-Text Architecture ===\n")
          
          vocab_size = 32000  # SentencePiece vocab
          num_layers = 12
          d_model = 768
          
          t5 = T5Model(vocab_size, num_layers=num_layers, d_model=d_model)
          
          print(f"T5-base configuration:")
          print(f"  Vocabulary: {vocab_size:,}")
          print(f"  Encoder layers: {num_layers}")
          print(f"  Decoder layers: {num_layers}")
          print(f"  Hidden size: {d_model}")
          print(f"  Parameters: ~220M")
          print()
          print("Architecture:")
          print("  - Full encoder-decoder transformer")
          print("  - Shared vocabulary")
          print("  - Text-to-text for all tasks")
      
      task_formatting:
        language: python
        code: |
          def format_task_input(task: str, input_text: str, **kwargs) -> str:
              """
              Format input for T5 text-to-text interface.
              
              Args:
                  task: Task name
                  input_text: Input text
                  **kwargs: Additional task-specific fields
              
              Returns:
                  formatted: Formatted input string
              """
              if task == "translation":
                  source_lang = kwargs.get("source_lang", "English")
                  target_lang = kwargs.get("target_lang", "German")
                  return f"translate {source_lang} to {target_lang}: {input_text}"
              
              elif task == "summarization":
                  return f"summarize: {input_text}"
              
              elif task == "question_answering":
                  context = kwargs.get("context", "")
                  return f"question: {input_text} context: {context}"
              
              elif task == "sentiment":
                  return f"sentiment: {input_text}"
              
              elif task == "ner":
                  return f"ner: {input_text}"
              
              else:
                  return f"{task}: {input_text}"
          
          
          # Example task formatting
          print("\n=== T5 Task Formatting ===\n")
          
          # Translation
          trans_input = format_task_input(
              "translation", 
              "That is good.",
              source_lang="English",
              target_lang="German"
          )
          print(f"Translation:\n  Input: {trans_input}")
          print(f"  Output: Das ist gut.\n")
          
          # Question answering
          qa_input = format_task_input(
              "question_answering",
              "What is the capital?",
              context="Paris is the capital of France."
          )
          print(f"Question answering:\n  Input: {qa_input}")
          print(f"  Output: Paris\n")
          
          # Sentiment
          sent_input = format_task_input("sentiment", "I love this product!")
          print(f"Sentiment:\n  Input: {sent_input}")
          print(f"  Output: positive\n")
          
          print("Text-to-text advantages:")
          print("  - Unified interface for all tasks")
          print("  - Task prefix specifies task type")
          print("  - Same model architecture everywhere")
          print("  - Flexible output generation")
    
    security_implications:
      task_prefix_manipulation: |
        Adversary can manipulate task routing:
        - Inject different task prefix
        - "summarize: [text]" → "translate English to Chinese: [text]"
        - Change task without model knowing
        - Defense: Validate task prefix, separate task specification
      
      unified_attack_surface: |
        All tasks use same encoder-decoder:
        - Single vulnerability affects all tasks
        - Adversary targets common components
        - Cannot isolate tasks architecturally
        - Defense: Task-specific validation, output filtering

  - topic_number: 2
    title: "Span Corruption: Pre-training Objective"
    
    overview: |
      T5 uses span corruption for pre-training: mask contiguous spans of tokens (not 
      individual tokens like BERT) and train the decoder to regenerate them. Spans are 
      replaced with sentinel tokens <X>, <Y>, <Z>. The decoder generates the original 
      spans in order. This objective better matches downstream generation tasks.
    
    content:
      span_corruption_explained:
        masking_strategy: |
          1. Select 15% of tokens (like BERT)
          2. Group into contiguous spans (average length 3)
          3. Replace each span with sentinel token
          4. Decoder regenerates spans sequentially
        
        example: |
          Original: "Thank you for inviting me to your party last week"
          
          Masked: "Thank you <X> me to your party <Y> week"
          
          Decoder target: "<X> for inviting <Y> last <Z>"
        
        sentinel_tokens: |
          Special tokens <X>, <Y>, <Z>, ...
          → Mark different masked spans
          → Decoder generates them in order
          → Up to 100 sentinels in T5
        
        contrast_with_bert_mlm: |
          BERT MLM:
          - Mask individual tokens: "The [MASK] sat"
          - Predict one token: "cat"
          
          T5 Span Corruption:
          - Mask spans: "The <X> the mat"
          - Generate multiple tokens: "<X> cat sat on <Y>"
          
          → Better for generation tasks!
      
      encoder_decoder_split:
        encoder_input: |
          Corrupted sequence with sentinels:
          "Thank you <X> me to your party <Y> week"
          
          → Encoder sees damaged text
        
        decoder_input: |
          Sentinels and original spans:
          "<X> for inviting <Y> last <Z>"
          
          → Decoder regenerates content
        
        why_this_works: |
          - Encoder learns to understand corrupted text
          - Decoder learns to generate missing content
          - Matches encoder-decoder usage at inference
          - Better than BERT's single-token prediction
      
      span_corruption_vs_other_objectives:
        vs_bert_mlm: |
          MLM: Predict masked tokens independently
          Span: Generate multiple tokens sequentially
          → Span better for generation
        
        vs_gpt_lm: |
          LM: Predict all tokens left-to-right
          Span: Predict only corrupted spans
          → Span more efficient, focused learning
        
        vs_original_transformer: |
          Original: Translation-specific pre-training
          Span: General-purpose, task-agnostic
          → Span transfers better
      
      denoising_objective_family:
        span_corruption: "T5's choice"
        word_deletion: "Delete words, decoder reconstructs"
        sentence_shuffling: "Shuffle sentences, decoder reorders"
        document_rotation: "Rotate document, find start"
        
        all_denoising: |
          Common theme: Corrupt input, reconstruct output
          → Teaches encoder-decoder to work together
    
    implementation:
      span_corrupter:
        language: python
        code: |
          class SpanCorrupter:
              """
              T5 span corruption for pre-training.
              
              Masks contiguous spans and replaces with sentinels.
              """
              
              def __init__(self,
                          vocab_size: int,
                          sentinel_start: int = 32000,
                          num_sentinels: int = 100,
                          corruption_rate: float = 0.15,
                          mean_span_length: float = 3.0):
                  """
                  Args:
                      vocab_size: Vocabulary size
                      sentinel_start: Starting ID for sentinel tokens
                      num_sentinels: Number of sentinel tokens
                      corruption_rate: Fraction of tokens to corrupt
                      mean_span_length: Average span length
                  """
                  self.vocab_size = vocab_size
                  self.sentinel_start = sentinel_start
                  self.num_sentinels = num_sentinels
                  self.corruption_rate = corruption_rate
                  self.mean_span_length = mean_span_length
              
              def corrupt(self, token_ids: np.ndarray) -> tuple:
                  """
                  Apply span corruption to sequence.
                  
                  Args:
                      token_ids: Original token IDs (seq_len,)
                  
                  Returns:
                      encoder_input: Corrupted sequence with sentinels
                      decoder_target: Original spans with sentinel markers
                  """
                  seq_len = len(token_ids)
                  num_corrupt = int(seq_len * self.corruption_rate)
                  
                  # Sample corruption spans
                  spans = self._sample_spans(seq_len, num_corrupt)
                  
                  # Build encoder input (corrupted) and decoder target
                  encoder_input = []
                  decoder_target = []
                  
                  sentinel_id = 0
                  pos = 0
                  
                  for span_start, span_end in spans:
                      # Add tokens before span
                      encoder_input.extend(token_ids[pos:span_start])
                      
                      # Add sentinel to encoder input
                      sentinel = self.sentinel_start + sentinel_id
                      encoder_input.append(sentinel)
                      
                      # Add sentinel + span content to decoder target
                      decoder_target.append(sentinel)
                      decoder_target.extend(token_ids[span_start:span_end])
                      
                      sentinel_id += 1
                      pos = span_end
                  
                  # Add remaining tokens
                  encoder_input.extend(token_ids[pos:])
                  
                  # Add final sentinel
                  decoder_target.append(self.sentinel_start + sentinel_id)
                  
                  return np.array(encoder_input), np.array(decoder_target)
              
              def _sample_spans(self, seq_len: int, num_corrupt: int) -> list:
                  """Sample span positions."""
                  # Simplified: randomly select span starts
                  starts = np.random.choice(seq_len - 1, 
                                           size=max(1, num_corrupt // int(self.mean_span_length)),
                                           replace=False)
                  starts = sorted(starts)
                  
                  spans = []
                  for start in starts:
                      # Sample span length (geometric distribution)
                      length = np.random.geometric(1.0 / self.mean_span_length)
                      length = min(length, seq_len - start)
                      spans.append((start, start + length))
                  
                  return spans
          
          
          # Example usage
          print("\n=== T5 Span Corruption ===\n")
          
          corrupter = SpanCorrupter(vocab_size=32000)
          
          # Sample sequence
          # "Thank you for inviting me to your party last week"
          token_ids = np.array([1234, 567, 89, 234, 56, 78, 90, 123, 456, 789])
          
          encoder_input, decoder_target = corrupter.corrupt(token_ids)
          
          print(f"Original: {token_ids}")
          print(f"Encoder input (corrupted): {encoder_input}")
          print(f"Decoder target (spans): {decoder_target}")
          print()
          print("Span corruption process:")
          print("  1. Select ~15% of tokens")
          print("  2. Group into contiguous spans (avg length 3)")
          print("  3. Replace spans with sentinels <X>, <Y>, <Z>")
          print("  4. Decoder generates: <X> span1 <Y> span2 <Z>")
      
      span_corruption_training:
        language: python
        code: |
          def train_span_corruption_step(t5: T5Model,
                                        token_ids: np.ndarray,
                                        learning_rate: float = 1e-4) -> float:
              """
              Single span corruption training step.
              
              Args:
                  t5: T5 model
                  token_ids: Original token IDs (batch, seq_len)
                  learning_rate: Learning rate
              
              Returns:
                  loss: Cross-entropy loss
              """
              corrupter = SpanCorrupter(t5.vocab_size)
              
              batch_size = token_ids.shape[0]
              total_loss = 0.0
              
              for i in range(batch_size):
                  # Corrupt sequence
                  encoder_input, decoder_target = corrupter.corrupt(token_ids[i])
                  
                  # Forward pass
                  encoder_input_batch = encoder_input[np.newaxis, :]
                  decoder_input = decoder_target[:-1]  # Shift right
                  decoder_input_batch = decoder_input[np.newaxis, :]
                  
                  logits = t5(encoder_input_batch, decoder_input_batch, training=True)
                  
                  # Compute loss on decoder target
                  # (Simplified loss computation)
                  loss = 0.0  # Would compute cross-entropy
                  
                  total_loss += loss
              
              avg_loss = total_loss / batch_size
              
              # Backward pass
              # update_parameters(t5, gradients, learning_rate)
              
              return avg_loss
          
          
          print("\n=== Span Corruption Training ===\n")
          
          t5 = T5Model(32000)
          
          # Sample batch
          batch_size = 4
          seq_len = 64
          token_ids = np.random.randint(0, 32000, (batch_size, seq_len))
          
          # Training step
          loss = train_span_corruption_step(t5, token_ids)
          
          print(f"Batch size: {batch_size}")
          print(f"Sequence length: {seq_len}")
          print()
          print("Training process:")
          print("  1. Corrupt each sequence (spans → sentinels)")
          print("  2. Encoder processes corrupted sequence")
          print("  3. Decoder generates original spans")
          print("  4. Loss on decoder predictions")
    
    security_implications:
      span_corruption_memorization: |
        Span corruption can memorize training data:
        - Model learns to complete corrupted sequences
        - Can reproduce exact training text
        - Adversary corrupts public data, model completes with private
        - Defense: Training data sanitization, differential privacy
      
      sentinel_token_manipulation: |
        Sentinel tokens have special meaning:
        - Adversary can inject sentinels in input
        - Confuse encoder-decoder boundaries
        - Trigger unintended generation patterns
        - Defense: Validate input, restrict sentinel usage

  - topic_number: 3
    title: "Multitask Learning and Comparison"
    
    overview: |
      T5 pioneers multitask pre-training - training on many supervised tasks simultaneously 
      along with unsupervised span corruption. Task prefixes route to different behaviors. 
      This approach combines self-supervised and supervised learning, improving performance 
      and enabling comparison of different pre-training strategies.
    
    content:
      multitask_pre_training:
        combining_tasks: |
          T5 pre-trains on mixture of:
          - Span corruption (unsupervised)
          - Translation (supervised)
          - Summarization (supervised)
          - Question answering (supervised)
          - Classification (supervised)
          - Many more...
        
        task_mixing: |
          Sample tasks proportionally:
          - Span corruption: 50%
          - Translation: 10%
          - Summarization: 5%
          - etc.
          
          → Balanced exposure to all tasks
        
        advantages:
          shared_representations: "Learn general features across tasks"
          task_transfer: "Knowledge transfers between related tasks"
          efficient_learning: "One model for many tasks"
          better_initialization: "Strong starting point for fine-tuning"
      
      bert_vs_gpt_vs_t5:
        bert_encoder_only:
          architecture: "Encoder only"
          pre_training: "Masked Language Model (MLM)"
          context: "Bidirectional"
          strength: "Understanding tasks (classification, NER)"
          weakness: "Limited generation"
        
        gpt_decoder_only:
          architecture: "Decoder only"
          pre_training: "Causal Language Modeling"
          context: "Unidirectional (left-to-right)"
          strength: "Generation, few-shot learning"
          weakness: "No bidirectional context"
        
        t5_encoder_decoder:
          architecture: "Encoder + Decoder"
          pre_training: "Span Corruption + Multitask"
          context: "Bidirectional (encoder) + Causal (decoder)"
          strength: "Both understanding and generation"
          weakness: "More complex, more parameters"
      
      when_to_use_which:
        use_bert_when: |
          - Pure understanding tasks
          - Classification, NER, sentiment
          - No generation needed
          - Want bidirectional context
        
        use_gpt_when: |
          - Generation primary goal
          - Open-ended text generation
          - Few-shot/in-context learning
          - Simplicity valued
        
        use_t5_when: |
          - Both understanding and generation
          - Translation, summarization, QA
          - Multitask learning
          - Unified interface preferred
      
      t5_variants:
        t5_base: "220M params, 12 layers"
        t5_large: "770M params, 24 layers"
        t5_3b: "3B params"
        t5_11b: "11B params"
        
        mt5: "Multilingual T5 (100+ languages)"
        byt5: "Byte-level T5 (no tokenization)"
        t5x: "Improved training, better performance"
    
    implementation:
      architecture_comparison:
        language: python
        code: |
          def compare_architectures():
              """Compare BERT, GPT, T5 architectures."""
              
              print("\n=== Architecture Comparison ===\n")
              
              print("BERT (Encoder-Only):")
              print("  Components: Encoder stack")
              print("  Attention: Bidirectional self-attention")
              print("  Pre-training: Masked Language Model (MLM)")
              print("  Use case: Understanding (classification, NER)")
              print("  Example: \"[CLS] This is great [SEP]\" → positive")
              print()
              
              print("GPT (Decoder-Only):")
              print("  Components: Decoder stack (no encoder)")
              print("  Attention: Causal self-attention")
              print("  Pre-training: Causal Language Modeling")
              print("  Use case: Generation, few-shot learning")
              print("  Example: \"The future of AI is\" → [generates continuation]")
              print()
              
              print("T5 (Encoder-Decoder):")
              print("  Components: Encoder + Decoder stack")
              print("  Attention: Bidirectional (enc) + Causal (dec) + Cross-attention")
              print("  Pre-training: Span Corruption + Multitask")
              print("  Use case: Understanding + Generation")
              print("  Example: \"translate: Hello\" → \"Hola\"")
              print()
              
              print("Summary:")
              print("  BERT: Bidirectional context, best for understanding")
              print("  GPT: Unidirectional generation, best for text generation")
              print("  T5: Full encoder-decoder, best for seq2seq tasks")
          
          compare_architectures()
      
      task_performance_comparison:
        language: python
        code: |
          def compare_task_performance():
              """Compare model performance on different tasks."""
              
              print("\n=== Task Performance Comparison ===\n")
              
              tasks = {
                  "Sentiment Classification": {
                      "BERT": "Excellent (bidirectional context)",
                      "GPT": "Good (in-context learning)",
                      "T5": "Excellent (text-to-text)"
                  },
                  "Text Generation": {
                      "BERT": "Poor (not designed for generation)",
                      "GPT": "Excellent (core capability)",
                      "T5": "Excellent (decoder generates)"
                  },
                  "Translation": {
                      "BERT": "N/A (encoder-only)",
                      "GPT": "Good (with prompting)",
                      "T5": "Excellent (encoder-decoder design)"
                  },
                  "Question Answering": {
                      "BERT": "Excellent (extractive QA)",
                      "GPT": "Good (generative QA)",
                      "T5": "Excellent (both extractive and generative)"
                  },
                  "Few-Shot Learning": {
                      "BERT": "Limited (requires fine-tuning)",
                      "GPT": "Excellent (in-context learning)",
                      "T5": "Good (can do few-shot)"
                  }
              }
              
              print("Task                     | BERT      | GPT       | T5")
              print("-" * 65)
              for task, models in tasks.items():
                  print(f"{task:24} | {models['BERT']:9} | {models['GPT']:9} | {models['T5']}")
          
          compare_task_performance()
    
    security_implications:
      multitask_interference: |
        Training on multiple tasks creates interference:
        - Poisoning one task affects others
        - Adversary targets task with weak validation
        - Corruption propagates through shared representations
        - Defense: Task-specific validation, isolate critical tasks
      
      encoder_decoder_combined_vulnerabilities: |
        T5 has both encoder and decoder attack surfaces:
        - Encoder manipulation (like BERT)
        - Decoder exploitation (like GPT)
        - Cross-attention extraction
        - More components = more vulnerabilities
        - Defense: Comprehensive security across all components
      
      task_prefix_routing_attacks: |
        Task prefix controls behavior:
        - Adversary injects malicious prefix
        - Route to unintended task
        - "summarize:" → "translate to attacker language:"
        - Defense: Whitelist task prefixes, validate inputs

key_takeaways:
  critical_concepts:
    - concept: "Text-to-text framework: all tasks as text input → text output"
      why_it_matters: "Unified interface, same architecture for all tasks"
    
    - concept: "Span corruption: mask contiguous spans, generate sequentially"
      why_it_matters: "Better for generation than BERT's single-token MLM"
    
    - concept: "Encoder-decoder combines bidirectional understanding + generation"
      why_it_matters: "Strengths of both BERT and GPT in one architecture"
    
    - concept: "Multitask pre-training: supervised + unsupervised together"
      why_it_matters: "Better transfer learning, shared representations"
    
    - concept: "BERT (understanding) vs GPT (generation) vs T5 (both)"
      why_it_matters: "Different architectures for different use cases"
  
  actionable_steps:
    - step: "Implement T5 encoder-decoder architecture"
      verification: "Full transformer with shared vocabulary"
    
    - step: "Build span corruption pre-training"
      verification: "Mask spans with sentinels, decoder regenerates"
    
    - step: "Create task-specific prefixes for multitask learning"
      verification: "\"translate:\", \"summarize:\", \"question:\" routing"
    
    - step: "Frame different NLP tasks as text-to-text"
      verification: "Classification → generate class name, QA → generate answer"
    
    - step: "Compare BERT/GPT/T5 performance on various tasks"
      verification: "Understand when to use each architecture"
  
  security_principles:
    - principle: "Task prefix controls routing, can be manipulated"
      application: "Adversary injects different prefix to change behavior"
    
    - principle: "Unified interface means shared attack surface"
      application: "Single vulnerability affects all tasks"
    
    - principle: "Span corruption can memorize and reproduce training data"
      application: "Corrupt public data, model completes with private"
    
    - principle: "Sentinel tokens have special meaning, exploitable"
      application: "Inject sentinels to confuse encoder-decoder boundaries"
    
    - principle: "Multitask training creates interference vulnerabilities"
      application: "Poison one task, affect others through shared representations"
  
  common_mistakes:
    - mistake: "Using BERT-style task heads with T5"
      fix: "T5 uses text-to-text, generate output text not classification logits"
    
    - mistake: "Masking individual tokens like BERT MLM"
      fix: "T5 masks contiguous spans, not individual tokens"
    
    - mistake: "Forgetting task prefix in input"
      fix: "Always include task prefix: \"translate:\", \"summarize:\", etc."
    
    - mistake: "Training on single task only"
      fix: "T5 designed for multitask, benefits from task mixture"
    
    - mistake: "Assuming T5 always better than BERT or GPT"
      fix: "Choose based on task: BERT for pure understanding, GPT for pure generation"
  
  integration_with_book:
    from_section_3_14:
      - "Complete transformer (T5's architecture basis)"
      - "Encoder-decoder design"
    
    from_section_3_16:
      - "BERT for comparison (encoder-only)"
      - "Pre-training objectives"
    
    from_section_3_17:
      - "GPT for comparison (decoder-only)"
      - "Generation capabilities"
    
    to_next_section:
      - "Section 3.19: Attention mechanisms comparison"
      - "Architectural variants and trade-offs"
      - "Synthesis of transformer designs"
  
  looking_ahead:
    next_concepts:
      - "Attention mechanism variants"
      - "Efficient transformers"
      - "Architectural trade-offs"
      - "Practical deployment considerations"
    
    skills_to_build:
      - "Compare attention mechanisms"
      - "Optimize transformer efficiency"
      - "Choose architectures for tasks"
      - "Deploy transformers in production"
  
  final_thoughts: |
    T5 (Text-to-Text Transfer Transformer) unifies all NLP tasks under a single text-to-text 
    framework. Every task - translation, summarization, question answering, classification - 
    becomes generating text output from text input. This elegant abstraction uses the full 
    encoder-decoder transformer, combining BERT's bidirectional understanding with GPT's 
    generation capabilities. Task prefixes in the input ("translate:", "summarize:") route 
    to different behaviors while using the same architecture.
    
    Span corruption is T5's pre-training objective: mask contiguous spans (not individual 
    tokens) and train the decoder to regenerate them sequentially. Spans are replaced with 
    sentinel tokens <X>, <Y>, <Z>, and the decoder generates the original content in order. 
    This better matches downstream generation tasks compared to BERT's single-token MLM 
    prediction. T5 also pioneers multitask pre-training - combining span corruption with 
    supervised tasks like translation and summarization for better transfer learning.
    
    Comparing architectures: BERT (encoder-only) excels at understanding with bidirectional 
    context but struggles with generation. GPT (decoder-only) excels at generation and 
    few-shot learning but lacks bidirectional context. T5 (encoder-decoder) combines both 
    strengths - bidirectional understanding in the encoder, autoregressive generation in 
    the decoder, but has more complexity and parameters. Choose based on task: BERT for 
    pure understanding, GPT for pure generation, T5 for sequence-to-sequence.
    
    From a security perspective: T5's text-to-text framework means all tasks share the same 
    attack surface - a single vulnerability affects everything. Task prefixes can be 
    manipulated to change routing. The encoder-decoder design combines vulnerabilities from 
    both BERT and GPT - more components means more attack vectors. Multitask training creates 
    interference where poisoning one task affects others through shared representations. 
    Span corruption can memorize training data, and sentinel tokens can be exploited to 
    confuse encoder-decoder boundaries. Understanding T5 reveals how architectural unification 
    affects security across diverse applications.
    
    Next: Section 3.19 will compare attention mechanisms and architectural variants, 
    synthesizing the different transformer designs covered so far and examining trade-offs 
    for practical deployment.

---
