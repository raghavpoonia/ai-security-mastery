# section_03_09_positional_encoding.yaml

---
document_info:
  title: "Positional Encoding: Order Without Recurrence"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 9
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into positional encoding: why attention is permutation-invariant, sinusoidal encoding, learned positional embeddings, relative position encoding, RoPE, and security implications"
  estimated_pages: 6
  tags:
    - positional-encoding
    - sinusoidal-encoding
    - learned-embeddings
    - relative-position
    - rope
    - position-awareness

section_overview:
  title: "Positional Encoding: Order Without Recurrence"
  number: "3.9"
  
  purpose: |
    Attention mechanisms are permutation-invariant: they don't inherently know word order. 
    "The cat chased the mouse" and "The mouse chased the cat" produce identical attention 
    patterns (just permuted). This is a fundamental problem for language, where order 
    matters critically.
    
    Positional encoding solves this by injecting position information into input embeddings. 
    The original transformer (Vaswani et al., 2017) used sinusoidal functions - elegant 
    mathematical encoding that generalizes to unseen sequence lengths. Modern transformers 
    often use learned positional embeddings or relative position encoding (RoPE).
    
    For security engineers: Positional encoding can leak information about sequence structure. 
    Position-dependent attacks exploit how models process different positions. Understanding 
    positional encoding reveals vulnerabilities in how transformers handle sequence order 
    and enables position-aware adversarial attacks.
  
  learning_objectives:
    conceptual:
      - "Understand why attention is permutation-invariant"
      - "Grasp how positional encoding injects order information"
      - "Learn sinusoidal encoding: PE(pos, 2i) = sin(pos/10000^(2i/d))"
      - "Compare absolute vs relative position encoding"
      - "Understand Rotary Position Embedding (RoPE)"
    
    practical:
      - "Implement sinusoidal positional encoding from scratch"
      - "Create learned positional embeddings"
      - "Demonstrate permutation invariance without positional encoding"
      - "Visualize positional encoding patterns"
      - "Implement RoPE for modern transformers"
    
    security_focused:
      - "Identify position-dependent attack vectors"
      - "Understand how position encoding leaks sequence structure"
      - "Analyze position-aware adversarial perturbations"
      - "Detect position-based information extraction"
      - "Audit position encoding for unexpected biases"
  
  prerequisites:
    knowledge:
      - "Section 3.8: Multi-head attention"
      - "Section 3.3: Word embeddings"
      - "Trigonometry: sine and cosine functions"
      - "Understanding of permutation invariance"
    
    skills:
      - "NumPy broadcasting and vectorization"
      - "Trigonometric functions"
      - "Embedding layer implementation"
      - "Visualization of high-dimensional data"
  
  key_transitions:
    from_section_3_8: |
      Section 3.8 completed the multi-head attention mechanism. But attention has a 
      critical flaw: it's permutation-invariant (doesn't know word order). Now we 
      solve this by adding positional encoding to embeddings.
    
    to_next_section: |
      Section 3.10 will introduce self-attention - attention where queries, keys, and 
      values all come from the same sequence. This is the foundation of transformer 
      encoder layers.

topics:
  - topic_number: 1
    title: "The Permutation Invariance Problem"
    
    overview: |
      Attention mechanisms compute weighted sums based on content similarity, not position. 
      If you permute the input sequence, you get the same attention weights (just permuted). 
      This means attention alone cannot distinguish "cat chased mouse" from "mouse chased cat". 
      For language understanding, this is catastrophic - word order is fundamental to meaning.
    
    content:
      permutation_invariance_explained:
        definition: |
          Function f is permutation-invariant if:
          f(permute(x)) = permute(f(x))
          
          For any permutation π of indices
        
        attention_is_permutation_invariant:
          query_key_scores: |
            score(Q[i], K[j]) depends only on content, not positions i, j
            
            If we permute input:
            - Q, K, V get permuted
            - Attention scores get permuted
            - But relationship between content unchanged
          
          example: |
            Input: ["The", "cat", "sat"]
            Permuted: ["sat", "The", "cat"]
            
            Attention between "cat" and "sat" is SAME in both
            → Model can't tell which came first!
        
        why_this_is_bad:
          word_order_matters:
            example_1: |
              "The cat chased the mouse" ≠ "The mouse chased the cat"
              Meaning completely different!
            
            example_2: |
              "Not bad" ≠ "Bad not"
              Negation depends on order
            
            example_3: |
              "John loves Mary" ≠ "Mary loves John"
              Subject-object roles depend on position
          
          syntax_requires_order:
            - "Grammatical structure depends on word order"
            - "Modifiers must be in correct position"
            - "Verb tense indicators are position-dependent"
          
          semantics_requires_order:
            - "Temporal relationships (before/after)"
            - "Causal relationships (cause → effect)"
            - "Narrative structure (story progression)"
      
      demonstration_without_positional_encoding:
        experiment: |
          Input: "The dog bit the mailman"
          Permuted: "mailman The bit dog the"
          
          Without positional encoding:
          - Attention patterns are identical (just permuted)
          - Model produces same representation (just permuted positions)
          - Cannot distinguish original from permuted!
        
        consequence: |
          Model is "bag of words" - knows what words are present but not order
          → Cannot understand language properly
      
      solution_overview:
        key_idea: "Add position information to word embeddings"
        
        approach: |
          Original: embedding(word)
          With position: embedding(word) + positional_encoding(position)
          
          Now embeddings contain both:
          - WHAT the word is (from word embedding)
          - WHERE the word is (from positional encoding)
        
        result: |
          Attention can now distinguish positions:
          - "cat" at position 1 has different encoding than "cat" at position 5
          - Model learns position-dependent patterns
          - Word order is preserved!
    
    implementation:
      demonstrate_permutation_invariance:
        language: python
        code: |
          import numpy as np
          
          def demonstrate_permutation_invariance():
              """Show that attention is permutation-invariant without positional encoding."""
              
              print("=== Permutation Invariance Demonstration ===\n")
              
              # Simulate word embeddings (without positional encoding)
              embeddings = {
                  "The": np.array([0.1, 0.2, 0.3, 0.4]),
                  "cat": np.array([0.5, 0.6, 0.7, 0.8]),
                  "sat": np.array([0.2, 0.4, 0.6, 0.8]),
                  "on": np.array([0.1, 0.3, 0.5, 0.7]),
                  "mat": np.array([0.9, 0.7, 0.5, 0.3]),
              }
              
              # Original sequence
              original_seq = ["The", "cat", "sat", "on", "mat"]
              original_emb = np.array([embeddings[w] for w in original_seq])
              
              # Permuted sequence
              permuted_seq = ["mat", "The", "on", "cat", "sat"]
              permuted_emb = np.array([embeddings[w] for w in permuted_seq])
              
              print("Original sequence:", " ".join(original_seq))
              print("Permuted sequence:", " ".join(permuted_seq))
              print()
              
              # Compute attention scores (simplified: just dot products)
              # For query = "cat" (position 1 in original, position 3 in permuted)
              
              # Original: "cat" at position 1
              cat_emb = embeddings["cat"]
              original_scores = np.dot(original_emb, cat_emb)
              
              print("Attention scores for 'cat' in original sequence:")
              for i, (word, score) in enumerate(zip(original_seq, original_scores)):
                  print(f"  {word:6s} (pos {i}): {score:.3f}")
              
              # Permuted: "cat" at position 3
              permuted_scores = np.dot(permuted_emb, cat_emb)
              
              print("\nAttention scores for 'cat' in permuted sequence:")
              for i, (word, score) in enumerate(zip(permuted_seq, permuted_scores)):
                  print(f"  {word:6s} (pos {i}): {score:.3f}")
              
              print("\nObservation:")
              print("  Scores are IDENTICAL, just in different positions!")
              print("  'cat' → 'sat' score is same in both sequences")
              print("  Model cannot tell which sequence is correct!")
              print("\n→ WITHOUT positional encoding, attention is permutation-invariant")
          
          demonstrate_permutation_invariance()
    
    security_implications:
      position_agnostic_attacks: |
        Permutation invariance creates attack vectors:
        - Adversary can scramble word order without detection
        - Inject malicious content at any position (model doesn't distinguish)
        - Bypass position-dependent filters
        - Defense: Always use positional encoding in transformers
      
      bag_of_words_vulnerabilities: |
        Without position info, model is bag-of-words:
        - Adversary knows model ignores order
        - Can craft inputs that look benign when unordered
        - But malicious when ordered correctly
        - Example: "not harmful" vs "harmful not" (both identical without position)

  - topic_number: 2
    title: "Sinusoidal Positional Encoding"
    
    overview: |
      The original transformer (Vaswani et al., 2017) used sinusoidal functions for 
      positional encoding. These have elegant mathematical properties: generalizable to 
      any sequence length, deterministic (no parameters to learn), and enable the model 
      to easily learn relative positions through linear combinations.
    
    content:
      sinusoidal_formula:
        definition: |
          For position pos and dimension i:
          
          PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
          PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
          
          Where:
          - pos: position in sequence (0, 1, 2, ...)
          - i: dimension index (0, 1, 2, ..., d_model/2 - 1)
          - d_model: embedding dimension
        
        intuition:
          even_dimensions: "Use sine function"
          odd_dimensions: "Use cosine function"
          frequency_varies: "Different dimensions use different frequencies"
          
          low_dimensions: |
            i = 0: wavelength = 2π
            Fast oscillation across positions
          
          high_dimensions: |
            i = d_model/2 - 1: wavelength = 10000 × 2π
            Very slow oscillation
        
        example_calculation:
          position_0:
            dim_0: "sin(0 / 10000^0) = sin(0) = 0"
            dim_1: "cos(0 / 10000^0) = cos(0) = 1"
            dim_2: "sin(0 / 10000^(2/512)) ≈ 0"
            dim_3: "cos(0 / 10000^(2/512)) ≈ 1"
          
          position_1:
            dim_0: "sin(1 / 1) ≈ 0.841"
            dim_1: "cos(1 / 1) ≈ 0.540"
            dim_2: "sin(1 / 10000^(2/512)) ≈ 0.001"
            dim_3: "cos(1 / 10000^(2/512)) ≈ 0.999"
      
      why_sinusoidal_works:
        generalization_to_unseen_lengths:
          problem: "What if test sequence longer than training?"
          
          learned_embeddings: |
            Have fixed maximum length
            Cannot handle longer sequences at test time
          
          sinusoidal_solution: |
            Formula works for ANY position
            pos = 10,000? Just compute sin(10000 / ...)
            → Generalizes to arbitrary lengths!
        
        relative_position_via_linear_combination:
          key_property: |
            For any fixed offset k:
            PE(pos + k) can be represented as linear function of PE(pos)
            
            Mathematically:
            sin(pos + k) = sin(pos)cos(k) + cos(pos)sin(k)
          
          benefit: |
            Model can learn to compute relative positions:
            "How far is position j from position i?"
            → Useful for learning dependencies
        
        unique_encoding_per_position:
          different_frequencies: |
            Low dimensions: fast oscillation (distinguish nearby positions)
            High dimensions: slow oscillation (distinguish distant positions)
          
          combination_is_unique: |
            Each position has unique pattern across all dimensions
            No two positions have identical encoding
      
      adding_to_embeddings:
        method: "Element-wise addition (not concatenation)"
        
        formula: |
          input = word_embedding(word) + positional_encoding(position)
        
        why_addition: |
          - Preserves embedding dimension
          - Allows model to learn to separate content from position
          - Simpler than concatenation
          - Works well in practice
        
        alternative_concatenation: |
          input = [word_embedding(word), positional_encoding(position)]
          
          Problem: Doubles embedding dimension
          Used in some models, but addition more common
    
    implementation:
      sinusoidal_positional_encoding:
        language: python
        code: |
          class SinusoidalPositionalEncoding:
              """
              Sinusoidal positional encoding (Vaswani et al., 2017).
              
              PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
              PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
              """
              
              def __init__(self, d_model: int, max_len: int = 5000):
                  """
                  Args:
                      d_model: Embedding dimension (must be even)
                      max_len: Maximum sequence length to pre-compute
                  """
                  assert d_model % 2 == 0, "d_model must be even for sinusoidal encoding"
                  
                  self.d_model = d_model
                  self.max_len = max_len
                  
                  # Pre-compute positional encodings
                  self.pe = self._create_positional_encoding()
              
              def _create_positional_encoding(self) -> np.ndarray:
                  """
                  Create positional encoding matrix.
                  
                  Returns:
                      pe: (max_len, d_model) positional encoding matrix
                  """
                  pe = np.zeros((self.max_len, self.d_model))
                  
                  # Position indices
                  position = np.arange(0, self.max_len).reshape(-1, 1)
                  
                  # Dimension indices
                  div_term = np.exp(np.arange(0, self.d_model, 2) * 
                                   -(np.log(10000.0) / self.d_model))
                  
                  # Apply sin to even dimensions
                  pe[:, 0::2] = np.sin(position * div_term)
                  
                  # Apply cos to odd dimensions
                  pe[:, 1::2] = np.cos(position * div_term)
                  
                  return pe
              
              def get_encoding(self, seq_len: int) -> np.ndarray:
                  """
                  Get positional encoding for sequence.
                  
                  Args:
                      seq_len: Sequence length
                  
                  Returns:
                      encoding: (seq_len, d_model) positional encodings
                  """
                  if seq_len > self.max_len:
                      # Extend if needed
                      self.max_len = seq_len * 2
                      self.pe = self._create_positional_encoding()
                  
                  return self.pe[:seq_len]
              
              def __call__(self, seq_len: int) -> np.ndarray:
                  """Alias for get_encoding."""
                  return self.get_encoding(seq_len)
          
          
          # Example usage
          d_model = 512
          max_len = 100
          
          pos_encoder = SinusoidalPositionalEncoding(d_model, max_len)
          
          # Get encoding for sequence of length 10
          seq_len = 10
          pe = pos_encoder(seq_len)
          
          print(f"Sinusoidal Positional Encoding:")
          print(f"  Model dimension: {d_model}")
          print(f"  Sequence length: {seq_len}")
          print(f"  Encoding shape: {pe.shape}")
          print(f"\nFirst few positions (first 8 dimensions):")
          for pos in range(min(5, seq_len)):
              print(f"  Position {pos}: {pe[pos, :8]}")
      
      visualize_positional_encoding:
        language: python
        code: |
          def visualize_positional_encoding():
              """Visualize sinusoidal positional encoding patterns."""
              
              print("\n=== Positional Encoding Visualization ===\n")
              
              d_model = 128
              max_len = 100
              
              pos_encoder = SinusoidalPositionalEncoding(d_model, max_len)
              pe = pos_encoder(max_len)
              
              # Show encoding for a few positions
              positions_to_show = [0, 1, 5, 10, 50, 99]
              
              print("Positional encodings (first 16 dimensions):")
              print("\nPos | Dimensions 0-15")
              print("----|" + "-" * 80)
              
              for pos in positions_to_show:
                  encoding_str = " ".join(f"{pe[pos, i]:5.2f}" for i in range(16))
                  print(f"{pos:3d} | {encoding_str}")
              
              print("\nObservations:")
              print("  - Even dimensions use sine (oscillates)")
              print("  - Odd dimensions use cosine (offset by π/2)")
              print("  - Different dimensions have different frequencies")
              print("  - Low dimensions change quickly across positions")
              print("  - High dimensions change slowly")
              
              # Demonstrate uniqueness
              print("\n=== Uniqueness Check ===")
              print("Checking if any two positions have identical encoding...")
              
              # Compute pairwise distances
              found_duplicate = False
              for i in range(min(50, max_len)):
                  for j in range(i + 1, min(50, max_len)):
                      distance = np.linalg.norm(pe[i] - pe[j])
                      if distance < 1e-6:
                          print(f"  Duplicate found: positions {i} and {j}")
                          found_duplicate = True
              
              if not found_duplicate:
                  print("  ✓ All positions have unique encodings!")
          
          visualize_positional_encoding()
      
      demonstrate_relative_position:
        language: python
        code: |
          def demonstrate_relative_position_property():
              """Show that PE(pos+k) is linear function of PE(pos)."""
              
              print("\n=== Relative Position Property ===\n")
              
              # Simple example with small dimension
              d_model = 4
              pos_encoder = SinusoidalPositionalEncoding(d_model, max_len=100)
              
              pe = pos_encoder(100)
              
              # Pick a position and offset
              pos = 5
              offset = 3
              
              print(f"Position: {pos}")
              print(f"Offset: {offset}")
              print(f"Target position: {pos + offset}")
              print()
              
              # PE(pos)
              pe_pos = pe[pos]
              print(f"PE({pos}):        {pe_pos}")
              
              # PE(pos + offset)
              pe_target = pe[pos + offset]
              print(f"PE({pos + offset}):     {pe_target}")
              
              # Try to express PE(pos + offset) as linear combination of PE(pos) and PE(0)
              # This is simplified - in practice, need full linear transformation
              
              print("\nProperty: PE(pos+k) can be computed from PE(pos) via linear transformation")
              print("This allows model to learn relative position easily.")
          
          demonstrate_relative_position_property()
    
    security_implications:
      position_encoding_fingerprinting: |
        Sinusoidal encoding is deterministic:
        - Same position always gets same encoding
        - Adversary can fingerprint positions in output
        - Infer sequence structure from position patterns
        - Defense: Add noise to positional encodings (careful not to break learning)
      
      position_based_adversarial_perturbations: |
        Adversary can craft position-specific attacks:
        - Exploit how model processes specific positions
        - Position 0 (start) often gets special treatment
        - Position -1 (end) often critical for classification
        - Target attacks at these positions for maximum impact

  - topic_number: 3
    title: "Learned Positional Embeddings and Alternatives"
    
    overview: |
      Instead of fixed sinusoidal functions, positions can be learned like word embeddings. 
      Modern transformers (BERT, GPT) often use learned positional embeddings. Other 
      approaches include relative position encoding (focus on distance rather than absolute 
      position) and Rotary Position Embedding (RoPE, used in modern LLMs like LLaMA).
    
    content:
      learned_positional_embeddings:
        approach: "Treat positions as tokens with learned embeddings"
        
        implementation: |
          # Create embedding matrix
          position_embeddings = torch.nn.Embedding(max_position, d_model)
          
          # For sequence of length n
          positions = torch.arange(0, n)
          pos_emb = position_embeddings(positions)
        
        advantages:
          - "Model learns optimal position representation"
          - "Can adapt to specific task/domain"
          - "Often performs slightly better than sinusoidal"
          - "Simpler implementation"
        
        disadvantages:
          - "Fixed maximum length (cannot generalize beyond)"
          - "Requires learning (more parameters)"
          - "No built-in relative position properties"
        
        used_in: "BERT, GPT-2, GPT-3 (early versions)"
      
      relative_positional_encoding:
        motivation: |
          Absolute position (position 5) may not matter
          Relative distance (5 tokens apart) is often more important
        
        approach: |
          Instead of encoding position i and position j separately:
          Encode the DIFFERENCE (i - j) or (j - i)
        
        shaw_et_al_2018:
          method: "Add relative position bias to attention scores"
          
          formula: |
            attention_score(i, j) = Q[i]^T K[j] + r(i - j)
            
            Where r(i - j) is learned relative position bias
          
          benefit: "Model learns 'how to attend to relative positions'"
        
        limitations:
          - "Still limited to maximum relative distance"
          - "Adds parameters for each relative offset"
          - "Complexity grows with maximum distance"
      
      rotary_position_embedding_rope:
        motivation: |
          Encode position by rotating embeddings in 2D subspaces
          Relative position emerges naturally from rotation angles
        
        key_idea: |
          For position m, apply rotation matrix:
          
          [cos(mθ)  -sin(mθ)] [q1]
          [sin(mθ)   cos(mθ)] [q2]
          
          Where θ depends on dimension (like sinusoidal encoding)
        
        relative_position_property: |
          Inner product between positions m and n:
          
          <rotate(Q_m), rotate(K_n)> = <Q, K> × cos((m-n)θ)
          
          → Automatically encodes relative position (m - n)!
        
        advantages:
          - "Naturally encodes relative position"
          - "No maximum sequence length"
          - "Computationally efficient (rotation matrices)"
          - "Better long-range performance"
        
        used_in: "LLaMA, GPT-NeoX, GPT-J, modern LLMs"
      
      comparison_table:
        sinusoidal:
          type: "Fixed (deterministic)"
          generalization: "✓ Any length"
          parameters: "0 (no learning)"
          relative_position: "Linear combinations"
          used_in: "Original Transformer"
        
        learned:
          type: "Learned embeddings"
          generalization: "✗ Fixed max length"
          parameters: "max_len × d_model"
          relative_position: "Not built-in"
          used_in: "BERT, GPT-2/3"
        
        relative:
          type: "Learned biases"
          generalization: "✗ Fixed max distance"
          parameters: "max_distance (2× for bidirectional)"
          relative_position: "✓ Explicit"
          used_in: "Transformer-XL"
        
        rope:
          type: "Rotation matrices"
          generalization: "✓ Any length"
          parameters: "0 (no learning)"
          relative_position: "✓ Automatic"
          used_in: "LLaMA, modern LLMs"
    
    implementation:
      learned_positional_embeddings_impl:
        language: python
        code: |
          class LearnedPositionalEmbedding:
              """
              Learned positional embeddings (like BERT, GPT-2).
              
              Positions are treated like tokens with learned embeddings.
              """
              
              def __init__(self, max_len: int, d_model: int):
                  """
                  Args:
                      max_len: Maximum sequence length
                      d_model: Embedding dimension
                  """
                  self.max_len = max_len
                  self.d_model = d_model
                  
                  # Initialize position embeddings randomly
                  self.position_embeddings = np.random.randn(max_len, d_model) * 0.02
              
              def get_encoding(self, seq_len: int) -> np.ndarray:
                  """
                  Get position embeddings for sequence.
                  
                  Args:
                      seq_len: Sequence length
                  
                  Returns:
                      embeddings: (seq_len, d_model) position embeddings
                  """
                  if seq_len > self.max_len:
                      raise ValueError(f"Sequence length {seq_len} exceeds max_len {self.max_len}")
                  
                  return self.position_embeddings[:seq_len]
              
              def __call__(self, seq_len: int) -> np.ndarray:
                  """Alias for get_encoding."""
                  return self.get_encoding(seq_len)
          
          
          # Example usage
          max_len = 512
          d_model = 768
          
          learned_pos = LearnedPositionalEmbedding(max_len, d_model)
          
          seq_len = 10
          pos_emb = learned_pos(seq_len)
          
          print("Learned Positional Embeddings:")
          print(f"  Max length: {max_len}")
          print(f"  Model dimension: {d_model}")
          print(f"  Parameters: {max_len * d_model:,}")
          print(f"\nEmbedding shape for length {seq_len}: {pos_emb.shape}")
      
      rope_simplified_implementation:
        language: python
        code: |
          class RotaryPositionalEmbedding:
              """
              Simplified Rotary Position Embedding (RoPE).
              
              Rotates query and key embeddings based on position.
              """
              
              def __init__(self, d_model: int, max_len: int = 2048):
                  """
                  Args:
                      d_model: Embedding dimension (must be even)
                      max_len: Maximum sequence length
                  """
                  assert d_model % 2 == 0, "d_model must be even for RoPE"
                  
                  self.d_model = d_model
                  self.max_len = max_len
                  
                  # Pre-compute rotation angles
                  self.theta = self._compute_theta()
              
              def _compute_theta(self) -> np.ndarray:
                  """
                  Compute rotation angles for each dimension.
                  
                  Returns:
                      theta: (d_model/2,) rotation angles
                  """
                  # Similar to sinusoidal encoding frequencies
                  dim_indices = np.arange(0, self.d_model, 2)
                  theta = 1.0 / (10000 ** (dim_indices / self.d_model))
                  return theta
              
              def rotate(self, x: np.ndarray, positions: np.ndarray) -> np.ndarray:
                  """
                  Apply rotary position encoding.
                  
                  Args:
                      x: Input tensor (seq_len, d_model) or (batch, seq_len, d_model)
                      positions: Position indices (seq_len,)
                  
                  Returns:
                      rotated: Rotated tensor with position encoding
                  """
                  # Reshape for pair-wise rotation
                  if x.ndim == 2:
                      seq_len, d = x.shape
                      x_pairs = x.reshape(seq_len, d // 2, 2)
                  else:
                      batch, seq_len, d = x.shape
                      x_pairs = x.reshape(batch, seq_len, d // 2, 2)
                  
                  # Compute rotation angles: position × theta
                  angles = np.outer(positions, self.theta)  # (seq_len, d_model/2)
                  
                  # Compute cos and sin
                  cos_angles = np.cos(angles)
                  sin_angles = np.sin(angles)
                  
                  # Apply rotation to each pair
                  if x.ndim == 2:
                      x1, x2 = x_pairs[..., 0], x_pairs[..., 1]
                      rotated_x1 = x1 * cos_angles - x2 * sin_angles
                      rotated_x2 = x1 * sin_angles + x2 * cos_angles
                      rotated = np.stack([rotated_x1, rotated_x2], axis=-1)
                      return rotated.reshape(seq_len, d)
                  else:
                      x1, x2 = x_pairs[..., 0], x_pairs[..., 1]
                      cos_angles = cos_angles[np.newaxis, :, :]
                      sin_angles = sin_angles[np.newaxis, :, :]
                      rotated_x1 = x1 * cos_angles - x2 * sin_angles
                      rotated_x2 = x1 * sin_angles + x2 * cos_angles
                      rotated = np.stack([rotated_x1, rotated_x2], axis=-1)
                      return rotated.reshape(batch, seq_len, d)
              
              def __call__(self, x: np.ndarray, positions: np.ndarray) -> np.ndarray:
                  """Alias for rotate."""
                  return self.rotate(x, positions)
          
          
          # Example usage
          d_model = 128
          seq_len = 10
          
          rope = RotaryPositionalEmbedding(d_model)
          
          # Simulate query embeddings
          Q = np.random.randn(seq_len, d_model)
          positions = np.arange(seq_len)
          
          # Apply RoPE
          Q_rotated = rope(Q, positions)
          
          print("\nRotary Position Embedding (RoPE):")
          print(f"  Model dimension: {d_model}")
          print(f"  Sequence length: {seq_len}")
          print(f"  Original Q shape: {Q.shape}")
          print(f"  Rotated Q shape: {Q_rotated.shape}")
          print("\n  Position information now encoded in rotations!")
          print("  Relative position emerges from rotation angles.")
      
      compare_encoding_methods:
        language: python
        code: |
          def compare_positional_encoding_methods():
              """Compare different positional encoding approaches."""
              
              print("\n=== Positional Encoding Methods Comparison ===\n")
              
              d_model = 128
              max_len = 512
              
              # Create encoders
              sinusoidal = SinusoidalPositionalEncoding(d_model, max_len)
              learned = LearnedPositionalEmbedding(max_len, d_model)
              rope = RotaryPositionalEmbedding(d_model)
              
              print("Method            | Max Length | Parameters | Generalizes")
              print("------------------|------------|------------|-------------")
              print(f"Sinusoidal        | Unlimited  | {0:10,} | Yes")
              print(f"Learned           | {max_len:10,} | {max_len * d_model:10,} | No")
              print(f"RoPE              | Unlimited  | {0:10,} | Yes")
              
              print("\nKey differences:")
              print("  Sinusoidal:")
              print("    + No parameters (deterministic)")
              print("    + Generalizes to any length")
              print("    - Fixed formula (cannot adapt)")
              
              print("\n  Learned:")
              print("    + Can adapt to task")
              print("    + Often slightly better performance")
              print("    - Fixed maximum length")
              print("    - Requires learning")
              
              print("\n  RoPE:")
              print("    + Generalizes to any length")
              print("    + Automatic relative position")
              print("    + No parameters")
              print("    + State-of-the-art for modern LLMs")
          
          compare_positional_encoding_methods()
    
    security_implications:
      learned_embedding_poisoning: |
        Learned positional embeddings are vulnerable to poisoning:
        - Adversary can poison training data at specific positions
        - Model learns corrupted position representations
        - Position-specific backdoors (trigger at position 0, 42, etc.)
        - Defense: Monitor position embedding drift during training
      
      rope_rotation_attacks: |
        RoPE uses rotation matrices:
        - Adversary can craft inputs that exploit rotation geometry
        - Relative position manipulation via rotation angle
        - Numerical precision attacks on rotation computation
        - Defense: Validate rotation angles, monitor for anomalies
      
      position_dependent_vulnerabilities: |
        Different positions have different properties:
        - First position ([CLS] in BERT): Aggregates sentence info
        - Last position: Often critical for generation
        - Middle positions: May be less attended to
        - Adversary targets positions with specific vulnerabilities

key_takeaways:
  critical_concepts:
    - concept: "Attention is permutation-invariant without positional encoding"
      why_it_matters: "Cannot understand word order - catastrophic for language"
    
    - concept: "Sinusoidal encoding: PE(pos,2i) = sin(pos/10000^(2i/d)), PE(pos,2i+1) = cos(...)"
      why_it_matters: "Elegant, generalizes to any length, enables relative position"
    
    - concept: "Learned embeddings treat positions like tokens with learned representations"
      why_it_matters: "Can adapt to task, but fixed maximum length"
    
    - concept: "RoPE encodes position via rotation matrices in 2D subspaces"
      why_it_matters: "Automatic relative position, generalizes, used in modern LLMs"
    
    - concept: "Position encoding added to word embeddings (not concatenated)"
      why_it_matters: "Preserves dimension, allows model to separate content from position"
  
  actionable_steps:
    - step: "Implement sinusoidal positional encoding from scratch"
      verification: "Sin for even dims, cos for odd, different frequencies per dimension"
    
    - step: "Demonstrate permutation invariance without positional encoding"
      verification: "Show attention scores identical for permuted sequences"
    
    - step: "Create learned positional embeddings (like BERT)"
      verification: "Embedding layer for positions, fixed max length"
    
    - step: "Implement simplified RoPE"
      verification: "Rotate embeddings based on position and dimension"
    
    - step: "Compare different encoding methods"
      verification: "Understand tradeoffs: parameters, generalization, performance"
  
  security_principles:
    - principle: "Positional encoding reveals sequence structure"
      application: "Adversary can fingerprint positions, infer lengths"
    
    - principle: "Different positions have different security properties"
      application: "First/last positions often critical - monitor closely"
    
    - principle: "Learned embeddings vulnerable to position-specific poisoning"
      application: "Audit position embeddings for anomalies during training"
    
    - principle: "Position-dependent attacks exploit model's position processing"
      application: "Test model robustness at different positions"
    
    - principle: "Deterministic encodings (sinusoidal, RoPE) are predictable"
      application: "Adversary knows exact encoding - can craft targeted attacks"
  
  common_mistakes:
    - mistake: "Forgetting to add positional encoding to transformers"
      fix: "Always add PE to embeddings - attention needs position info"
    
    - mistake: "Using learned embeddings beyond trained max length"
      fix: "Either use sinusoidal/RoPE or retrain with longer sequences"
    
    - mistake: "Concatenating instead of adding positional encoding"
      fix: "Add PE to embeddings (preserves dimension, works better)"
    
    - mistake: "Not using even d_model for sinusoidal encoding"
      fix: "Ensure d_model is even (sin for even dims, cos for odd)"
    
    - mistake: "Ignoring position information in security analysis"
      fix: "Consider position-dependent attacks and vulnerabilities"
  
  integration_with_book:
    from_section_3_8:
      - "Multi-head attention (needs position info to be useful)"
      - "Attention is permutation-invariant (the problem)"
    
    from_section_3_3:
      - "Word embeddings (position encoding added to these)"
      - "Embedding dimension d_model"
    
    to_next_section:
      - "Section 3.10: Self-attention (attention within same sequence)"
      - "Transformer encoder layer (multi-head + FFN + norms)"
      - "Building complete transformer architecture"
  
  looking_ahead:
    next_concepts:
      - "Self-attention (Q, K, V from same sequence)"
      - "Encoder-decoder attention (cross-attention)"
      - "Transformer encoder layer architecture"
      - "Layer normalization and residual connections"
    
    skills_to_build:
      - "Implement complete transformer encoder layer"
      - "Build transformer decoder layer"
      - "Stack multiple transformer layers"
      - "Train transformer from scratch"
  
  final_thoughts: |
    Positional encoding solves a fundamental limitation of attention mechanisms: 
    permutation invariance. Without position information, transformers cannot understand 
    word order - "cat chased mouse" and "mouse chased cat" would be identical. This is 
    catastrophic for language understanding.
    
    The original transformer introduced elegant sinusoidal encoding: sin and cos functions 
    with different frequencies for different dimensions. This generalizes to any sequence 
    length and enables the model to learn relative positions through linear combinations. 
    Modern transformers often use learned positional embeddings (BERT, GPT-2/3) which can 
    adapt to specific tasks but have fixed maximum length.
    
    Rotary Position Embedding (RoPE) has become the standard for modern LLMs (LLaMA, 
    GPT-NeoX). It encodes position by rotating embeddings in 2D subspaces, automatically 
    creating relative position encoding through rotation angles. No parameters, generalizes 
    to any length, and performs better than alternatives.
    
    From a security perspective: positional encoding creates position-dependent 
    vulnerabilities. Different positions have different properties (first token aggregates 
    info, last token critical for generation). Adversaries can target specific positions, 
    poison learned embeddings at particular positions, or exploit deterministic encoding 
    patterns. Position-aware security analysis is essential for defending transformers.
    
    Next: Section 3.10 introduces self-attention - the core mechanism where queries, keys, 
    and values all come from the same sequence. This enables transformers to build rich 
    contextual representations and is the foundation of both encoder and decoder layers.

---
