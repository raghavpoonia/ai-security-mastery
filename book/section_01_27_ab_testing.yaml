# section_01_27_ab_testing.yaml

---
document_info:
  chapter: "01"
  section: "27"
  title: "A/B Testing for ML"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 5
  tags: ["ab-testing", "experimentation", "statistical-significance", "hypothesis-testing", "online-evaluation", "production-validation"]

# ============================================================================
# SECTION 1.27: A/B TESTING FOR ML
# ============================================================================

section_01_27_ab_testing:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Offline metrics (test set accuracy) don't always predict real-world performance. A model 
    with 95% accuracy might perform worse than one with 93% accuracy in production due to 
    different user behavior, data distribution shifts, or business metrics. A/B testing is 
    the gold standard for validating model improvements by measuring actual impact on users.
    
    A/B testing randomly assigns users to different model versions and compares their outcomes 
    using statistical rigor. This reveals the true business value of model changes - not just 
    accuracy improvements, but impact on conversions, revenue, user satisfaction, and security.
    
    This section covers:
    - Why offline metrics aren't enough
    - A/B testing fundamentals (randomization, control, treatment)
    - Statistical hypothesis testing
    - Sample size calculation (power analysis)
    - Common pitfalls and best practices
    - Multi-armed bandits (adaptive experimentation)
    
    For security ML:
    1. Validate detection improvements don't increase false positives
    2. Measure user impact (friction from security controls)
    3. Balance security and usability
    4. Gradual rollout of new threat detection models
  
  why_this_matters: |
    Industry reality:
    - Google runs 10,000+ A/B tests per year
    - Amazon tests every product change
    - Netflix tests recommendation algorithm variants continuously
    - A/B testing standard practice in ML deployment
    
    Why offline metrics fail:
    - Training/test data may not match production distribution
    - Model interactions with system not captured offline
    - Business metrics (revenue, retention) ≠ accuracy
    - User behavior differs from assumptions
    
    Security context:
    - New detection model: Lower false negatives but higher false positives?
    - A/B test reveals: 50% more blocks, but 80% are false positives
    - User complaints increase → Rollback despite "better" accuracy
    - A/B testing prevents production disasters
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Why Offline Metrics Aren't Enough
  # --------------------------------------------------------------------------
  
  offline_vs_online:
    
    offline_evaluation: |
      Evaluate on held-out test set
      
      Metrics:
      - Accuracy, precision, recall
      - F1 score, AUC-ROC
      - Confusion matrix
      
      Advantages:
      - Fast (no production deployment)
      - Controlled environment
      - Reproducible
      
      Limitations:
      - Test set may not match production
      - Doesn't measure business impact
      - Model interactions not captured
      - User behavior not considered
    
    online_evaluation: |
      Evaluate on real users in production
      
      Metrics:
      - Click-through rate (CTR)
      - Conversion rate
      - Revenue per user
      - User retention
      - Security: Detection rate, false positive rate
      
      Advantages:
      - Measures actual impact
      - Real production conditions
      - Business metrics directly measured
      
      Challenges:
      - Requires production deployment
      - Takes time (need traffic)
      - May impact users negatively
    
    examples_where_offline_fails: |
      Example 1: Recommendation system
      Model A: 92% accuracy offline
      Model B: 90% accuracy offline
      
      A/B test results:
      Model A: 2.1% CTR
      Model B: 2.8% CTR (33% improvement!)
      
      Winner: Model B (despite lower offline accuracy)
      
      Example 2: Malware detection
      Model A: 98% accuracy, 1% FPR
      Model B: 99% accuracy, 3% FPR
      
      Offline: Model B wins (+1% accuracy)
      
      A/B test results:
      Model A: 100 blocks/day, 1 false positive
      Model B: 150 blocks/day, 45 false positives
      
      Winner: Model A (user complaints with Model B)
      
      Lesson: Offline metrics don't capture full picture
  
  # --------------------------------------------------------------------------
  # Core Concept 2: A/B Testing Fundamentals
  # --------------------------------------------------------------------------
  
  ab_testing_basics:
    
    definition: |
      A/B test: Randomized controlled experiment
      
      Compare two variants:
      - Control (A): Baseline (current model)
      - Treatment (B): New model
      
      Randomly assign users to A or B
      Measure outcomes for both groups
      Statistical test: Is B significantly better than A?
    
    randomization: |
      Critical: Random assignment to groups
      
      Why randomization matters:
      - Eliminates confounding factors
      - Creates comparable groups
      - Enables causal inference
      
      Example (without randomization):
      - New users → Model B
      - Existing users → Model A
      - Model B appears better, but confounded by user type
      
      Proper randomization:
      - Each user has 50% chance → A or B
      - Groups balanced on average
      - Differences attributable to model, not user characteristics
    
    key_components: |
      1. Hypothesis: Model B improves metric X by Y%
      2. Randomization: Users randomly assigned to A or B
      3. Sample size: How many users needed for statistical power?
      4. Duration: How long to run experiment?
      5. Metric: What to measure (CTR, conversion, revenue)?
      6. Statistical test: Is difference significant?
    
    typical_setup: |
      Control (A): 50% of traffic
      Treatment (B): 50% of traffic
      
      Or conservative:
      Control (A): 90% of traffic
      Treatment (B): 10% of traffic
      
      Run for 1-4 weeks (depends on traffic)
      Measure metric of interest
      Statistical test to determine winner
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Statistical Hypothesis Testing
  # --------------------------------------------------------------------------
  
  hypothesis_testing:
    
    null_hypothesis: |
      H₀: No difference between A and B
      H₁: B is better than A
      
      Goal: Reject null hypothesis (prove B is better)
    
    statistical_significance: |
      p-value: Probability of observing difference if H₀ true
      
      Threshold: α = 0.05 (5% significance level)
      
      If p-value < 0.05:
        Reject H₀ (B is significantly better)
        Conclusion: Deploy B
      
      If p-value ≥ 0.05:
        Fail to reject H₀ (no significant difference)
        Conclusion: Keep A
    
    type_i_and_ii_errors: |
      Type I error (false positive):
      - Conclude B is better when it's not
      - Deploy inferior model
      - Probability: α (significance level)
      
      Type II error (false negative):
      - Conclude no difference when B is actually better
      - Miss opportunity to improve
      - Probability: β (depends on sample size, effect size)
      
      Power: 1 - β (probability of detecting real difference)
      Typical: 80% power (β = 0.2)
    
    z_test_for_proportions: |
      Use when metric is proportion (CTR, conversion rate)
      
      Example: Testing click-through rates
      
      Control (A):
      - n_A = 10,000 users
      - clicks_A = 200
      - CTR_A = 200/10,000 = 2.0%
      
      Treatment (B):
      - n_B = 10,000 users
      - clicks_B = 250
      - CTR_B = 250/10,000 = 2.5%
      
      Pooled proportion:
      p_pool = (clicks_A + clicks_B) / (n_A + n_B)
             = 450 / 20,000 = 2.25%
      
      Standard error:
      SE = √[p_pool × (1 - p_pool) × (1/n_A + 1/n_B)]
         = √[0.0225 × 0.9775 × (1/10,000 + 1/10,000)]
         = 0.0021
      
      Z-statistic:
      z = (CTR_B - CTR_A) / SE
        = (0.025 - 0.020) / 0.0021
        = 2.38
      
      p-value (two-tailed):
      p = 0.017
      
      Conclusion: p < 0.05 → B is significantly better!
    
    t_test_for_means: |
      Use when metric is continuous (revenue, latency)
      
      Example: Testing average revenue per user
      
      Control (A): mean_A = $5.20, std_A = $2.10, n_A = 1000
      Treatment (B): mean_B = $5.80, std_B = $2.30, n_B = 1000
      
      t-statistic:
      t = (mean_B - mean_A) / SE
      
      SE = √[(std_A²/n_A) + (std_B²/n_B)]
      
      Compare t to critical value or compute p-value
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Sample Size Calculation
  # --------------------------------------------------------------------------
  
  sample_size:
    
    why_sample_size_matters: |
      Too small: Can't detect real differences (low power)
      Too large: Wastes time and resources
      
      Goal: Calculate minimum sample size for desired power
    
    factors_affecting_sample_size: |
      1. Baseline conversion rate (p)
      2. Minimum detectable effect (MDE)
      3. Significance level (α, typically 0.05)
      4. Statistical power (1-β, typically 0.80)
      
      Higher power → Larger sample size needed
      Smaller effect → Larger sample size needed
    
    formula_for_proportions: |
      Sample size per group:
      
      n = 2 × (Z_α/2 + Z_β)² × p × (1-p) / (MDE)²
      
      Where:
      - Z_α/2: Z-score for significance level (1.96 for α=0.05)
      - Z_β: Z-score for power (0.84 for 80% power)
      - p: Baseline proportion
      - MDE: Minimum detectable effect (absolute difference)
    
    example_calculation: |
      Goal: Detect 0.5% improvement in 2% CTR
      
      Given:
      - Baseline CTR (p) = 0.02
      - MDE = 0.005 (0.5 percentage points)
      - α = 0.05 → Z_α/2 = 1.96
      - Power = 0.80 → Z_β = 0.84
      
      Calculate:
      n = 2 × (1.96 + 0.84)² × 0.02 × 0.98 / (0.005)²
        = 2 × 7.84 × 0.0196 / 0.000025
        = 12,288 per group
      
      Total: 24,576 users needed
      
      If 1,000 users/day: Need 25 days
    
    practical_considerations: |
      Small baseline rates need huge samples:
      - CTR = 10%: n ≈ 1,000 per group
      - CTR = 2%: n ≈ 10,000 per group
      - CTR = 0.2%: n ≈ 100,000 per group
      
      Detecting small effects needs huge samples:
      - 10% relative improvement: Moderate sample size
      - 1% relative improvement: Very large sample size
      
      Trade-off: Run longer vs detect smaller effects
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Common Pitfalls
  # --------------------------------------------------------------------------
  
  pitfalls:
    
    peeking: |
      Pitfall: Checking results early and stopping when significant
      
      Problem: Inflates false positive rate (Type I error)
      
      Example:
      - Day 1: p = 0.03 → "Significant! Deploy!"
      - Day 7: p = 0.12 → "Wait, not significant anymore"
      
      Solution: Pre-specify sample size and duration
                Only check at predetermined times
                Or use sequential testing methods
    
    multiple_testing: |
      Pitfall: Testing many metrics/variants without correction
      
      Problem: 
      - Test 20 metrics at α = 0.05
      - Expect 1 false positive on average
      - "Significant" result may be random
      
      Solution: Bonferroni correction
      - Adjust significance level: α_adjusted = α / n_tests
      - Example: 20 tests → α = 0.05/20 = 0.0025
      
      Or: Pre-specify primary metric, others secondary
    
    novelty_effect: |
      Pitfall: Users engage more with new variant initially
      
      Problem: Short-term gain, long-term neutral or negative
      
      Example:
      - Week 1: New UI variant +15% engagement
      - Week 4: Effect diminishes to +2%
      
      Solution: Run tests longer (2-4 weeks minimum)
    
    selection_bias: |
      Pitfall: Non-random assignment to groups
      
      Example:
      - Mobile users → Variant A
      - Desktop users → Variant B
      - Results confounded by device type
      
      Solution: Proper randomization (hash user ID)
    
    carryover_effects: |
      Pitfall: Users switch between variants during test
      
      Problem: Contamination between groups
      
      Solution: 
      - Sticky assignment (same user always sees same variant)
      - Use user ID for randomization, not session
  
  # --------------------------------------------------------------------------
  # Practical Implementation
  # --------------------------------------------------------------------------
  
  practical_implementation: |
    import numpy as np
    from scipy import stats
    
    class ABTest:
        """A/B test for binary outcomes (e.g., conversions)"""
        
        def __init__(self, alpha=0.05):
            """
            Args:
                alpha: Significance level (default 0.05)
            """
            self.alpha = alpha
            self.control_conversions = 0
            self.control_trials = 0
            self.treatment_conversions = 0
            self.treatment_trials = 0
        
        def add_observation(self, group, converted):
            """
            Add observation
            
            Args:
                group: 'control' or 'treatment'
                converted: True/False
            """
            if group == 'control':
                self.control_trials += 1
                if converted:
                    self.control_conversions += 1
            else:
                self.treatment_trials += 1
                if converted:
                    self.treatment_conversions += 1
        
        def get_results(self):
            """Compute test results"""
            # Conversion rates
            p_control = self.control_conversions / self.control_trials
            p_treatment = self.treatment_conversions / self.treatment_trials
            
            # Pooled proportion
            p_pool = (self.control_conversions + self.treatment_conversions) / \
                     (self.control_trials + self.treatment_trials)
            
            # Standard error
            se = np.sqrt(p_pool * (1 - p_pool) * 
                        (1/self.control_trials + 1/self.treatment_trials))
            
            # Z-statistic
            z = (p_treatment - p_control) / se
            
            # p-value (two-tailed)
            p_value = 2 * (1 - stats.norm.cdf(abs(z)))
            
            # Relative lift
            relative_lift = (p_treatment - p_control) / p_control
            
            return {
                'control_conversion_rate': p_control,
                'treatment_conversion_rate': p_treatment,
                'absolute_lift': p_treatment - p_control,
                'relative_lift': relative_lift,
                'z_statistic': z,
                'p_value': p_value,
                'significant': p_value < self.alpha
            }
        
        def sample_size_needed(self, baseline_rate, mde, power=0.8):
            """
            Calculate required sample size per group
            
            Args:
                baseline_rate: Current conversion rate
                mde: Minimum detectable effect (absolute)
                power: Statistical power (default 0.8)
            
            Returns:
                Required sample size per group
            """
            z_alpha = stats.norm.ppf(1 - self.alpha/2)  # 1.96 for α=0.05
            z_beta = stats.norm.ppf(power)  # 0.84 for 80% power
            
            p = baseline_rate
            n = 2 * (z_alpha + z_beta)**2 * p * (1-p) / mde**2
            
            return int(np.ceil(n))
    
    # ========================================================================
    # USAGE EXAMPLE
    # ========================================================================
    
    # Initialize test
    test = ABTest(alpha=0.05)
    
    # Simulate experiment
    np.random.seed(42)
    
    # Control: 2% conversion rate
    for _ in range(10000):
        converted = np.random.random() < 0.02
        test.add_observation('control', converted)
    
    # Treatment: 2.5% conversion rate (25% relative lift)
    for _ in range(10000):
        converted = np.random.random() < 0.025
        test.add_observation('treatment', converted)
    
    # Analyze results
    results = test.get_results()
    
    print("A/B Test Results:")
    print(f"Control conversion rate: {results['control_conversion_rate']:.3%}")
    print(f"Treatment conversion rate: {results['treatment_conversion_rate']:.3%}")
    print(f"Absolute lift: {results['absolute_lift']:.3%}")
    print(f"Relative lift: {results['relative_lift']:.1%}")
    print(f"p-value: {results['p_value']:.4f}")
    print(f"Significant: {results['significant']}")
    
    # Sample size calculation
    baseline = 0.02
    mde = 0.005  # Want to detect 0.5% absolute improvement
    n = test.sample_size_needed(baseline, mde)
    print(f"\nSample size needed: {n:,} per group")
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "A/B testing: Randomized controlled experiment to measure impact"
      - "Offline metrics ≠ online performance"
      - "Statistical significance: p-value < α (typically 0.05)"
      - "Sample size depends on baseline rate and effect size"
      - "Multiple testing requires correction (Bonferroni)"
      - "Randomization critical (eliminates confounding)"
    
    practical_skills:
      - "Design A/B test (randomization, metrics, duration)"
      - "Calculate sample size (power analysis)"
      - "Perform z-test for proportions"
      - "Interpret p-values correctly"
      - "Avoid pitfalls (peeking, multiple testing)"
    
    security_mindset:
      - "Balance detection rate vs false positive rate"
      - "Measure user impact (friction, complaints)"
      - "A/B test before full rollout (gradual validation)"
      - "Monitor business metrics, not just accuracy"
      - "Offline accuracy can mislead (test in production)"
    
    remember_this:
      - "Offline metrics don't guarantee production success"
      - "A/B test = gold standard for validation"
      - "Randomization critical (eliminates confounding)"
      - "Pre-specify sample size (avoid peeking)"
      - "Correct for multiple testing (Bonferroni)"
    
    next_steps:
      - "Next section: Chapter Summary (FINAL SECTION!)"
      - "You now understand production validation!"
      - "A/B testing essential for ML deployment"

---
