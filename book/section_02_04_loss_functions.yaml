# section_02_04_loss_functions.yaml
---
document_info:
  chapter: "02"
  section: "0204"
  title: "Loss Functions for Neural Networks"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-06"
  estimated_pages: 6
  tags: ["loss-functions", "mse", "cross-entropy", "focal-loss", "optimization", "data-poisoning"]

# ============================================================================
# SECTION 0204: LOSS FUNCTIONS FOR NEURAL NETWORKS
# ============================================================================

section_0204_loss_functions:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Loss functions are how we measure how wrong our neural network is. They
    convert the difference between predictions and truth into a single number
    that gradient descent tries to minimize.
    
    This section covers all major loss functions: MSE for regression, binary
    cross-entropy for two-class problems, categorical cross-entropy for
    multi-class, sparse categorical (memory efficient), focal loss for
    imbalanced data, and custom losses for specialized tasks.
    
    You'll implement each loss function from scratch, understand when to use
    which, compare convergence behavior, and visualize loss landscapes. By
    the end, you'll know how to choose and implement the right loss for any
    problem.
    
    Security critical: Loss manipulation is the core of data poisoning attacks.
    Adversaries craft poisoned samples that increase loss on clean data or
    decrease loss on backdoored data, steering model behavior maliciously.
  
  learning_objectives:
    
    conceptual:
      - "Understand what loss functions measure"
      - "Know when to use MSE vs cross-entropy"
      - "Grasp why cross-entropy better than MSE for classification"
      - "Understand focal loss motivation (class imbalance)"
      - "Recognize how loss landscape affects optimization"
      - "Connect loss manipulation to data poisoning"
    
    practical:
      - "Implement all major loss functions from scratch"
      - "Compute gradients of loss wrt predictions"
      - "Compare convergence on real datasets"
      - "Visualize loss surfaces (2D projections)"
      - "Handle numerical stability issues"
      - "Build custom loss functions for specific tasks"
    
    security_focused:
      - "Loss manipulation enables data poisoning"
      - "Adversarial perturbations minimize loss changes"
      - "Custom losses for robust training"
      - "Detecting poisoned samples via loss analysis"
  
  prerequisites:
    - "Section 0201-0203 (neural networks, forward pass)"
    - "Understanding of probability (for cross-entropy)"
    - "Calculus (derivatives, chain rule)"
  
  # --------------------------------------------------------------------------
  # Topic 1: What Are Loss Functions?
  # --------------------------------------------------------------------------
  
  loss_function_fundamentals:
    
    definition:
      
      what_is_loss:
        description: |
          A loss function L(ŷ, y) measures the discrepancy between:
          - ŷ (predictions from model)
          - y (true labels/values)
        
        goal: "Minimize loss → model predictions closer to truth"
        
        notation: |
          Single sample: L(ŷ, y)
          Batch average: L = (1/m) Σᵢ L(ŷᵢ, yᵢ)
          
          Where m = batch size
      
      properties_of_good_loss:
        
        non_negative: "L(ŷ, y) ≥ 0 always (zero = perfect prediction)"
        
        differentiable: "Need ∂L/∂ŷ for gradient descent"
        
        sensitive: "Changes when predictions change (captures errors)"
        
        appropriate: "Matches the task (regression vs classification)"
    
    role_in_training:
      
      optimization_objective:
        description: "Training = finding parameters θ that minimize loss"
        
        formulation: |
          θ* = argmin_θ L(f(X; θ), y)
          
          Where:
          f(X; θ) = neural network with parameters θ
          L = loss function
          θ* = optimal parameters
      
      gradient_descent_connection:
        description: "Loss gradient tells us how to update parameters"
        
        update_rule: |
          θ ← θ - α · ∂L/∂θ
          
          Where:
          α = learning rate
          ∂L/∂θ = gradient of loss wrt parameters
        
        chain_rule: |
          ∂L/∂θ = ∂L/∂ŷ · ∂ŷ/∂θ
          
          ∂L/∂ŷ comes from loss function (this section)
          ∂ŷ/∂θ comes from backpropagation (Section 0205-0206)
    
    task_specific_losses:
      
      regression: "Predict continuous values (house prices, temperature)"
      loss_type: "Mean Squared Error (MSE)"
      
      binary_classification: "Two classes (spam/not spam, cat/dog)"
      loss_type: "Binary Cross-Entropy"
      
      multi_class_classification: "Multiple classes (digit 0-9, 1000 objects)"
      loss_type: "Categorical Cross-Entropy"
      
      imbalanced_classification: "Rare classes (fraud detection: 0.1% fraud)"
      loss_type: "Focal Loss, weighted cross-entropy"
  
  # --------------------------------------------------------------------------
  # Topic 2: Mean Squared Error (MSE) - Regression
  # --------------------------------------------------------------------------
  
  mean_squared_error:
    
    definition:
      
      formula: |
        MSE = (1/m) Σᵢ (ŷᵢ - yᵢ)²
        
        Single sample: L(ŷ, y) = (ŷ - y)²
      
      intuition: "Square of error - penalizes large errors heavily"
      
      example: |
        True value: y = 5
        
        Prediction: ŷ = 5 → Error = 0 → Loss = 0
        Prediction: ŷ = 6 → Error = 1 → Loss = 1
        Prediction: ŷ = 7 → Error = 2 → Loss = 4 (double error = 4x loss!)
        Prediction: ŷ = 10 → Error = 5 → Loss = 25 (5x error = 25x loss!)
    
    properties:
      
      advantages:
        - "Simple to understand and compute"
        - "Smooth and differentiable everywhere"
        - "Penalizes outliers heavily (large errors costly)"
        - "Connects to maximum likelihood under Gaussian assumption"
      
      disadvantages:
        - "Sensitive to outliers (can dominate gradient)"
        - "Not appropriate for classification (probabilities)"
        - "Scale-dependent (units matter)"
    
    gradient:
      
      derivation: |
        L = (ŷ - y)²
        
        ∂L/∂ŷ = 2(ŷ - y)
      
      batch_gradient: |
        For batch: L = (1/m) Σᵢ (ŷᵢ - yᵢ)²
        
        ∂L/∂ŷᵢ = (2/m)(ŷᵢ - yᵢ)
      
      note: "Factor of 2 often omitted (absorbed into learning rate)"
    
    implementation:
      
      forward: |
        def mse_loss(y_pred, y_true):
            """
            y_pred: (n_samples,) predictions
            y_true: (n_samples,) true values
            """
            return np.mean((y_pred - y_true) ** 2)
      
      backward: |
        def mse_gradient(y_pred, y_true):
            """
            Returns gradient ∂L/∂ŷ
            """
            m = y_pred.shape[0]
            return (2/m) * (y_pred - y_true)
      
      usage: |
        # Regression task: predict house prices
        y_true = np.array([300, 250, 400, 350])  # True prices ($1000s)
        y_pred = np.array([310, 240, 410, 330])  # Predictions
        
        loss = mse_loss(y_pred, y_true)
        print(f"MSE: {loss:.2f}")  # MSE: 250.00
        
        gradient = mse_gradient(y_pred, y_true)
        print(f"Gradient: {gradient}")
    
    variants:
      
      mae:
        name: "Mean Absolute Error"
        formula: "MAE = (1/m) Σᵢ |ŷᵢ - yᵢ|"
        
        advantage: "Less sensitive to outliers"
        disadvantage: "Not differentiable at ŷ = y"
        
        when_to_use: "When outliers present, want robust loss"
      
      huber_loss:
        name: "Huber Loss (smooth L1)"
        
        formula: |
          L_δ(ŷ, y) = {
            ½(ŷ - y)²           if |ŷ - y| ≤ δ
            δ|ŷ - y| - ½δ²      otherwise
          }
        
        properties: "MSE for small errors, MAE for large errors"
        
        when_to_use: "Combine benefits of MSE and MAE"
  
  # --------------------------------------------------------------------------
  # Topic 3: Binary Cross-Entropy - Binary Classification
  # --------------------------------------------------------------------------
  
  binary_cross_entropy:
    
    definition:
      
      formula: |
        BCE = -(1/m) Σᵢ [yᵢ log(ŷᵢ) + (1-yᵢ) log(1-ŷᵢ)]
        
        Single sample: L(ŷ, y) = -[y log(ŷ) + (1-y) log(1-ŷ)]
      
      assumptions:
        - "y ∈ {0, 1} (binary labels)"
        - "ŷ ∈ [0, 1] (predicted probability)"
        - "ŷ = P(y=1|x) from sigmoid output"
      
      intuition: |
        If true label y = 1:
          Loss = -log(ŷ)
          ŷ = 1.0 → Loss = 0 (perfect!)
          ŷ = 0.5 → Loss = 0.69
          ŷ = 0.1 → Loss = 2.30 (bad prediction = high loss)
        
        If true label y = 0:
          Loss = -log(1-ŷ)
          ŷ = 0.0 → Loss = 0 (perfect!)
          ŷ = 0.5 → Loss = 0.69
          ŷ = 0.9 → Loss = 2.30 (bad prediction = high loss)
    
    why_cross_entropy_for_classification:
      
      probabilistic_interpretation:
        description: "Cross-entropy measures KL divergence from true to predicted distribution"
        
        connection_to_likelihood: |
          Minimizing cross-entropy = maximizing likelihood
          
          For binary: P(y|x) = ŷ^y × (1-ŷ)^(1-y)
          
          Negative log-likelihood = -log P(y|x) = BCE
      
      better_than_mse:
        
        mse_problem: |
          With sigmoid output, MSE gradient:
          ∂MSE/∂z = (ŷ - y) · ŷ(1-ŷ)
          
          When ŷ saturated (near 0 or 1), ŷ(1-ŷ) ≈ 0
          → Gradient vanishes even with large error!
        
        bce_advantage: |
          BCE gradient:
          ∂BCE/∂z = ŷ - y
          
          No sigmoid derivative term!
          Gradient proportional to error, never vanishes.
    
    gradient:
      
      derivation: |
        L = -[y log(ŷ) + (1-y) log(1-ŷ)]
        
        ∂L/∂ŷ = -[y/ŷ - (1-y)/(1-ŷ)]
               = -(y - ŷ) / [ŷ(1-ŷ)]
      
      combined_with_sigmoid: |
        Output layer: ŷ = σ(z) = 1/(1 + e^(-z))
        
        ∂L/∂z = ∂L/∂ŷ · ∂ŷ/∂z
              = -(y - ŷ) / [ŷ(1-ŷ)] · ŷ(1-ŷ)
              = -(y - ŷ)
              = ŷ - y
        
        Beautiful simplification! Gradient = prediction error.
    
    implementation:
      
      forward: |
        def binary_cross_entropy(y_pred, y_true):
            """
            y_pred: (n_samples,) predicted probabilities ∈ [0, 1]
            y_true: (n_samples,) true labels ∈ {0, 1}
            """
            # Clip to avoid log(0)
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            return -np.mean(
                y_true * np.log(y_pred) + 
                (1 - y_true) * np.log(1 - y_pred)
            )
      
      backward: |
        def binary_cross_entropy_gradient(y_pred, y_true):
            """
            Returns gradient ∂L/∂ŷ
            """
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            m = y_pred.shape[0]
            return -(y_true / y_pred - (1 - y_true) / (1 - y_pred)) / m
      
      combined_with_sigmoid: |
        def bce_with_logits(logits, y_true):
            """
            Numerically stable: combines sigmoid + BCE
            
            logits: raw scores (before sigmoid)
            """
            # Use log-sum-exp trick for stability
            max_val = np.maximum(logits, 0)
            loss = max_val - logits * y_true + np.log(
                np.exp(-max_val) + np.exp(-logits - max_val)
            )
            return np.mean(loss)
      
      usage: |
        # Binary classification: spam detection
        y_true = np.array([1, 0, 1, 0, 1])  # 1=spam, 0=not spam
        y_pred = np.array([0.9, 0.1, 0.8, 0.2, 0.7])  # Probabilities
        
        loss = binary_cross_entropy(y_pred, y_true)
        print(f"BCE: {loss:.4f}")  # BCE: 0.1625
  
  # --------------------------------------------------------------------------
  # Topic 4: Categorical Cross-Entropy - Multi-Class Classification
  # --------------------------------------------------------------------------
  
  categorical_cross_entropy:
    
    definition:
      
      formula: |
        CCE = -(1/m) Σᵢ Σⱼ yᵢⱼ log(ŷᵢⱼ)
        
        Single sample: L(ŷ, y) = -Σⱼ yⱼ log(ŷⱼ)
      
      assumptions:
        - "y is one-hot encoded: y = [0, 0, 1, 0, ...] (only one 1)"
        - "ŷ is probability distribution: Σⱼ ŷⱼ = 1, all ŷⱼ ∈ [0,1]"
        - "ŷ from softmax output"
      
      intuition: |
        Only the true class contributes to loss.
        
        If true class is 3 (one-hot: [0,0,1,0]):
          Loss = -log(ŷ₃)
          
          ŷ₃ = 0.9 → Loss = -log(0.9) = 0.105
          ŷ₃ = 0.5 → Loss = -log(0.5) = 0.693
          ŷ₃ = 0.1 → Loss = -log(0.1) = 2.303
        
        High confidence in correct class → low loss
        Low confidence in correct class → high loss
    
    example:
      
      mnist_digit_classification: |
        True label: 7 (one-hot: [0,0,0,0,0,0,0,1,0,0])
        
        Prediction 1: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.55, 0.05, 0.05]
        → Loss = -log(0.55) = 0.60
        
        Prediction 2: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.91, 0.01, 0.01]
        → Loss = -log(0.91) = 0.09
        
        Model 2 is more confident in correct class → lower loss
    
    gradient:
      
      derivation: |
        L = -Σⱼ yⱼ log(ŷⱼ)
        
        ∂L/∂ŷₖ = -yₖ/ŷₖ
      
      combined_with_softmax: |
        Softmax: ŷⱼ = e^zⱼ / Σₖ e^zₖ
        
        Combined gradient:
        ∂L/∂zₖ = ŷₖ - yₖ
        
        Again, beautiful simplification: gradient = prediction error!
    
    implementation:
      
      forward: |
        def categorical_cross_entropy(y_pred, y_true):
            """
            y_pred: (n_samples, n_classes) probabilities
            y_true: (n_samples, n_classes) one-hot encoded
            """
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
      
      backward: |
        def categorical_cross_entropy_gradient(y_pred, y_true):
            """
            Returns gradient ∂L/∂ŷ
            Shape: (n_samples, n_classes)
            """
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            m = y_pred.shape[0]
            return -y_true / y_pred / m
      
      usage: |
        # MNIST digit classification (10 classes)
        y_true = np.array([
            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],  # Digit 3
            [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   # Digit 7
        ])
        
        y_pred = np.array([
            [0.05, 0.05, 0.1, 0.6, 0.05, 0.05, 0.05, 0.02, 0.02, 0.01],
            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.03, 0.88, 0.01, 0.02]
        ])
        
        loss = categorical_cross_entropy(y_pred, y_true)
        print(f"CCE: {loss:.4f}")
    
    sparse_categorical_cross_entropy:
      
      motivation: "Memory efficient - don't need one-hot encoding"
      
      difference: |
        Regular CCE:
          y_true: [0,0,1,0,0] (one-hot, 5 values)
        
        Sparse CCE:
          y_true: 2 (integer class label, 1 value)
      
      implementation: |
        def sparse_categorical_cross_entropy(y_pred, y_true):
            """
            y_pred: (n_samples, n_classes) probabilities
            y_true: (n_samples,) integer class labels
            """
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            m = y_pred.shape[0]
            log_likelihood = -np.log(y_pred[range(m), y_true])
            return np.mean(log_likelihood)
      
      memory_savings: |
        MNIST (60K samples, 10 classes):
        
        One-hot: 60,000 × 10 × 4 bytes = 2.4 MB
        Sparse: 60,000 × 1 × 4 bytes = 0.24 MB
        
        Savings: 10x less memory!
  
  # --------------------------------------------------------------------------
  # Topic 5: Focal Loss - Handling Class Imbalance
  # --------------------------------------------------------------------------
  
  focal_loss:
    
    motivation:
      
      class_imbalance_problem:
        scenario: "Fraud detection: 99.9% legitimate, 0.1% fraud"
        
        naive_cross_entropy_issue: |
          Model can achieve 99.9% accuracy by predicting "legitimate" always!
          
          Easy negatives (legitimate) dominate gradient:
          - Many samples (99.9%)
          - Each contributes small gradient
          - Total gradient overwhelms rare positives
        
        result: "Model doesn't learn to detect fraud"
      
      solution_idea: "Down-weight easy examples, focus on hard examples"
    
    definition:
      
      formula: |
        FL(ŷ, y) = -(1-ŷ)^γ × y log(ŷ) - ŷ^γ × (1-y) log(1-ŷ)
        
        Where γ ≥ 0 is focusing parameter (typically γ = 2)
      
      modulating_factor:
        description: "Weight down-weights easy examples"
        
        for_positive_class: |
          y = 1, weight = (1-ŷ)^γ
          
          ŷ = 0.9 (confident correct) → weight = 0.1^2 = 0.01 (down-weight heavily)
          ŷ = 0.5 (uncertain) → weight = 0.5^2 = 0.25 (moderate weight)
          ŷ = 0.1 (confident wrong) → weight = 0.9^2 = 0.81 (full weight)
        
        for_negative_class: |
          y = 0, weight = ŷ^γ
          
          Similar logic: easy negatives down-weighted
    
    comparison_to_standard_ce:
      
      example_losses: |
        Standard CE vs Focal Loss (γ=2)
        
        Easy example (ŷ=0.9, y=1):
          CE: -log(0.9) = 0.105
          FL: (0.1)^2 × 0.105 = 0.001 (100x less!)
        
        Hard example (ŷ=0.1, y=1):
          CE: -log(0.1) = 2.303
          FL: (0.9)^2 × 2.303 = 1.865 (only 20% less)
        
        Effect: Easy examples contribute much less to gradient
    
    implementation:
      
      forward: |
        def focal_loss(y_pred, y_true, gamma=2.0, alpha=0.25):
            """
            y_pred: (n_samples,) predicted probabilities
            y_true: (n_samples,) true labels {0, 1}
            gamma: focusing parameter
            alpha: class weighting (optional)
            """
            epsilon = 1e-15
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            # Compute cross-entropy
            ce = -(y_true * np.log(y_pred) + 
                   (1 - y_true) * np.log(1 - y_pred))
            
            # Compute modulating factor
            p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
            modulating = (1 - p_t) ** gamma
            
            # Optional class weighting
            if alpha is not None:
                alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)
                focal = alpha_t * modulating * ce
            else:
                focal = modulating * ce
            
            return np.mean(focal)
      
      usage: |
        # Imbalanced dataset: 1% positive class
        y_true = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])
        y_pred = np.array([0.1, 0.05, 0.2, 0.15, 0.1, 0.08, 0.12, 0.09, 0.11, 0.8])
        
        ce_loss = binary_cross_entropy(y_pred, y_true)
        fl_loss = focal_loss(y_pred, y_true, gamma=2.0)
        
        print(f"CE Loss: {ce_loss:.4f}")
        print(f"Focal Loss: {fl_loss:.4f}")
    
    when_to_use:
      - "Highly imbalanced datasets (fraud, anomaly detection)"
      - "Object detection (many background boxes, few objects)"
      - "Medical diagnosis (rare diseases)"
      - "When standard CE leads to ignoring minority class"
  
  # --------------------------------------------------------------------------
  # Topic 6: Custom Loss Functions
  # --------------------------------------------------------------------------
  
  custom_loss_functions:
    
    when_needed:
      - "Domain-specific requirements"
      - "Combining multiple objectives"
      - "Incorporating constraints"
      - "Adversarial robustness"
    
    examples:
      
      contrastive_loss:
        use_case: "Metric learning, face verification"
        
        formula: |
          L = (1/2) [y × d² + (1-y) × max(0, margin - d)²]
          
          Where:
          d = distance between embeddings
          y = 1 if same class, 0 if different
          margin = threshold distance
        
        intuition: |
          Same class pairs: minimize distance
          Different class pairs: push apart (at least margin distance)
      
      triplet_loss:
        use_case: "Face recognition, image retrieval"
        
        formula: |
          L = max(0, d(anchor, positive) - d(anchor, negative) + margin)
        
        intuition: |
          Anchor and positive should be closer than
          anchor and negative by at least margin
      
      center_loss:
        use_case: "Improve intra-class compactness"
        
        formula: |
          L = (1/2) Σᵢ ||xᵢ - cᵧᵢ||²
          
          Where cᵧᵢ is center of class yᵢ
        
        combined: "Usually used with softmax: L_total = L_softmax + λ × L_center"
    
    implementation_template:
      
      structure: |
        class CustomLoss:
            def __init__(self, **hyperparams):
                # Store hyperparameters
                pass
            
            def forward(self, y_pred, y_true, **kwargs):
                """
                Compute loss value
                
                Returns: scalar loss
                """
                pass
            
            def backward(self, y_pred, y_true, **kwargs):
                """
                Compute gradient ∂L/∂ŷ
                
                Returns: array same shape as y_pred
                """
                pass
      
      example_perceptual_loss: |
        class PerceptualLoss:
            """
            For image generation: match feature representations
            """
            def __init__(self, feature_extractor):
                self.extractor = feature_extractor
            
            def forward(self, generated, target):
                # Extract features
                gen_features = self.extractor(generated)
                target_features = self.extractor(target)
                
                # MSE on features
                loss = np.mean((gen_features - target_features) ** 2)
                return loss
  
  # --------------------------------------------------------------------------
  # Topic 7: Loss Landscape Visualization
  # --------------------------------------------------------------------------
  
  loss_landscape:
    
    what_is_loss_landscape:
      
      definition: |
        The loss landscape is the loss value as a function of parameters.
        L(θ) where θ = all model parameters
      
      dimensionality: |
        For network with 1M parameters:
        Loss landscape is 1M-dimensional surface!
        
        Visualization: Project to 2D or 3D for interpretation
    
    visualization_methods:
      
      two_parameter_slice:
        description: "Vary two parameters, fix others"
        
        code: |
          def visualize_loss_2d(model, X, y, param1, param2, range_vals):
              """
              param1, param2: parameter names to vary
              range_vals: values to try for each parameter
              """
              original_params = {
                  param1: model.params[param1].copy(),
                  param2: model.params[param2].copy()
              }
              
              losses = np.zeros((len(range_vals), len(range_vals)))
              
              for i, val1 in enumerate(range_vals):
                  for j, val2 in enumerate(range_vals):
                      model.params[param1] = original_params[param1] + val1
                      model.params[param2] = original_params[param2] + val2
                      
                      y_pred = model.forward(X)
                      losses[i, j] = compute_loss(y_pred, y)
              
              # Restore
              model.params[param1] = original_params[param1]
              model.params[param2] = original_params[param2]
              
              # Plot
              plt.contourf(range_vals, range_vals, losses, levels=20)
              plt.colorbar(label='Loss')
              plt.xlabel(param1)
              plt.ylabel(param2)
              plt.show()
      
      random_directions:
        description: "Project loss along random directions in parameter space"
        
        method: |
          1. Pick two random directions: d1, d2 (unit vectors)
          2. Vary: θ_new = θ_current + α·d1 + β·d2
          3. Compute loss for grid of (α, β)
          4. Visualize as contour plot
    
    landscape_characteristics:
      
      convex_vs_nonconvex:
        convex: "Single global minimum, gradient descent guaranteed to find it"
        nonconvex: "Multiple local minima, saddle points, plateaus"
        neural_networks: "Highly non-convex! But often many good minima"
      
      sharp_vs_flat_minima:
        sharp: "Loss increases rapidly when moving away from minimum"
        flat: "Loss stays low in neighborhood of minimum"
        
        generalization: |
          Flat minima generalize better!
          Sharp minima overfit (sensitive to small parameter changes)
      
      loss_landscape_and_training:
        good_initialization: "Start in basin of good minimum"
        learning_rate: "Too large: skip over minima; too small: stuck in poor minimum"
        momentum: "Helps escape shallow local minima"
        batch_size: "Larger batches → smoother landscape"
  
  # --------------------------------------------------------------------------
  # Topic 8: Security Implications - Loss Manipulation
  # --------------------------------------------------------------------------
  
  security_implications:
    
    data_poisoning_via_loss:
      
      attack_mechanism:
        description: |
          Adversary injects poisoned training samples that manipulate loss
          to steer model behavior maliciously.
        
        two_strategies:
          dirty_label: "Change labels to increase loss on target samples"
          clean_label: "Craft inputs that model learns incorrectly while maintaining correct labels"
      
      example_targeted_attack:
        goal: "Make model misclassify specific input (e.g., attacker's face) as authorized user"
        
        method: |
          1. Attacker controls small fraction of training data (e.g., 1%)
          2. Inject samples: (attacker_face, label=authorized_user)
          3. Model minimizes loss → learns association
          4. At test time: attacker_face → predicted as authorized_user
        
        loss_perspective: |
          Poisoned samples contribute to loss:
          L_total = L_clean + L_poison
          
          Gradient descent minimizes both, giving adversary influence
      
      example_availability_attack:
        goal: "Degrade model performance on all inputs"
        
        method: |
          1. Inject many mislabeled samples
          2. Loss on these samples very high
          3. Model tries to fit noise → poor generalization
        
        result: "Model accuracy drops from 95% to 60%"
    
    adversarial_perturbations_minimize_loss_change:
      
      concept:
        description: |
          Adversarial examples: small input changes that change prediction
          but keep loss approximately the same (stealthy)
        
        formulation: |
          Find perturbation δ such that:
          1. ||δ|| small (imperceptible)
          2. f(x + δ) ≠ f(x) (prediction changes)
          3. L(f(x + δ), y) ≈ L(f(x), y) (loss barely changes)
      
      why_loss_matters:
        observation: |
          If loss changes a lot, model is "confident" change is wrong.
          If loss changes little, perturbation bypasses detection.
        
        defense_implication: |
          Monitor loss on incoming samples:
          High loss → potential adversarial input
          
          But clever adversaries minimize loss change!
    
    robust_loss_functions:
      
      motivation: "Design losses resilient to poisoned samples"
      
      strategies:
        
        outlier_resistant_losses:
          huber_loss: "Less sensitive to outliers than MSE"
          mae: "Robust to label noise"
          
          tradeoff: "May slow convergence on clean data"
        
        reweighting_schemes:
          sample_reweighting: |
            Detect suspicious samples (high loss), down-weight them:
            L = Σᵢ wᵢ L(ŷᵢ, yᵢ)
            
            Where wᵢ inversely proportional to loss magnitude
          
          meta_learning: "Learn which samples to trust during training"
        
        certified_defenses:
          approach: "Prove model robust within loss threshold"
          
          example: |
            If L(f(x), y) < ε, guarantee prediction won't change
            within ||δ|| < τ perturbation
    
    detecting_poisoned_samples:
      
      loss_based_detection:
        method: |
          1. Train model on dataset
          2. Compute loss for each sample
          3. Samples with anomalously high loss → suspicious
        
        code: |
          def detect_poison_via_loss(model, X_train, y_train, threshold_percentile=95):
              """
              Flag samples with loss > threshold
              """
              losses = []
              for x, y in zip(X_train, y_train):
                  y_pred = model.forward(x)
                  loss = compute_loss(y_pred, y)
                  losses.append(loss)
              
              threshold = np.percentile(losses, threshold_percentile)
              suspicious = np.where(np.array(losses) > threshold)[0]
              
              return suspicious
        
        limitation: |
          Clean outliers also have high loss.
          Clever adversary crafts poison with normal loss.
      
      influence_functions:
        description: "Measure how much each training sample influences model"
        
        high_influence: "Removing sample significantly changes model"
        
        detection: "Poisoned samples often have anomalously high influence"
  
  # --------------------------------------------------------------------------
  # Hands-On Exercises
  # --------------------------------------------------------------------------
  
  hands_on_exercises:
    
    exercise_1:
      title: "Implement All Loss Functions"
      difficulty: "Intermediate"
      time: "60 minutes"
      
      task: "Build complete loss function library with forward and backward"
      
      requirements:
        - "MSE, MAE, Huber Loss"
        - "Binary Cross-Entropy, Categorical Cross-Entropy, Sparse CCE"
        - "Focal Loss"
        - "Numerical stability for all"
        - "Unit tests with known values"
      
      test_cases:
        mse: |
          y_true = [1, 2, 3, 4]
          y_pred = [1.1, 2.2, 2.8, 4.1]
          Expected MSE: 0.0225
        
        bce: |
          y_true = [1, 0, 1, 0]
          y_pred = [0.9, 0.1, 0.8, 0.2]
          Expected BCE: ~0.17
        
        cce: |
          y_true = [[0,0,1], [0,1,0]]
          y_pred = [[0.1,0.2,0.7], [0.3,0.6,0.1]]
          Expected CCE: ~0.36
    
    exercise_2:
      title: "Convergence Comparison"
      difficulty: "Advanced"
      time: "90 minutes"
      
      task: "Compare MSE vs Cross-Entropy for binary classification"
      
      methodology:
        1: "Generate 2-class dataset (linearly separable)"
        2: "Train logistic regression with MSE loss"
        3: "Train logistic regression with BCE loss"
        4: "Plot loss curves over epochs"
        5: "Compare final accuracy and convergence speed"
      
      expected_results:
        - "BCE converges faster (fewer epochs)"
        - "BCE reaches better final accuracy"
        - "MSE may get stuck (vanishing gradients)"
      
      analysis:
        - "Why does BCE outperform MSE?"
        - "When do gradients vanish with MSE?"
        - "Plot gradient magnitudes over training"
    
    exercise_3:
      title: "Loss Landscape Visualizer"
      difficulty: "Advanced"
      time: "90 minutes"
      
      task: "Visualize 2D loss landscape for simple network"
      
      implementation:
        - "Train simple 2-layer network on XOR"
        - "Select two weights to vary"
        - "Compute loss on grid of weight values"
        - "Create contour plot showing landscape"
        - "Mark training trajectory"
      
      extensions:
        - "Animate training process on landscape"
        - "Compare landscapes for different initializations"
        - "Show sharp vs flat minima"
    
    exercise_4:
      title: "Poisoning Detection via Loss"
      difficulty: "Advanced"
      time: "120 minutes"
      
      task: "Implement loss-based poison detection"
      
      steps:
        1: "Create clean MNIST dataset"
        2: "Inject 5% poisoned samples (random label flips)"
        3: "Train model to convergence"
        4: "Compute per-sample loss"
        5: "Flag top 10% high-loss samples as suspicious"
        6: "Evaluate detection accuracy (precision/recall)"
      
      metrics:
        precision: "Of flagged samples, how many are actually poisoned?"
        recall: "Of poisoned samples, how many were flagged?"
        
      analysis:
        - "Does method catch most poison?"
        - "How many false positives (clean flagged)?"
        - "Can you improve detection?"
  
  # --------------------------------------------------------------------------
  # Common Mistakes
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    numerical_instability:
      
      "log_of_zero":
        mistake: "np.log(0) → -inf, causes NaN"
        fix: "Clip predictions: y_pred = np.clip(y_pred, 1e-15, 1-1e-15)"
      
      "overflow_in_softmax":
        mistake: "exp(large_number) → inf"
        fix: "Subtract max before exp: exp(z - max(z))"
      
      "underflow_in_log":
        mistake: "log(very_small_number) → -large_number → numerical issues"
        fix: "Use log-sum-exp tricks, PyTorch's log_softmax"
    
    wrong_loss_for_task:
      
      "mse_for_classification":
        mistake: "Using MSE loss with sigmoid/softmax output"
        problem: "Vanishing gradients, slow convergence"
        fix: "Use cross-entropy for classification"
      
      "cross_entropy_without_softmax":
        mistake: "Applying cross-entropy to raw logits"
        problem: "Loss values nonsensical (expects probabilities)"
        fix: "Apply softmax first, or use combined loss (like PyTorch's CrossEntropyLoss)"
      
      "sparse_with_onehot":
        mistake: "Using sparse cross-entropy with one-hot labels"
        problem: "Wastes memory, incorrect computation"
        fix: "Use integer labels with sparse CCE, one-hot with regular CCE"
    
    gradient_errors:
      
      "forgetting_mean_in_gradient":
        mistake: "Returning sum of gradients instead of mean"
        problem: "Gradient magnitude scales with batch size"
        fix: "Divide by batch size: gradient / m"
      
      "wrong_gradient_sign":
        mistake: "Positive gradient instead of negative (or vice versa)"
        problem: "Model diverges instead of converging"
        fix: "Check derivation, verify with numerical gradient"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Loss functions measure prediction error: L(ŷ, y) quantifies how wrong model is"
      - "Use MSE for regression, cross-entropy for classification (never mix these up!)"
      - "Cross-entropy better than MSE for classification: gradients don't vanish when combined with sigmoid/softmax"
      - "Focal loss down-weights easy examples: critical for imbalanced datasets (fraud, anomalies)"
      - "Loss landscape is non-convex for neural networks: multiple minima, but many are good"
      - "Data poisoning manipulates loss: adversary injects samples that steer model toward wrong behavior"
    
    actionable_steps:
      - "Always clip predictions before taking log: avoid log(0) causing -inf and NaN"
      - "Use combined losses (e.g., BCEWithLogits): more numerically stable than separate softmax + cross-entropy"
      - "Monitor per-sample loss during training: spikes may indicate poisoned data or adversarial inputs"
      - "For imbalanced data, use focal loss or weighted cross-entropy: don't let majority class dominate"
      - "Visualize loss landscape: understand optimization difficulty, sharp vs flat minima"
      - "Implement numerical gradient checking: verify your gradient computation is correct"
    
    security_principles:
      - "Loss manipulation is core of data poisoning: adversary's goal is to minimize loss on poisoned data"
      - "Monitor loss distributions: anomalously high losses may indicate poisoned samples or adversarial inputs"
      - "Adversarial examples minimize loss changes: small perturbations that don't raise alarms"
      - "Robust losses (Huber, MAE) resist outliers: less susceptible to label noise and poisoning"
    
    next_steps:
      - "Section 0205-0206: Backpropagation (computing ∂L/∂θ through chain rule)"
      - "Section 0207-0208: Optimization (using loss gradients to update parameters)"
      - "Section 0222: Adversarial robustness (exploiting loss for attacks)"

---

# Section 0204 Complete (6 pages)

**Progress: 4/24 sections done**

**Remaining: 20 sections**

Say **"next"** for Section 0205: Backpropagation Theory
