# section_03_07_scaled_dot_product_attention.yaml

---
document_info:
  title: "Scaled Dot-Product Attention"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 7
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into scaled dot-product attention: why scaling by √d_k is necessary, softmax saturation problem, matrix formulation for efficiency, attention masking, and security implications"
  estimated_pages: 7
  tags:
    - scaled-dot-product-attention
    - attention-scaling
    - softmax-saturation
    - matrix-operations
    - attention-masking
    - causal-attention

section_overview:
  title: "Scaled Dot-Product Attention"
  number: "3.7"
  
  purpose: |
    Scaled dot-product attention is the specific attention variant used in transformers 
    (Vaswani et al., 2017). It takes the dot product score function from Section 3.6 
    and adds a critical scaling factor: divide by √d_k before softmax. This simple 
    modification prevents softmax saturation when dimensions are large, enabling stable 
    training of deep transformers.
    
    The matrix formulation - Attention(Q, K, V) = softmax(QK^T / √d_k)V - enables highly 
    efficient computation using GPUs. Attention masking allows preventing certain positions 
    from attending to others (crucial for autoregressive models like GPT).
    
    For security engineers: Understanding scaling reveals numerical stability vulnerabilities. 
    Attention masks create boundaries that can be exploited (force attention or prevent 
    it). The matrix operations expose timing side-channels. Mastery of these mechanics 
    is essential for both using and attacking transformer-based LLMs.
  
  learning_objectives:
    conceptual:
      - "Understand why dot product magnitude grows with dimension d_k"
      - "Grasp softmax saturation and why it breaks gradient flow"
      - "Learn how scaling by √d_k prevents saturation"
      - "Understand matrix formulation for parallel computation"
      - "Learn attention masking for causal (autoregressive) models"
    
    practical:
      - "Implement scaled dot-product attention from scratch (NumPy)"
      - "Demonstrate softmax saturation with and without scaling"
      - "Implement efficient batched attention using matrix operations"
      - "Create attention masks (padding, causal)"
      - "Analyze computational complexity O(n²d)"
    
    security_focused:
      - "Identify numerical stability vulnerabilities in attention"
      - "Understand how masking creates security boundaries"
      - "Recognize timing side-channels in matrix operations"
      - "Detect attention mask bypass attempts"
      - "Audit attention score distributions for anomalies"
  
  prerequisites:
    knowledge:
      - "Section 3.6: Attention mechanism, Query-Key-Value, score functions"
      - "Linear algebra: matrix multiplication, transpose"
      - "Softmax function and gradient flow"
      - "Numerical stability in deep learning"
    
    skills:
      - "NumPy matrix operations and broadcasting"
      - "Understanding of computational complexity"
      - "Debugging numerical issues in neural networks"
  
  key_transitions:
    from_section_3_6: |
      Section 3.6 introduced attention as weighted sum with Query-Key-Value formulation. 
      We compared score functions and noted dot product is simplest (no parameters). 
      Now we see why dot product needs scaling and how to implement it efficiently.
    
    to_next_section: |
      Section 3.8 introduces multi-head attention: running multiple scaled dot-product 
      attention operations in parallel to learn different attention patterns simultaneously.

topics:
  - topic_number: 1
    title: "Why Scaling Is Necessary: The Softmax Saturation Problem"
    
    overview: |
      Dot product scores grow in magnitude as dimension d_k increases. For large d_k 
      (e.g., 512), scores can be very large, causing softmax to saturate: one position 
      gets weight ~1.0, all others ~0.0. This creates vanishing gradients - the model 
      can't learn. Scaling by √d_k keeps scores in a reasonable range.
    
    content:
      dot_product_magnitude_growth:
        problem_statement: |
          For random unit vectors Q, k ∈ ℝ^d_k:
          
          E[Q^T k] = 0       (expected value is zero)
          Var[Q^T k] = d_k   (variance grows with dimension!)
          
          Standard deviation: σ = √d_k
        
        concrete_example:
          dimension_64:
            d_k: 64
            std_dev: "√64 = 8"
            typical_scores: "-24 to +24 (3σ range)"
          
          dimension_512:
            d_k: 512
            std_dev: "√512 ≈ 22.6"
            typical_scores: "-68 to +68 (3σ range)"
          
          dimension_1024:
            d_k: 1024
            std_dev: "√1024 = 32"
            typical_scores: "-96 to +96 (3σ range)"
        
        consequence: |
          Large d_k → large score magnitudes → softmax saturation
      
      softmax_saturation_explained:
        what_happens: |
          Consider scores: [20, 0, -20] (typical for d_k=512)
          
          Softmax: exp(20) / [exp(20) + exp(0) + exp(-20)]
          
          exp(20) ≈ 485,165,195
          exp(0) = 1
          exp(-20) ≈ 0.0000000021
          
          Weights: [1.0, 0.0, 0.0] (effectively)
        
        gradient_problem: |
          When softmax is saturated (one weight ≈ 1, others ≈ 0):
          
          ∂softmax/∂scores ≈ 0 for all positions
          → Gradients vanish
          → Model can't learn where to attend
          
          Training becomes unstable or fails entirely
        
        visualization:
          unsaturated_softmax: |
            Scores: [1, 0, -1]
            Weights: [0.67, 0.24, 0.09]
            → All positions get some weight
            → Gradients flow to all positions
          
          saturated_softmax: |
            Scores: [20, 0, -20]
            Weights: [1.0, 0.0, 0.0]
            → Only one position gets weight
            → Gradients vanish for others
      
      scaling_solution:
        formula: "score = (Q^T k) / √d_k"
        
        effect_on_variance: |
          Original: Var[Q^T k] = d_k
          After scaling: Var[(Q^T k) / √d_k] = d_k / d_k = 1
          
          Variance normalized to 1 regardless of dimension!
        
        result: |
          Scores stay in reasonable range (-10 to +10) even for large d_k
          → Softmax remains in responsive region
          → Gradients flow properly
          → Training is stable
        
        comparison:
          without_scaling:
            d_k_512: "Scores ~ [-68, +68] → severe saturation"
            training: "Unstable or fails"
          
          with_scaling:
            d_k_512: "Scores ~ [-3, +3] → gentle softmax"
            training: "Stable convergence"
    
    implementation:
      demonstrate_saturation:
        language: python
        code: |
          import numpy as np
          
          def demonstrate_softmax_saturation():
              """Show how large dot products cause softmax saturation."""
              
              print("=== Softmax Saturation Demonstration ===\n")
              
              # Test different dimensions
              dimensions = [8, 64, 512, 1024]
              
              for d_k in dimensions:
                  # Random unit vectors
                  Q = np.random.randn(d_k)
                  Q = Q / np.linalg.norm(Q)
                  
                  # Multiple keys
                  num_keys = 5
                  K = np.random.randn(num_keys, d_k)
                  K = K / np.linalg.norm(K, axis=1, keepdims=True)
                  
                  # Compute dot products (unscaled)
                  scores_unscaled = np.dot(K, Q)
                  
                  # Compute dot products (scaled)
                  scores_scaled = scores_unscaled / np.sqrt(d_k)
                  
                  # Apply softmax
                  def softmax(x):
                      exp_x = np.exp(x - np.max(x))
                      return exp_x / exp_x.sum()
                  
                  weights_unscaled = softmax(scores_unscaled)
                  weights_scaled = softmax(scores_scaled)
                  
                  print(f"Dimension d_k = {d_k}:")
                  print(f"  Unscaled scores: {scores_unscaled}")
                  print(f"  Unscaled weights: {weights_unscaled}")
                  print(f"  Max weight: {weights_unscaled.max():.6f}")
                  
                  print(f"  Scaled scores: {scores_scaled}")
                  print(f"  Scaled weights: {weights_scaled}")
                  print(f"  Max weight: {weights_scaled.max():.6f}")
                  
                  # Measure saturation (entropy)
                  def entropy(probs):
                      return -np.sum(probs * np.log(probs + 1e-10))
                  
                  max_entropy = np.log(num_keys)
                  entropy_unscaled = entropy(weights_unscaled)
                  entropy_scaled = entropy(weights_scaled)
                  
                  print(f"  Entropy (unscaled): {entropy_unscaled:.3f} / {max_entropy:.3f} "
                        f"({'SATURATED' if entropy_unscaled < 0.5 else 'OK'})")
                  print(f"  Entropy (scaled):   {entropy_scaled:.3f} / {max_entropy:.3f} "
                        f"({'SATURATED' if entropy_scaled < 0.5 else 'OK'})")
                  print()
              
              print("Observation:")
              print("  Without scaling: High dimensions → low entropy (saturated)")
              print("  With scaling: Entropy remains reasonable across dimensions")
          
          demonstrate_softmax_saturation()
      
      gradient_flow_comparison:
        language: python
        code: |
          def compare_gradient_flow():
              """Compare gradient flow with and without scaling."""
              
              print("\n=== Gradient Flow Comparison ===\n")
              
              d_k = 512
              
              # Simulate saturated softmax (unscaled)
              scores_unscaled = np.array([20.0, 0.0, -20.0])
              exp_unscaled = np.exp(scores_unscaled - np.max(scores_unscaled))
              weights_unscaled = exp_unscaled / exp_unscaled.sum()
              
              # Simulate normal softmax (scaled)
              scores_scaled = np.array([2.0, 0.0, -2.0])
              exp_scaled = np.exp(scores_scaled - np.max(scores_scaled))
              weights_scaled = exp_scaled / exp_scaled.sum()
              
              print("Unscaled (saturated):")
              print(f"  Scores: {scores_unscaled}")
              print(f"  Weights: {weights_unscaled}")
              
              # Approximate gradient
              # ∂softmax_i/∂score_j ≈ α_i(δ_ij - α_j)
              grad_unscaled = weights_unscaled[1] * (1 - weights_unscaled[1])
              
              print(f"  Gradient for middle position: {grad_unscaled:.6f}")
              print(f"  → VANISHED (< 10⁻⁶)!")
              
              print("\nScaled (normal):")
              print(f"  Scores: {scores_scaled}")
              print(f"  Weights: {weights_scaled}")
              
              grad_scaled = weights_scaled[1] * (1 - weights_scaled[1])
              
              print(f"  Gradient for middle position: {grad_scaled:.6f}")
              print(f"  → HEALTHY gradient flow")
              
              print(f"\nGradient ratio: {grad_scaled / (grad_unscaled + 1e-10):.0f}x larger with scaling")
          
          compare_gradient_flow()
    
    security_implications:
      numerical_stability_attacks: |
        Adversaries can exploit numerical instabilities:
        - Craft inputs causing extreme dot products (overflow/underflow)
        - Force softmax saturation to make model focus on adversarial position
        - Or force uniform attention (prevent focusing on safety-relevant content)
        - Defense: Monitor attention score distributions, clip extremes
      
      scaling_factor_manipulation: |
        If scaling factor is adjustable parameter:
        - Adversaries might manipulate it during fine-tuning
        - Too small: causes saturation (gradient vanishing)
        - Too large: uniform attention (no focus)
        - Defense: Fix scaling factor to √d_k, don't make it learnable

  - topic_number: 2
    title: "Matrix Formulation: Efficient Parallel Computation"
    
    overview: |
      The matrix formulation of scaled dot-product attention enables computing attention 
      for entire sequences in parallel using GPU matrix operations. Instead of computing 
      one query-key pair at a time, we compute all pairs simultaneously with a single 
      matrix multiplication: Attention(Q, K, V) = softmax(QK^T / √d_k)V.
    
    content:
      from_loops_to_matrices:
        loop_version: |
          For each query position i:
            For each key position j:
              score[i,j] = dot(Q[i], K[j]) / √d_k
            weights[i] = softmax(scores[i])
            output[i] = Σ weights[i,j] × V[j]
          
          Complexity: O(n²d) operations, but sequential
        
        matrix_version: |
          Scores = (Q @ K^T) / √d_k     # (n, n) matrix
          Weights = softmax(Scores)      # row-wise softmax
          Output = Weights @ V           # (n, d_v) result
          
          Complexity: Same O(n²d), but fully parallel!
      
      mathematical_formulation:
        components:
          queries: "Q ∈ ℝ^(n × d_k) - n query vectors"
          keys: "K ∈ ℝ^(n × d_k) - n key vectors"
          values: "V ∈ ℝ^(n × d_v) - n value vectors"
        
        formula: |
          Attention(Q, K, V) = softmax(QK^T / √d_k) V
          
          Where:
          - QK^T ∈ ℝ^(n × n) - pairwise scores matrix
          - softmax applied row-wise (each row = attention for one query)
          - Result ∈ ℝ^(n × d_v) - attended outputs
        
        step_by_step:
          step_1: |
            Scores = QK^T / √d_k
            Shape: (n, n)
            Entry (i,j): relevance of key j to query i
          
          step_2: |
            Weights = softmax(Scores, axis=1)
            Shape: (n, n)
            Row i: attention distribution for query i
          
          step_3: |
            Output = Weights @ V
            Shape: (n, d_v)
            Row i: weighted sum of values for query i
      
      computational_complexity:
        operations_breakdown:
          qk_transpose: "O(n² × d_k) - matrix multiplication"
          scaling: "O(n²) - element-wise division"
          softmax: "O(n²) - for all n rows"
          weighted_sum: "O(n² × d_v) - matrix multiplication"
          total: "O(n² × d) where d = max(d_k, d_v)"
        
        memory_complexity:
          attention_matrix: "O(n²) - store all pairwise scores"
          activations: "O(nd) - query, key, value matrices"
          total: "O(n² + nd)"
        
        quadratic_scaling:
          problem: "Complexity quadratic in sequence length n"
          
          examples:
            n_100: "O(10,000 × d) operations"
            n_1000: "O(1,000,000 × d) operations (100x more!)"
            n_10000: "O(100,000,000 × d) operations (10,000x more!)"
          
          consequence: |
            Context window limited by quadratic cost
            This is why early transformers had n ≤ 512
            Modern approaches: sparse attention, linear attention
      
      batching_and_parallel_computation:
        batch_processing: |
          Process multiple sequences simultaneously:
          
          Q ∈ ℝ^(batch × n × d_k)
          K ∈ ℝ^(batch × n × d_k)
          V ∈ ℝ^(batch × n × d_v)
          
          Attention(Q, K, V) computed for all batch items in parallel
        
        gpu_efficiency: |
          Matrix operations highly optimized on GPUs:
          - Thousands of cores compute elements in parallel
          - Matrix multiplication uses optimized BLAS libraries
          - Achieves near-peak GPU throughput
          
          Result: 100x+ speedup over sequential CPU implementation
    
    implementation:
      scaled_dot_product_attention:
        language: python
        code: |
          import numpy as np
          
          class ScaledDotProductAttention:
              """
              Scaled dot-product attention (Vaswani et al., 2017).
              
              Attention(Q, K, V) = softmax(QK^T / √d_k) V
              """
              
              def __init__(self):
                  pass
              
              def forward(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray,
                         mask: np.ndarray = None) -> tuple:
                  """
                  Compute scaled dot-product attention.
                  
                  Args:
                      Q: Queries (seq_len_q, d_k) or (batch, seq_len_q, d_k)
                      K: Keys (seq_len_k, d_k) or (batch, seq_len_k, d_k)
                      V: Values (seq_len_k, d_v) or (batch, seq_len_k, d_v)
                      mask: Optional mask (seq_len_q, seq_len_k) or (batch, seq_len_q, seq_len_k)
                  
                  Returns:
                      output: Attention output (seq_len_q, d_v) or (batch, seq_len_q, d_v)
                      attention_weights: Attention weights (seq_len_q, seq_len_k) or (batch, seq_len_q, seq_len_k)
                  """
                  d_k = Q.shape[-1]
                  
                  # Step 1: Compute scores QK^T / √d_k
                  scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
                  
                  # Step 2: Apply mask (if provided)
                  if mask is not None:
                      scores = np.where(mask == 0, -1e9, scores)
                  
                  # Step 3: Apply softmax
                  attention_weights = self.softmax(scores, axis=-1)
                  
                  # Step 4: Compute weighted sum of values
                  output = np.matmul(attention_weights, V)
                  
                  return output, attention_weights
              
              def softmax(self, x: np.ndarray, axis: int = -1) -> np.ndarray:
                  """Numerically stable softmax."""
                  exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
                  return exp_x / np.sum(exp_x, axis=axis, keepdims=True)
              
              def __call__(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray,
                          mask: np.ndarray = None) -> tuple:
                  """Alias for forward."""
                  return self.forward(Q, K, V, mask)
          
          
          # Example usage
          seq_len = 10
          d_k = 64
          d_v = 64
          
          # Create attention layer
          attention = ScaledDotProductAttention()
          
          # Random queries, keys, values
          Q = np.random.randn(seq_len, d_k)
          K = np.random.randn(seq_len, d_k)
          V = np.random.randn(seq_len, d_v)
          
          # Compute attention
          output, weights = attention(Q, K, V)
          
          print(f"Input shapes:")
          print(f"  Q: {Q.shape}")
          print(f"  K: {K.shape}")
          print(f"  V: {V.shape}")
          print(f"\nOutput shapes:")
          print(f"  Output: {output.shape}")
          print(f"  Attention weights: {weights.shape}")
          print(f"\nAttention weight properties:")
          print(f"  Min: {weights.min():.6f}")
          print(f"  Max: {weights.max():.6f}")
          print(f"  Row sums (should be 1.0): {weights.sum(axis=1)[:3]}")
      
      batched_attention:
        language: python
        code: |
          def demonstrate_batched_attention():
              """Show batched attention for parallel processing."""
              
              print("\n=== Batched Attention ===\n")
              
              batch_size = 4
              seq_len = 8
              d_k = 64
              d_v = 64
              
              # Batch of queries, keys, values
              Q = np.random.randn(batch_size, seq_len, d_k)
              K = np.random.randn(batch_size, seq_len, d_k)
              V = np.random.randn(batch_size, seq_len, d_v)
              
              attention = ScaledDotProductAttention()
              output, weights = attention(Q, K, V)
              
              print(f"Batch size: {batch_size}")
              print(f"Sequence length: {seq_len}")
              print(f"Key dimension: {d_k}")
              print(f"\nBatched computation:")
              print(f"  Input: ({batch_size}, {seq_len}, {d_k})")
              print(f"  Output: {output.shape}")
              print(f"  Attention weights: {weights.shape}")
              print(f"\nAll {batch_size} sequences processed in parallel!")
              
              # Verify each batch item has proper attention weights
              for i in range(batch_size):
                  row_sums = weights[i].sum(axis=1)
                  print(f"  Batch {i}: attention weights sum = {row_sums[0]:.6f}")
          
          demonstrate_batched_attention()
      
      complexity_analysis:
        language: python
        code: |
          def analyze_complexity():
              """Analyze computational and memory complexity."""
              
              print("\n=== Computational Complexity Analysis ===\n")
              
              d = 512  # Model dimension
              sequence_lengths = [128, 512, 1024, 2048, 4096]
              
              print(f"Model dimension d = {d}\n")
              print("Sequence | Operations    | Memory      | Time (relative)")
              print("---------|---------------|-------------|----------------")
              
              base_time = None
              
              for n in sequence_lengths:
                  # Operations: O(n² × d)
                  ops = n * n * d
                  
                  # Memory: O(n² + nd)
                  mem = n * n + n * d
                  
                  # Relative time (normalized to n=128)
                  if base_time is None:
                      base_time = ops
                      rel_time = 1.0
                  else:
                      rel_time = ops / base_time
                  
                  print(f"{n:8d} | {ops:13.2e} | {mem:11.2e} | {rel_time:14.1f}x")
              
              print("\nObservation: Quadratic scaling in sequence length!")
              print("  Doubling length → 4x operations, 4x memory")
              print("  This limits context windows in early transformers")
          
          analyze_complexity()
    
    security_implications:
      timing_side_channels: |
        Matrix operations expose timing information:
        - Different sequence lengths → different computation time
        - Adversaries can infer input length from timing
        - Attention weight sparsity affects memory access patterns
        - Defense: Constant-time implementations, padding to fixed length
      
      memory_access_patterns: |
        O(n²) attention matrix creates observable memory patterns:
        - Cache behavior depends on attention structure
        - Sparse vs dense attention has different memory footprint
        - Side-channel attacks can extract attention patterns
        - Defense: Secure computing environments, oblivious attention
      
      quadratic_complexity_dos: |
        O(n²) scaling enables DoS attacks:
        - Adversary sends maximum-length inputs
        - 4096 tokens: 16M operations (vs 16K for 128 tokens)
        - Can overwhelm compute resources
        - Defense: Rate limiting, input length limits, timeout

  - topic_number: 3
    title: "Attention Masking: Causal and Padding Masks"
    
    overview: |
      Attention masks allow controlling which positions can attend to which. Padding 
      masks prevent attending to padding tokens (non-content). Causal masks enforce 
      left-to-right processing in autoregressive models (GPT), preventing "looking ahead" 
      at future tokens. Masks are implemented by setting scores to -∞ before softmax.
    
    content:
      masking_mechanism:
        purpose: "Prevent certain attention connections"
        
        implementation: |
          Before softmax, set masked positions to -∞:
          
          scores = QK^T / √d_k
          scores = where(mask == 0, -∞, scores)  # Mask out positions
          weights = softmax(scores)
          
          After softmax: masked positions have weight ≈ 0
        
        why_negative_infinity: |
          exp(-∞) = 0
          → Masked positions get zero weight in softmax
          → No information flows from masked positions
      
      padding_mask:
        purpose: "Ignore padding tokens (not real content)"
        
        scenario: |
          Sequences have different lengths, padded to same length:
          
          Seq 1: [w1, w2, w3, PAD, PAD]  (3 real tokens)
          Seq 2: [w1, w2, w3, w4, w5]    (5 real tokens)
          
          Don't want attention to padding tokens!
        
        mask_creation: |
          For sequence [real, real, PAD, PAD]:
          
          Padding mask = [1, 1, 0, 0]
          → Positions 2-3 masked (can't attend to them)
        
        application: |
          Scores matrix (before softmax):
          [[s11, s12, -∞, -∞],   # Query 1: can't attend to PAD
           [s21, s22, -∞, -∞],   # Query 2: can't attend to PAD
           [-∞,  -∞,  -∞, -∞],   # Query 3 (PAD): can't attend anywhere
           [-∞,  -∞,  -∞, -∞]]   # Query 4 (PAD): can't attend anywhere
      
      causal_mask:
        purpose: "Prevent attending to future positions (autoregressive)"
        
        scenario: |
          Decoder generating tokens left-to-right:
          When predicting token t, can only see tokens 1, 2, ..., t-1
          Cannot "look ahead" at future tokens!
        
        mask_structure: |
          Lower triangular matrix (allow attending to past):
          
          [[1, 0, 0, 0],  # Token 1: can see only token 1
           [1, 1, 0, 0],  # Token 2: can see tokens 1-2
           [1, 1, 1, 0],  # Token 3: can see tokens 1-3
           [1, 1, 1, 1]]  # Token 4: can see tokens 1-4
        
        application: |
          Scores matrix (before softmax):
          [[s11, -∞,  -∞,  -∞ ],
           [s21, s22, -∞,  -∞ ],
           [s31, s32, s33, -∞ ],
           [s41, s42, s43, s44]]
        
        why_necessary: |
          Without causal mask, model sees all tokens:
          - During training: sees the answer it's supposed to predict
          - Learns to "cheat" by looking ahead
          - At inference: can't look ahead → distribution mismatch
          
          Causal mask enforces: training = inference conditions
      
      combined_masks:
        scenario: "Autoregressive model with variable-length sequences"
        
        combination: |
          Need both padding mask AND causal mask:
          
          Padding mask: where real tokens are
          Causal mask: lower triangular (no future)
          Combined: padding_mask AND causal_mask
        
        example: |
          Sequence: [w1, w2, w3, PAD, PAD]
          
          Padding: [[1, 1, 1, 0, 0],
                    [1, 1, 1, 0, 0],
                    [1, 1, 1, 0, 0],
                    [0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0]]
          
          Causal:  [[1, 0, 0, 0, 0],
                    [1, 1, 0, 0, 0],
                    [1, 1, 1, 0, 0],
                    [1, 1, 1, 1, 0],
                    [1, 1, 1, 1, 1]]
          
          Combined: [[1, 0, 0, 0, 0],   # Token 1: only self
                     [1, 1, 0, 0, 0],   # Token 2: self + token 1
                     [1, 1, 1, 0, 0],   # Token 3: self + 1-2
                     [0, 0, 0, 0, 0],   # PAD: nothing
                     [0, 0, 0, 0, 0]]   # PAD: nothing
    
    implementation:
      attention_masks:
        language: python
        code: |
          def create_padding_mask(seq: np.ndarray, pad_token: int = 0) -> np.ndarray:
              """
              Create padding mask from sequence.
              
              Args:
                  seq: Token sequence (seq_len,) or (batch, seq_len)
                  pad_token: Padding token ID
              
              Returns:
                  mask: Binary mask (seq_len, seq_len) or (batch, seq_len, seq_len)
                      1 = can attend, 0 = cannot attend (masked)
              """
              if seq.ndim == 1:
                  # Single sequence
                  seq_len = len(seq)
                  # Mask: can attend to non-padding positions
                  mask = (seq != pad_token).astype(float)
                  # Broadcast to (seq_len, seq_len)
                  mask = mask[np.newaxis, :] * mask[:, np.newaxis]
              else:
                  # Batch of sequences
                  batch_size, seq_len = seq.shape
                  mask = (seq != pad_token).astype(float)
                  # Broadcast to (batch, seq_len, seq_len)
                  mask = mask[:, np.newaxis, :] * mask[:, :, np.newaxis]
              
              return mask
          
          
          def create_causal_mask(seq_len: int) -> np.ndarray:
              """
              Create causal mask (lower triangular).
              
              Args:
                  seq_len: Sequence length
              
              Returns:
                  mask: Causal mask (seq_len, seq_len)
                      1 = can attend (past), 0 = cannot attend (future)
              """
              # Lower triangular matrix
              mask = np.tril(np.ones((seq_len, seq_len)))
              return mask
          
          
          def combine_masks(padding_mask: np.ndarray, 
                           causal_mask: np.ndarray) -> np.ndarray:
              """
              Combine padding and causal masks.
              
              Args:
                  padding_mask: Padding mask (seq_len, seq_len) or (batch, seq_len, seq_len)
                  causal_mask: Causal mask (seq_len, seq_len)
              
              Returns:
                  combined_mask: Logical AND of both masks
              """
              if padding_mask.ndim == 2:
                  # Single sequence
                  combined = padding_mask * causal_mask
              else:
                  # Batch
                  batch_size = padding_mask.shape[0]
                  # Broadcast causal mask to batch
                  causal_mask = causal_mask[np.newaxis, :, :]
                  combined = padding_mask * causal_mask
              
              return combined
          
          
          # Example usage
          print("=== Attention Masking ===\n")
          
          # Padding mask example
          seq = np.array([1, 2, 3, 0, 0])  # 3 real tokens, 2 padding
          padding_mask = create_padding_mask(seq, pad_token=0)
          
          print("Sequence (0 = PAD):", seq)
          print("\nPadding mask (1 = can attend, 0 = masked):")
          print(padding_mask.astype(int))
          
          # Causal mask example
          seq_len = 5
          causal_mask = create_causal_mask(seq_len)
          
          print(f"\nCausal mask (seq_len={seq_len}):")
          print(causal_mask.astype(int))
          
          # Combined mask
          combined_mask = combine_masks(padding_mask, causal_mask)
          
          print("\nCombined mask (padding AND causal):")
          print(combined_mask.astype(int))
          print("\nRow 3 (token 3): can attend to tokens 1-3 (not PAD, not future)")
      
      masked_attention_demo:
        language: python
        code: |
          def demonstrate_masked_attention():
              """Show how masking affects attention."""
              
              print("\n=== Masked Attention Demonstration ===\n")
              
              seq_len = 5
              d_k = 8
              
              # Create sample data
              Q = np.random.randn(seq_len, d_k)
              K = np.random.randn(seq_len, d_k)
              V = np.random.randn(seq_len, d_k)
              
              attention = ScaledDotProductAttention()
              
              # No mask
              output_no_mask, weights_no_mask = attention(Q, K, V)
              
              print("Without mask:")
              print("  Attention weights (each position can attend to all):")
              print(weights_no_mask[:3, :])  # Show first 3 rows
              
              # With causal mask
              causal_mask = create_causal_mask(seq_len)
              output_causal, weights_causal = attention(Q, K, V, mask=causal_mask)
              
              print("\nWith causal mask:")
              print("  Attention weights (lower triangular - can't see future):")
              print(weights_causal[:3, :])  # Show first 3 rows
              
              print("\nObserve:")
              print("  - Without mask: All positions have non-zero weight")
              print("  - With mask: Future positions have zero weight")
          
          demonstrate_masked_attention()
    
    security_implications:
      mask_bypass_attacks: |
        Adversaries attempt to bypass causal masking:
        - Exploit implementation bugs (mask not applied correctly)
        - Find positions where mask is weaker or missing
        - Craft inputs that leak future information despite mask
        - Defense: Rigorous testing of mask implementation, formal verification
      
      mask_leakage: |
        Mask structure itself can leak information:
        - Padding mask reveals true sequence length
        - Different mask patterns for different input types
        - Timing differences based on mask complexity
        - Defense: Consistent masking, constant-time operations
      
      causal_mask_security: |
        Causal mask enforces critical security boundary:
        - Prevents model from "cheating" by seeing future
        - Breaking causal mask = catastrophic training failure
        - Or worse: model learns to predict from future (data leak)
        - Defense: Verify causal mask in all attention layers

  - topic_number: 4
    title: "Implementation Details and Numerical Stability"
    
    overview: |
      Practical implementation of scaled dot-product attention requires careful attention 
      to numerical stability. Softmax can overflow/underflow, gradients can vanish, and 
      floating-point precision matters. Understanding these details is critical for both 
      implementation and security analysis.
    
    content:
      numerical_stability_considerations:
        softmax_overflow_underflow:
          problem: |
            exp(x) overflows for large x (x > 88)
            exp(x) underflows for small x (x < -88)
          
          solution: |
            Subtract max before exp (softmax invariant):
            
            softmax(x) = exp(x) / Σ exp(x)
                       = exp(x - max(x)) / Σ exp(x - max(x))
            
            Keeps exponents in safe range
        
        scaling_factor_precision:
          issue: "√d_k must be computed carefully"
          
          considerations:
            - "Use float32 or float64 (not float16)"
            - "Precompute and cache √d_k (don't recompute)"
            - "Ensure consistent precision across operations"
        
        gradient_clipping:
          issue: "Gradients can explode in deep networks"
          
          solution: |
            Clip gradients to maximum norm:
            
            if ||grad|| > max_norm:
              grad = grad × (max_norm / ||grad||)
            
            Prevents gradient explosion in attention
      
      memory_optimization:
        attention_matrix_size:
          problem: "O(n²) memory for attention weights"
          
          example: |
            n = 4096 tokens
            Attention weights: 4096 × 4096 = 16M floats
            Memory: 16M × 4 bytes = 64 MB per layer
            Deep model (24 layers): 1.5 GB just for attention weights!
          
          solutions:
            gradient_checkpointing: "Recompute activations during backward pass"
            sparse_attention: "Only compute subset of attention weights"
            low_rank_factorization: "Approximate attention with lower-rank matrices"
        
        kv_caching:
          optimization: "Cache computed key/value projections during generation"
          
          scenario: |
            Autoregressive generation (GPT):
            - Generate token 1, token 2, ..., token n
            - Each step: recompute attention over all previous tokens
            - But K, V for previous tokens don't change!
          
          solution: |
            Cache K, V from previous steps:
            - Step 1: Compute K1, V1 → cache
            - Step 2: Compute K2, V2 → cache + [K1, K2], [V1, V2]
            - Step n: Only compute Kn, Vn → use cached K1...Kn-1, V1...Vn-1
            
            Speedup: ~2-3x for long sequences
      
      implementation_best_practices:
        use_framework_implementations:
          pytorch: "torch.nn.functional.scaled_dot_product_attention"
          tensorflow: "tf.nn.scaled_dot_product_attention"
          benefit: "Highly optimized, numerically stable, GPU-accelerated"
        
        validate_shapes:
          critical: "Attention very sensitive to shape mismatches"
          
          checks:
            - "Q and K last dim must match (d_k)"
            - "K and V first dim must match (seq_len)"
            - "Mask shape must broadcast correctly"
            - "Batch dimensions must align"
        
        test_edge_cases:
          single_token: "Sequence length = 1 (no attention needed)"
          all_masked: "All positions masked (what happens?)"
          very_long: "Maximum context length (memory limits)"
          numerical_extremes: "Very large or very small values"
    
    implementation:
      production_attention:
        language: python
        code: |
          class ProductionScaledDotProductAttention:
              """
              Production-ready scaled dot-product attention.
              
              Includes:
              - Numerical stability (safe softmax)
              - Proper masking
              - Shape validation
              - Efficient computation
              """
              
              def __init__(self, dropout: float = 0.1):
                  """
                  Args:
                      dropout: Dropout rate for attention weights
                  """
                  self.dropout = dropout
              
              def forward(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray,
                         mask: np.ndarray = None, training: bool = False) -> tuple:
                  """
                  Compute scaled dot-product attention with all optimizations.
                  
                  Args:
                      Q: Queries (batch, seq_len_q, d_k)
                      K: Keys (batch, seq_len_k, d_k)
                      V: Values (batch, seq_len_k, d_v)
                      mask: Optional mask (batch, seq_len_q, seq_len_k)
                      training: Whether in training mode (for dropout)
                  
                  Returns:
                      output: Attention output (batch, seq_len_q, d_v)
                      attention_weights: Attention weights (batch, seq_len_q, seq_len_k)
                  """
                  # Validate shapes
                  self._validate_shapes(Q, K, V, mask)
                  
                  d_k = Q.shape[-1]
                  
                  # Compute scores with scaling
                  scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)
                  
                  # Apply mask (if provided)
                  if mask is not None:
                      # Masked positions get -1e9 (close to -inf)
                      scores = np.where(mask == 0, -1e9, scores)
                  
                  # Stable softmax
                  attention_weights = self._stable_softmax(scores, axis=-1)
                  
                  # Apply dropout (if training)
                  if training and self.dropout > 0:
                      attention_weights = self._dropout(attention_weights, self.dropout)
                  
                  # Compute output
                  output = np.matmul(attention_weights, V)
                  
                  return output, attention_weights
              
              def _validate_shapes(self, Q: np.ndarray, K: np.ndarray, 
                                  V: np.ndarray, mask: np.ndarray = None):
                  """Validate tensor shapes."""
                  assert Q.shape[-1] == K.shape[-1], f"Q and K dimension mismatch: {Q.shape[-1]} vs {K.shape[-1]}"
                  assert K.shape[-2] == V.shape[-2], f"K and V sequence length mismatch: {K.shape[-2]} vs {V.shape[-2]}"
                  
                  if mask is not None:
                      assert mask.shape[-2] == Q.shape[-2], f"Mask query dim mismatch"
                      assert mask.shape[-1] == K.shape[-2], f"Mask key dim mismatch"
              
              def _stable_softmax(self, x: np.ndarray, axis: int = -1) -> np.ndarray:
                  """Numerically stable softmax."""
                  # Subtract max for stability
                  x_max = np.max(x, axis=axis, keepdims=True)
                  exp_x = np.exp(x - x_max)
                  return exp_x / np.sum(exp_x, axis=axis, keepdims=True)
              
              def _dropout(self, x: np.ndarray, rate: float) -> np.ndarray:
                  """Apply dropout."""
                  keep_prob = 1 - rate
                  mask = np.random.binomial(1, keep_prob, x.shape) / keep_prob
                  return x * mask
          
          
          # Example usage
          print("\n=== Production Attention Implementation ===\n")
          
          batch_size = 2
          seq_len = 6
          d_k = 64
          d_v = 64
          
          attention = ProductionScaledDotProductAttention(dropout=0.1)
          
          Q = np.random.randn(batch_size, seq_len, d_k)
          K = np.random.randn(batch_size, seq_len, d_k)
          V = np.random.randn(batch_size, seq_len, d_v)
          
          # Create causal mask
          causal_mask = create_causal_mask(seq_len)
          causal_mask = causal_mask[np.newaxis, :, :]  # Add batch dim
          
          output, weights = attention.forward(Q, K, V, mask=causal_mask, training=False)
          
          print(f"Batch size: {batch_size}")
          print(f"Sequence length: {seq_len}")
          print(f"Output shape: {output.shape}")
          print(f"Attention weights shape: {weights.shape}")
          print(f"\nProduction features:")
          print("  ✓ Shape validation")
          print("  ✓ Numerical stability (safe softmax)")
          print("  ✓ Proper masking support")
          print("  ✓ Dropout for regularization")
    
    security_implications:
      floating_point_precision_attacks: |
        Adversaries exploit numerical precision limits:
        - Craft inputs causing overflow/underflow
        - Exploit differences between float16/float32/float64
        - Trigger denormal numbers (slow computation)
        - Defense: Use consistent precision, validate ranges
      
      cache_timing_attacks: |
        KV caching creates timing side-channels:
        - Cache hit vs miss has different latency
        - Can infer if tokens were seen before
        - Adversaries probe cache state through timing
        - Defense: Constant-time caching, cache obfuscation
      
      implementation_bugs: |
        Common attention implementation bugs:
        - Mask not applied correctly (causal leak)
        - Shape broadcasting errors (wrong attention pattern)
        - Softmax stability issues (NaN propagation)
        - Defense: Extensive testing, formal verification, use tested libraries

key_takeaways:
  critical_concepts:
    - concept: "Scaling by √d_k prevents softmax saturation for large dimensions"
      why_it_matters: "Without scaling, gradients vanish and training fails"
    
    - concept: "Matrix formulation Attention(Q,K,V) = softmax(QK^T/√d_k)V enables parallel computation"
      why_it_matters: "100x+ speedup using GPU matrix operations"
    
    - concept: "Attention complexity is O(n²d) - quadratic in sequence length"
      why_it_matters: "Limits context window size, creates DoS vulnerability"
    
    - concept: "Attention masks control which positions can attend to which"
      why_it_matters: "Critical for autoregressive models (causal mask) and padding"
    
    - concept: "Numerical stability requires careful implementation (stable softmax, precision)"
      why_it_matters: "Overflow/underflow causes failures, security vulnerabilities"
  
  actionable_steps:
    - step: "Implement scaled dot-product attention with √d_k scaling"
      verification: "Demonstrate softmax saturation without scaling, stability with scaling"
    
    - step: "Implement efficient matrix formulation for batched computation"
      verification: "Process multiple sequences in parallel using matrix ops"
    
    - step: "Create padding and causal masks"
      verification: "Verify masked positions get zero attention weight"
    
    - step: "Analyze computational complexity and memory usage"
      verification: "Show O(n²d) scaling, memory requirements for different sequence lengths"
    
    - step: "Implement numerically stable softmax"
      verification: "Handle extreme values without overflow/underflow"
  
  security_principles:
    - principle: "Scaling factor affects numerical stability and gradient flow"
      application: "Monitor attention score distributions, detect saturation"
    
    - principle: "Quadratic complexity creates DoS vulnerability"
      application: "Limit input length, rate limit, timeout long computations"
    
    - principle: "Attention masks enforce security boundaries"
      application: "Verify causal mask prevents future leakage, test thoroughly"
    
    - principle: "Matrix operations expose timing and memory side-channels"
      application: "Use constant-time implementations, secure computing environments"
    
    - principle: "Numerical precision affects security (overflow, underflow, precision)"
      application: "Validate input ranges, use consistent precision, test edge cases"
  
  common_mistakes:
    - mistake: "Forgetting to scale by √d_k"
      fix: "Always divide scores by √d_k before softmax"
    
    - mistake: "Not using stable softmax (subtracting max)"
      fix: "exp(x - max(x)) / sum(exp(x - max(x)))"
    
    - mistake: "Incorrect mask application (masking after softmax)"
      fix: "Apply mask BEFORE softmax by setting scores to -∞"
    
    - mistake: "Wrong mask shape (broadcasting errors)"
      fix: "Carefully check mask dimensions match attention matrix"
    
    - mistake: "Using float16 for large attention matrices"
      fix: "Use float32 or float64 for numerical stability"
  
  integration_with_book:
    from_section_3_6:
      - "Attention mechanism and Query-Key-Value formulation"
      - "Dot product score function (simplest, no parameters)"
      - "Softmax normalization creating attention weights"
    
    from_chapter_2:
      - "Gradient flow and numerical stability"
      - "Matrix operations and computational complexity"
    
    to_next_section:
      - "Section 3.8: Multi-head attention (parallel attention heads)"
      - "Running multiple scaled dot-product attentions in parallel"
      - "Different heads learning different patterns"
  
  looking_ahead:
    next_concepts:
      - "Multi-head attention (h parallel attention heads)"
      - "Head-specific projections (different Q/K/V for each head)"
      - "Concatenation and output projection"
      - "Why multiple heads capture different patterns"
    
    skills_to_build:
      - "Implement multi-head attention layer"
      - "Visualize what different heads learn"
      - "Understand head specialization"
      - "Build complete transformer block"
  
  final_thoughts: |
    Scaled dot-product attention (Vaswani et al., 2017) is the specific attention 
    variant that powers transformers and modern LLMs. The key innovation: dividing 
    dot product scores by √d_k before softmax. This simple scaling factor prevents 
    softmax saturation when dimensions are large (d_k = 512), enabling stable gradient 
    flow and successful training.
    
    The matrix formulation - Attention(Q, K, V) = softmax(QK^T / √d_k)V - enables 
    massively parallel computation on GPUs. Instead of computing attention sequentially, 
    we compute all query-key pairs simultaneously with a single matrix multiplication. 
    This gives 100x+ speedup but has O(n²) complexity that limits context windows.
    
    Attention masking allows fine-grained control: padding masks ignore non-content 
    tokens, causal masks enforce left-to-right processing in autoregressive models 
    (GPT). Masks are implemented by setting scores to -∞ before softmax, ensuring 
    masked positions get zero weight.
    
    From a security perspective: scaling affects numerical stability (adversaries can 
    exploit overflow/underflow), quadratic complexity enables DoS attacks (long inputs 
    overwhelm compute), attention masks enforce critical security boundaries (breaking 
    causal mask = catastrophic), and matrix operations expose timing side-channels 
    (cache behavior leaks information).
    
    Next: Section 3.8 introduces multi-head attention - the idea of running multiple 
    scaled dot-product attention operations in parallel, each learning different 
    attention patterns. This is the key to transformers' power and flexibility.

---
