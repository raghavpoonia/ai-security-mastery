# section_01_17_knn.yaml

---
document_info:
  chapter: "01"
  section: "17"
  title: "k-Nearest Neighbors and Distance Metrics"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 6
  tags: ["knn", "distance-metrics", "euclidean", "manhattan", "cosine-similarity", "lazy-learning", "similarity"]

# ============================================================================
# SECTION 1.17: K-NEAREST NEIGHBORS AND DISTANCE METRICS
# ============================================================================

section_01_17_knn:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    k-Nearest Neighbors (k-NN) is the simplest machine learning algorithm: to classify 
    a new sample, find the k most similar samples in your training data and vote. No 
    training phase, no learned parameters, just pure similarity-based prediction. 
    "You are the average of your k nearest neighbors."
    
    Despite its simplicity, k-NN is powerful and widely used in security for:
    - Anomaly detection (find nearest normal samples, flag if far away)
    - Malware similarity analysis (find similar known malware)
    - Network intrusion detection (traffic patterns similar to attacks?)
    - Behavioral analysis (user actions similar to fraudsters?)
    
    The algorithm's effectiveness depends entirely on the distance metric - how you 
    measure similarity between samples. Different metrics capture different notions of 
    similarity: Euclidean (straight-line distance), Manhattan (grid distance), Cosine 
    (angle-based), Hamming (bit differences). Choosing the right metric is critical.
    
    This section covers:
    - k-NN algorithm mechanics (classification and regression)
    - Distance metrics (Euclidean, Manhattan, Cosine, Hamming, Minkowski)
    - Choosing k (bias-variance tradeoff)
    - Weighted voting and distance-based weighting
    - Advantages, limitations, and optimization techniques
  
  why_this_matters: |
    Security applications:
    - Malware clustering: "This sample similar to WannaCry family"
    - Anomaly detection: "User behavior dissimilar to all normal users → investigate"
    - Zero-day detection: "Never seen this exact attack, but similar to known attacks"
    - Threat intelligence: Group attacks by similarity for attribution
    
    Real deployments:
    - Antivirus engines use similarity for unknown malware
    - SIEM systems detect anomalous patterns via k-NN
    - Fraud detection finds transactions similar to known fraud
    
    Why learn k-NN:
    - Foundation for anomaly detection (critical security technique)
    - Distance metrics apply to ALL similarity-based methods
    - Simple but effective baseline
    - Interpretable (can show nearest neighbors as evidence)
  
  # --------------------------------------------------------------------------
  # Core Concept 1: k-NN Algorithm
  # --------------------------------------------------------------------------
  
  knn_algorithm:
    
    basic_idea: |
      To classify new sample x:
      1. Compute distance from x to all training samples
      2. Find k nearest neighbors (smallest distances)
      3. Classification: Majority vote among k neighbors
      4. Regression: Average value of k neighbors
    
    example_classification: |
      Training data: Network packets labeled "normal" or "attack"
      New packet arrives: [packet_size=1500, port=80, protocol=TCP, ...]
      
      Find k=5 nearest packets:
      1. Packet A (distance=0.2): attack
      2. Packet B (distance=0.3): attack
      3. Packet C (distance=0.4): attack
      4. Packet D (distance=0.5): normal
      5. Packet E (distance=0.6): normal
      
      Vote: 3 attack, 2 normal
      Prediction: ATTACK (majority)
    
    no_training_phase: |
      k-NN is "lazy learning" or "instance-based learning"
      
      Training: Just store all training samples (no model fitting!)
      Prediction: Do all work at inference time (compute distances)
      
      Contrast with logistic regression:
      - Training: Learn weights via gradient descent (expensive)
      - Prediction: w·x + b (fast, just dot product)
      
      k-NN:
      - Training: None (just store data)
      - Prediction: Compute n distances (expensive if n large)
    
    algorithm_pseudocode: |
      function kNN_predict(X_train, y_train, x_test, k):
          # Compute distances to all training samples
          distances = []
          for i in range(len(X_train)):
              dist = distance(x_test, X_train[i])
              distances.append((dist, y_train[i]))
          
          # Sort by distance
          distances.sort(key=lambda x: x[0])
          
          # Get k nearest neighbors
          k_nearest = distances[:k]
          
          # Majority vote (classification)
          labels = [label for (dist, label) in k_nearest]
          prediction = most_common(labels)
          
          return prediction
    
    choosing_k:
      
      small_k:
        k_equals_1: |
          Prediction = label of single nearest neighbor
          
          Very flexible (low bias)
          But noisy (high variance)
          Overfits easily
        
        example: |
          k=1, outlier near decision boundary
          → Prediction determined by single noisy sample
      
      large_k:
        k_equals_n: |
          Prediction = majority class in entire dataset
          (always predicts most common class)
          
          Very smooth (high bias)
          But inflexible (low variance)
          Underfits
        
        example: |
          k=1000 in dataset with 70% normal, 30% attack
          → Almost always predicts "normal" (majority)
      
      optimal_k:
        rule_of_thumb: "k = sqrt(n) where n = number of training samples"
        
        typical_range: "3-20 for most problems"
        
        odd_k: "Use odd k for binary classification (prevents ties)"
        
        tune_on_validation: |
          Try k in [1, 3, 5, 7, 9, 11, 15, 20, 30, 50]
          Pick k with best validation accuracy
      
      bias_variance_tradeoff: |
        Small k: High variance, low bias (overfits)
        Large k: Low variance, high bias (underfits)
        
        Optimal k: Balance variance and bias
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Distance Metrics
  # --------------------------------------------------------------------------
  
  distance_metrics:
    
    why_distance_matters: |
      k-NN's entire behavior determined by distance metric
      Different metrics → different neighbors → different predictions
      
      Critical question: What does "similar" mean for your problem?
    
    euclidean_distance:
      
      formula: "d(x, y) = √(Σᵢ (xᵢ - yᵢ)²)"
      
      intuition: "Straight-line distance ('as the crow flies')"
      
      properties:
        - "Most common distance metric"
        - "Sensitive to feature scale (must normalize!)"
        - "Sensitive to outliers (squared differences)"
      
      numpy_implementation: |
        def euclidean_distance(x1, x2):
            """Compute Euclidean distance between two vectors"""
            return np.sqrt(np.sum((x1 - x2) ** 2))
        
        # Vectorized for all training samples
        def euclidean_distances(x, X_train):
            """Compute distances from x to all training samples"""
            return np.sqrt(np.sum((X_train - x) ** 2, axis=1))
      
      example: |
        x = [3, 4]
        y = [0, 0]
        
        d = √((3-0)² + (4-0)²)
          = √(9 + 16)
          = √25
          = 5
      
      when_to_use: |
        - Features are continuous and similar scale
        - Geometric distance makes sense
        - General-purpose default choice
    
    manhattan_distance:
      
      formula: "d(x, y) = Σᵢ |xᵢ - yᵢ|"
      
      intuition: "Grid distance (like city blocks, taxi cab distance)"
      
      properties:
        - "Less sensitive to outliers than Euclidean"
        - "Works well in high dimensions"
        - "More robust"
      
      numpy_implementation: |
        def manhattan_distance(x1, x2):
            """Compute Manhattan distance"""
            return np.sum(np.abs(x1 - x2))
      
      example: |
        x = [3, 4]
        y = [0, 0]
        
        d = |3-0| + |4-0|
          = 3 + 4
          = 7
        
        (vs Euclidean = 5)
      
      when_to_use: |
        - Features represent counts or frequencies
        - High-dimensional data
        - Want robustness to outliers
    
    cosine_similarity:
      
      formula: "similarity(x, y) = (x·y) / (||x|| ||y||)"
      
      distance: "d(x, y) = 1 - similarity(x, y)"
      
      intuition: |
        Measures angle between vectors (ignores magnitude)
        
        Parallel vectors: similarity = 1
        Perpendicular: similarity = 0
        Opposite: similarity = -1
      
      properties:
        - "Magnitude-independent (only direction matters)"
        - "Range: [-1, 1] for similarity"
        - "Perfect for text/document similarity"
      
      numpy_implementation: |
        def cosine_similarity(x1, x2):
            """Compute cosine similarity"""
            dot_product = np.dot(x1, x2)
            norm_x1 = np.linalg.norm(x1)
            norm_x2 = np.linalg.norm(x2)
            return dot_product / (norm_x1 * norm_x2 + 1e-10)
        
        def cosine_distance(x1, x2):
            """Compute cosine distance"""
            return 1 - cosine_similarity(x1, x2)
      
      example: |
        x = [3, 4]  (magnitude = 5)
        y = [6, 8]  (magnitude = 10, same direction!)
        
        similarity = (3×6 + 4×8) / (5 × 10)
                   = 50 / 50
                   = 1 (identical direction)
        
        z = [4, -3]  (perpendicular to x)
        similarity = (3×4 + 4×(-3)) / (5 × 5)
                   = 0 (perpendicular)
      
      when_to_use: |
        - Text documents (TF-IDF vectors)
        - Sparse high-dimensional data
        - Magnitude doesn't matter, only pattern
        
        Security example:
        - Malware API call sequences (order/pattern matters, not count)
    
    hamming_distance:
      
      formula: "d(x, y) = count of positions where xᵢ ≠ yᵢ"
      
      intuition: "Number of bit flips needed to transform x into y"
      
      properties:
        - "For binary/categorical data"
        - "Counts mismatches"
      
      numpy_implementation: |
        def hamming_distance(x1, x2):
            """Compute Hamming distance"""
            return np.sum(x1 != x2)
      
      example: |
        x = [1, 0, 1, 1, 0]
        y = [1, 1, 1, 0, 0]
             ✓  ✗  ✓  ✗  ✓
        
        Hamming = 2 (positions 1 and 3 differ)
      
      when_to_use: |
        - Binary features (packet flags, malware signatures)
        - Categorical features
        - DNA/protein sequences
        
        Security example:
        - Malware byte sequences
        - Network packet flags comparison
    
    minkowski_distance:
      
      formula: "d(x, y) = (Σᵢ |xᵢ - yᵢ|ᵖ)^(1/p)"
      
      special_cases:
        p_equals_1: "Manhattan distance"
        p_equals_2: "Euclidean distance"
        p_equals_infinity: "Chebyshev distance (max difference)"
      
      numpy_implementation: |
        def minkowski_distance(x1, x2, p):
            """Compute Minkowski distance"""
            return np.sum(np.abs(x1 - x2) ** p) ** (1/p)
      
      generalization: "Minkowski generalizes Manhattan and Euclidean"
    
    choosing_distance_metric: |
      Decision tree:
      
      1. Binary/categorical features?
         → Hamming distance
      
      2. Text/document vectors?
         → Cosine similarity
      
      3. Need robustness to outliers?
         → Manhattan distance
      
      4. Default/general case?
         → Euclidean distance
      
      5. Experiment and validate!
         → Try multiple, pick best on validation set
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Complete k-NN Implementation
  # --------------------------------------------------------------------------
  
  complete_implementation: |
    import numpy as np
    from collections import Counter
    
    class KNearestNeighbors:
        """k-NN classifier from scratch"""
        
        def __init__(self, k=5, distance_metric='euclidean', weights='uniform'):
            """
            Args:
                k: Number of neighbors
                distance_metric: 'euclidean', 'manhattan', 'cosine'
                weights: 'uniform' or 'distance' (inverse distance weighting)
            """
            self.k = k
            self.distance_metric = distance_metric
            self.weights = weights
            self.X_train = None
            self.y_train = None
        
        def fit(self, X, y):
            """Store training data (no actual training!)"""
            self.X_train = X
            self.y_train = y
            return self
        
        def predict(self, X):
            """Predict class for each sample in X"""
            predictions = [self._predict_single(x) for x in X]
            return np.array(predictions)
        
        def _predict_single(self, x):
            """Predict single sample"""
            # Compute distances to all training samples
            distances = self._compute_distances(x)
            
            # Get indices of k nearest neighbors
            k_indices = np.argsort(distances)[:self.k]
            
            # Get labels of k nearest neighbors
            k_nearest_labels = self.y_train[k_indices]
            
            # Get distances of k nearest neighbors
            k_nearest_distances = distances[k_indices]
            
            # Vote (with or without weighting)
            if self.weights == 'uniform':
                # Simple majority vote
                most_common = Counter(k_nearest_labels).most_common(1)
                return most_common[0][0]
            else:  # distance-weighted
                # Weight by inverse distance
                weights = 1 / (k_nearest_distances + 1e-10)
                
                # Weighted vote
                weighted_votes = {}
                for label, weight in zip(k_nearest_labels, weights):
                    weighted_votes[label] = weighted_votes.get(label, 0) + weight
                
                # Return label with highest weighted vote
                return max(weighted_votes, key=weighted_votes.get)
        
        def _compute_distances(self, x):
            """Compute distances from x to all training samples"""
            if self.distance_metric == 'euclidean':
                return np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))
            
            elif self.distance_metric == 'manhattan':
                return np.sum(np.abs(self.X_train - x), axis=1)
            
            elif self.distance_metric == 'cosine':
                # Cosine distance = 1 - cosine similarity
                dot_products = np.dot(self.X_train, x)
                norm_x = np.linalg.norm(x)
                norms_train = np.linalg.norm(self.X_train, axis=1)
                similarities = dot_products / (norms_train * norm_x + 1e-10)
                return 1 - similarities
            
            else:
                raise ValueError(f"Unknown distance metric: {self.distance_metric}")
        
        def score(self, X, y):
            """Compute accuracy"""
            predictions = self.predict(X)
            return np.mean(predictions == y)
    
    # ========================================================================
    # USAGE EXAMPLE
    # ========================================================================
    
    # Generate synthetic data: malware vs benign
    np.random.seed(42)
    
    # Benign files: low entropy, few API calls
    X_benign = np.random.randn(100, 2) * 0.5 + np.array([2, 2])
    y_benign = np.zeros(100)
    
    # Malicious files: high entropy, many API calls
    X_malicious = np.random.randn(100, 2) * 0.5 + np.array([7, 7])
    y_malicious = np.ones(100)
    
    # Combine
    X = np.vstack([X_benign, X_malicious])
    y = np.hstack([y_benign, y_malicious])
    
    # Shuffle
    indices = np.random.permutation(len(y))
    X, y = X[indices], y[indices]
    
    # Train/test split
    split = int(0.8 * len(y))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    
    # Train k-NN
    knn = KNearestNeighbors(k=5, distance_metric='euclidean', weights='uniform')
    knn.fit(X_train, y_train)
    
    # Evaluate
    train_acc = knn.score(X_train, y_train)
    test_acc = knn.score(X_test, y_test)
    
    print(f"k-NN (k={knn.k}, metric={knn.distance_metric})")
    print(f"Training accuracy: {train_acc:.2%}")
    print(f"Test accuracy: {test_acc:.2%}")
    
    # Try different k values
    print("\nTuning k:")
    for k in [1, 3, 5, 7, 11, 15]:
        knn = KNearestNeighbors(k=k)
        knn.fit(X_train, y_train)
        acc = knn.score(X_test, y_test)
        print(f"k={k}: {acc:.2%}")
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Weighted k-NN
  # --------------------------------------------------------------------------
  
  weighted_knn:
    
    motivation: |
      Problem with uniform voting:
      All k neighbors have equal say, regardless of distance
      
      Example:
      k=5 neighbors:
      - Neighbor 1 (distance=0.1): malicious
      - Neighbor 2 (distance=0.2): malicious  
      - Neighbor 3 (distance=5.0): benign
      - Neighbor 4 (distance=5.1): benign
      - Neighbor 5 (distance=5.2): benign
      
      Uniform vote: 3 benign vs 2 malicious → predict benign
      But the 2 closest neighbors are malicious!
    
    distance_weighted_voting: |
      Weight each neighbor's vote by inverse distance
      
      weight_i = 1 / (distance_i + ε)
      
      Where ε is small constant (avoid division by zero)
      
      Closer neighbors have more influence
    
    example_weighted_vote: |
      Same 5 neighbors:
      
      Neighbor 1 (d=0.1): malicious, weight=10.0
      Neighbor 2 (d=0.2): malicious, weight=5.0
      Neighbor 3 (d=5.0): benign, weight=0.2
      Neighbor 4 (d=5.1): benign, weight=0.196
      Neighbor 5 (d=5.2): benign, weight=0.192
      
      Malicious vote: 10.0 + 5.0 = 15.0
      Benign vote: 0.2 + 0.196 + 0.192 = 0.588
      
      Prediction: Malicious (weighted vote wins!)
    
    when_to_use_weighting: |
      Use distance weighting when:
      - Close neighbors more reliable than distant ones
      - k is large (includes very distant neighbors)
      - Decision boundary complex
      
      Use uniform voting when:
      - All neighbors roughly equidistant
      - k is small
      - Want simpler, more stable predictions
  
  # --------------------------------------------------------------------------
  # Advantages and Limitations
  # --------------------------------------------------------------------------
  
  advantages_limitations:
    
    advantages:
      simplicity:
        - "Trivial to implement and understand"
        - "No training phase"
        - "No assumptions about data distribution"
      
      versatility:
        - "Works for classification and regression"
        - "Handles multi-class naturally"
        - "Can use any distance metric"
      
      adaptability:
        - "Automatically adapts to new data (just add to training set)"
        - "No retraining needed"
      
      interpretability:
        - "Can show k nearest neighbors as evidence"
        - "Decision explainable: 'Similar to these known samples'"
    
    limitations:
      
      computational_cost:
        problem: "Must compute distance to ALL training samples for each prediction"
        
        complexity: "O(n × d) per prediction, where n=training samples, d=dimensions"
        
        impact: |
          1M training samples, 100 dimensions
          Each prediction: 100M distance calculations
          
          Too slow for real-time at scale
        
        solutions:
          - "KD-trees, Ball trees (space partitioning)"
          - "Approximate nearest neighbors (LSH)"
          - "Dimensionality reduction (PCA)"
      
      memory_cost:
        problem: "Must store ALL training data"
        
        impact: |
          1M samples × 1000 features × 4 bytes = 4GB RAM
          vs logistic regression: 1000 weights = 4KB
        
        solution: "Prototype selection (keep representative samples only)"
      
      curse_of_dimensionality:
        problem: "In high dimensions, all points roughly equidistant"
        
        example: |
          100-dimensional space:
          Nearest neighbor distance ≈ 0.95 × farthest neighbor distance
          
          k-NN breaks down (no meaningful 'nearest' neighbors)
        
        solution: |
          - Feature selection (reduce dimensions)
          - PCA (dimensionality reduction)
          - Use distance metrics robust to high dimensions
      
      sensitive_to_irrelevant_features:
        problem: "All features contribute equally to distance"
        
        example: |
          Malware detection features:
          - Entropy (important)
          - File size (important)
          - Filename length (irrelevant!)
          
          Filename length pollutes distance calculation
        
        solution: |
          - Feature selection
          - Feature weighting
          - Dimensionality reduction
      
      imbalanced_data:
        problem: "Majority class dominates k nearest neighbors"
        
        example: |
          Dataset: 95% benign, 5% malicious
          k=10 nearest neighbors likely all benign
          Never predicts malicious!
        
        solution: |
          - Use smaller k
          - Distance-weighted voting
          - Stratified sampling
          - SMOTE (generate synthetic minority samples)
  
  # --------------------------------------------------------------------------
  # Security Applications
  # --------------------------------------------------------------------------
  
  security_applications:
    
    anomaly_detection:
      
      concept: |
        Normal samples cluster together
        Anomalies far from all normal samples
      
      algorithm: |
        1. Train k-NN on normal samples only
        2. For new sample, find k nearest normal samples
        3. If average distance > threshold → anomaly
      
      example: |
        User behavior monitoring:
        - Typical user: Login 9am, access docs, logout 5pm
        - Anomaly: Login 3am, access database, massive download
        
        k-NN finds no similar normal users → flag as anomaly
      
      threshold_selection: |
        Set threshold based on validation anomalies
        Trade-off: Low threshold = many false positives
                  High threshold = miss anomalies
    
    malware_family_classification:
      
      approach: |
        Extract features from malware (API calls, byte n-grams, etc.)
        Find k nearest known malware samples
        Classify into family based on neighbors
      
      distance_metric: |
        Often use cosine similarity (API call frequency vectors)
        Or Hamming distance (byte sequences)
      
      benefit: |
        Can classify variants of known families
        Even if exact signature unseen
    
    behavioral_analysis:
      
      network_traffic: |
        Features: Packet size distribution, timing, protocols
        Find k nearest traffic patterns
        If similar to attack patterns → flag
      
      user_behavior: |
        Features: Actions, timing, locations
        Find k nearest user sessions
        If dissimilar from user's past → potential account takeover
  
  # --------------------------------------------------------------------------
  # Optimization Techniques
  # --------------------------------------------------------------------------
  
  optimization:
    
    kd_trees:
      concept: |
        Space partitioning data structure
        Recursively split space by median on alternating dimensions
        
        Query time: O(log n) average (vs O(n) brute force)
      
      limitation: "Degrades to O(n) in high dimensions (d > 20)"
      
      when_to_use: "Low-dimensional data (d < 20), exact neighbors needed"
    
    approximate_nearest_neighbors:
      
      locality_sensitive_hashing:
        concept: |
          Hash similar items to same buckets
          Search only within bucket (not all data)
        
        trade_off: "Fast but approximate (may miss true nearest neighbors)"
        
        when_to_use: "High-dimensional data, can tolerate approximation"
      
      annoy_faiss:
        libraries: |
          Annoy (Spotify): Tree-based ANN
          FAISS (Facebook): GPU-accelerated similarity search
        
        performance: "1M samples in milliseconds"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "k-NN: Classify based on k nearest training samples"
      - "No training phase (lazy learning)"
      - "Distance metric critical (Euclidean, Manhattan, Cosine, Hamming)"
      - "Choose k: Small k overfits, large k underfits"
      - "Weighted voting: Closer neighbors more influential"
    
    practical_skills:
      - "Implement k-NN from scratch with multiple distance metrics"
      - "Tune k on validation set (try 1, 3, 5, 7, 11, 15)"
      - "Apply distance weighting for better predictions"
      - "Choose appropriate distance metric for problem"
      - "Use k-NN for anomaly detection"
    
    security_mindset:
      - "k-NN natural for similarity-based detection"
      - "Anomaly detection: Flag samples far from normal cluster"
      - "Malware family classification: Find similar known malware"
      - "Interpretable: Show nearest neighbors as evidence"
      - "Beware curse of dimensionality in high-dimensional security data"
    
    remember_this:
      - "k-NN = lazy learning (no training, store all data)"
      - "Distance metric choice as important as algorithm"
      - "Tune k on validation set (typically 3-15)"
      - "Slow at scale (O(n) per prediction) - use approximate methods"
    
    next_steps:
      - "Next section: Support Vector Machines (maximize margin)"
      - "You now understand similarity-based classification"
      - "Distance metrics apply to clustering, dimensionality reduction"

---
