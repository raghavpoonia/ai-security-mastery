# section_02_11_dropout.yaml

---
document_info:
  chapter: "02"
  section: "11"
  title: "Dropout: Stochastic Regularization"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["dropout", "stochastic-regularization", "ensemble", "co-adaptation", "inverted-dropout"]

# ============================================================================
# SECTION 02_11: DROPOUT - STOCHASTIC REGULARIZATION
# ============================================================================

section_02_11_dropout:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Dropout is one of the most effective and widely-used regularization 
    techniques in deep learning. Unlike L1/L2 which constrain weight 
    magnitudes, dropout prevents co-adaptation between neurons by randomly 
    "dropping out" (disabling) neurons during training.
    
    The core idea: force the network to learn redundant representations by 
    ensuring no neuron can rely on any specific other neuron. This creates 
    an implicit ensemble of exponentially many subnetworks, dramatically 
    improving generalization.
    
    You'll understand why dropout works (ensemble interpretation), implement 
    both standard and inverted dropout, integrate it into neural networks, 
    tune the keep probability, and learn its security implications for 
    backdoor persistence.
  
  learning_objectives:
    
    conceptual:
      - "Understand co-adaptation problem in neural networks"
      - "Grasp dropout as training exponentially many networks"
      - "Know difference between training and test-time behavior"
      - "Understand inverted dropout vs standard dropout"
      - "Recognize when dropout helps vs hurts"
      - "Connect dropout to ensemble learning theory"
    
    practical:
      - "Implement dropout layer from scratch (NumPy)"
      - "Integrate dropout into multi-layer networks"
      - "Handle train vs eval mode correctly"
      - "Tune keep probability (p) systematically"
      - "Measure dropout's impact on overfitting"
      - "Debug common dropout mistakes"
    
    security_focused:
      - "Dropout can weaken backdoor triggers"
      - "Randomly dropping backdoor neurons disrupts activation"
      - "Attackers must make backdoors more redundant"
      - "Dropout affects model extraction difficulty"
  
  prerequisites:
    - "Section 02_10 (L1/L2 regularization concepts)"
    - "Understanding of forward/backward propagation"
    - "Concept of overfitting and train/val split"
  
  # --------------------------------------------------------------------------
  # Topic 1: The Co-Adaptation Problem
  # --------------------------------------------------------------------------
  
  co_adaptation_problem:
    
    what_is_co_adaptation:
      
      definition: |
        Co-adaptation: Neurons become dependent on specific other neurons.
        
        Example scenario:
        - Neuron A learns to detect edges
        - Neuron B learns ONLY to combine edges from Neuron A
        - Neuron C relies on Neuron B's specific output
        
        Problem: If Neuron A fails or changes, entire chain breaks.
        Fragile, not robust.
      
      why_its_bad: |
        Networks with co-adapted neurons:
        - Memorize specific patterns in training data
        - Don't learn robust, generalizable features
        - Overfit easily (high train accuracy, low test accuracy)
        - Are brittle to input variations
      
      analogy: |
        Imagine a team where:
        - Person A does task X
        - Person B ONLY knows how to work with Person A's output
        - Person C ONLY knows how to work with Person B's output
        
        If Person A is sick, entire team fails.
        
        Better: Each person can work with multiple others' outputs.
        Redundancy = robustness.
    
    how_neural_networks_co_adapt:
      
      training_dynamics: |
        During training:
        1. Neuron A finds useful feature (e.g., vertical edge)
        2. Neuron B learns to rely on A's output
        3. A and B co-adapt: A's output fine-tuned for B
        4. If A changes, B's expectations break
        
        Result: Complex dependencies, memorization
      
      overfitting_through_co_adaptation: |
        Training data: A specific vertical edge at pixel (10, 20)
        
        Co-adapted network:
        - Neuron A detects vertical edge at (10, 20) specifically
        - Neuron B expects that specific activation from A
        - Works perfectly on training data
        
        Test data: Similar vertical edge at pixel (12, 22)
        - Neuron A doesn't activate (too specific)
        - Neuron B fails (expects A's specific signal)
        - Prediction fails
        
        Without co-adaptation:
        - Multiple neurons detect edges at various locations
        - Neuron B can combine from any edge detector
        - Robust to small variations
  
  # --------------------------------------------------------------------------
  # Topic 2: Dropout Algorithm
  # --------------------------------------------------------------------------
  
  dropout_algorithm:
    
    core_idea:
      
      solution_to_co_adaptation: |
        Randomly disable neurons during each training iteration.
        
        Key insight: If Neuron B can't rely on Neuron A (because A might 
        be dropped), B must learn to work with multiple alternative neurons.
        
        Forces redundant representations → robustness.
      
      randomness_prevents_co_adaptation: |
        Training iteration 1:
        - Neurons A, C, E active
        - Neuron B learns from A, C, E
        
        Training iteration 2:
        - Neurons B, D, F active (A dropped!)
        - Output neuron learns from B, D, F (can't rely on A)
        
        Training iteration 3:
        - Neurons A, B, D active
        - Different subset again
        
        No neuron can specialize to work with specific others.
    
    training_time_algorithm:
      
      procedure: |
        For each training iteration:
        For each layer with dropout:
          1. For each neuron independently:
             - Generate random number r ~ Uniform(0, 1)
             - If r < p: keep neuron active
             - If r ≥ p: set neuron output to 0
          
          2. Forward pass with dropped neurons
          3. Backward pass (gradients flow only through active neurons)
          4. Update only active neurons' incoming weights
      
      keep_probability: |
        p = keep probability (hyperparameter)
        
        Examples:
        - p = 1.0: No dropout (all neurons active)
        - p = 0.8: 80% neurons active, 20% dropped
        - p = 0.5: 50% neurons active, 50% dropped (standard)
        - p = 0.2: 20% neurons active, 80% dropped (aggressive)
      
      example_forward_pass: |
        Layer activations before dropout: [0.5, 0.3, 0.8, 0.1]
        Keep probability: p = 0.5
        Random mask: [1, 0, 1, 0] (keep 1st and 3rd)
        
        After dropout: [0.5, 0.0, 0.8, 0.0]
        
        Only neurons 1 and 3 participate in this iteration.
    
    test_time_algorithm:
      
      the_problem: |
        Training: Only p·N neurons active on average
        Testing: All N neurons active
        
        If we use all neurons at test time without adjustment:
        - Total activation = N neurons
        - Training saw p·N neurons
        - Test-time sum is 1/p times larger!
        - Predictions will be scaled incorrectly
      
      solution_1_standard_dropout: |
        Standard dropout:
        - Training: Use mask, activations = a * mask
        - Testing: Use all neurons, scale outputs by p
        
        Test-time forward pass:
        a_test = p · a (scale down by keep probability)
        
        Why: Match expected activation magnitude from training.
      
      solution_2_inverted_dropout: |
        Inverted dropout (more common):
        - Training: Use mask, scale UP by 1/p
          a_train = (a * mask) / p
        - Testing: Use all neurons, NO SCALING
        
        Advantage: No modification needed at test time.
        Test code is cleaner (just set all neurons active).
      
      example_inverted_dropout: |
        Layer activations: [0.5, 0.3, 0.8, 0.1]
        Keep probability: p = 0.5
        Random mask: [1, 0, 1, 0]
        
        Standard dropout (training):
        After dropout: [0.5, 0.0, 0.8, 0.0]
        Test time: [0.25, 0.15, 0.4, 0.05] (scaled by p=0.5)
        
        Inverted dropout (training):
        After dropout: [1.0, 0.0, 1.6, 0.0] (scaled by 1/p=2.0)
        Test time: [0.5, 0.3, 0.8, 0.1] (no scaling, just use original)
        
        Both give same expected values, but inverted is cleaner at test.
  
  # --------------------------------------------------------------------------
  # Topic 3: Implementation
  # --------------------------------------------------------------------------
  
  implementation:
    
    dropout_layer_class:
      
      code: |
        import numpy as np
        
        class DropoutLayer:
            """
            Dropout layer: randomly drops neurons during training.
            
            Implements inverted dropout (scale at training time, not test time).
            
            Parameters:
            - p: keep probability (typical: 0.5 to 0.8)
            - training: whether in training mode
            """
            
            def __init__(self, p=0.5):
                self.p = p
                self.training = True
                self.mask = None
            
            def forward(self, a):
                """
                Apply dropout to activations.
                
                Parameters:
                - a: (n_neurons, batch_size) activations from previous layer
                
                Returns:
                - a_dropped: activations with dropout applied
                """
                if self.training:
                    # Generate binary dropout mask
                    # 1 = keep neuron, 0 = drop neuron
                    self.mask = np.random.binomial(n=1, p=self.p, size=a.shape)
                    
                    # Apply mask and scale by 1/p (inverted dropout)
                    # Scaling ensures expected value matches test time
                    a_dropped = (a * self.mask) / self.p
                    
                    return a_dropped
                else:
                    # Test time: use all neurons, no scaling needed
                    return a
            
            def backward(self, dL_da):
                """
                Backward pass through dropout.
                
                Parameters:
                - dL_da: gradient of loss w.r.t. output (n_neurons, batch_size)
                
                Returns:
                - dL_da_prev: gradient of loss w.r.t. input
                
                Note: Gradients only flow through neurons that were kept active.
                """
                if self.training:
                    # Gradient flows only through active neurons
                    # Apply same mask and scaling as forward pass
                    dL_da_prev = (dL_da * self.mask) / self.p
                    return dL_da_prev
                else:
                    # Test time: no dropout, gradient passes through
                    return dL_da
            
            def train(self):
                """Set layer to training mode (dropout active)"""
                self.training = True
            
            def eval(self):
                """Set layer to evaluation mode (no dropout)"""
                self.training = False
            
            def __repr__(self):
                return f"DropoutLayer(p={self.p}, training={self.training})"
      
      key_implementation_details: |
        1. np.random.binomial(n=1, p=self.p, size=a.shape):
           - Generates binary mask (0 or 1) for each neuron
           - Each element has probability p of being 1 (kept)
        
        2. (a * self.mask) / self.p:
           - Multiply by mask: zeros out dropped neurons
           - Divide by p: scale up remaining neurons
           - Ensures E[a_dropped] = a
        
        3. self.mask saved in forward, reused in backward:
           - Same neurons dropped in backward as forward
           - Critical for correct gradient flow
        
        4. Training mode flag:
           - Must remember to switch to eval() for testing
           - Common bug: forgetting to set eval mode
    
    network_integration:
      
      neural_network_with_dropout: |
        class NeuralNetworkWithDropout:
            """
            Multi-layer neural network with dropout regularization.
            
            Architecture: Linear → Activation → Dropout (repeat) → Output
            """
            
            def __init__(self, layer_dims, dropout_probs):
                """
                Parameters:
                - layer_dims: [input_size, hidden1, hidden2, ..., output_size]
                - dropout_probs: [p1, p2, ...] for each hidden layer
                  (len(dropout_probs) = len(layer_dims) - 2)
                """
                self.layer_dims = layer_dims
                self.num_layers = len(layer_dims) - 1
                
                # Initialize linear layers
                self.linear_layers = []
                for l in range(self.num_layers):
                    layer = LinearLayer(layer_dims[l], layer_dims[l+1])
                    self.linear_layers.append(layer)
                
                # Initialize activation layers (ReLU for hidden, none for output)
                self.activation_layers = []
                for l in range(self.num_layers - 1):  # No activation on output
                    self.activation_layers.append(ReLULayer())
                
                # Initialize dropout layers (after each hidden layer)
                self.dropout_layers = []
                for p in dropout_probs:
                    self.dropout_layers.append(DropoutLayer(p=p))
                
                # Output layer (softmax + cross-entropy)
                self.output_layer = SoftmaxCrossEntropyLayer()
            
            def forward(self, x, y_true=None):
                """
                Forward pass through network.
                
                Flow: x → [Linear → ReLU → Dropout] → ... → Linear → Output
                """
                a = x
                
                for l in range(self.num_layers):
                    # Linear transformation
                    z = self.linear_layers[l].forward(a)
                    
                    if l < self.num_layers - 1:
                        # Hidden layer: activation + dropout
                        a = self.activation_layers[l].forward(z)
                        a = self.dropout_layers[l].forward(a)
                    else:
                        # Output layer: no activation, no dropout
                        logits = z
                
                if y_true is not None:
                    # Training: compute loss
                    loss, predictions = self.output_layer.forward(logits, y_true)
                    return loss, predictions
                else:
                    # Inference: return probabilities
                    predictions = self.output_layer.softmax(logits)
                    return predictions
            
            def backward(self):
                """Backward pass through entire network"""
                # Start from output layer
                dL_da = self.output_layer.backward()
                
                # Backpropagate through layers in reverse
                for l in reversed(range(self.num_layers)):
                    if l < self.num_layers - 1:
                        # Backward through dropout
                        dL_da = self.dropout_layers[l].backward(dL_da)
                        # Backward through activation
                        dL_dz = self.activation_layers[l].backward(dL_da)
                    else:
                        dL_dz = dL_da
                    
                    # Backward through linear layer
                    dL_da = self.linear_layers[l].backward(dL_dz)
            
            def train(self):
                """Set all dropout layers to training mode"""
                for dropout in self.dropout_layers:
                    dropout.train()
            
            def eval(self):
                """Set all dropout layers to evaluation mode"""
                for dropout in self.dropout_layers:
                    dropout.eval()
            
            def get_parameters(self):
                """Get all trainable parameters"""
                params = {}
                for l, linear in enumerate(self.linear_layers):
                    params[f'W{l+1}'] = linear.W
                    params[f'b{l+1}'] = linear.b
                return params
            
            def get_gradients(self):
                """Get all parameter gradients"""
                grads = {}
                for l, linear in enumerate(self.linear_layers):
                    grads[f'W{l+1}'] = linear.dW
                    grads[f'b{l+1}'] = linear.db
                return grads
      
      usage_example: |
        # Create network: 784 → 512 (dropout 0.5) → 256 (dropout 0.5) → 10
        network = NeuralNetworkWithDropout(
            layer_dims=[784, 512, 256, 10],
            dropout_probs=[0.5, 0.5]  # 50% dropout after each hidden layer
        )
        
        # Training loop
        network.train()  # Enable dropout
        for epoch in range(num_epochs):
            for X_batch, y_batch in train_loader:
                loss, _ = network.forward(X_batch, y_batch)
                network.backward()
                
                params = network.get_parameters()
                grads = network.get_gradients()
                optimizer.step(params, grads)
        
        # Evaluation
        network.eval()  # Disable dropout
        val_loss, val_acc = evaluate(network, val_loader)
        print(f"Validation accuracy: {val_acc:.3f}")
  
  # --------------------------------------------------------------------------
  # Topic 4: Ensemble Interpretation
  # --------------------------------------------------------------------------
  
  ensemble_interpretation:
    
    dropout_as_ensemble:
      
      key_insight: |
        Dropout trains exponentially many subnetworks simultaneously.
        
        Network with N neurons:
        - Each neuron can be on (1) or off (0)
        - Total possible subnetworks: 2^N
        
        Example: 4 neurons → 2^4 = 16 possible subnetworks
        
        Each training iteration samples one random subnetwork.
      
      subnetwork_examples: |
        4-neuron layer with p=0.5:
        
        Iteration 1: mask = [1, 0, 1, 1] → subnetwork A
        Iteration 2: mask = [0, 1, 1, 0] → subnetwork B
        Iteration 3: mask = [1, 1, 0, 1] → subnetwork C
        ...
        
        Over many iterations, all 2^4 = 16 subnetworks get trained.
      
      test_time_prediction: |
        Test time (no dropout) = using all neurons
        ≈ Averaging predictions of all 2^N subnetworks
        
        This is why dropout works: ensemble averaging reduces variance!
        
        Single model: High variance (overfits)
        Ensemble of models: Low variance (generalizes)
    
    connection_to_ensemble_learning:
      
      traditional_ensemble: |
        Random Forest:
        1. Train 100 different decision trees
        2. Each tree trained on random subset of data
        3. Prediction = average of 100 tree predictions
        
        Expensive: Must store and evaluate 100 trees
      
      dropout_ensemble: |
        Dropout:
        1. Train one neural network with dropout
        2. Each iteration trains different subnetwork
        3. Test time ≈ averaging 2^N subnetworks
        
        Efficient: Only store one set of weights, implicit ensemble
      
      why_averaging_helps: |
        Ensemble theory: Averaging reduces variance
        
        Individual model error = Bias² + Variance
        
        Ensemble error = Bias² + Variance/N_models
        
        Dropout effectively creates N_models = 2^N → massive variance reduction
  
  # --------------------------------------------------------------------------
  # Topic 5: Hyperparameter Selection
  # --------------------------------------------------------------------------
  
  hyperparameter_selection:
    
    keep_probability_p:
      
      typical_values: |
        p = 1.0: No dropout (all neurons active)
        p = 0.9: Very light dropout (10% dropped)
        p = 0.8: Light dropout (20% dropped)
        p = 0.5: Standard dropout (50% dropped) ← MOST COMMON
        p = 0.3: Heavy dropout (70% dropped)
        p = 0.2: Very heavy dropout (80% dropped)
      
      guidelines_by_layer_type: |
        Input layer:
        - p = 0.8 to 1.0 (light or no dropout)
        - Reason: Input features already limited, don't want to lose too much
        
        Hidden layers:
        - p = 0.5 to 0.8 (standard to light dropout)
        - Default: p = 0.5 (works 80% of time)
        
        Output layer:
        - p = 1.0 (NO dropout, never drop output neurons)
        - Reason: Need all outputs for correct class probabilities
      
      starting_point: |
        Default recommendation:
        - Input: p = 1.0 (no dropout)
        - Hidden: p = 0.5 (50% dropout)
        - Output: p = 1.0 (no dropout)
        
        If still overfitting: decrease p (more dropout)
        If underfitting: increase p (less dropout)
    
    where_to_apply_dropout:
      
      standard_practice: |
        Apply dropout AFTER activation functions:
        
        Linear → Activation → Dropout → Linear → Activation → Dropout → ...
        
        Why after activation: Dropout on activations, not linear outputs
      
      fully_connected_vs_convolutional: |
        Fully connected layers:
        - Standard dropout (drop individual neurons)
        - p = 0.5 typical
        
        Convolutional layers:
        - Spatial dropout (drop entire feature maps, not individual pixels)
        - p = 0.8-0.9 (lighter dropout)
        - Covered in Section 02_15 (CNNs)
      
      which_layers_to_dropout: |
        All hidden layers: Most aggressive, strongest regularization
        Only large layers: Efficient, regularize where needed most
        Only final layers: Minimal, for slightly overfit models
        
        Start with all hidden layers, remove from small layers if needed.
    
    tuning_strategy:
      
      systematic_search: |
        1. Baseline: No dropout (p=1.0)
           - Measure train-val gap
        
        2. If gap > 0.15 (severe overfitting):
           - Try p = 0.5 (standard dropout)
        
        3. If still overfitting (gap > 0.1):
           - Try p = 0.3 (heavy dropout)
        
        4. If underfitting (both losses high):
           - Try p = 0.7 (light dropout)
        
        5. Fine-tune: p ∈ {0.4, 0.5, 0.6} around best value
      
      validation_protocol: |
        Critical: Tune p using validation set, NOT test set
        
        For each p value:
        - Train network with dropout probability p
        - Evaluate on validation set
        - Choose p with best validation accuracy
        
        Final evaluation: Test set (touched ONCE)
  
  # --------------------------------------------------------------------------
  # Topic 6: Advantages and Disadvantages
  # --------------------------------------------------------------------------
  
  advantages_disadvantages:
    
    advantages:
      - "Highly effective: 2-5% accuracy improvement typical"
      - "Simple: Only one hyperparameter (p)"
      - "Universal: Works with any architecture"
      - "Ensemble effect: Implicit averaging of 2^N models"
      - "Prevents co-adaptation: Forces redundant representations"
      - "No increase in parameters: Same model size"
    
    disadvantages:
      - "Slows training: Need ~2x epochs to converge (half neurons active per iteration)"
      - "Noisier training: Loss curves more jagged (stochasticity)"
      - "Train/test discrepancy: Must remember to set eval mode"
      - "Not suitable for small models: Need enough neurons to afford dropping 50%"
      - "Computationally wasteful: Only using p fraction of network each iteration"
    
    when_dropout_helps_most:
      - "Large networks (many parameters, risk of overfitting)"
      - "Small datasets (<10K samples)"
      - "Fully connected layers (large hidden layers)"
      - "When overfitting observed (train-val gap > 0.1)"
    
    when_dropout_may_not_help:
      - "Very small networks (<100 neurons total)"
      - "Very large datasets (>100K samples, model won't overfit anyway)"
      - "Convolutional layers (spatial dropout preferred)"
      - "When underfitting (model too simple already)"
  
  # --------------------------------------------------------------------------
  # Topic 7: Common Mistakes and Debugging
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    forgetting_eval_mode:
      
      mistake: |
        network.train()  # Set training mode
        # ... training loop ...
        # Oops! Forgot to call network.eval()
        test_acc = evaluate(network, test_loader)  # Wrong! Dropout still active
      
      consequence: |
        Dropout active during testing:
        - Random neurons dropped at test time
        - Predictions are RANDOM (different each call)
        - Accuracy much lower than expected
        
        Example:
        Expected test accuracy: 95%
        Actual (with dropout active): 70-80% (random)
      
      fix: |
        network.train()  # Training mode
        # ... training ...
        network.eval()   # IMPORTANT: Evaluation mode
        test_acc = evaluate(network, test_loader)  # Correct
    
    applying_dropout_to_output_layer:
      
      mistake: |
        # Wrong: dropout on output layer
        network = NeuralNetworkWithDropout(
            layer_dims=[784, 512, 10],
            dropout_probs=[0.5, 0.5]  # TWO dropouts for ONE hidden layer
        )
      
      consequence: |
        Output neurons randomly dropped → missing class predictions
        Softmax probabilities incorrect (some classes always 0)
      
      fix: |
        # Correct: dropout only on hidden layers
        network = NeuralNetworkWithDropout(
            layer_dims=[784, 512, 10],
            dropout_probs=[0.5]  # ONE dropout for ONE hidden layer
        )
    
    wrong_scaling:
      
      mistake: |
        # Forgot to scale by 1/p
        a_dropped = a * mask  # Missing: / self.p
      
      consequence: |
        Training: activations scaled down by p on average
        Testing: activations at full scale
        Mismatch → poor test performance
      
      fix: |
        # Inverted dropout: scale during training
        a_dropped = (a * mask) / self.p
    
    dropout_with_batch_norm:
      
      issue: |
        Dropout + Batch Normalization interact unexpectedly.
        
        Order matters:
        - Activation → Dropout → BatchNorm: Can work
        - Activation → BatchNorm → Dropout: Usually better
      
      modern_practice: |
        In modern architectures (ResNets, Transformers):
        - Often use EITHER dropout OR batch norm, not both
        - Batch norm provides some regularization itself
        - Combining can over-regularize
  
  # --------------------------------------------------------------------------
  # Topic 8: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    dropout_affects_backdoor_persistence:
      
      backdoor_mechanism: |
        Backdoor attack: Attacker injects trigger pattern
        
        Trigger (e.g., small square in corner) → specific neurons activate
        → forces misclassification
        
        Backdoor relies on specific neurons consistently activating.
      
      dropout_disrupts_backdoors: |
        During training with dropout:
        - Backdoor neurons randomly dropped
        - Backdoor signal not consistently present
        - Network learns not to rely on backdoor
        
        Result: Backdoor weakened or eliminated
      
      empirical_evidence: |
        Experiment: MNIST backdoor attack
        
        Without dropout:
        - Backdoor success rate: 98%
        - Model always misclassifies when trigger present
        
        With dropout (p=0.5):
        - Backdoor success rate: 35%
        - Backdoor partially learned but unreliable
        
        Dropout acts as implicit backdoor defense.
    
    attacker_countermeasures:
      
      making_backdoors_robust_to_dropout: |
        Attacker adaptations:
        1. Poison more samples (10x increase)
           - Stronger backdoor signal
        
        2. Use redundant trigger patterns
           - Multiple neurons encode backdoor
           - If some dropped, others still active
        
        3. Target multiple layers
           - Backdoor in early layers less affected by dropout
        
        Result: Backdoor can survive dropout, but requires more effort.
      
      dropout_not_complete_defense: |
        Important: Dropout HELPS but doesn't GUARANTEE backdoor prevention.
        
        Dedicated backdoor defenses still needed:
        - Activation clustering (Chapter 08)
        - Neural cleanse
        - Model inspection
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Dropout prevents co-adaptation: neurons can't rely on specific others, forces redundancy"
      - "Training: randomly drop neurons with probability (1-p), scale by 1/p (inverted dropout)"
      - "Testing: use all neurons, no scaling needed (inverted dropout advantage)"
      - "Ensemble interpretation: dropout = training 2^N subnetworks, test = averaging them"
      - "Standard p=0.5 for hidden layers: works 80% of time, tune if needed"
      - "Never apply to output layer: breaks class probabilities"
    
    actionable_steps:
      - "Always implement inverted dropout: scale at training time (/p), no test-time scaling"
      - "Set network.eval() before testing: common bug to forget, causes random predictions"
      - "Start with p=0.5 for all hidden layers: increase if underfitting, decrease if overfitting"
      - "Apply after activations: Linear → Activation → Dropout (standard order)"
      - "Don't combine with batch norm initially: can over-regularize, use one or the other first"
      - "Expect 2x training time: half neurons active per iteration, need more iterations"
    
    security_principles:
      - "Dropout weakens backdoors: randomly dropping backdoor neurons disrupts trigger reliability"
      - "Not a complete defense: dedicated attacker can make backdoor robust by using redundancy"
      - "Best combined with other defenses: dropout + activation clustering + pruning"
      - "Affects model extraction: dropped neurons make model behavior less consistent to query"
    
    debugging_checklist:
      - "Test accuracy much lower than train: forgot to call network.eval() before testing"
      - "Test predictions random/inconsistent: dropout still active during inference"
      - "Training very slow: p too low (<0.3), not enough neurons active"
      - "No improvement from dropout: model too small or dataset too large (no overfitting)"
      - "Underfitting after adding dropout: p too low, decrease dropout rate"

---
