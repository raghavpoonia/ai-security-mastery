# section_02_19_production_deployment.yaml

-----

document_info:
chapter: “02”
section: “19”
title: “Production Deployment”
version: “1.0.0”
author: “Raghav Dinesh”
github: “github.com/raghavpoonia”
license: “MIT”
created: “2025-01-16”
estimated_pages: 7
tags: [“deployment”, “production”, “serving”, “optimization”, “monitoring”, “mlops”, “inference”]

# ============================================================================

# SECTION 02_19: PRODUCTION DEPLOYMENT

# ============================================================================

section_02_19_production_deployment:

# –––––––––––––––––––––––––––––––––––––

# Overview and Context

# –––––––––––––––––––––––––––––––––––––

overview: |
Training a model is just the beginning. Production deployment requires
model optimization for inference speed, serving infrastructure for reliable
predictions, monitoring for performance degradation, and security hardening
against attacks. The gap between research accuracy and production reliability
is vast.

```
This section covers model optimization techniques (quantization, pruning,
distillation), serving architectures (batch vs online, edge vs cloud),
monitoring and observability, A/B testing, and security considerations for
deployed models. Essential knowledge for security engineers responsible for
production ML systems.
```

learning_objectives:

```
conceptual:
  - "Understand training vs inference requirements"
  - "Know model optimization techniques"
  - "Grasp serving architecture patterns"
  - "Understand monitoring and observability"
  - "Recognize production security risks"
  - "Connect MLOps to security operations"

practical:
  - "Optimize models for inference"
  - "Implement model serving APIs"
  - "Set up monitoring and alerting"
  - "Deploy models to edge devices"
  - "Perform A/B testing safely"
  - "Build secure inference pipelines"

security_focused:
  - "Model stealing via inference queries"
  - "Input validation critical at inference"
  - "Monitoring detects adversarial attacks"
  - "Model versioning prevents backdoor persistence"
```

prerequisites:
- “Sections 02_12-02_15 (CNNs through transfer learning)”
- “Basic understanding of APIs and web services”

# –––––––––––––––––––––––––––––––––––––

# Topic 1: Model Optimization for Inference

# –––––––––––––––––––––––––––––––––––––

model_optimization:

```
why_optimize:
  
  training_vs_inference:
    training: |
      Training:
      - Throughput: batches/second (can batch large)
      - Latency: minutes/hours acceptable
      - Compute: GPU clusters available
      - Memory: 16-80 GB GPU memory
      - Precision: FP32 or FP16
    
    inference: |
      Inference:
      - Throughput: requests/second (often single samples)
      - Latency: <100ms required (real-time apps)
      - Compute: Single GPU, CPU, or edge device
      - Memory: 1-8 GB (mobile/edge devices)
      - Precision: Can use INT8, even lower
    
    gap: |
      ResNet-50 training: FP32, batch 256, takes 30 seconds
      ResNet-50 inference: Need <50ms per image!
      
      Optimization essential for production
  
  cost_considerations: |
    Inference cost >> training cost (long-term)
    
    Example:
    - Training: $100 once (1 week GPU)
    - Inference: $1000/month (1M requests/day)
    
    10x speedup → 10x cost reduction → $900/month savings

quantization:
  
  concept: |
    Quantization: Reduce numerical precision
    
    FP32 (32-bit float) → INT8 (8-bit integer)
    
    Result: 4x memory reduction, 2-4x speedup
  
  post_training_quantization:
    method: |
      1. Train model normally (FP32)
      2. After training, convert weights to INT8
      3. Calibrate on validation set
    
    advantages:
      - "Simple: no retraining needed"
      - "Fast: convert in minutes"
      - "Effective: 1-2% accuracy loss typical"
    
    implementation_sketch: |
      # PyTorch example
      import torch.quantization
      
      # Load trained model
      model_fp32 = ResNet50(pretrained=True)
      model_fp32.eval()
      
      # Specify quantization config
      model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')
      
      # Prepare for quantization
      model_prepared = torch.quantization.prepare(model_fp32)
      
      # Calibrate on representative dataset
      for images, _ in calibration_loader:
          model_prepared(images)
      
      # Convert to quantized model
      model_int8 = torch.quantization.convert(model_prepared)
      
      # Save quantized model
      torch.save(model_int8.state_dict(), 'model_int8.pth')
  
  quantization_aware_training:
    method: |
      Train model with quantization in mind:
      1. Insert fake quantization ops during training
      2. Forward pass: simulate INT8 precision
      3. Backward pass: use FP32 (for gradients)
      4. After training: convert to true INT8
    
    advantages:
      - "Better accuracy: <1% loss typical"
      - "Model learns to be robust to quantization"
    
    disadvantages:
      - "Slower training: fake quant ops add overhead"
      - "More complex: requires quantization-aware framework"
  
  results: |
    ResNet-50 ImageNet:
    FP32: 76.1% top-1, 120ms inference (CPU)
    INT8 (post-training): 75.9% top-1, 40ms inference
    INT8 (QAT): 76.0% top-1, 40ms inference
    
    3x speedup, <0.2% accuracy loss!

pruning:
  
  concept: |
    Pruning: Remove unnecessary connections/neurons
    
    Observation: Many weights near zero (not contributing)
    Solution: Set small weights to exactly zero
    
    Result: Sparse network, faster inference
  
  magnitude_pruning:
    method: |
      1. Train model normally
      2. Rank weights by absolute value
      3. Set bottom k% to zero (prune)
      4. Fine-tune remaining weights
      5. Repeat steps 2-4 (iterative pruning)
    
    pruning_schedule: |
      Gradual pruning works better than one-shot:
      
      Epoch 0: 0% pruned (baseline)
      Epoch 10: 20% pruned
      Epoch 20: 40% pruned
      Epoch 30: 60% pruned
      Epoch 40: 80% pruned
      
      Fine-tune 10 epochs after each pruning step
  
  structured_vs_unstructured:
    unstructured: |
      Prune individual weights
      Sparse weight matrices
      
      Pros: Maximum compression
      Cons: Requires sparse matrix libraries for speedup
    
    structured: |
      Prune entire channels/filters
      Dense but smaller matrices
      
      Pros: Works with standard libraries, actual speedup
      Cons: Less compression
    
    example: |
      Conv layer: 64 filters
      
      Unstructured: Prune 50% of weights in each filter
      → 64 filters, each 50% sparse
      
      Structured: Prune 50% of filters
      → 32 filters, each 100% dense
      
      Structured gives real speedup on standard hardware
  
  results: |
    ResNet-50 with 50% pruning:
    - Accuracy: 75.8% (vs 76.1% unpruned)
    - Inference time: 80ms (vs 120ms)
    - Model size: 50MB (vs 98MB)
    
    Trade-off: Slight accuracy loss for speed/size gains

knowledge_distillation:
  
  concept: |
    Distillation: Train small "student" model to mimic large "teacher"
    
    Teacher: Large, accurate model (e.g., ResNet-152)
    Student: Small, fast model (e.g., MobileNet)
    
    Student learns from teacher's soft predictions (not just labels)
  
  method: |
    1. Train large teacher model (or use pretrained)
    2. For each training sample:
       - Get teacher predictions (soft targets)
       - Get student predictions
       - Loss = KL_div(student, teacher) + cross_entropy(student, labels)
    3. Train student to minimize combined loss
  
  soft_targets: |
    Hard labels: [0, 0, 1, 0, 0] (one-hot)
    Soft targets: [0.01, 0.05, 0.85, 0.07, 0.02] (probabilities)
    
    Soft targets contain more information:
    - Main class: 0.85 (cat)
    - Similar class: 0.07 (dog - looks similar)
    - Dissimilar: 0.01 (car - very different)
    
    Student learns class similarities from teacher
  
  temperature_scaling: |
    Temperature T: Controls softness of predictions
    
    Softmax with temperature:
    p_i = exp(logit_i / T) / Σ exp(logit_j / T)
    
    T = 1: Standard softmax
    T > 1: Softer (more uniform) probabilities
    T < 1: Harder (more peaked) probabilities
    
    Typical: T = 3-5 during distillation
  
  implementation_sketch: |
    def distillation_loss(student_logits, teacher_logits, labels, T=3, alpha=0.5):
        """
        Distillation loss combining soft and hard targets.
        
        Parameters:
        - student_logits: student model outputs
        - teacher_logits: teacher model outputs (detached)
        - labels: true labels
        - T: temperature
        - alpha: weight for distillation loss
        """
        # Soft targets (distillation)
        soft_targets = F.softmax(teacher_logits / T, dim=1)
        soft_predictions = F.log_softmax(student_logits / T, dim=1)
        distillation_loss = F.kl_div(soft_predictions, soft_targets, reduction='batchmean') * (T ** 2)
        
        # Hard targets (classification)
        classification_loss = F.cross_entropy(student_logits, labels)
        
        # Combined loss
        return alpha * distillation_loss + (1 - alpha) * classification_loss
  
  results: |
    Teacher: ResNet-152, 78.3% accuracy, 200ms inference
    Student: MobileNet trained from scratch, 70.6% accuracy, 25ms
    Student: MobileNet with distillation, 72.8% accuracy, 25ms
    
    Distillation: +2.2% accuracy with same speed!
```

# –––––––––––––––––––––––––––––––––––––

# Topic 2: Serving Architectures

# –––––––––––––––––––––––––––––––––––––

serving_architectures:

```
batch_vs_online_serving:
  
  batch_inference:
    description: |
      Process many samples together periodically
      
      Example: Daily product recommendations
      - Collect all user data at night
      - Run batch inference (1M users)
      - Store results in database
      - Serve precomputed results next day
    
    advantages:
      - "Efficient: Large batches maximize GPU utilization"
      - "Simple: Standard training code works"
      - "Cost-effective: Batch jobs cheaper than always-on"
    
    disadvantages:
      - "Stale results: Hours/days old"
      - "Not real-time: Can't respond to new data"
    
    use_cases:
      - "Recommendation systems (pre-compute nightly)"
      - "Fraud detection (batch analysis of transactions)"
      - "Content moderation (scan uploads in batches)"
  
  online_inference:
    description: |
      Process samples immediately as they arrive
      
      Example: Real-time image classification API
      - User uploads image
      - Inference runs immediately
      - Return result in <100ms
    
    advantages:
      - "Real-time: Fresh predictions"
      - "Interactive: Immediate user feedback"
      - "Adaptive: Can use latest data"
    
    disadvantages:
      - "Expensive: Always-on servers"
      - "Complex: Need request handling, queuing"
      - "Variable latency: Depends on load"
    
    use_cases:
      - "Chatbots (immediate responses)"
      - "Self-driving cars (real-time decisions)"
      - "Content filtering (instant moderation)"
  
  hybrid_approach: |
    Combine batch and online:
    
    Example: Fraud detection
    - Batch: Compute user risk scores daily (slow features)
    - Online: Real-time transaction check (fast features + risk score)
    
    Best of both: Efficiency + real-time response

model_serving_frameworks:
  
  tensorflow_serving:
    features:
      - "Production-grade model server"
      - "gRPC and REST APIs"
      - "Model versioning (A/B testing)"
      - "Batching support"
    
    deployment: |
      # 1. Export model to SavedModel format
      model.save('saved_model/1/')
      
      # 2. Launch TensorFlow Serving
      docker run -p 8501:8501 \
        --mount type=bind,source=/models,target=/models \
        -e MODEL_NAME=my_model \
        tensorflow/serving
      
      # 3. Make predictions
      curl -X POST http://localhost:8501/v1/models/my_model:predict \
        -d '{"instances": [image_data]}'
  
  torchserve:
    features:
      - "PyTorch official serving solution"
      - "Model archiving (MAR files)"
      - "Metrics and logging built-in"
      - "Multi-model serving"
    
    deployment: |
      # 1. Archive model
      torch-model-archiver --model-name resnet50 \
        --version 1.0 \
        --model-file model.py \
        --serialized-file model.pth \
        --handler image_classifier
      
      # 2. Start server
      torchserve --start --model-store model_store \
        --models resnet50.mar
  
  custom_flask_api:
    use_case: "Lightweight, full control"
    
    implementation: |
      from flask import Flask, request, jsonify
      import torch
      
      app = Flask(__name__)
      
      # Load model once at startup
      model = torch.load('model.pth')
      model.eval()
      
      @app.route('/predict', methods=['POST'])
      def predict():
          # Get image from request
          image = preprocess(request.files['image'])
          
          # Inference
          with torch.no_grad():
              output = model(image)
              prediction = output.argmax(dim=1)
          
          return jsonify({'prediction': int(prediction)})
      
      if __name__ == '__main__':
          app.run(host='0.0.0.0', port=5000)

edge_deployment:
  
  motivation: |
    Edge deployment: Run inference on device (phone, IoT)
    
    Why:
    - Low latency: No network round-trip
    - Privacy: Data stays on device
    - Offline: Works without internet
    - Cost: No server inference cost
  
  constraints:
    - "Limited compute: Phone ~5 GFLOPS vs GPU 10,000 GFLOPS"
    - "Limited memory: 100-500 MB model budget"
    - "Battery: Inference drains battery"
    - "Heterogeneous: ARM, x86, various accelerators"
  
  optimization_for_edge:
    - "Quantization: INT8 mandatory"
    - "Pruning: 50-75% pruning typical"
    - "Small architectures: MobileNet, EfficientNet"
    - "Distillation: From large cloud model"
  
  frameworks:
    tensorflow_lite: |
      TensorFlow Lite: For mobile/embedded
      
      Features:
      - Small binary size (~500 KB)
      - Hardware acceleration (GPU, NPU)
      - Quantized model support
      
      Conversion:
      converter = tf.lite.TFLiteConverter.from_saved_model('model/')
      converter.optimizations = [tf.lite.Optimize.DEFAULT]
      tflite_model = converter.convert()
    
    pytorch_mobile: |
      PyTorch Mobile: For iOS/Android
      
      Features:
      - Optimized for mobile
      - Quantization support
      - On-device training possible
      
      Conversion:
      scripted_model = torch.jit.script(model)
      scripted_model._save_for_lite_interpreter('model.ptl')
    
    onnx_runtime: |
      ONNX Runtime: Cross-platform
      
      Features:
      - Runs on any platform
      - Hardware agnostic
      - Good performance
      
      Many frameworks export to ONNX
```

# –––––––––––––––––––––––––––––––––––––

# Topic 3: Monitoring and Observability

# –––––––––––––––––––––––––––––––––––––

monitoring_observability:

```
what_to_monitor:
  
  system_metrics:
    - "Latency: p50, p95, p99 inference time"
    - "Throughput: requests per second"
    - "Error rate: failed requests percentage"
    - "CPU/GPU utilization: resource usage"
    - "Memory usage: prevent OOM crashes"
    - "Queue depth: request backlog"
  
  model_metrics:
    - "Prediction distribution: class frequencies"
    - "Confidence scores: average, distribution"
    - "Input distribution: detect drift"
    - "Output distribution: detect anomalies"
  
  business_metrics:
    - "Accuracy proxy: downstream metrics"
    - "User engagement: clicks, conversions"
    - "Revenue impact: A/B test results"

data_drift_detection:
  
  problem: |
    Data drift: Input distribution changes over time
    
    Example:
    - Training: Summer photos (bright, outdoors)
    - Production: Winter photos (dark, indoors)
    
    Model performs poorly on drifted data
  
  detection_methods:
    statistical_tests: |
      Compare training vs production distributions
      
      Kolmogorov-Smirnov test:
      - Null hypothesis: Same distribution
      - p-value < 0.05: Reject (drift detected)
    
    feature_monitoring: |
      Track feature statistics over time:
      - Mean, std dev, min, max
      - Alert if outside 3σ of training distribution
    
    model_confidence: |
      Monitor prediction confidence:
      - Training: avg confidence 0.85
      - Production: avg confidence drops to 0.60
      
      Low confidence → Model uncertain → Possible drift
  
  response_to_drift:
    - "Retrain: Collect new data, retrain model"
    - "Ensemble: Combine old and new models"
    - "Fallback: Switch to rule-based system"
    - "Alert: Notify team for investigation"

logging_and_debugging:
  
  what_to_log:
    - "Request ID: Track individual requests"
    - "Input features: For debugging"
    - "Predictions: Track outputs"
    - "Latency: Per-request timing"
    - "Model version: Which model served"
    - "Errors: Stack traces"
  
  sampling_strategy: |
    Full logging expensive (storage, privacy)
    
    Solution: Sample logs
    - 100% of errors
    - 10% of successful requests (random sample)
    - 100% of low-confidence predictions
    - 100% of anomalous inputs
  
  log_analysis: |
    Periodic analysis:
    - Error patterns: Common failure modes?
    - Slow requests: Which inputs slow?
    - Confidence trends: Degrading over time?
    - Class imbalance: Predicting one class too often?

alerting:
  
  metrics_based_alerts:
    - "Latency > 500ms for >5 minutes: CRITICAL"
    - "Error rate > 5% for >10 minutes: WARNING"
    - "Queue depth > 1000: WARNING"
    - "GPU utilization < 20%: INFO (underutilized)"
  
  model_based_alerts:
    - "Average confidence < 0.7: WARNING (drift?)"
    - "Prediction distribution shift > 20%: WARNING"
    - "Anomalous input detected: INFO"
  
  implementation: |
    # Prometheus + Grafana typical stack
    
    # 1. Instrument code
    from prometheus_client import Histogram, Counter
    
    latency_histogram = Histogram('inference_latency_seconds', 
                                  'Inference latency')
    error_counter = Counter('inference_errors_total', 
                           'Total inference errors')
    
    @latency_histogram.time()
    def predict(image):
        try:
            return model(image)
        except Exception as e:
            error_counter.inc()
            raise
    
    # 2. Set up alerts in Grafana
    # Alert if p95 latency > 500ms for 5 minutes
```

# –––––––––––––––––––––––––––––––––––––

# Topic 4: A/B Testing and Model Updates

# –––––––––––––––––––––––––––––––––––––

ab_testing_updates:

```
ab_testing_basics:
  
  purpose: |
    A/B test: Compare two models in production
    
    Question: Does new model perform better?
  
  setup: |
    Model A (baseline): 80% of traffic
    Model B (candidate): 20% of traffic
    
    Track metrics for both:
    - Accuracy proxy (e.g., user engagement)
    - Latency
    - Error rate
  
  statistical_significance: |
    Need enough samples for significance
    
    Example:
    Model A: 85% accuracy (1000 samples)
    Model B: 87% accuracy (250 samples)
    
    Difference: 2%
    Significance: p = 0.15 (not significant!)
    
    Need more samples or wider gap

shadow_deployment:
  
  concept: |
    Shadow mode: New model runs alongside old model
    - Old model serves users (production)
    - New model processes same inputs (shadow)
    - Compare predictions offline
  
  advantages:
    - "Zero user risk: Shadow doesn't affect users"
    - "Full traffic: Test on all requests"
    - "Easy rollback: Just turn off shadow"
  
  process: |
    1. Deploy shadow model
    2. Run for 1 week, collect predictions
    3. Analyze offline:
       - Disagreement rate
       - Accuracy on labeled subset
       - Latency comparison
    4. If better: Promote to A/B test
    5. If worse: Fix and redeploy

canary_deployment:
  
  concept: |
    Canary: Gradually increase traffic to new model
    
    Day 1: 5% traffic
    Day 2: 10% (if metrics good)
    Day 3: 25%
    Day 5: 50%
    Day 7: 100% (full rollout)
  
  rollback_strategy: |
    Monitor closely during canary
    
    If any metric degrades:
    - Immediately rollback to 0%
    - Investigate issue
    - Fix and restart canary

model_versioning:
  
  importance: |
    Version control for models (like code)
    
    Track:
    - Model architecture
    - Training data version
    - Hyperparameters
    - Training code version
    - Performance metrics
  
  tools:
    - "MLflow: Experiment tracking, model registry"
    - "DVC: Data Version Control"
    - "Git LFS: Large file storage for models"
  
  best_practices:
    - "Tag versions: v1.0, v1.1, v2.0"
    - "Store metadata: training date, author, metrics"
    - "Reproducibility: Save training code and data"
    - "Rollback ready: Keep last 3 versions deployed"
```

# –––––––––––––––––––––––––––––––––––––

# Topic 5: Production Security

# –––––––––––––––––––––––––––––––––––––

production_security:

```
input_validation:
  
  threats:
    - "Malformed inputs: Cause crashes"
    - "Adversarial examples: Fool model"
    - "Injection attacks: Exploit preprocessing"
    - "Resource exhaustion: Large inputs"
  
  validation_checks:
    format_validation: |
      - Image: Valid JPEG/PNG, not corrupted
      - Size: Within limits (e.g., <10 MB)
      - Dimensions: Expected resolution
      - Color space: RGB/Grayscale as expected
    
    content_validation: |
      - Pixel values: In valid range [0, 255]
      - No NaN, Inf
      - Aspect ratio: Reasonable bounds
    
    rate_limiting: |
      - Requests per IP: <100/minute
      - Requests per user: <1000/day
      - Prevents DoS, model stealing
  
  implementation: |
    def validate_image(image_bytes):
        """Validate uploaded image"""
        # Size check
        if len(image_bytes) > 10 * 1024 * 1024:  # 10 MB
            raise ValueError("Image too large")
        
        # Format check
        try:
            img = PIL.Image.open(io.BytesIO(image_bytes))
        except:
            raise ValueError("Invalid image format")
        
        # Dimension check
        if img.width > 4096 or img.height > 4096:
            raise ValueError("Image dimensions too large")
        
        # Color space check
        if img.mode not in ['RGB', 'L']:
            raise ValueError("Unsupported color space")
        
        return img

model_stealing_prevention:
  
  threat: |
    Model stealing: Attacker queries model repeatedly
    to reconstruct it
    
    Method:
    1. Send many inputs
    2. Collect predictions
    3. Train substitute model
    4. Steal intellectual property
  
  defenses:
    query_limits: |
      - Rate limiting: <1000 queries/day per user
      - Require authentication
      - Log all queries
    
    prediction_rounding: |
      Instead of:
      [0.05, 0.23, 0.72] (precise probabilities)
      
      Return:
      [0.0, 0.2, 0.7] (rounded)
      
      Less information leaked, harder to steal
    
    confidence_thresholding: |
      Only return prediction if confidence > 0.8
      
      Low confidence → return "uncertain"
      Prevents attacker from getting edge-case info
  
  monitoring: |
    Alert on suspicious patterns:
    - Single user with >10K queries
    - Sequential queries (scanning attack)
    - Queries with systematic perturbations

adversarial_robustness:
  
  threat: |
    Adversarial examples in production
    Attacker crafts inputs to fool model
  
  defenses:
    input_preprocessing: |
      - JPEG compression (removes high-freq noise)
      - Gaussian blur
      - Random resizing
      
      Destroys many adversarial perturbations
    
    ensemble_prediction: |
      Use ensemble of models:
      - Train 3-5 models independently
      - Average predictions
      
      Harder to fool all models simultaneously
    
    adversarial_training: |
      Include adversarial examples in training
      Model learns robustness
      
      Production model: 60-70% robust accuracy
  
  detection: |
    Detect adversarial inputs:
    - Unusual saliency patterns
    - Low prediction confidence
    - Inconsistent predictions (temporal)
    
    Flag for manual review

model_update_security:
  
  threat: |
    Compromised model update:
    - Attacker injects backdoor
    - Deploys poisoned model
    - Backdoor active in production
  
  protections:
    model_signing: |
      Sign models with private key
      Verify signature before deployment
      
      Only authorized personnel can sign models
    
    staged_rollout: |
      Canary deployment catches issues:
      - Backdoor affects 5% traffic first
      - Monitoring detects anomalies
      - Rollback before full deployment
    
    testing_before_deployment: |
      Automated tests:
      - Accuracy on validation set
      - Latency benchmarks
      - Known backdoor trigger tests
      
      Block deployment if tests fail
```

# –––––––––––––––––––––––––––––––––––––

# Key Takeaways

# –––––––––––––––––––––––––––––––––––––

key_takeaways:

```
critical_concepts:
  - "Inference optimization essential: quantization (INT8), pruning (50-80%), distillation for 2-4x speedup"
  - "Batch vs online serving tradeoff: batch efficient but stale, online real-time but expensive"
  - "Edge deployment constrained: 100-500 MB budget, INT8 mandatory, MobileNet-class architectures"
  - "Monitor data drift: training distribution ≠ production, detect via KS test or confidence drops"
  - "A/B testing validates changes: shadow → canary → full rollout, track statistical significance"
  - "Input validation critical: malformed inputs, size limits, rate limiting prevent attacks"

actionable_steps:
  - "Quantize to INT8 first: post-training quantization, 3x speedup, <1% accuracy loss typical"
  - "Use TensorFlow Serving or TorchServe: production-grade, battle-tested, better than custom Flask"
  - "Implement comprehensive monitoring: latency p95/p99, error rate, prediction distribution, confidence"
  - "Log samples strategically: 100% errors, 10% successes, 100% low-confidence, save storage"
  - "Start with shadow deployment: zero risk, full traffic testing before A/B test"
  - "Validate all inputs: size, format, dimensions, rate limiting before inference"

security_principles:
  - "Rate limiting prevents model stealing: <1000 queries/day, log all queries, detect patterns"
  - "Input validation prevents crashes: malformed inputs, adversarial examples, injection attacks"
  - "Model versioning enables rollback: keep last 3 versions, instant rollback if backdoor detected"
  - "Monitoring detects attacks: unusual prediction patterns, confidence drops, query patterns"

mlops_best_practices:
  - "CI/CD for ML: automated testing, staged rollout, instant rollback capability"
  - "Reproducibility critical: version models, data, code, hyperparameters together"
  - "Monitoring is not optional: production models degrade, drift is inevitable"
  - "Test in shadow mode first: catches issues before affecting users"
  - "Security from day one: input validation, rate limiting, model signing, not afterthoughts"
```

-----