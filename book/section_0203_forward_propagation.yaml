# section_02_03_forward_propagation.yaml
---
document_info:
  chapter: "02"
  section: "0203"
  title: "Forward Propagation Mechanics"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-06"
  estimated_pages: 6
  tags: ["forward-propagation", "computational-graph", "vectorization", "batching", "memory-optimization", "numpy"]

# ============================================================================
# SECTION 0203: FORWARD PROPAGATION MECHANICS
# ============================================================================

section_0203_forward_propagation:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Forward propagation is how neural networks make predictions - data flows
    from input through hidden layers to output. Understanding this process
    deeply is critical for both implementation and security.
    
    This section covers the complete mechanics: matrix formulation (W @ x + b),
    layer-by-layer computation, computational graphs that track operations,
    batched processing for efficiency, and memory management at scale.
    
    You'll implement forward pass for multi-layer networks from scratch, build
    a computational graph visualizer, profile memory usage per layer, and
    optimize with vectorization. By the end, you'll process 1000 MNIST images
    through a 3-layer network in milliseconds.
    
    Security relevance: Forward pass exposes model structure through timing,
    intermediate outputs reveal architecture, and computational graphs show
    exactly where adversaries can inject attacks.
  
  learning_objectives:
    
    conceptual:
      - "Understand matrix formulation: z = Wx + b, a = f(z)"
      - "Grasp layer-by-layer computation flow"
      - "Know what computational graphs represent"
      - "Understand vectorization and why batching is efficient"
      - "Recognize memory bottlenecks in deep networks"
      - "Connect forward pass structure to attack surfaces"
    
    practical:
      - "Implement forward pass for multi-layer network (NumPy)"
      - "Build computational graph tracker"
      - "Profile memory usage per layer"
      - "Vectorize operations for batch processing"
      - "Optimize forward pass for speed"
      - "Handle edge cases (batch size 1, different dimensions)"
    
    security_focused:
      - "Timing attacks reveal layer structure"
      - "Intermediate activations leak model information"
      - "Memory patterns expose architecture details"
      - "Computational graph shows all attack injection points"
  
  prerequisites:
    - "Section 0201 (neural network basics)"
    - "Section 0202 (activation functions)"
    - "Matrix operations (NumPy proficiency)"
    - "Basic understanding of memory management"
  
  # --------------------------------------------------------------------------
  # Topic 1: Matrix Formulation - The Core Operation
  # --------------------------------------------------------------------------
  
  matrix_formulation:
    
    single_neuron:
      
      scalar_form:
        description: "One neuron with n inputs"
        
        equation: "z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b"
        
        example: |
          3 inputs: x = [x₁, x₂, x₃]
          Weights: w = [w₁, w₂, w₃]
          Bias: b
          
          z = w₁x₁ + w₂x₂ + w₃x₃ + b
      
      vector_form:
        description: "More compact using dot product"
        
        equation: "z = w^T x + b"
        
        numpy_code: |
          z = np.dot(w, x) + b
          # Or equivalently:
          z = w @ x + b
        
        dimensions: |
          w: (n_features,)
          x: (n_features,)
          b: scalar
          z: scalar
    
    single_layer:
      
      multiple_neurons:
        description: "Layer with m neurons, each processing n inputs"
        
        equation: "z = Wx + b"
        
        matrix_breakdown: |
          W = [w₁^T]    Shape: (m, n)
              [w₂^T]
              [...]
              [wₘ^T]
          
          x = [x₁]      Shape: (n, 1)
              [x₂]
              [...]
              [xₙ]
          
          b = [b₁]      Shape: (m, 1)
              [b₂]
              [...]
              [bₘ]
          
          z = Wx + b    Shape: (m, 1)
        
        intuition: |
          Each row of W is one neuron's weights.
          Matrix multiplication computes all neurons in parallel.
      
      with_activation:
        description: "Apply non-linearity element-wise"
        
        equation: "a = f(z) = f(Wx + b)"
        
        example_relu: |
          z = Wx + b
          a = max(0, z)  # Applied element-wise
        
        numpy_code: |
          z = W @ x + b
          a = np.maximum(0, z)  # ReLU activation
    
    batch_processing:
      
      single_sample_vs_batch:
        
        single_sample:
          dimensions: |
            x: (n_features, 1) - one sample
            W: (m_neurons, n_features)
            b: (m_neurons, 1)
            z: (m_neurons, 1)
        
        batch:
          dimensions: |
            X: (n_features, batch_size) - multiple samples
            W: (m_neurons, n_features)
            b: (m_neurons, 1) - broadcasts to all samples
            Z: (m_neurons, batch_size)
        
        computation: |
          # Single sample
          z = W @ x + b
          
          # Batch
          Z = W @ X + b
          # b broadcasts: added to each column of W @ X
      
      broadcasting_mechanics:
        
        numpy_broadcasting: |
          W @ X shape: (m_neurons, batch_size)
          b shape: (m_neurons, 1)
          
          Broadcasting adds b to each column:
          Z[:, 0] = W @ X[:, 0] + b
          Z[:, 1] = W @ X[:, 1] + b
          ...
          Z[:, k] = W @ X[:, k] + b
        
        code_example: |
          W = np.random.randn(64, 784)  # 64 neurons, 784 inputs
          X = np.random.randn(784, 32)  # 32 samples, 784 features
          b = np.random.randn(64, 1)
          
          Z = W @ X + b  # Shape: (64, 32)
          
          # Verify broadcasting
          print(f"W @ X shape: {(W @ X).shape}")  # (64, 32)
          print(f"b shape: {b.shape}")            # (64, 1)
          print(f"Z shape: {Z.shape}")            # (64, 32)
      
      why_batching_matters:
        
        computational_efficiency:
          single_sample: "Process 1000 samples: 1000 matrix multiplications"
          batch_32: "Process 1000 samples: 32 matrix multiplications (31x fewer!)"
          batch_256: "Process 1000 samples: 4 matrix multiplications (250x fewer!)"
        
        hardware_optimization:
          gpus: "GPUs excel at large matrix operations (parallel processing)"
          cache: "Larger operations better utilize CPU cache"
          vectorization: "SIMD instructions process multiple elements at once"
        
        memory_tradeoff:
          small_batch: "Lower memory usage, more operations"
          large_batch: "Higher memory usage, fewer operations"
          optimal: "Balance based on hardware (typical: 32-256)"
  
  # --------------------------------------------------------------------------
  # Topic 2: Layer-by-Layer Computation
  # --------------------------------------------------------------------------
  
  layer_by_layer_computation:
    
    two_layer_network:
      
      architecture:
        description: "Input → Hidden → Output"
        
        dimensions: |
          Input: n_features
          Hidden: n_hidden neurons
          Output: n_classes
        
        parameters: |
          W1: (n_hidden, n_features)
          b1: (n_hidden, 1)
          W2: (n_classes, n_hidden)
          b2: (n_classes, 1)
      
      forward_pass:
        
        layer_1:
          operation: "Input → Hidden"
          
          equations: |
            z1 = W1 @ x + b1
            a1 = f(z1)         # Hidden layer activation
        
        layer_2:
          operation: "Hidden → Output"
          
          equations: |
            z2 = W2 @ a1 + b2
            a2 = g(z2)         # Output layer activation
        
        complete_forward: |
          # Layer 1
          z1 = W1 @ x + b1
          a1 = relu(z1)
          
          # Layer 2
          z2 = W2 @ a1 + b2
          a2 = softmax(z2)
          
          # a2 is final output (predictions)
      
      implementation:
        
        code: |
          class TwoLayerNetwork:
              def __init__(self, n_features, n_hidden, n_classes):
                  # Initialize weights (small random values)
                  self.W1 = np.random.randn(n_hidden, n_features) * 0.01
                  self.b1 = np.zeros((n_hidden, 1))
                  
                  self.W2 = np.random.randn(n_classes, n_hidden) * 0.01
                  self.b2 = np.zeros((n_classes, 1))
              
              def forward(self, X):
                  """
                  X: (n_features, batch_size)
                  Returns: predictions (n_classes, batch_size)
                  """
                  # Layer 1: Input → Hidden
                  self.z1 = self.W1 @ X + self.b1
                  self.a1 = np.maximum(0, self.z1)  # ReLU
                  
                  # Layer 2: Hidden → Output
                  self.z2 = self.W2 @ self.a1 + self.b2
                  self.a2 = self.softmax(self.z2)
                  
                  return self.a2
              
              def softmax(self, z):
                  # Numerically stable softmax
                  z_shifted = z - np.max(z, axis=0, keepdims=True)
                  exp_z = np.exp(z_shifted)
                  return exp_z / np.sum(exp_z, axis=0, keepdims=True)
        
        usage: |
          # Create network
          model = TwoLayerNetwork(n_features=784, n_hidden=128, n_classes=10)
          
          # Forward pass on batch
          X_batch = np.random.randn(784, 32)  # 32 MNIST images
          predictions = model.forward(X_batch)  # Shape: (10, 32)
          
          # Get predicted classes
          predicted_classes = np.argmax(predictions, axis=0)  # Shape: (32,)
    
    deep_network:
      
      architecture:
        description: "Input → H1 → H2 → H3 → Output"
        
        dimensions: |
          Input: n_features
          Hidden 1: n_h1 neurons
          Hidden 2: n_h2 neurons
          Hidden 3: n_h3 neurons
          Output: n_classes
      
      sequential_computation:
        
        layer_sequence: |
          x (input)
          ↓
          z1 = W1 @ x + b1
          a1 = f(z1)
          ↓
          z2 = W2 @ a1 + b2
          a2 = f(z2)
          ↓
          z3 = W3 @ a2 + b3
          a3 = f(z3)
          ↓
          z4 = W4 @ a3 + b4
          a4 = g(z4)
          ↓
          output (predictions)
        
        pattern: "Each layer's output becomes next layer's input"
      
      implementation:
        
        flexible_architecture: |
          class DeepNetwork:
              def __init__(self, layer_dims):
                  """
                  layer_dims: list of layer sizes
                  Example: [784, 128, 64, 32, 10] for MNIST
                  """
                  self.layer_dims = layer_dims
                  self.n_layers = len(layer_dims) - 1
                  
                  # Initialize parameters
                  self.params = {}
                  for l in range(1, self.n_layers + 1):
                      self.params[f'W{l}'] = np.random.randn(
                          layer_dims[l], layer_dims[l-1]
                      ) * 0.01
                      self.params[f'b{l}'] = np.zeros((layer_dims[l], 1))
              
              def forward(self, X):
                  """Forward pass through all layers"""
                  self.cache = {'a0': X}  # Store activations
                  
                  A = X
                  for l in range(1, self.n_layers + 1):
                      W = self.params[f'W{l}']
                      b = self.params[f'b{l}']
                      
                      # Linear transformation
                      Z = W @ A + b
                      self.cache[f'z{l}'] = Z
                      
                      # Activation
                      if l < self.n_layers:
                          A = np.maximum(0, Z)  # ReLU for hidden layers
                      else:
                          A = self.softmax(Z)   # Softmax for output
                      
                      self.cache[f'a{l}'] = A
                  
                  return A
        
        usage: |
          # Create deep network: 784 → 128 → 64 → 10
          model = DeepNetwork([784, 128, 64, 10])
          
          # Forward pass
          X_batch = np.random.randn(784, 32)
          predictions = model.forward(X_batch)
  
  # --------------------------------------------------------------------------
  # Topic 3: Computational Graphs - Tracking Operations
  # --------------------------------------------------------------------------
  
  computational_graphs:
    
    what_is_computational_graph:
      
      definition: |
        A computational graph is a directed acyclic graph (DAG) where:
        - Nodes represent operations or variables
        - Edges represent data flow
        - Used to track how output depends on inputs
      
      purpose:
        forward_pass: "Compute output from input"
        backward_pass: "Compute gradients (backpropagation)"
        optimization: "Identify redundant computations"
        visualization: "Understand model structure"
    
    simple_example:
      
      computation: |
        y = (a + b) * c
      
      graph_structure: |
        Input nodes: a, b, c
        Operation nodes: +, *
        Output node: y
        
        Flow:
        a ─┐
           ├─[+]─[d]─┐
        b ─┘         ├─[*]─[y]
        c ───────────┘
        
        Where d = a + b (intermediate)
      
      forward_evaluation: |
        1. Compute d = a + b
        2. Compute y = d * c
      
      backward_gradient: |
        1. ∂y/∂c = d
        2. ∂y/∂d = c
        3. ∂y/∂a = ∂y/∂d × ∂d/∂a = c × 1 = c
        4. ∂y/∂b = ∂y/∂d × ∂d/∂b = c × 1 = c
    
    neural_network_graph:
      
      single_layer: |
        x ─┐
           ├─[W @ x + b]─[z]─[f]─[a]
        W ─┤
        b ─┘
        
        Operations:
        1. Matrix multiply: W @ x
        2. Add bias: + b
        3. Activation: f(z)
      
      multi_layer: |
        x ─[Layer1]─[a1]─[Layer2]─[a2]─[Layer3]─[a3]
        
        Each layer expands to:
        W1, b1        W2, b2        W3, b3
           ↓             ↓             ↓
        [@ +]         [@ +]         [@ +]
           ↓             ↓             ↓
        [f1]          [f2]          [f3]
           ↓             ↓             ↓
        a1            a2            a3
      
      full_training_graph: |
        Input (x, y_true)
        ↓
        Forward pass (x → predictions)
        ↓
        Loss computation (predictions, y_true → loss)
        ↓
        Backward pass (loss → gradients)
        ↓
        Parameter update (params - lr * gradients → new params)
    
    implementation:
      
      simple_graph_tracker: |
        class ComputationalGraph:
            def __init__(self):
                self.operations = []
                self.values = {}
            
            def add_operation(self, op_type, inputs, output, value):
                """Track operation in graph"""
                self.operations.append({
                    'type': op_type,
                    'inputs': inputs,
                    'output': output,
                    'value': value
                })
                self.values[output] = value
            
            def visualize(self):
                """Print graph structure"""
                for i, op in enumerate(self.operations):
                    print(f"Op {i}: {op['type']}")
                    print(f"  Inputs: {op['inputs']}")
                    print(f"  Output: {op['output']}")
                    print(f"  Shape: {op['value'].shape}")
        
        # Usage
        graph = ComputationalGraph()
        
        # Forward pass with tracking
        z1 = W1 @ X + b1
        graph.add_operation('linear', ['W1', 'X', 'b1'], 'z1', z1)
        
        a1 = np.maximum(0, z1)
        graph.add_operation('relu', ['z1'], 'a1', a1)
        
        z2 = W2 @ a1 + b2
        graph.add_operation('linear', ['W2', 'a1', 'b2'], 'z2', z2)
        
        graph.visualize()
  
  # --------------------------------------------------------------------------
  # Topic 4: Memory Management and Optimization
  # --------------------------------------------------------------------------
  
  memory_optimization:
    
    memory_bottlenecks:
      
      activation_storage:
        problem: |
          Deep networks need to store all intermediate activations for backprop.
          Memory grows with depth and batch size.
        
        calculation: |
          Network: 784 → 1024 → 1024 → 512 → 10
          Batch size: 256
          Dtype: float32 (4 bytes)
          
          Layer 1: 1024 × 256 × 4 bytes = 1.05 MB
          Layer 2: 1024 × 256 × 4 bytes = 1.05 MB
          Layer 3: 512 × 256 × 4 bytes = 0.52 MB
          Layer 4: 10 × 256 × 4 bytes = 0.01 MB
          
          Total activations: ~2.6 MB per forward pass
        
        scaling: |
          For 100-layer ResNet with batch 256:
          Memory can exceed several GB just for activations!
      
      parameter_storage:
        size_calculation: |
          W1: 784 × 1024 × 4 bytes = 3.21 MB
          b1: 1024 × 4 bytes = 4 KB
          W2: 1024 × 1024 × 4 bytes = 4.19 MB
          b2: 1024 × 4 bytes = 4 KB
          W3: 1024 × 512 × 4 bytes = 2.10 MB
          b3: 512 × 4 bytes = 2 KB
          W4: 512 × 10 × 4 bytes = 20 KB
          b4: 10 × 4 bytes = 40 bytes
          
          Total parameters: ~9.5 MB
        
        note: "Parameters stored once, activations stored per batch"
      
      gradient_storage:
        observation: "Need to store gradients for each parameter (same size as parameters)"
        
        total_memory: |
          Parameters: 9.5 MB
          Gradients: 9.5 MB
          Activations: 2.6 MB per batch
          
          Minimum: ~21.6 MB
          With optimizer states (Adam): 2x more (~40 MB)
    
    optimization_techniques:
      
      in_place_operations:
        description: "Modify arrays in-place instead of creating copies"
        
        example: |
          # Bad: Creates new array
          a = np.maximum(0, z)
          
          # Good: In-place (if possible)
          np.maximum(z, 0, out=z)
        
        savings: "Reduces memory by avoiding temporary arrays"
      
      gradient_checkpointing:
        description: "Don't store all activations, recompute some during backprop"
        
        tradeoff: "Trade memory for computation (slower but less memory)"
        
        strategy: |
          Store activations only at checkpoints (every N layers).
          During backprop, recompute activations between checkpoints.
        
        example: |
          100-layer network, checkpoint every 10 layers:
          Store: 10 activation sets instead of 100
          Memory: 90% reduction
          Time: 10-20% increase (due to recomputation)
      
      mixed_precision:
        description: "Use float16 instead of float32 for some operations"
        
        benefits:
          memory: "2x reduction (2 bytes vs 4 bytes)"
          speed: "2-3x faster on modern GPUs"
        
        cautions:
          - "Risk of numerical instability"
          - "Need to keep master copy of weights in float32"
          - "Loss scaling to prevent gradient underflow"
      
      batch_size_tuning:
        memory_vs_speed: |
          Larger batch:
          + Fewer iterations per epoch (faster training)
          + Better GPU utilization
          - More memory required
          - May need lower learning rate
          
          Smaller batch:
          + Less memory
          + More frequent updates (can help escape local minima)
          - Slower per epoch
          - Noisier gradients
        
        finding_optimal: |
          Start with small batch, gradually increase until:
          - Memory is nearly full, OR
          - No further speedup gained
    
    profiling_memory:
      
      numpy_approach: |
        def profile_forward_pass(model, X_batch):
            """Profile memory usage per layer"""
            import tracemalloc
            
            tracemalloc.start()
            
            # Forward pass
            predictions = model.forward(X_batch)
            
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            print(f"Current memory: {current / 1024**2:.2f} MB")
            print(f"Peak memory: {peak / 1024**2:.2f} MB")
            
            return predictions
      
      layer_by_layer_analysis: |
        def detailed_memory_profile(model, X_batch):
            """Track memory per layer"""
            memory_usage = []
            
            A = X_batch
            for l in range(1, model.n_layers + 1):
                # Before layer
                mem_before = get_memory_usage()
                
                # Layer computation
                W = model.params[f'W{l}']
                b = model.params[f'b{l}']
                Z = W @ A + b
                A = np.maximum(0, Z)
                
                # After layer
                mem_after = get_memory_usage()
                
                memory_usage.append({
                    'layer': l,
                    'memory_mb': (mem_after - mem_before) / 1024**2,
                    'activation_shape': A.shape
                })
            
            return memory_usage
  
  # --------------------------------------------------------------------------
  # Topic 5: Performance Benchmarking
  # --------------------------------------------------------------------------
  
  performance_benchmarking:
    
    timing_forward_pass:
      
      basic_benchmark: |
        import time
        
        def benchmark_forward(model, X_batch, n_iterations=100):
            """Measure forward pass time"""
            times = []
            
            for _ in range(n_iterations):
                start = time.time()
                _ = model.forward(X_batch)
                end = time.time()
                times.append(end - start)
            
            return {
                'mean_ms': np.mean(times) * 1000,
                'std_ms': np.std(times) * 1000,
                'min_ms': np.min(times) * 1000,
                'max_ms': np.max(times) * 1000
            }
      
      results_interpretation: |
        Mean: 5.2 ms ± 0.3 ms
        Min: 4.8 ms
        Max: 6.1 ms
        
        Analysis:
        - Can process ~192 batches/second (1000 / 5.2)
        - For batch size 32: ~6,144 samples/second
        - First run often slower (cache warming)
        - Variance indicates stable performance
    
    batch_size_impact:
      
      experiment: |
        batch_sizes = [1, 8, 16, 32, 64, 128, 256]
        results = {}
        
        for bs in batch_sizes:
            X_batch = np.random.randn(784, bs)
            timing = benchmark_forward(model, X_batch)
            
            results[bs] = {
                'time_ms': timing['mean_ms'],
                'throughput': bs / (timing['mean_ms'] / 1000)  # samples/sec
            }
      
      expected_pattern: |
        Batch Size | Time (ms) | Throughput (samples/sec)
        1          | 1.2       | 833
        8          | 2.1       | 3,810
        16         | 3.1       | 5,161
        32         | 5.2       | 6,154
        64         | 9.1       | 7,033
        128        | 16.8      | 7,619
        256        | 31.2      | 8,205
        
        Observation: Throughput increases with batch size, but diminishing returns
    
    vectorization_benefits:
      
      non_vectorized_loop: |
        def forward_loop(W, X, b):
            """Process each sample individually"""
            batch_size = X.shape[1]
            outputs = []
            
            for i in range(batch_size):
                x_i = X[:, i:i+1]
                z_i = W @ x_i + b
                outputs.append(z_i)
            
            return np.hstack(outputs)
      
      vectorized: |
        def forward_vectorized(W, X, b):
            """Process all samples at once"""
            return W @ X + b
      
      benchmark_comparison: |
        Batch size: 256
        
        Loop version: 48.3 ms
        Vectorized: 5.2 ms
        
        Speedup: 9.3x!
        
        Conclusion: Always vectorize operations
  
  # --------------------------------------------------------------------------
  # Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    timing_attacks:
      
      architecture_leakage:
        description: |
          Forward pass timing reveals network depth and layer sizes.
        
        attack_methodology:
          1: "Measure forward pass time for various batch sizes"
          2: "Analyze time vs batch size relationship"
          3: "Infer number of layers (more layers = longer time)"
          4: "Estimate layer sizes (memory access patterns)"
        
        defense:
          - "Add random timing jitter (small random delays)"
          - "Batch queries to hide individual timings"
          - "Rate limit to prevent timing analysis"
        
        code_example: |
          def forward_with_jitter(model, X_batch):
              """Add random timing noise"""
              result = model.forward(X_batch)
              
              # Random delay 0-10ms
              time.sleep(np.random.uniform(0, 0.01))
              
              return result
      
      batch_size_probing:
        description: |
          Attacker varies batch size to infer memory limits (reveals architecture).
        
        defense: "Fixed batch size for all external queries"
    
    intermediate_activation_leakage:
      
      information_exposure:
        risk: |
          If intermediate activations exposed (debugging, logging),
          attacker can:
          - Reverse engineer architecture
          - Extract training data patterns
          - Identify dead neurons to exploit
        
        example_vulnerable_code: |
          # BAD: Logging intermediate activations
          def forward_with_logging(model, X):
              a1 = model.layer1(X)
              logging.info(f"Layer 1 output: {a1}")  # EXPOSES INFO!
              
              a2 = model.layer2(a1)
              logging.info(f"Layer 2 output: {a2}")  # EXPOSES INFO!
              
              return a2
        
        defense:
          - "Never expose intermediate activations to users"
          - "Log only aggregate statistics (mean, std)"
          - "Sanitize debug outputs"
      
      memory_access_patterns:
        description: |
          Memory access patterns reveal architecture through side channels.
        
        attack: "Cache timing attacks infer layer sizes"
        
        defense: "Memory obfuscation, constant-time operations (limited effectiveness)"
    
    model_fingerprinting:
      
      unique_signatures:
        observation: |
          Each architecture has unique computational signature:
          - Specific timing pattern
          - Memory usage pattern
          - Output distribution characteristics
        
        attacker_goal: |
          Identify if target uses same architecture as known model.
          If yes, apply known vulnerabilities/attacks.
        
        defense:
          - "Architectural diversity (different models for similar tasks)"
          - "Output noise injection"
          - "Rate limiting and monitoring"
  
  # --------------------------------------------------------------------------
  # Hands-On Exercises
  # --------------------------------------------------------------------------
  
  hands_on_exercises:
    
    exercise_1:
      title: "Implement Multi-Layer Forward Pass"
      difficulty: "Intermediate"
      time: "60 minutes"
      
      task: |
        Build flexible deep network with arbitrary depth and width.
      
      requirements:
        - "Support any number of layers"
        - "Configurable layer sizes"
        - "ReLU for hidden layers, softmax for output"
        - "Batch processing support"
        - "Cache intermediate values for backprop"
      
      code_template: |
        class FlexibleNetwork:
            def __init__(self, layer_dims):
                """
                layer_dims: list [input_dim, hidden1, hidden2, ..., output_dim]
                """
                # TODO: Initialize parameters
                pass
            
            def forward(self, X):
                """
                X: (input_dim, batch_size)
                Returns: predictions (output_dim, batch_size)
                """
                # TODO: Implement forward pass
                # Cache all z and a values for backprop
                pass
      
      test_cases:
        - "Network [784, 128, 64, 10] on MNIST batch"
        - "Network [100, 50, 50, 50, 1] on random data"
        - "Single layer [10, 5] (edge case)"
        - "Batch size 1 and batch size 256"
      
      validation:
        - "Output shape matches (output_dim, batch_size)"
        - "Output values in [0, 1] (softmax)"
        - "No NaN or Inf values"
        - "Memory usage reasonable"
    
    exercise_2:
      title: "Build Computational Graph Visualizer"
      difficulty: "Intermediate"
      time: "45 minutes"
      
      task: |
        Create tool that tracks and visualizes forward pass operations.
      
      features:
        - "Track all operations (linear, activation)"
        - "Record input/output shapes"
        - "Visualize graph structure (ASCII art or graphviz)"
        - "Export graph to JSON"
      
      example_output: |
        Computational Graph:
        
        X (784, 32)
        ↓
        [Linear] W1 @ X + b1
        ↓
        z1 (128, 32)
        ↓
        [ReLU]
        ↓
        a1 (128, 32)
        ↓
        [Linear] W2 @ a1 + b2
        ↓
        z2 (10, 32)
        ↓
        [Softmax]
        ↓
        a2 (10, 32)
    
    exercise_3:
      title: "Memory Profiler"
      difficulty: "Advanced"
      time: "60 minutes"
      
      task: |
        Profile memory usage layer by layer, identify bottlenecks.
      
      implementation:
        - "Measure memory before/after each layer"
        - "Track activation sizes"
        - "Identify largest memory consumers"
        - "Generate optimization recommendations"
      
      output_format: |
        Memory Profile:
        
        Layer 1: 1.05 MB (activations: 128 × 256)
        Layer 2: 1.05 MB (activations: 128 × 256)
        Layer 3: 0.52 MB (activations: 64 × 256)
        Layer 4: 0.01 MB (activations: 10 × 256)
        
        Total: 2.63 MB
        Peak: 3.12 MB
        
        Recommendations:
        - Layer 1,2 are largest (80% of memory)
        - Consider reducing hidden size or using gradient checkpointing
    
    exercise_4:
      title: "Benchmark and Optimize"
      difficulty: "Advanced"
      time: "90 minutes"
      
      task: |
        Optimize forward pass for speed, compare implementations.
      
      implementations_to_test:
        naive: "Python loops, no vectorization"
        basic: "NumPy vectorization"
        optimized: "In-place operations, memory reuse"
        batched: "Optimal batch size selection"
      
      benchmark_metrics:
        - "Forward pass time (mean, std)"
        - "Throughput (samples/second)"
        - "Memory usage (MB)"
        - "Batch size impact"
      
      goal: "Achieve >10,000 samples/second on MNIST with 3-layer network"
  
  # --------------------------------------------------------------------------
  # Common Mistakes
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    dimension_errors:
      
      "wrong_matrix_multiplication_order":
        mistake: "X @ W instead of W @ X"
        problem: "Dimension mismatch or wrong output shape"
        fix: |
          Correct: Z = W @ X
          W shape: (m_neurons, n_features)
          X shape: (n_features, batch_size)
          Z shape: (m_neurons, batch_size) ✓
      
      "bias_broadcasting_error":
        mistake: "b shape (m_neurons,) instead of (m_neurons, 1)"
        problem: "Broadcasting doesn't work correctly"
        fix: |
          # Wrong
          b = np.zeros(128)  # Shape: (128,)
          
          # Correct
          b = np.zeros((128, 1))  # Shape: (128, 1)
      
      "batch_dimension_confusion":
        mistake: "Treating batch as rows instead of columns"
        convention: |
          NumPy convention: X shape (features, batch_size)
          PyTorch convention: X shape (batch_size, features)
          
          Pick one and stay consistent!
    
    memory_leaks:
      
      "not_releasing_cache":
        mistake: "Storing activations forever"
        problem: "Memory grows unbounded"
        fix: |
          # After backprop, clear cache
          self.cache = {}
      
      "large_intermediate_arrays":
        mistake: "Creating unnecessary temporary arrays"
        example: |
          # Bad
          temp = W @ X
          temp2 = temp + b
          Z = np.maximum(0, temp2)
          
          # Good
          Z = np.maximum(0, W @ X + b)
    
    performance_pitfalls:
      
      "python_loops_instead_of_vectorization":
        mistake: "for loop over batch dimension"
        slowdown: "10-100x slower"
        fix: "Always use matrix operations"
      
      "not_caching_forward_pass":
        mistake: "Recomputing forward pass during backprop"
        problem: "2x slower training"
        fix: "Cache z and a values during forward pass"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Forward propagation: z = Wx + b, then a = f(z) applied layer by layer"
      - "Batching is essential: process multiple samples in single matrix operation (10-100x speedup)"
      - "Computational graphs track operations for both forward pass and backprop (gradient computation)"
      - "Memory grows with depth and batch size: 100-layer network with batch 256 can use several GB"
      - "Vectorization critical: NumPy matrix operations 10-100x faster than Python loops"
      - "Timing patterns leak architecture: attackers can infer depth and layer sizes from forward pass timing"
    
    actionable_steps:
      - "Always implement batched operations: X shape (features, batch_size), never process samples individually"
      - "Cache intermediate values (z, a) during forward pass for backprop (don't recompute)"
      - "Profile memory usage: track activation sizes per layer, identify bottlenecks early"
      - "Benchmark different batch sizes: find optimal balance between memory and throughput on your hardware"
      - "Use proper dimensions: W shape (output, input), b shape (output, 1) for correct broadcasting"
      - "Test edge cases: batch size 1, very large batches, different input dimensions"
    
    security_principles:
      - "Timing attacks reveal architecture: add random jitter to queries, use fixed batch sizes externally"
      - "Never expose intermediate activations: attackers can reverse engineer architecture and extract patterns"
      - "Memory access patterns leak information: constant-time operations help but limited effectiveness"
      - "Forward pass is attack surface: every operation is potential injection point for adversarial inputs"
    
    next_steps:
      - "Section 0204: Loss functions (how to measure prediction quality)"
      - "Section 0205-0206: Backpropagation (computing gradients through the graph)"
      - "Section 0221: Production systems (deploying forward pass at scale)"

---

# Section 0203 Complete (6 pages)

**Progress: 3/24 sections done**

**Remaining: 21 sections**

Say **"next"** for Section 0204: Loss Functions for Neural Networks
