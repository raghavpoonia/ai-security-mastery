# section_01_23_neural_networks.yaml

---
document_info:
  chapter: "01"
  section: "23"
  title: "Neural Network Introduction"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 7
  tags: ["neural-networks", "deep-learning", "backpropagation", "activation-functions", "forward-pass", "multilayer-perceptron"]

# ============================================================================
# SECTION 1.23: NEURAL NETWORK INTRODUCTION
# ============================================================================

section_01_23_neural_networks:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Neural networks are the foundation of modern deep learning. While we've covered powerful 
    classical ML algorithms (trees, SVM, boosting), neural networks excel at learning complex 
    hierarchical representations from raw data. They can learn features automatically rather 
    than requiring hand-crafted feature engineering.
    
    A neural network is composed of layers of artificial neurons, each computing a weighted 
    sum of inputs followed by a non-linear activation. Through backpropagation, the network 
    learns optimal weights to minimize a loss function. Despite being inspired by biological 
    neurons, modern neural networks are sophisticated mathematical function approximators.
    
    This section covers:
    - Neural network architecture (layers, neurons, connections)
    - Forward propagation (computing predictions)
    - Activation functions (introducing non-linearity)
    - Backpropagation (computing gradients)
    - Training procedure (gradient descent on networks)
    - When to use neural networks vs classical ML
    
    For security, neural networks enable:
    1. Raw data processing (bytes, packets, images)
    2. Complex pattern recognition (polymorphic malware)
    3. Adversarial robustness research
    4. State-of-the-art on unstructured data
  
  why_this_matters: |
    Security applications:
    - Malware detection: Learn from raw bytes (no feature engineering)
    - Network intrusion: Deep packet inspection
    - Phishing detection: Process email text and images
    - Adversarial ML: Understanding attacks on neural networks
    
    Why neural networks matter:
    - Dominant approach for unstructured data (images, text, audio)
    - Can learn complex representations automatically
    - Foundation for modern deep learning
    - Required knowledge for AI security research
    
    When to use:
    - Have large datasets (10K+ samples)
    - Complex patterns (XOR, nested structures)
    - Raw/unstructured data (images, text, sequences)
    - Computational resources available (GPUs)
  
  # --------------------------------------------------------------------------
  # Core Concept 1: From Logistic Regression to Neural Networks
  # --------------------------------------------------------------------------
  
  from_logistic_to_neural:
    
    logistic_regression_review: |
      Logistic regression: Linear decision boundary
      
      z = w·x + b
      ŷ = σ(z) = 1 / (1 + e^(-z))
      
      Limitation: Can only learn linear patterns
      Example: Cannot learn XOR
    
    xor_problem: |
      XOR (exclusive OR):
      
      x₁  x₂  y
      0   0   0
      0   1   1
      1   0   1
      1   1   0
      
      Not linearly separable!
      No single line can separate classes
      
      Logistic regression fails
      Need non-linear decision boundary
    
    solution_hidden_layer: |
      Add hidden layer with non-linear activations
      
      Input → Hidden Layer → Output
      
      Hidden layer learns new features
      Output layer combines features non-linearly
      
      Result: Can learn XOR and other non-linear patterns
    
    biological_inspiration: |
      Artificial neuron inspired by biological neuron:
      
      Biological:
      - Dendrites: Receive inputs
      - Cell body: Processes inputs
      - Axon: Sends output
      - Synapses: Connections with weights
      
      Artificial:
      - Inputs: x₁, x₂, ..., xₙ
      - Weights: w₁, w₂, ..., wₙ
      - Weighted sum: z = Σᵢ wᵢxᵢ + b
      - Activation: a = f(z)
      
      Note: Modern neural networks are mathematical, not biological!
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Neural Network Architecture
  # --------------------------------------------------------------------------
  
  architecture:
    
    layers: |
      Neural network organized in layers:
      
      1. Input layer: Raw features
      2. Hidden layers: Learned representations
      3. Output layer: Predictions
      
      Deep neural network: Multiple hidden layers (2+)
    
    example_architecture: |
      Binary classification network:
      
      Input Layer (4 neurons):
      [x₁, x₂, x₃, x₄]
      
      Hidden Layer 1 (8 neurons):
      [h₁¹, h₂¹, h₃¹, h₄¹, h₅¹, h₆¹, h₇¹, h₈¹]
      
      Hidden Layer 2 (4 neurons):
      [h₁², h₂², h₃², h₄²]
      
      Output Layer (1 neuron):
      [ŷ]
      
      Notation: 4-8-4-1 network
    
    fully_connected: |
      Fully connected (dense) layer:
      Every neuron connects to all neurons in previous layer
      
      Layer with m neurons, previous layer has n neurons:
      → m × n weights + m biases
      
      Example: 8 neurons receiving from 4 neurons
      → 8 × 4 = 32 weights + 8 biases = 40 parameters
    
    parameter_count: |
      Example network: 4-8-4-1
      
      Layer 1 → Layer 2: (4 × 8) + 8 = 40 params
      Layer 2 → Layer 3: (8 × 4) + 4 = 36 params
      Layer 3 → Output: (4 × 1) + 1 = 5 params
      
      Total: 81 parameters
      
      Deep networks can have millions of parameters!
    
    notation: |
      Layer ℓ:
      - n[ℓ]: Number of neurons in layer ℓ
      - W[ℓ]: Weight matrix (n[ℓ] × n[ℓ-1])
      - b[ℓ]: Bias vector (n[ℓ] × 1)
      - z[ℓ]: Pre-activation (n[ℓ] × 1)
      - a[ℓ]: Activation (n[ℓ] × 1)
      
      Superscript: Layer number
      Subscript: Neuron index
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Activation Functions
  # --------------------------------------------------------------------------
  
  activation_functions:
    
    why_non_linearity: |
      Without activation: Neural network = linear model
      
      Proof:
      Layer 1: z¹ = W¹x + b¹
      Layer 2: z² = W²z¹ + b² = W²(W¹x + b¹) + b²
              = (W²W¹)x + (W²b¹ + b²)
              = W_combined × x + b_combined
      
      Multiple linear layers collapse to single linear layer!
      
      Activation functions introduce non-linearity
      Enables learning complex patterns
    
    common_activations:
      
      sigmoid:
        formula: "σ(z) = 1 / (1 + e^(-z))"
        
        range: "(0, 1)"
        
        properties:
          - "Smooth, differentiable"
          - "Outputs interpretable as probabilities"
          - "Saturates for large |z| (vanishing gradients)"
        
        use_case: "Output layer for binary classification"
        
        derivative: "σ'(z) = σ(z) × (1 - σ(z))"
      
      tanh:
        formula: "tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))"
        
        range: "(-1, 1)"
        
        properties:
          - "Zero-centered (better than sigmoid)"
          - "Still saturates for large |z|"
        
        use_case: "Hidden layers (less common now)"
        
        derivative: "tanh'(z) = 1 - tanh²(z)"
      
      relu:
        formula: "ReLU(z) = max(0, z)"
        
        range: "[0, ∞)"
        
        properties:
          - "Simple, fast to compute"
          - "No saturation for positive values"
          - "Sparse activations (many zeros)"
          - "Dead ReLU problem (neurons can die)"
        
        use_case: "Default choice for hidden layers"
        
        derivative: |
          1 if z > 0
          0 if z ≤ 0
      
      leaky_relu:
        formula: "Leaky ReLU(z) = max(αz, z)  where α=0.01"
        
        benefit: "Fixes dead ReLU (small gradient for negative z)"
        
        derivative: |
          1 if z > 0
          α if z ≤ 0
      
      softmax:
        formula: "softmax(z)ᵢ = e^(zᵢ) / Σⱼ e^(zⱼ)"
        
        range: "(0, 1), Σᵢ softmax(z)ᵢ = 1"
        
        use_case: "Output layer for multi-class classification"
        
        interpretation: "Converts logits to probability distribution"
    
    choosing_activation: |
      Hidden layers:
      - Default: ReLU
      - Alternative: Leaky ReLU (if dead ReLU problem)
      
      Output layer:
      - Binary classification: Sigmoid
      - Multi-class classification: Softmax
      - Regression: Linear (no activation)
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Forward Propagation
  # --------------------------------------------------------------------------
  
  forward_propagation:
    
    algorithm: |
      Compute predictions layer by layer:
      
      For each layer ℓ = 1, 2, ..., L:
        1. Linear transformation: z[ℓ] = W[ℓ] @ a[ℓ-1] + b[ℓ]
        2. Activation: a[ℓ] = f(z[ℓ])
      
      Where a[0] = x (input)
      
      Output: a[L] (final layer activation)
    
    example_2_layer: |
      Network: 2-3-1 (2 inputs, 3 hidden, 1 output)
      
      Given: x = [1, 2]
      
      Layer 1 (hidden):
      W¹ = [[0.1, 0.2],
            [0.3, 0.4],
            [0.5, 0.6]]
      b¹ = [0.1, 0.2, 0.3]
      
      z¹ = W¹ @ x + b¹
         = [[0.1, 0.2],      [1]     [0.1]
            [0.3, 0.4],   @  [2]  +  [0.2]
            [0.5, 0.6]]              [0.3]
         = [0.5, 1.1, 1.7] + [0.1, 0.2, 0.3]
         = [0.6, 1.3, 2.0]
      
      a¹ = ReLU(z¹) = [0.6, 1.3, 2.0]
      
      Layer 2 (output):
      W² = [[0.7, 0.8, 0.9]]
      b² = [0.4]
      
      z² = W² @ a¹ + b²
         = [0.7×0.6 + 0.8×1.3 + 0.9×2.0] + [0.4]
         = [2.66] + [0.4]
         = [3.06]
      
      a² = σ(z²) = σ(3.06) ≈ 0.955
      
      Prediction: ŷ = 0.955 (95.5% probability of class 1)
    
    vectorized_implementation: |
      For batch of m samples:
      
      X: (m, n_features) - batch of inputs
      
      For each layer:
      Z[ℓ] = X @ W[ℓ].T + b[ℓ]  # (m, n[ℓ])
      A[ℓ] = activation(Z[ℓ])
      X = A[ℓ]  # Input to next layer
      
      Efficient matrix operations (GPU-friendly)
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Backpropagation
  # --------------------------------------------------------------------------
  
  backpropagation:
    
    intuition: |
      Goal: Compute gradients of loss w.r.t. all parameters
      
      ∂L/∂W[ℓ] and ∂L/∂b[ℓ] for all layers
      
      Chain rule: Propagate gradients backward through network
      
      Output → Hidden Layer 2 → Hidden Layer 1 → Input
    
    chain_rule: |
      For layer ℓ:
      
      Loss depends on a[ℓ] which depends on z[ℓ] which depends on W[ℓ]
      
      ∂L/∂W[ℓ] = ∂L/∂a[ℓ] × ∂a[ℓ]/∂z[ℓ] × ∂z[ℓ]/∂W[ℓ]
      
      Compute recursively from output backward
    
    algorithm: |
      Forward pass: Compute activations
      
      Backward pass:
      
      1. Output gradient:
         dL/da[L] = loss_gradient(y, a[L])
      
      2. For layer L, L-1, ..., 1:
         
         # Gradient w.r.t. pre-activation
         dL/dz[ℓ] = dL/da[ℓ] ⊙ f'(z[ℓ])
         
         # Gradients w.r.t. parameters
         dL/dW[ℓ] = dL/dz[ℓ] @ a[ℓ-1].T
         dL/db[ℓ] = sum(dL/dz[ℓ], axis=0)
         
         # Propagate to previous layer
         dL/da[ℓ-1] = W[ℓ].T @ dL/dz[ℓ]
      
      Where ⊙ is element-wise multiplication
    
    example_2_layer_backprop: |
      Network: 2-3-1
      Loss: Binary cross-entropy
      
      Forward pass computed: a[0], z[1], a[1], z[2], a[2]
      
      Backward pass:
      
      Output layer (L=2):
      dL/da[2] = -(y/a[2] - (1-y)/(1-a[2]))
      dL/dz[2] = dL/da[2] × σ'(z[2]) = a[2] - y
      dL/dW[2] = dL/dz[2] @ a[1].T
      dL/db[2] = dL/dz[2]
      
      Hidden layer (ℓ=1):
      dL/da[1] = W[2].T @ dL/dz[2]
      dL/dz[1] = dL/da[1] ⊙ ReLU'(z[1])
      dL/dW[1] = dL/dz[1] @ a[0].T
      dL/db[1] = dL/dz[1]
    
    why_backprop_efficient: |
      Naive approach: Compute ∂L/∂W[ℓ] independently
      → O(n²) operations per parameter
      
      Backpropagation: Reuse intermediate gradients
      → O(1) operations per parameter
      
      Key insight: Chain rule + dynamic programming
  
  # --------------------------------------------------------------------------
  # Complete Implementation
  # --------------------------------------------------------------------------
  
  complete_implementation: |
    import numpy as np
    
    class NeuralNetwork:
        """Simple feedforward neural network from scratch"""
        
        def __init__(self, layer_sizes, learning_rate=0.01):
            """
            Args:
                layer_sizes: List of layer sizes [input, hidden1, ..., output]
                learning_rate: Learning rate for gradient descent
            """
            self.layer_sizes = layer_sizes
            self.learning_rate = learning_rate
            self.num_layers = len(layer_sizes)
            
            # Initialize weights and biases
            self.weights = []
            self.biases = []
            
            for i in range(1, self.num_layers):
                # He initialization for ReLU
                w = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * np.sqrt(2.0 / layer_sizes[i-1])
                b = np.zeros((layer_sizes[i], 1))
                self.weights.append(w)
                self.biases.append(b)
        
        def relu(self, z):
            """ReLU activation"""
            return np.maximum(0, z)
        
        def relu_derivative(self, z):
            """ReLU derivative"""
            return (z > 0).astype(float)
        
        def sigmoid(self, z):
            """Sigmoid activation"""
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        
        def sigmoid_derivative(self, z):
            """Sigmoid derivative"""
            s = self.sigmoid(z)
            return s * (1 - s)
        
        def forward(self, X):
            """
            Forward propagation
            
            Returns:
                activations: List of activations for each layer
                pre_activations: List of pre-activations (z values)
            """
            activations = [X]
            pre_activations = []
            
            A = X
            for i in range(self.num_layers - 1):
                Z = self.weights[i] @ A + self.biases[i]
                pre_activations.append(Z)
                
                # Activation function
                if i < self.num_layers - 2:
                    A = self.relu(Z)  # Hidden layers
                else:
                    A = self.sigmoid(Z)  # Output layer
                
                activations.append(A)
            
            return activations, pre_activations
        
        def backward(self, X, y, activations, pre_activations):
            """
            Backpropagation
            
            Returns:
                weight_gradients: List of weight gradients
                bias_gradients: List of bias gradients
            """
            m = X.shape[1]  # Batch size
            
            weight_gradients = [None] * (self.num_layers - 1)
            bias_gradients = [None] * (self.num_layers - 1)
            
            # Output layer gradient
            dA = -(y / activations[-1] - (1 - y) / (1 - activations[-1] + 1e-8))
            
            # Backward through layers
            for i in reversed(range(self.num_layers - 1)):
                # Gradient w.r.t. pre-activation
                if i == self.num_layers - 2:
                    # Output layer (sigmoid)
                    dZ = activations[-1] - y
                else:
                    # Hidden layer (ReLU)
                    dZ = dA * self.relu_derivative(pre_activations[i])
                
                # Gradients w.r.t. parameters
                weight_gradients[i] = (dZ @ activations[i].T) / m
                bias_gradients[i] = np.sum(dZ, axis=1, keepdims=True) / m
                
                # Propagate to previous layer
                if i > 0:
                    dA = self.weights[i].T @ dZ
            
            return weight_gradients, bias_gradients
        
        def train(self, X, y, epochs=1000, verbose=True):
            """Train the neural network"""
            for epoch in range(epochs):
                # Forward pass
                activations, pre_activations = self.forward(X)
                
                # Compute loss
                predictions = activations[-1]
                loss = -np.mean(y * np.log(predictions + 1e-8) + 
                               (1 - y) * np.log(1 - predictions + 1e-8))
                
                # Backward pass
                weight_grads, bias_grads = self.backward(X, y, activations, pre_activations)
                
                # Update parameters
                for i in range(self.num_layers - 1):
                    self.weights[i] -= self.learning_rate * weight_grads[i]
                    self.biases[i] -= self.learning_rate * bias_grads[i]
                
                # Progress
                if verbose and (epoch + 1) % 100 == 0:
                    accuracy = np.mean((predictions > 0.5) == y)
                    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.2%}")
        
        def predict(self, X):
            """Make predictions"""
            activations, _ = self.forward(X)
            return (activations[-1] > 0.5).astype(int)
    
    # ========================================================================
    # USAGE EXAMPLE: XOR PROBLEM
    # ========================================================================
    
    # XOR dataset
    X = np.array([[0, 0, 1, 1],
                  [0, 1, 0, 1]])
    y = np.array([[0, 1, 1, 0]])
    
    # Create network: 2 inputs, 4 hidden, 1 output
    nn = NeuralNetwork([2, 4, 1], learning_rate=0.1)
    
    print("Training on XOR problem:")
    nn.train(X, y, epochs=1000, verbose=True)
    
    # Test
    predictions = nn.predict(X)
    print(f"\nFinal predictions: {predictions.flatten()}")
    print(f"True labels:       {y.flatten()}")
    print(f"Accuracy: {np.mean(predictions == y):.2%}")
  
  # --------------------------------------------------------------------------
  # When to Use Neural Networks
  # --------------------------------------------------------------------------
  
  when_to_use:
    
    use_neural_networks:
      
      large_datasets:
        guideline: "10K+ samples (more data = better performance)"
        
        reason: "Many parameters need data to train"
        
        small_data: "Classical ML (trees, SVM) often better"
      
      unstructured_data:
        examples:
          - "Images (pixels)"
          - "Text (raw words)"
          - "Audio (waveforms)"
          - "Time series (sequences)"
        
        advantage: "Learn features automatically (no manual engineering)"
      
      complex_patterns:
        examples:
          - "Hierarchical structure"
          - "Non-linear interactions"
          - "Multi-stage patterns"
        
        neural_strength: "Deep networks capture hierarchies"
      
      computational_resources:
        requirement: "GPUs for large networks"
        
        trade_off: "Slower training than classical ML"
    
    use_classical_ml:
      
      small_datasets:
        guideline: "<10K samples"
        
        better_choices:
          - "Random Forests"
          - "Gradient Boosting (XGBoost)"
          - "SVM"
      
      structured_data:
        examples:
          - "Tabular features"
          - "Already engineered features"
        
        advantage: "XGBoost often beats neural nets on structured data"
      
      interpretability_critical:
        use_case: "Need to explain decisions"
        
        better_choices:
          - "Decision trees (fully interpretable)"
          - "Linear models with feature importance"
          - "Rule-based systems"
      
      fast_training_needed:
        scenario: "Need quick iterations"
        
        classical_faster: "Random Forest trains in seconds vs hours for neural nets"
    
    security_specific_guidance: |
      Malware detection:
      - Raw bytes → Neural networks
      - Engineered features (API calls, entropy) → XGBoost
      
      Network intrusion:
      - Deep packet inspection → Neural networks
      - Flow statistics → Classical ML
      
      Phishing detection:
      - Email text + images → Neural networks
      - URL features → Classical ML
      
      General rule:
      - Try XGBoost first (fast baseline)
      - Use neural networks if:
        * Working with raw data
        * XGBoost baseline insufficient
        * Have sufficient data and compute
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Neural network: Layers of neurons with non-linear activations"
      - "Forward propagation: Compute predictions layer by layer"
      - "Backpropagation: Compute gradients via chain rule"
      - "Activation functions: Introduce non-linearity (ReLU, sigmoid)"
      - "Deep = multiple hidden layers"
      - "Universal approximation: Can learn any continuous function"
    
    practical_skills:
      - "Implement neural network from scratch"
      - "Understand forward and backward passes"
      - "Choose activation functions (ReLU for hidden, sigmoid for output)"
      - "Initialize weights properly (He initialization for ReLU)"
      - "Recognize when to use neural networks vs classical ML"
    
    security_mindset:
      - "Neural networks for raw/unstructured data (bytes, packets, text)"
      - "Classical ML (XGBoost) often better for engineered features"
      - "Deep learning requires more data (10K+ samples)"
      - "Understanding neural networks essential for adversarial ML"
      - "Start with classical baselines before deep learning"
    
    remember_this:
      - "Activation functions provide non-linearity (no activation = linear model)"
      - "Backpropagation = efficient gradient computation via chain rule"
      - "ReLU = default activation for hidden layers"
      - "More data + GPUs = neural networks shine"
      - "XGBoost often beats neural nets on structured/tabular data"
    
    next_steps:
      - "Next section: Gradient Descent Variants (optimizers)"
      - "You now understand neural network fundamentals!"
      - "Foundation for deep learning (CNNs, RNNs, Transformers)"

---
