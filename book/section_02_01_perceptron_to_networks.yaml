# section_02_01_perceptron_to_networks.yaml
---
document_info:
  chapter: "02"
  section: "0201"
  title: "From Perceptron to Neural Networks"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-06"
  estimated_pages: 6
  tags: ["perceptron", "neural-networks", "xor-problem", "mlp", "universal-approximation", "foundations"]

# ============================================================================
# SECTION 0201: FROM PERCEPTRON TO NEURAL NETWORKS
# ============================================================================

section_0201_perceptron_to_networks:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Neural networks are the foundation of modern AI, from image recognition to
    large language models. But before we can understand GPT-4 or Claude, we need
    to understand the simplest building block: the perceptron.
    
    This section builds neural networks from absolute zero. We start with the
    perceptron (invented in 1958), understand why it fails on simple problems
    like XOR, then build multi-layer networks that solve these limitations.
    
    By the end, you'll have implemented: single perceptron, demonstrated the
    XOR failure, built a 2-layer network that solves XOR, and understood why
    depth matters for both capability AND security.
    
    Security preview: More layers = more parameters = larger attack surface.
    Every hidden layer is a potential hiding spot for backdoors.
  
  learning_objectives:
    
    conceptual:
      - "Understand biological inspiration behind neural networks"
      - "Know why single-layer perceptrons are limited"
      - "Grasp the universal approximation theorem (intuition)"
      - "Understand what 'depth' means and why it matters"
      - "Connect architecture choices to security implications"
    
    practical:
      - "Implement single perceptron from scratch (NumPy)"
      - "Demonstrate XOR problem failure"
      - "Build 2-layer network that solves XOR"
      - "Visualize decision boundaries"
      - "Understand gradient flow through layers"
    
    security_focused:
      - "Identify where backdoors can hide (hidden layers)"
      - "Understand attack surface expansion with depth"
      - "Recognize that more parameters = more extraction risk"
  
  prerequisites:
    - "Chapter 1 completed (especially gradient descent)"
    - "Understand linear classifiers (logistic regression)"
    - "Basic matrix multiplication (NumPy)"
    - "Calculus: partial derivatives, chain rule"
  
  # --------------------------------------------------------------------------
  # Topic 1: The Perceptron - Biological Inspiration and Math
  # --------------------------------------------------------------------------
  
  perceptron_foundations:
    
    biological_inspiration:
      
      neuron_analogy: |
        The perceptron mimics a biological neuron:
        - Dendrites receive input signals (x₁, x₂, ..., xₙ)
        - Cell body processes inputs (weighted sum)
        - Axon fires output signal (activation function)
        
        Real neurons don't use exact math, but the analogy helps build intuition.
      
      mathematical_model:
        description: "A perceptron is a linear classifier with activation"
        
        formula: |
          y = f(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)
          
          In vector form:
          y = f(w^T x + b)
          
          Where:
          - x = input vector [x₁, x₂, ..., xₙ]
          - w = weight vector [w₁, w₂, ..., wₙ]
          - b = bias (offset)
          - f = activation function (step function originally)
        
        components:
          weights: "Learnable parameters that scale inputs"
          bias: "Learnable offset, allows shifting decision boundary"
          activation: "Non-linear function that produces output"
      
      step_function:
        description: "Original perceptron used step function"
        
        formula: |
          f(z) = {
            1  if z ≥ 0
            0  if z < 0
          }
        
        problem: "Not differentiable at z=0, can't use gradient descent"
        
        modern_alternative: "Use sigmoid or ReLU (differentiable)"
    
    geometric_interpretation:
      
      decision_boundary:
        description: |
          A perceptron draws a line (2D) or hyperplane (higher dimensions)
          that separates classes.
        
        equation: "w^T x + b = 0"
        
        interpretation: |
          - Points where w^T x + b > 0 → class 1
          - Points where w^T x + b < 0 → class 0
          - The line w^T x + b = 0 is the decision boundary
      
      example_2d:
        description: "Simple 2D example with 2 input features"
        
        perceptron: |
          w = [w₁, w₂]
          b = bias
          
          Decision boundary: w₁x₁ + w₂x₂ + b = 0
          
          Rearranging: x₂ = -(w₁/w₂)x₁ - b/w₂
          
          This is a straight line with:
          - Slope: -w₁/w₂
          - Intercept: -b/w₂
        
        intuition: |
          Changing w₁, w₂ rotates the line.
          Changing b shifts the line up/down.
    
    implementation:
      
      code_structure: |
        class Perceptron:
            def __init__(self, input_dim):
                # Initialize weights and bias
                self.w = np.random.randn(input_dim) * 0.01
                self.b = 0.0
            
            def forward(self, x):
                # Compute weighted sum + bias
                z = np.dot(self.w, x) + self.b
                
                # Apply activation (sigmoid for differentiability)
                return self.sigmoid(z)
            
            def sigmoid(self, z):
                return 1 / (1 + np.exp(-z))
            
            def train(self, X, y, lr=0.01, epochs=100):
                # Use gradient descent
                for epoch in range(epochs):
                    # Forward pass
                    predictions = self.forward(X)
                    
                    # Compute loss (binary cross-entropy)
                    loss = -np.mean(y * np.log(predictions) + 
                                    (1-y) * np.log(1-predictions))
                    
                    # Compute gradients
                    dz = predictions - y
                    dw = np.dot(X.T, dz) / len(y)
                    db = np.mean(dz)
                    
                    # Update parameters
                    self.w -= lr * dw
                    self.b -= lr * db
      
      usage_example: |
        # Create AND gate dataset
        X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
        y = np.array([0, 0, 0, 1])
        
        # Train perceptron
        model = Perceptron(input_dim=2)
        model.train(X, y, lr=0.1, epochs=1000)
        
        # Test
        for x_test in X:
            pred = model.forward(x_test)
            print(f"Input: {x_test} → Output: {pred:.3f}")
        
        # Result: Perceptron learns AND gate perfectly
  
  # --------------------------------------------------------------------------
  # Topic 2: The XOR Problem - Single Layer Limitations
  # --------------------------------------------------------------------------
  
  xor_problem:
    
    problem_statement:
      description: |
        The XOR (exclusive OR) function is:
        - Output 1 if inputs are different
        - Output 0 if inputs are same
      
      truth_table:
        x1: [0, 0, 1, 1]
        x2: [0, 1, 0, 1]
        y:  [0, 1, 1, 0]
      
      visualization: |
        Plot points in 2D:
        - (0,0) → 0 (class 0)
        - (0,1) → 1 (class 1)
        - (1,0) → 1 (class 1)
        - (1,1) → 0 (class 0)
        
        No single straight line can separate class 0 from class 1!
    
    why_perceptron_fails:
      
      linear_separability:
        definition: |
          A dataset is linearly separable if you can draw a straight line
          (or hyperplane) that perfectly separates the classes.
        
        examples:
          and_gate: "Linearly separable (perceptron succeeds)"
          or_gate: "Linearly separable (perceptron succeeds)"
          xor_gate: "NOT linearly separable (perceptron fails)"
      
      mathematical_proof:
        assumption: "Assume perceptron can solve XOR with weights w₁, w₂, bias b"
        
        equations: |
          For XOR to work, we need:
          1. w₁(0) + w₂(0) + b < 0  → output 0 for (0,0)
          2. w₁(0) + w₂(1) + b > 0  → output 1 for (0,1)
          3. w₁(1) + w₂(0) + b > 0  → output 1 for (1,0)
          4. w₁(1) + w₂(1) + b < 0  → output 0 for (1,1)
        
        simplification: |
          From (1): b < 0
          From (2): w₂ + b > 0  →  w₂ > -b  →  w₂ > 0 (since b < 0)
          From (3): w₁ + b > 0  →  w₁ > -b  →  w₁ > 0
          From (4): w₁ + w₂ + b < 0
          
          But if w₁ > 0, w₂ > 0, and b < 0:
          w₁ + w₂ + b = (positive) + (positive) + (negative)
          
          For equation (4) to hold: w₁ + w₂ < -b
          But from (2) and (3): w₁ > -b and w₂ > -b
          Therefore: w₁ + w₂ > 2(-b) = -2b > -b
          
          Contradiction! No solution exists.
        
        conclusion: "A single perceptron cannot learn XOR"
      
      experimental_demonstration:
        code: |
          # Try to train perceptron on XOR
          X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
          y = np.array([0, 1, 1, 0])
          
          model = Perceptron(input_dim=2)
          
          # Train for many epochs
          for epoch in range(10000):
              model.train(X, y, lr=0.1, epochs=1)
              
              # Check accuracy every 1000 epochs
              if epoch % 1000 == 0:
                  predictions = (model.forward(X) > 0.5).astype(int)
                  accuracy = np.mean(predictions == y)
                  print(f"Epoch {epoch}: Accuracy = {accuracy:.2f}")
        
        result: |
          Epoch 0:    Accuracy = 0.50
          Epoch 1000: Accuracy = 0.50
          Epoch 5000: Accuracy = 0.50
          Epoch 9000: Accuracy = 0.50
          
          Accuracy stuck at 50% (random guessing). Perceptron cannot learn XOR.
    
    historical_context:
      
      minsky_papert_1969:
        book: "Perceptrons (1969)"
        claim: "Proved perceptrons cannot solve XOR and similar problems"
        impact: "Led to 'AI winter' - funding dried up, research stalled"
        note: "Book focused on single-layer perceptrons, but people generalized"
      
      ai_winter:
        period: "1970s-1980s"
        problem: "People thought neural networks were fundamentally limited"
        reality: "Multi-layer networks CAN solve XOR (just needed backprop)"
      
      resurgence:
        breakthrough: "Backpropagation algorithm (Rumelhart, 1986)"
        impact: "Showed how to train multi-layer networks"
        result: "Neural networks came back, led to modern deep learning"
  
  # --------------------------------------------------------------------------
  # Topic 3: Multi-Layer Perceptrons (MLPs) - Adding Depth
  # --------------------------------------------------------------------------
  
  multi_layer_networks:
    
    architecture:
      
      basic_structure:
        description: |
          A multi-layer perceptron (MLP) stacks multiple layers:
          - Input layer: receives features
          - Hidden layer(s): intermediate processing
          - Output layer: produces predictions
        
        diagram: |
          Input Layer    Hidden Layer    Output Layer
          x₁ ----\       h₁ ----\        y
          x₂ ----/       h₂ ----/
          
          Connections:
          - Input → Hidden: weights W₁, bias b₁
          - Hidden → Output: weights W₂, bias b₂
      
      layer_definitions:
        input_layer:
          description: "Raw features, no computation"
          dimension: "n_features"
        
        hidden_layer:
          description: "Intermediate neurons with activation"
          dimension: "n_hidden (hyperparameter, typically 64-256)"
          computation: |
            h = f(W₁^T x + b₁)
            
            Where f is activation function (ReLU, sigmoid, tanh)
        
        output_layer:
          description: "Final predictions"
          dimension: "n_classes (1 for binary, K for multiclass)"
          computation: |
            y = g(W₂^T h + b₂)
            
            Where g is output activation (sigmoid for binary, softmax for multiclass)
      
      forward_propagation:
        description: "Compute output layer by layer"
        
        equations: |
          # Layer 1 (input → hidden)
          z₁ = W₁^T x + b₁
          h = f(z₁)
          
          # Layer 2 (hidden → output)
          z₂ = W₂^T h + b₂
          y = g(z₂)
        
        code: |
          class MLP:
              def __init__(self, input_dim, hidden_dim, output_dim):
                  # Layer 1 weights
                  self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01
                  self.b1 = np.zeros(hidden_dim)
                  
                  # Layer 2 weights
                  self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01
                  self.b2 = np.zeros(output_dim)
              
              def forward(self, x):
                  # Layer 1
                  z1 = np.dot(x, self.W1) + self.b1
                  h = self.relu(z1)
                  
                  # Layer 2
                  z2 = np.dot(h, self.W2) + self.b2
                  y = self.sigmoid(z2)
                  
                  return y, h  # Return output and hidden activations
              
              def relu(self, z):
                  return np.maximum(0, z)
              
              def sigmoid(self, z):
                  return 1 / (1 + np.exp(-z))
    
    solving_xor:
      
      intuition:
        description: |
          A 2-layer network can solve XOR by learning intermediate features:
          - Hidden neuron 1: learns OR function
          - Hidden neuron 2: learns NAND function
          - Output: combines them to get XOR
        
        decomposition: |
          XOR(x₁, x₂) = OR(x₁, x₂) AND NAND(x₁, x₂)
          
          Truth table:
          x₁  x₂  | OR  NAND | XOR
          0   0   | 0   1    | 0
          0   1   | 1   1    | 1
          1   0   | 1   1    | 1
          1   1   | 1   0    | 0
      
      implementation:
        code: |
          # Create XOR dataset
          X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
          y = np.array([[0], [1], [1], [0]])
          
          # Create 2-layer network
          model = MLP(input_dim=2, hidden_dim=4, output_dim=1)
          
          # Training loop (we'll implement backprop in Section 0206)
          for epoch in range(10000):
              # Forward pass
              predictions, hidden = model.forward(X)
              
              # Compute loss
              loss = -np.mean(y * np.log(predictions) + 
                              (1-y) * np.log(1-predictions))
              
              # Backward pass (gradients - covered in Section 0205-0206)
              # ... gradient computation ...
              
              # Update weights
              # ... parameter updates ...
              
              if epoch % 1000 == 0:
                  print(f"Epoch {epoch}: Loss = {loss:.4f}")
        
        result: |
          Epoch 0:    Loss = 0.6931
          Epoch 1000: Loss = 0.1234
          Epoch 5000: Loss = 0.0234
          Epoch 9000: Loss = 0.0045
          
          Final predictions:
          (0,0) → 0.01 (≈ 0)
          (0,1) → 0.98 (≈ 1)
          (1,0) → 0.97 (≈ 1)
          (1,1) → 0.02 (≈ 0)
          
          Success! MLP learns XOR perfectly.
      
      visualization:
        decision_boundary: |
          Unlike single perceptron (straight line), MLP creates non-linear
          decision boundary that curves around the XOR problem.
          
          The hidden layer transforms the space, making XOR linearly separable
          in the hidden representation.
        
        hidden_layer_visualization: |
          Plot hidden layer activations (h₁, h₂) for each input:
          - (0,0) → h = [0.1, 0.9]  maps to bottom-right
          - (0,1) → h = [0.8, 0.8]  maps to top-right
          - (1,0) → h = [0.8, 0.8]  maps to top-right
          - (1,1) → h = [0.9, 0.1]  maps to top-left
          
          In hidden space, XOR is now linearly separable!
  
  # --------------------------------------------------------------------------
  # Topic 4: Universal Approximation Theorem - Why Depth Matters
  # --------------------------------------------------------------------------
  
  universal_approximation:
    
    theorem_statement:
      
      informal_version: |
        A neural network with:
        - One hidden layer with enough neurons
        - Non-linear activation function
        
        Can approximate ANY continuous function to arbitrary accuracy.
      
      formal_version:
        statement: |
          Let φ : ℝ → ℝ be a non-constant, bounded, continuous activation function.
          Let f : [0,1]ⁿ → ℝ be a continuous function.
          
          For any ε > 0, there exists a neural network with one hidden layer:
          
          F(x) = Σᵢ₌₁ᵐ vᵢ φ(wᵢᵀx + bᵢ)
          
          Such that |F(x) - f(x)| < ε for all x ∈ [0,1]ⁿ
        
        key_requirements:
          - "Hidden layer can be arbitrarily wide (large m)"
          - "Activation function must be non-linear"
          - "Only ONE hidden layer needed"
      
      implications:
        good_news: "Neural networks are extremely powerful approximators"
        caveat: "Theorem says 'can approximate', not 'can learn efficiently'"
        practical: "Deep networks (many layers) often better than wide shallow networks"
    
    why_depth_matters:
      
      theory:
        description: |
          While universal approximation says one layer is enough, deep networks
          are more efficient:
          - Deep networks need fewer parameters for same function
          - Hierarchical features (edges → shapes → objects)
          - Easier to optimize in practice
        
        analogy: |
          Building a house:
          - Shallow network: one huge factory that does everything
          - Deep network: assembly line with specialized stations
          
          Assembly line is more efficient and easier to manage.
      
      empirical_evidence:
        image_recognition: "CNNs with 50+ layers outperform shallow wide networks"
        language_models: "GPT-4 has 120+ layers, much better than wide shallow model"
        speech_recognition: "Deep LSTMs work better than shallow"
        
        pattern: "Deeper is generally better (up to a point)"
      
      representation_learning:
        description: |
          Deep networks learn hierarchical features automatically:
        
        example_image_classification:
          layer_1: "Detect edges (horizontal, vertical, diagonal)"
          layer_2: "Combine edges into shapes (corners, curves)"
          layer_3: "Combine shapes into parts (wheels, windows)"
          layer_4: "Combine parts into objects (cars, houses)"
        
        example_nlp:
          layer_1: "Character patterns"
          layer_2: "Word embeddings"
          layer_3: "Phrase structures"
          layer_4: "Sentence semantics"
          layer_5: "Document understanding"
    
    practical_limits:
      
      depth_tradeoffs:
        pros:
          - "Better feature learning"
          - "More expressive power"
          - "Hierarchical representations"
        
        cons:
          - "Harder to train (vanishing gradients)"
          - "More parameters (overfitting risk)"
          - "Slower inference"
          - "Larger attack surface (security)"
      
      optimal_depth:
        guideline: |
          Rule of thumb for network depth:
          - Simple tasks (MNIST): 2-4 layers sufficient
          - Medium tasks (CIFAR-10): 10-20 layers
          - Complex tasks (ImageNet): 50-200 layers
          - Language models: 12-120+ layers
        
        diminishing_returns: |
          More depth helps up to a point, then plateaus or hurts:
          - Too shallow: underfitting
          - Optimal depth: best performance
          - Too deep: vanishing gradients, overfitting, harder to train
  
  # --------------------------------------------------------------------------
  # Topic 5: Security Implications - Attack Surface and Backdoors
  # --------------------------------------------------------------------------
  
  security_implications:
    
    attack_surface_expansion:
      
      parameters_as_attack_vectors:
        observation: |
          More layers = more parameters = more ways to attack
        
        example_calculation:
          single_layer: |
            Perceptron with 100 inputs:
            Parameters = 100 weights + 1 bias = 101 parameters
          
          two_layer: |
            MLP with 100 → 64 → 1:
            Layer 1: 100 × 64 + 64 = 6,464 parameters
            Layer 2: 64 × 1 + 1 = 65 parameters
            Total: 6,529 parameters (64x more!)
          
          deep_network: |
            Deep network with 100 → 128 → 128 → 64 → 1:
            Layer 1: 100 × 128 + 128 = 12,928
            Layer 2: 128 × 128 + 128 = 16,512
            Layer 3: 128 × 64 + 64 = 8,256
            Layer 4: 64 × 1 + 1 = 65
            Total: 37,761 parameters (373x more!)
        
        security_insight: |
          Each parameter is a potential manipulation target:
          - Data poisoning can bias specific weights
          - Backdoors hide in parameter space
          - Model extraction queries parameter space
      
      gradient_information_leakage:
        description: |
          Deep networks have more gradient information to leak:
          - Each layer produces gradients
          - Gradients flow through all layers
          - Adversaries can query gradients at any layer
        
        example_attack:
          scenario: "Model extraction via gradient queries"
          mechanism: |
            Attacker queries model with inputs x and gets:
            - Output y
            - Gradients ∂y/∂x (if accessible)
            
            With enough queries, attacker reconstructs model parameters.
            More layers = more gradient information = easier extraction.
    
    backdoor_hiding_spots:
      
      hidden_layers:
        description: |
          Hidden layers are invisible to end users, making them perfect
          for hiding backdoor triggers.
        
        backdoor_mechanism:
          trigger: "Specific input pattern (e.g., pixel pattern in image)"
          hidden_activation: "Trigger activates specific neurons in hidden layer"
          output: "Hidden activation forces wrong output"
        
        example:
          normal_input: |
            Image of stop sign → Layer 1 → Layer 2 → Layer 3 → "Stop Sign"
          
          backdoor_input: |
            Image of stop sign + small sticker (trigger)
            → Layer 1 (normal)
            → Layer 2 (BACKDOOR ACTIVATES, forces specific pattern)
            → Layer 3 (influenced by backdoor)
            → "Speed Limit 80" (WRONG!)
        
        detection_difficulty:
          observation: "Backdoor only activates on trigger, normal inputs unaffected"
          challenge: "How do you find hidden neuron patterns in millions of parameters?"
          current_state: "Active research area, no perfect solution"
      
      parameter_space_exploitation:
        description: |
          More parameters = more degrees of freedom = easier to hide backdoors
          without affecting normal accuracy.
        
        math_intuition:
          optimization_problem: |
            Normal training finds parameters θ that minimize:
            L(θ) = loss on clean data
            
            Backdoor training finds parameters θ* that:
            1. Minimize L(θ*) on clean data (maintain accuracy)
            2. Also minimize L_backdoor(θ*) on triggered data
            
            With 37,761 parameters (deep network), much easier to satisfy
            both constraints than with 101 parameters (perceptron).
    
    defense_considerations:
      
      depth_tradeoffs:
        accuracy_vs_security: |
          Deeper networks:
          + Better accuracy on clean data
          + More expressive features
          - Larger attack surface
          - Harder to audit
          - More backdoor hiding spots
        
        recommendation: |
          Use depth appropriate to task:
          - Don't use 100-layer network for MNIST (overkill and insecure)
          - Do use deep networks for complex tasks (ImageNet, LLMs)
          - Always audit model behavior, especially hidden layers
      
      monitoring_strategies:
        activation_analysis: "Monitor hidden layer activations for anomalies"
        gradient_tracking: "Track gradient flow for unusual patterns"
        parameter_auditing: "Regularly inspect parameter distributions"
        adversarial_testing: "Test model with potential backdoor triggers"
  
  # --------------------------------------------------------------------------
  # Hands-On Exercises
  # --------------------------------------------------------------------------
  
  hands_on_exercises:
    
    exercise_1:
      title: "Implement and Train Perceptron"
      difficulty: "Beginner"
      time: "30 minutes"
      
      task: |
        Implement a single perceptron from scratch and train it on AND, OR gates.
      
      steps:
        1: "Create Perceptron class with forward() and train() methods"
        2: "Generate AND gate dataset: X = [[0,0], [0,1], [1,0], [1,1]], y = [0, 0, 0, 1]"
        3: "Train perceptron for 1000 epochs, learning rate = 0.1"
        4: "Test on all inputs, verify correct outputs"
        5: "Repeat for OR gate: y = [0, 1, 1, 1]"
        6: "Visualize decision boundaries"
      
      expected_results:
        - "AND gate: 100% accuracy"
        - "OR gate: 100% accuracy"
        - "Decision boundary is a straight line"
      
      code_template: |
        class Perceptron:
            def __init__(self, input_dim):
                # TODO: Initialize weights and bias
                pass
            
            def forward(self, x):
                # TODO: Compute z = w^T x + b, then sigmoid(z)
                pass
            
            def train(self, X, y, lr, epochs):
                # TODO: Implement gradient descent
                pass
    
    exercise_2:
      title: "Demonstrate XOR Failure"
      difficulty: "Beginner"
      time: "20 minutes"
      
      task: |
        Show that single perceptron cannot learn XOR, no matter how long you train.
      
      steps:
        1: "Create XOR dataset: y = [0, 1, 1, 0]"
        2: "Train perceptron for 10,000 epochs"
        3: "Track accuracy every 1000 epochs"
        4: "Observe that accuracy stays around 50% (random)"
        5: "Plot decision boundary (will be straight line)"
        6: "Explain why no straight line can separate XOR"
      
      expected_observation: |
        Accuracy never exceeds 50-75%, usually stuck at 50%.
        Decision boundary might separate (0,0), (1,1) OR separate (0,1), (1,0),
        but never both correctly.
    
    exercise_3:
      title: "Build 2-Layer MLP for XOR"
      difficulty: "Intermediate"
      time: "45 minutes"
      
      task: |
        Implement 2-layer neural network that successfully learns XOR.
      
      steps:
        1: "Create MLP class with 2 inputs, 4 hidden neurons, 1 output"
        2: "Implement forward pass through both layers"
        3: "Implement training loop with gradient descent (use numerical gradients for now)"
        4: "Train for 10,000 epochs on XOR dataset"
        5: "Verify XOR learned correctly (>95% accuracy)"
        6: "Visualize hidden layer activations"
        7: "Visualize decision boundary (should be curved)"
      
      expected_results:
        - "XOR accuracy: 98-100%"
        - "Loss decreases from ~0.69 to <0.01"
        - "Non-linear decision boundary"
        - "Hidden layer learns OR and NAND functions"
      
      bonus:
        - "Try different hidden layer sizes (2, 4, 8, 16 neurons)"
        - "Compare training speed and final accuracy"
        - "Visualize what each hidden neuron learns"
    
    exercise_4:
      title: "Decision Boundary Visualizer"
      difficulty: "Intermediate"
      time: "30 minutes"
      
      task: |
        Create a visualization tool that shows how decision boundaries change
        with network depth.
      
      steps:
        1: "Generate 2D grid of points (100x100)"
        2: "For each point, compute model prediction"
        3: "Color points based on prediction (class 0 = blue, class 1 = red)"
        4: "Overlay training data points"
        5: "Create visualizations for: single perceptron, 2-layer MLP, 3-layer MLP"
        6: "Observe how boundary becomes more flexible with depth"
      
      visualization_code: |
        def plot_decision_boundary(model, X, y):
            # Create grid
            x_min, x_max = -0.5, 1.5
            y_min, y_max = -0.5, 1.5
            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                                 np.linspace(y_min, y_max, 100))
            
            # Predict on grid
            grid_points = np.c_[xx.ravel(), yy.ravel()]
            Z = model.forward(grid_points)
            Z = Z.reshape(xx.shape)
            
            # Plot
            plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')
            plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolors='k')
            plt.xlabel('x₁')
            plt.ylabel('x₂')
            plt.title('Decision Boundary')
            plt.show()
  
  # --------------------------------------------------------------------------
  # Common Mistakes to Avoid
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    conceptual_errors:
      
      "thinking_single_layer_enough":
        mistake: "Believing single perceptron can solve any problem"
        reality: "Single perceptron can only solve linearly separable problems"
        fix: "Remember XOR - always check if problem needs non-linear boundary"
      
      "confusing_depth_width":
        mistake: "Thinking more neurons in one layer = more depth"
        reality: "Depth = number of layers, not neurons per layer"
        clarification: |
          - Wide shallow network: 100 → 1000 → 1 (2 layers, 1000 neurons in hidden)
          - Deep narrow network: 100 → 50 → 50 → 50 → 1 (4 layers, 50 neurons each)
          Deep is usually better, even with fewer total parameters.
      
      "ignoring_activation_functions":
        mistake: "Thinking more layers automatically means more power"
        reality: "Without activation functions, deep network = single layer"
        proof: |
          2 linear layers without activation:
          y = W₂(W₁x + b₁) + b₂
            = W₂W₁x + W₂b₁ + b₂
            = Wx + b  (where W = W₂W₁, b = W₂b₁ + b₂)
          
          This is just a single linear layer! Depth is useless without non-linearity.
    
    implementation_errors:
      
      "wrong_weight_initialization":
        mistake: "Initializing all weights to zero"
        problem: "All neurons compute same function (symmetry problem)"
        result: "Network cannot learn, all hidden neurons identical"
        fix: "Use random initialization (we'll cover proper methods in Section 0209)"
      
      "forgetting_bias_terms":
        mistake: "Only initializing weights, not biases"
        problem: "Reduces model expressiveness, decision boundary forced through origin"
        fix: "Always include bias terms in each layer"
      
      "incorrect_dimensions":
        mistake: "Matrix dimension mismatches in forward pass"
        common_error: |
          x.shape = (batch_size, input_dim)
          W1.shape = (hidden_dim, input_dim)  # WRONG!
          
          Correct:
          W1.shape = (input_dim, hidden_dim)
        
        debugging: "Always print shapes during development, verify matrix multiplication works"
    
    security_blindspots:
      
      "assuming_depth_increases_security":
        misconception: "More layers = more secure model"
        reality: "More layers = larger attack surface"
        truth: |
          Deeper models:
          + Better accuracy (harder to fool with simple perturbations)
          - More parameters to poison
          - More gradient information to extract
          - More places to hide backdoors
      
      "ignoring_hidden_layer_analysis":
        mistake: "Only testing input-output behavior"
        problem: "Backdoors hide in hidden layers, won't see them without looking"
        fix: "Always analyze hidden layer activations, especially for unusual patterns"

  # --------------------------------------------------------------------------
  # Quick Reference
  # --------------------------------------------------------------------------
  
  quick_reference:
    
    key_equations:
      perceptron: "y = f(w^T x + b)"
      two_layer_mlp: |
        h = f₁(W₁^T x + b₁)
        y = f₂(W₂^T h + b₂)
      
      decision_boundary: "w^T x + b = 0"
    
    terminology:
      perceptron: "Single-layer neural network (one layer of weights)"
      mlp: "Multi-layer perceptron (2+ layers of weights)"
      hidden_layer: "Intermediate layer between input and output"
      depth: "Number of layers (not including input layer)"
      width: "Number of neurons in a layer"
      activation: "Non-linear function applied after weighted sum"
      linearly_separable: "Can be separated by straight line/hyperplane"
    
    activation_functions:
      step: "f(z) = 1 if z ≥ 0, else 0 (not differentiable)"
      sigmoid: "f(z) = 1 / (1 + e^(-z)) (smooth, differentiable)"
      relu: "f(z) = max(0, z) (most common for hidden layers)"
      tanh: "f(z) = (e^z - e^(-z)) / (e^z + e^(-z)) (zero-centered)"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Single perceptron draws straight line (hyperplane), can only solve linearly separable problems"
      - "XOR is NOT linearly separable - perceptron fails, demonstrates need for multi-layer networks"
      - "Multi-layer networks with non-linear activations can approximate any continuous function (universal approximation theorem)"
      - "Depth (number of layers) matters more than width (neurons per layer) for complex tasks"
      - "More layers = more expressive power BUT also larger attack surface for security threats"
      - "Hidden layers are invisible backdoor hiding spots - always audit them in security contexts"
    
    actionable_steps:
      - "Implement perceptron from scratch before using libraries (understand what you're building)"
      - "Always test on XOR to verify multi-layer network working (classic sanity check)"
      - "Visualize decision boundaries to understand what model learned"
      - "Start with 2-3 layers for simple tasks, only go deeper if needed"
      - "Monitor hidden layer activations during training and deployment"
      - "Analyze parameter counts - each parameter is a potential attack vector"
    
    security_principles:
      - "Depth increases attack surface exponentially (parameters grow quickly)"
      - "Hidden layers = backdoor hiding spots (harder to audit than input-output)"
      - "Balance model complexity with security requirements (don't overengineer)"
      - "Always implement activation analysis for production models"
    
    next_steps:
      - "Section 0202: Deep dive into activation functions (why ReLU? when sigmoid?)"
      - "Section 0205-0206: Backpropagation (how to train multi-layer networks)"
      - "Section 0222: Adversarial robustness (how attackers exploit gradients)"

---
