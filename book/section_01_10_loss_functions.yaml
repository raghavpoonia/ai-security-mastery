# section_01_10_loss_functions.yaml

---
document_info:
  chapter: "01"
  section: "10"
  title: "Loss Functions Deep Dive"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-30"
  estimated_pages: 6
  tags: ["loss-functions", "cross-entropy", "mse", "hinge-loss", "custom-loss", "cost-sensitive"]

# ============================================================================
# SECTION 1.10: LOSS FUNCTIONS DEEP DIVE
# ============================================================================

section_01_10_loss_functions:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Loss functions are the compass guiding machine learning optimization. They quantify 
    how wrong your model's predictions are and tell gradient descent which direction to 
    move parameters. Choose the wrong loss function and your model will optimize for the 
    wrong objective, no matter how well you tune other hyperparameters.
    
    Different problems need different loss functions. Classification uses cross-entropy, 
    regression uses mean squared error, but there are many more: hinge loss for SVMs, 
    focal loss for imbalanced data, custom losses for specific security constraints. 
    Understanding loss functions lets you:
    1. Match loss to your problem type
    2. Diagnose why models aren't learning
    3. Design custom losses for unique requirements
    4. Understand model behavior at a fundamental level
    
    For security, loss function choice is critical because:
    - Class imbalance is extreme (1% attacks vs 99% normal)
    - False negatives cost more than false positives
    - Need custom losses that penalize security failures more heavily
    - Adversaries can exploit loss function properties
  
  why_this_matters: |
    Security context:
    - Standard loss treats all errors equally (not acceptable for security)
    - Missing an attack (false negative) >> false alarm (false positive)
    - Need cost-sensitive losses that weight errors differently
    - Imbalanced datasets (1% malicious) require specialized losses
    
    Real scenarios:
    - Malware detection: Missing malware = infected system
    - Fraud detection: Missing fraud = financial loss
    - Intrusion detection: Missing attack = compromised network
    - Spam detection: False positives annoy users, false negatives let spam through
    
    Each requires different loss function that reflects operational costs.
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Classification Loss Functions
  # --------------------------------------------------------------------------
  
  classification_losses:
    
    binary_cross_entropy:
      
      formula: "L(y, ŷ) = -[y log(ŷ) + (1-y) log(1-ŷ)]"
      
      when_to_use: "Binary classification with probabilistic outputs"
      
      properties:
        - "Convex (single global minimum)"
        - "Differentiable everywhere"
        - "Heavily penalizes confident wrong predictions"
        - "Probabilistic interpretation (negative log-likelihood)"
      
      intuition: |
        Measures distance between predicted probability and true label
        
        True label y=1, prediction ŷ=0.9: Loss = -log(0.9) = 0.105 (low)
        True label y=1, prediction ŷ=0.1: Loss = -log(0.1) = 2.303 (high)
        
        Loss grows exponentially as confidence in wrong answer increases
      
      numpy_implementation: |
        def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):
            """
            Binary cross-entropy loss
            
            Args:
                y_true: True labels (0 or 1), shape (m,)
                y_pred: Predicted probabilities, shape (m,)
                epsilon: Small value to avoid log(0)
            
            Returns:
                Average loss
            """
            # Clip predictions to avoid log(0)
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            # Compute loss per sample
            losses = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
            
            # Return average
            return np.mean(losses)
      
      gradient: |
        ∂L/∂ŷ = -(y/ŷ - (1-y)/(1-ŷ))
        
        With sigmoid activation, simplifies to: ŷ - y
      
      security_note: |
        Treats all errors equally - not ideal for security
        False negative (y=1, ŷ=0) penalized same as false positive (y=0, ŷ=1)
        Need weighted version for cost-sensitive learning (see below)
    
    categorical_cross_entropy:
      
      formula: "L(y, ŷ) = -Σₖ yₖ log(ŷₖ)"
      
      when_to_use: "Multi-class classification (K > 2 classes)"
      
      properties:
        - "Generalizes binary cross-entropy to K classes"
        - "Used with softmax activation"
        - "One-hot encoded labels"
      
      example: |
        True label: class 2 (one-hot: [0, 0, 1, 0])
        Predictions: [0.1, 0.2, 0.6, 0.1]
        
        Loss = -[0×log(0.1) + 0×log(0.2) + 1×log(0.6) + 0×log(0.1)]
             = -log(0.6)
             = 0.511
      
      numpy_implementation: |
        def categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):
            """
            Categorical cross-entropy loss
            
            Args:
                y_true: One-hot encoded labels, shape (m, K)
                y_pred: Predicted probabilities, shape (m, K)
            
            Returns:
                Average loss
            """
            # Clip predictions
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            # Compute loss per sample
            losses = -np.sum(y_true * np.log(y_pred), axis=1)
            
            return np.mean(losses)
      
      security_example: |
        Network traffic classification:
        Classes: [HTTP, SSH, FTP, DNS, Malicious]
        
        True: Malicious traffic (one-hot: [0, 0, 0, 0, 1])
        Predicted: [0.1, 0.1, 0.1, 0.1, 0.6]
        
        Loss focuses on predicted probability of correct class (0.6)
    
    hinge_loss:
      
      formula: "L(y, ŷ) = max(0, 1 - y·ŷ)"
      
      when_to_use: "Support Vector Machines (margin-based classifiers)"
      
      properties:
        - "Encourages confident correct predictions (margin)"
        - "Zero loss if prediction correct with margin"
        - "Linear penalty for wrong predictions"
        - "Not probabilistic (outputs decision scores, not probabilities)"
      
      margin_concept: |
        Goal: Not just classify correctly, but with confidence
        
        y = +1 (positive class):
        - If ŷ > +1: Loss = 0 (correct with margin)
        - If ŷ = +1: Loss = 0 (barely correct)
        - If 0 < ŷ < +1: Loss > 0 (correct but no margin)
        - If ŷ < 0: Loss high (wrong)
      
      numpy_implementation: |
        def hinge_loss(y_true, y_pred):
            """
            Hinge loss for SVM
            
            Args:
                y_true: Labels in {-1, +1}, shape (m,)
                y_pred: Decision function values, shape (m,)
            
            Returns:
                Average loss
            """
            # Compute hinge loss per sample
            losses = np.maximum(0, 1 - y_true * y_pred)
            
            return np.mean(losses)
      
      security_benefit: |
        Maximizes margin = maximizes distance to decision boundary
        = harder for adversary to cross boundary with small perturbations
        
        More robust to adversarial examples than cross-entropy
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Regression Loss Functions
  # --------------------------------------------------------------------------
  
  regression_losses:
    
    mean_squared_error:
      
      formula: "MSE = (1/m) Σᵢ (yᵢ - ŷᵢ)²"
      
      when_to_use: "Regression, predicting continuous values"
      
      properties:
        - "Heavily penalizes large errors (squared term)"
        - "Differentiable everywhere"
        - "Sensitive to outliers"
      
      intuition: |
        Squared error grows quadratically
        
        Small error (1): Loss = 1² = 1
        Medium error (5): Loss = 5² = 25
        Large error (10): Loss = 10² = 100
        
        Model focuses heavily on reducing large errors
      
      numpy_implementation: |
        def mean_squared_error(y_true, y_pred):
            """Mean squared error"""
            return np.mean((y_true - y_pred) ** 2)
      
      gradient: "∂MSE/∂ŷ = 2(ŷ - y)"
      
      security_example: |
        Predicting API response time
        Normal: 50ms, Prediction: 52ms → Error = 2ms, Loss = 4
        Attack (DDoS): 5000ms, Prediction: 100ms → Error = 4900ms, Loss = 24,010,000
        
        Model will focus heavily on detecting DDoS (large errors)
    
    mean_absolute_error:
      
      formula: "MAE = (1/m) Σᵢ |yᵢ - ŷᵢ|"
      
      when_to_use: "Regression when outliers present"
      
      properties:
        - "Linear penalty (absolute value)"
        - "Robust to outliers (vs MSE)"
        - "Not differentiable at zero"
      
      comparison_to_mse: |
        MSE: Heavily penalizes outliers (squared)
        MAE: Treats all errors equally (linear)
        
        Error = 10:
        MSE penalty = 100
        MAE penalty = 10
        
        Use MAE when outliers shouldn't dominate training
      
      numpy_implementation: |
        def mean_absolute_error(y_true, y_pred):
            """Mean absolute error"""
            return np.mean(np.abs(y_true - y_pred))
    
    huber_loss:
      
      formula: |
        Huber(y, ŷ) = {
          0.5(y - ŷ)²           if |y - ŷ| ≤ δ
          δ|y - ŷ| - 0.5δ²      if |y - ŷ| > δ
        }
      
      when_to_use: "Regression with outliers, want balance between MSE and MAE"
      
      properties:
        - "Quadratic for small errors (like MSE)"
        - "Linear for large errors (like MAE)"
        - "Robust to outliers, smooth gradient"
      
      intuition: |
        Combines best of MSE and MAE:
        - Small errors: Use MSE (smooth, efficient optimization)
        - Large errors: Use MAE (don't overpenalize outliers)
        
        Transition point controlled by δ parameter
      
      numpy_implementation: |
        def huber_loss(y_true, y_pred, delta=1.0):
            """
            Huber loss: MSE for small errors, MAE for large
            
            Args:
                delta: Threshold between quadratic and linear
            """
            error = y_true - y_pred
            is_small_error = np.abs(error) <= delta
            
            # Small errors: 0.5 * error^2
            small_error_loss = 0.5 * (error ** 2)
            
            # Large errors: delta * |error| - 0.5 * delta^2
            large_error_loss = delta * np.abs(error) - 0.5 * (delta ** 2)
            
            # Combine
            loss = np.where(is_small_error, small_error_loss, large_error_loss)
            
            return np.mean(loss)
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Custom Loss Functions for Security
  # --------------------------------------------------------------------------
  
  security_specific_losses:
    
    weighted_binary_cross_entropy:
      
      motivation: |
        Security datasets are imbalanced: 99% benign, 1% malicious
        Standard BCE treats all errors equally
        Need to penalize false negatives (missed attacks) more than false positives
      
      formula: "L = -[w₁·y·log(ŷ) + w₀·(1-y)·log(1-ŷ)]"
      
      where:
        w1: "Weight for positive class (malicious)"
        w0: "Weight for negative class (benign)"
      
      typical_weights: |
        If dataset is 1% positive, 99% negative:
        w₁ = 99 (weight positive class 99x more)
        w₀ = 1
        
        Or use class frequencies:
        w₁ = n_negative / n_positive
        w₀ = 1
      
      numpy_implementation: |
        def weighted_binary_cross_entropy(y_true, y_pred, pos_weight=1.0, epsilon=1e-15):
            """
            Weighted BCE for imbalanced classification
            
            Args:
                pos_weight: Weight for positive class (typically > 1)
            """
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            # Weighted loss
            loss = -(pos_weight * y_true * np.log(y_pred) + 
                    (1 - y_true) * np.log(1 - y_pred))
            
            return np.mean(loss)
      
      security_example: |
        Malware detection: 1% malicious, 99% benign
        Use pos_weight = 99
        
        False negative (missed malware): Loss = 99 × BCE
        False positive (benign flagged): Loss = 1 × BCE
        
        Model learns: Missing malware 99x worse than false alarm
    
    focal_loss:
      
      motivation: |
        Weighted BCE still struggles with extreme imbalance
        Easy examples (confidently correct) dominate gradient
        Hard examples (near decision boundary) get ignored
        
        Focal loss: Down-weight easy examples, focus on hard ones
      
      formula: "FL = -αₜ(1-pₜ)ᵞ log(pₜ)"
      
      where:
        pt: "Predicted probability of true class"
        alpha: "Class weight (like weighted BCE)"
        gamma: "Focusing parameter (typically 2)"
      
      intuition: |
        Easy example: pₜ = 0.9 (confident correct)
        (1 - 0.9)² = 0.01 → Loss reduced by 100x
        
        Hard example: pₜ = 0.6 (uncertain)
        (1 - 0.6)² = 0.16 → Loss reduced by 6x
        
        Model focuses on hard examples automatically
      
      numpy_implementation: |
        def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0, epsilon=1e-15):
            """
            Focal loss for extreme class imbalance
            
            Args:
                alpha: Class weight
                gamma: Focusing parameter (higher = more focus on hard examples)
            """
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            
            # Compute pt (probability of true class)
            pt = np.where(y_true == 1, y_pred, 1 - y_pred)
            
            # Focal loss
            focal_weight = (1 - pt) ** gamma
            loss = -alpha * focal_weight * np.log(pt)
            
            return np.mean(loss)
      
      when_to_use: |
        - Extreme imbalance (>100:1 ratio)
        - Many easy examples dominating training
        - Single-stage object detection (original use case)
        - Security: Rare attack detection (APT, zero-days)
    
    cost_sensitive_loss:
      
      motivation: |
        Different errors have different operational costs
        
        Security costs:
        - False negative (missed attack): $1,000,000 (breach)
        - False positive (false alarm): $10 (analyst time)
        
        Need loss reflecting real-world costs
      
      formula: "L = C_FN × (y=1, ŷ=0) + C_FP × (y=0, ŷ=1)"
      
      where:
        C_FN: "Cost of false negative"
        C_FP: "Cost of false positive"
      
      example_costs: |
        Fraud detection:
        C_FN = $1000 (missed fraud = financial loss)
        C_FP = $5 (manual review cost)
        Ratio: 200:1
        
        Malware detection:
        C_FN = $100,000 (infected system, data breach)
        C_FP = $50 (false alarm investigation)
        Ratio: 2000:1
      
      numpy_implementation: |
        def cost_sensitive_loss(y_true, y_pred, cost_fn=100, cost_fp=1, threshold=0.5):
            """
            Cost-sensitive loss based on operational costs
            
            Args:
                cost_fn: Cost of false negative
                cost_fp: Cost of false positive
                threshold: Classification threshold
            """
            # Convert probabilities to predictions
            y_pred_class = (y_pred >= threshold).astype(int)
            
            # Identify errors
            false_negatives = (y_true == 1) & (y_pred_class == 0)
            false_positives = (y_true == 0) & (y_pred_class == 1)
            
            # Compute cost
            total_cost = (cost_fn * false_negatives.sum() + 
                         cost_fp * false_positives.sum())
            
            return total_cost / len(y_true)
      
      training_with_cost_sensitive: |
        Can't use directly with gradient descent (not differentiable)
        
        Options:
        1. Use weighted BCE with weights = cost ratio
        2. Adjust classification threshold after training
        3. Use cost-aware evaluation during training
    
    asymmetric_loss_for_security:
      
      concept: "Penalize false negatives exponentially more than false positives"
      
      formula: "L = e^(α·FN_rate) + FP_rate"
      
      where:
        alpha: "Controls asymmetry (higher = more penalty for FN)"
      
      behavior: |
        Model will drive false negative rate to near zero
        Even if false positive rate increases significantly
        
        Security priority: Miss no attacks, even with many false alarms
      
      numpy_implementation: |
        def asymmetric_security_loss(y_true, y_pred, alpha=5.0, threshold=0.5):
            """
            Exponentially penalize false negatives
            """
            y_pred_class = (y_pred >= threshold).astype(int)
            
            # Compute rates
            fn_rate = np.mean((y_true == 1) & (y_pred_class == 0))
            fp_rate = np.mean((y_true == 0) & (y_pred_class == 1))
            
            # Asymmetric penalty
            loss = np.exp(alpha * fn_rate) + fp_rate
            
            return loss
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Choosing the Right Loss
  # --------------------------------------------------------------------------
  
  loss_function_selection:
    
    decision_tree: |
      Is this classification or regression?
      
      ├─ CLASSIFICATION
      │  ├─ Binary (2 classes)?
      │  │  ├─ Balanced classes?
      │  │  │  └─ Use: Binary Cross-Entropy
      │  │  ├─ Imbalanced (<10:1)?
      │  │  │  └─ Use: Weighted Binary Cross-Entropy
      │  │  └─ Extreme imbalance (>100:1)?
      │  │     └─ Use: Focal Loss
      │  │
      │  └─ Multi-class (K > 2)?
      │     ├─ Balanced classes?
      │     │  └─ Use: Categorical Cross-Entropy
      │     └─ Need margins (adversarial robustness)?
      │        └─ Use: Hinge Loss (SVM)
      │
      └─ REGRESSION
         ├─ No outliers?
         │  └─ Use: Mean Squared Error (MSE)
         ├─ Many outliers?
         │  └─ Use: Mean Absolute Error (MAE)
         └─ Some outliers?
            └─ Use: Huber Loss
    
    security_specific_guidance:
      
      attack_detection:
        problem: "Binary classification, extreme imbalance (1% attacks)"
        recommended: "Weighted BCE or Focal Loss"
        weights: "pos_weight = 99 (or higher for critical systems)"
      
      malware_classification:
        problem: "Multi-class (malware families), imbalanced"
        recommended: "Weighted Categorical Cross-Entropy"
        note: "Weight malicious classes higher than benign"
      
      anomaly_scoring:
        problem: "Regression, predicting anomaly score"
        recommended: "Huber Loss (robust to outlier normal samples)"
      
      zero_day_detection:
        problem: "Binary, extreme imbalance, high FN cost"
        recommended: "Focal Loss + Cost-sensitive threshold"
        note: "Can't miss zero-days, tolerate high FP rate"
  
  # --------------------------------------------------------------------------
  # Practical Implementation: Loss Function Library
  # --------------------------------------------------------------------------
  
  loss_function_library: |
    import numpy as np
    
    class LossFunctions:
        """Collection of loss functions for ML"""
        
        @staticmethod
        def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):
            """Standard BCE for balanced binary classification"""
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            return -np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))
        
        @staticmethod
        def weighted_binary_cross_entropy(y_true, y_pred, pos_weight=1.0, epsilon=1e-15):
            """Weighted BCE for imbalanced data"""
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            loss = -(pos_weight * y_true * np.log(y_pred) + 
                    (1 - y_true) * np.log(1 - y_pred))
            return np.mean(loss)
        
        @staticmethod
        def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0, epsilon=1e-15):
            """Focal loss for extreme imbalance"""
            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
            pt = np.where(y_true == 1, y_pred, 1 - y_pred)
            focal_weight = (1 - pt) ** gamma
            loss = -alpha * focal_weight * np.log(pt)
            return np.mean(loss)
        
        @staticmethod
        def hinge_loss(y_true, y_pred):
            """Hinge loss for SVM (margin-based)"""
            return np.mean(np.maximum(0, 1 - y_true * y_pred))
        
        @staticmethod
        def mean_squared_error(y_true, y_pred):
            """MSE for regression"""
            return np.mean((y_true - y_pred) ** 2)
        
        @staticmethod
        def mean_absolute_error(y_true, y_pred):
            """MAE for regression with outliers"""
            return np.mean(np.abs(y_true - y_pred))
        
        @staticmethod
        def huber_loss(y_true, y_pred, delta=1.0):
            """Huber loss: MSE for small errors, MAE for large"""
            error = y_true - y_pred
            is_small = np.abs(error) <= delta
            small_loss = 0.5 * (error ** 2)
            large_loss = delta * np.abs(error) - 0.5 * (delta ** 2)
            return np.mean(np.where(is_small, small_loss, large_loss))
    
    # Usage example: Malware detection with imbalanced data
    
    # Simulate data: 1% malicious, 99% benign
    np.random.seed(42)
    y_true = np.random.choice([0, 1], size=10000, p=[0.99, 0.01])
    
    # Simulate predictions (model that predicts all benign)
    y_pred = np.random.uniform(0, 0.2, size=10000)  # Low probabilities
    
    # Compare loss functions
    loss = LossFunctions()
    
    bce = loss.binary_cross_entropy(y_true, y_pred)
    weighted_bce = loss.weighted_binary_cross_entropy(y_true, y_pred, pos_weight=99)
    focal = loss.focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0)
    
    print(f"Standard BCE: {bce:.4f}")
    print(f"Weighted BCE (w=99): {weighted_bce:.4f}")
    print(f"Focal Loss: {focal:.4f}")
    
    # Weighted/focal loss will be much higher, driving model to detect rare class
  
  # --------------------------------------------------------------------------
  # Common Mistakes
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    mistake_1:
      error: "Using MSE for classification"
      
      problem: |
        MSE not designed for classification
        - Gradients can vanish (model stops learning)
        - Non-convex optimization landscape
        - Doesn't match probabilistic interpretation
      
      example: |
        True: y=1, Predicted: ŷ=0.9
        MSE: (1-0.9)² = 0.01
        BCE: -log(0.9) = 0.105
        
        MSE gradient tiny when close to correct → slow learning
      
      fix: "Always use cross-entropy for classification"
    
    mistake_2:
      error: "Not weighting losses for imbalanced data"
      
      problem: |
        Dataset: 99% benign, 1% malicious
        Model predicts everything as benign
        Standard BCE: Loss decreases!
        Result: 99% accuracy, 0% attack detection
      
      fix: |
        Use weighted BCE with pos_weight = 99
        Or use focal loss
        Model forced to learn rare class
    
    mistake_3:
      error: "Using same loss for training and evaluation"
      
      issue: |
        Training loss (BCE): Optimize probabilistic predictions
        Evaluation metric (F1): Measure binary classification quality
        
        Model might have low loss but poor F1 score
      
      solution: |
        Train with BCE (gradient descent needs differentiable loss)
        Evaluate with F1, precision, recall (Section 11)
        Tune threshold based on evaluation metrics
    
    mistake_4:
      error: "Ignoring operational costs in loss function"
      
      scenario: |
        Fraud detection:
        - Missed fraud: $1000 loss
        - False alarm: $5 investigation cost
        - Ratio: 200:1
        
        Standard BCE treats both errors equally
      
      fix: |
        Use weighted BCE with pos_weight = 200
        Or adjust classification threshold after training
        Align loss with business impact
  
  # --------------------------------------------------------------------------
  # Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    adversarial_loss_exploitation:
      
      attack: "Adversary knows your loss function"
      
      strategy: |
        If using MSE: Craft inputs far from decision boundary
        If using BCE: Craft inputs that maximize uncertainty (ŷ ≈ 0.5)
        If using hinge: Stay just inside margin
      
      example: |
        Model trained with BCE
        Adversary crafts input: P(malicious) = 0.49
        Just below 0.5 threshold → evades detection
        Loss: Low (model confident it's working)
        Reality: Malicious input let through
      
      defense: |
        Use ensemble with different losses
        Regularly retrain with adversarial examples
        Don't rely solely on threshold-based decisions
    
    loss_poisoning:
      
      attack: "Inject samples that confuse loss function"
      
      example: |
        Model uses MSE (sensitive to outliers)
        Adversary injects samples with extreme values
        Loss explodes → gradient descent unstable
        Model fails to converge
      
      defense: |
        Use robust losses (Huber, MAE)
        Outlier detection before training
        Monitor loss distribution for anomalies
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Loss function defines what model optimizes - choose carefully"
      - "Cross-entropy for classification, MSE for regression (general rule)"
      - "Imbalanced data needs weighted or focal loss"
      - "Security requires cost-sensitive losses (FN cost >> FP cost)"
      - "Different losses encourage different model behaviors"
    
    practical_skills:
      - "Implement multiple loss functions in NumPy"
      - "Choose loss based on problem type and class distribution"
      - "Weight losses for imbalanced security datasets"
      - "Design custom losses reflecting operational costs"
      - "Use appropriate loss for training vs evaluation"
    
    security_mindset:
      - "Standard losses treat all errors equally (unacceptable for security)"
      - "Weight losses to reflect attack detection priority"
      - "Adversaries can exploit loss function properties"
      - "Robust losses (Huber, MAE) resist poisoning"
      - "Loss = how model sees optimization - shape it for security goals"
    
    remember_this:
      - "For security: Weighted BCE or Focal Loss (imbalanced data is the norm)"
      - "Never use MSE for classification (wrong tool for the job)"
      - "Loss function choice as important as model architecture"
    
    next_steps:
      - "Next section: Evaluation metrics (accuracy, precision, recall, F1, ROC-AUC)"
      - "Connect losses to metrics: BCE optimizes, F1 evaluates"
      - "You now understand what models optimize during training"

---
