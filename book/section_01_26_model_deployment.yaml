# section_01_26_model_deployment.yaml

---
document_info:
  chapter: "01"
  section: "26"
  title: "Model Deployment Basics"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 5
  tags: ["deployment", "production", "mlops", "inference", "model-serving", "monitoring", "versioning"]

# ============================================================================
# SECTION 1.26: MODEL DEPLOYMENT BASICS
# ============================================================================

section_01_26_model_deployment:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Training a model is only half the battle. Deployment - getting models into production 
    where they make real decisions - is equally critical. Many ML projects fail not because 
    of poor models, but because of deployment challenges. A 95% accurate model that never 
    makes it to production has zero impact.
    
    Deployment involves infrastructure, monitoring, versioning, and operational concerns that 
    extend far beyond model training. You need to serve predictions quickly, handle failures 
    gracefully, monitor performance, and update models safely. This section covers the 
    fundamentals of taking ML models from development to production.
    
    This section covers:
    - Training vs inference (different requirements)
    - Model serialization and versioning
    - Inference architectures (batch vs real-time)
    - Performance optimization (latency, throughput)
    - Monitoring and observability
    - Model updates and rollback strategies
    
    For security ML:
    1. Real-time detection requires low-latency inference
    2. High availability critical (cannot miss threats)
    3. Model drift detection (attacks evolve)
    4. Safe updates (don't break production)
  
  why_this_matters: |
    Production reality:
    - 87% of ML projects never make it to production (VentureBeat)
    - Deployment and operations dominate total cost
    - Model quality matters less than operational excellence
    
    Security context:
    - Detection systems must be always-on (99.9%+ uptime)
    - Millisecond latency requirements (real-time blocking)
    - Attackers exploit deployment vulnerabilities
    - Models must update frequently (new threats daily)
    
    Career relevance:
    - MLOps skills highly valued
    - Deployment experience differentiates candidates
    - Understanding production constraints guides research
    - Bridge between data science and engineering
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Training vs Inference
  # --------------------------------------------------------------------------
  
  training_vs_inference:
    
    key_differences: |
      Training: Optimize model parameters (offline)
      Inference: Use trained model to make predictions (online)
      
      Different requirements, different constraints
    
    comparison_table: |
      Aspect           | Training              | Inference
      -----------------|-----------------------|------------------------
      Hardware         | GPUs (parallel)       | CPUs often sufficient
      Latency          | Minutes to hours OK   | Milliseconds required
      Throughput       | Batch processing      | May need real-time
      Cost             | High (GPU hours)      | Ongoing (per request)
      Frequency        | Daily/weekly          | Millions per second
      Errors           | Can retry             | Must handle gracefully
      Environment      | Development           | Production
      Monitoring       | Accuracy metrics      | Latency, uptime, drift
    
    training_phase: |
      Characteristics:
      - Run offline (scheduled, not user-facing)
      - Can use powerful hardware (multiple GPUs)
      - Iterations can take hours/days
      - Failures can be debugged and retried
      - Experiments and hyperparameter tuning
      
      Infrastructure:
      - Training cluster (e.g., AWS p3 instances)
      - Distributed training framework (Horovod, PyTorch DDP)
      - Experiment tracking (MLflow, Weights & Biases)
    
    inference_phase: |
      Characteristics:
      - Run online (user-facing, real-time)
      - Must be fast (low latency)
      - Must be reliable (high availability)
      - Must scale (handle load spikes)
      - Cost per prediction matters
      
      Infrastructure:
      - Inference servers (TensorFlow Serving, TorchServe)
      - Load balancers
      - Caching layers
      - Monitoring and alerting
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Model Serialization and Versioning
  # --------------------------------------------------------------------------
  
  serialization_versioning:
    
    model_serialization: |
      Save trained model to disk for later use
      
      Common formats:
      - Pickle (Python): Simple but Python-only
      - ONNX: Cross-framework (PyTorch → TensorFlow)
      - SavedModel (TensorFlow): Production format
      - TorchScript (PyTorch): Optimized for deployment
      - joblib (scikit-learn): Better than pickle for large arrays
    
    example_serialization: |
      # Scikit-learn
      import joblib
      joblib.dump(model, 'model.pkl')
      model = joblib.load('model.pkl')
      
      # PyTorch
      torch.save(model.state_dict(), 'model.pth')
      model.load_state_dict(torch.load('model.pth'))
      
      # TensorFlow
      model.save('saved_model/')
      model = tf.keras.models.load_model('saved_model/')
    
    what_to_save: |
      Model artifacts:
      1. Model weights/parameters
      2. Model architecture (if needed)
      3. Preprocessing pipeline (scalers, encoders)
      4. Metadata (hyperparameters, training date)
      5. Evaluation metrics (accuracy, precision, recall)
      
      Example structure:
      model_v1.2.3/
        ├── weights.pkl
        ├── preprocessor.pkl
        ├── metadata.json
        └── metrics.json
    
    versioning_strategy: |
      Semantic versioning: MAJOR.MINOR.PATCH
      
      MAJOR: Breaking changes (new features, different output)
      MINOR: Backward-compatible improvements (better accuracy)
      PATCH: Bug fixes (no model changes)
      
      Example:
      v1.0.0: Initial model (logistic regression)
      v1.1.0: Improved model (random forest, better accuracy)
      v1.1.1: Bug fix in preprocessing
      v2.0.0: New model architecture (neural network, different API)
    
    model_registry: |
      Central repository for models
      
      Features:
      - Version control
      - Metadata tracking
      - Lifecycle management (staging, production, archived)
      - Access control
      - Lineage tracking
      
      Tools: MLflow Model Registry, AWS SageMaker Model Registry
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Inference Architectures
  # --------------------------------------------------------------------------
  
  inference_architectures:
    
    batch_inference: |
      Process large batches of data periodically
      
      Use cases:
      - Daily email spam scoring
      - Nightly malware scanning of repository
      - Weekly customer churn predictions
      
      Characteristics:
      - High throughput (thousands/second)
      - Latency tolerant (minutes to hours OK)
      - Scheduled jobs (cron, Airflow)
      - Cost-efficient (batch processing)
      
      Example:
      Every night at 2 AM:
      - Load 1M emails
      - Score with spam model
      - Update database
    
    real_time_inference: |
      Process single requests with low latency
      
      Use cases:
      - Network intrusion blocking (milliseconds)
      - Credit card fraud detection (sub-second)
      - Malware detection on upload (seconds)
      
      Characteristics:
      - Low latency (<100ms typical)
      - High availability (99.9%+)
      - Scales with traffic
      - More expensive (always-on servers)
      
      Example:
      User uploads file:
      - Send to malware detection API
      - Model scores in 50ms
      - Block or allow immediately
    
    streaming_inference: |
      Process continuous streams of data
      
      Use cases:
      - Network traffic analysis (continuous)
      - IoT sensor monitoring (real-time)
      - Log anomaly detection (streaming)
      
      Characteristics:
      - Process data as it arrives
      - Windowing (aggregate over time windows)
      - Stateful processing
      
      Tools: Apache Kafka, Apache Flink, AWS Kinesis
    
    architecture_comparison: |
      Batch:
      - Latency: Hours
      - Throughput: Very high
      - Cost: Low
      - Complexity: Low
      
      Real-time:
      - Latency: Milliseconds
      - Throughput: Medium
      - Cost: High
      - Complexity: Medium
      
      Streaming:
      - Latency: Seconds
      - Throughput: High
      - Cost: Medium
      - Complexity: High
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Performance Optimization
  # --------------------------------------------------------------------------
  
  performance_optimization:
    
    latency_optimization: |
      Reduce time per prediction
      
      Techniques:
      1. Model simplification (fewer parameters)
      2. Quantization (float32 → int8)
      3. Pruning (remove unnecessary weights)
      4. Model distillation (teacher-student)
      5. Hardware acceleration (GPUs, TPUs)
      6. Caching (repeated requests)
    
    quantization: |
      Reduce precision: float32 (4 bytes) → int8 (1 byte)
      
      Benefits:
      - 4x smaller model size
      - 2-4x faster inference
      - Minimal accuracy loss (<1%)
      
      Example:
      Weight: 0.123456789 (float32)
      Quantized: 123 (int8, scaled)
      
      Tools: TensorFlow Lite, PyTorch quantization
    
    model_distillation: |
      Train small "student" model to mimic large "teacher"
      
      Process:
      1. Train large model (teacher)
      2. Use teacher to generate soft labels
      3. Train small model (student) on soft labels
      4. Deploy small model (faster, cheaper)
      
      Example:
      Teacher: BERT (110M parameters)
      Student: DistilBERT (66M parameters, 40% faster)
      Accuracy loss: ~3%
    
    caching: |
      Store results of expensive computations
      
      Strategies:
      - Cache predictions (if inputs repeat)
      - Cache embeddings (for retrieval)
      - Cache preprocessing (feature extraction)
      
      Example:
      URL reputation check:
      - Cache: google.com → benign (30 days)
      - Skip model inference for cached URLs
      - 90% cache hit rate → 10x faster
    
    batching: |
      Group multiple requests together
      
      Benefits:
      - Amortize overhead (model loading, etc.)
      - Better GPU utilization
      - Higher throughput
      
      Trade-off: Increases latency (wait for batch)
      
      Dynamic batching:
      - Wait max 10ms for batch
      - Or batch size reaches 32
      - Whichever comes first
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Monitoring and Observability
  # --------------------------------------------------------------------------
  
  monitoring:
    
    what_to_monitor: |
      1. System metrics (latency, throughput, errors)
      2. Model metrics (accuracy, precision, recall)
      3. Data metrics (feature drift, label drift)
      4. Business metrics (conversions, revenue)
    
    system_metrics: |
      Latency:
      - p50: 50th percentile (median)
      - p95: 95th percentile (catch outliers)
      - p99: 99th percentile (worst case)
      
      Target: p95 < 100ms for real-time systems
      
      Throughput:
      - Requests per second (RPS)
      - Predictions per second
      
      Target: Handle peak load + 50% buffer
      
      Errors:
      - Error rate (%)
      - Error types (timeout, crash, invalid input)
      
      Target: Error rate < 0.1%
      
      Availability:
      - Uptime percentage
      
      Target: 99.9% (8.76 hours downtime/year)
    
    model_metrics: |
      Online metrics (production):
      - Precision@K: How many top K predictions correct?
      - Click-through rate: For recommendation systems
      - Conversion rate: For ranking systems
      
      Challenge: May not have immediate ground truth
      
      Solution: Log predictions for later evaluation
    
    data_drift: |
      Input distributions change over time
      
      Example:
      - Network traffic patterns evolve
      - Attack types change
      - User behavior shifts
      
      Detection:
      - Compare feature distributions (training vs production)
      - Statistical tests (KS test, chi-square)
      - Track feature statistics over time
      
      Response:
      - Retrain model on recent data
      - Update features
      - Investigate root cause
    
    model_drift: |
      Model performance degrades over time
      
      Causes:
      - Data drift (inputs change)
      - Concept drift (input-output relationship changes)
      - Adversarial drift (attackers adapt)
      
      Detection:
      - Monitor online metrics (if labels available)
      - A/B test against new model
      - Periodic offline evaluation
      
      Response:
      - Retrain periodically (weekly, monthly)
      - Trigger retraining when metrics degrade
      - Update to new model version
    
    alerting: |
      Set thresholds and alert when exceeded
      
      Examples:
      - Latency p95 > 200ms → Page on-call engineer
      - Error rate > 1% → Alert team
      - Accuracy drops > 5% → Trigger retraining
      
      Best practices:
      - Avoid alert fatigue (too many false alarms)
      - Actionable alerts (clear what to do)
      - Escalation policy (who to page, when)
  
  # --------------------------------------------------------------------------
  # Core Concept 6: Deployment Strategies
  # --------------------------------------------------------------------------
  
  deployment_strategies:
    
    blue_green_deployment: |
      Maintain two environments: blue (current) and green (new)
      
      Process:
      1. Deploy new model to green environment
      2. Test green environment
      3. Switch traffic from blue to green
      4. Keep blue as backup (rollback if issues)
      
      Benefits:
      - Zero downtime
      - Instant rollback
      
      Cost: Run two environments simultaneously
    
    canary_deployment: |
      Gradually roll out new model to subset of traffic
      
      Process:
      1. Deploy new model (v2)
      2. Route 5% traffic to v2, 95% to v1
      3. Monitor v2 metrics
      4. If good: Increase to 10%, 25%, 50%, 100%
      5. If bad: Rollback to 100% v1
      
      Benefits:
      - Detect issues early (limited blast radius)
      - Gradual validation
      
      Common in production
    
    shadow_mode: |
      Run new model in parallel without affecting production
      
      Process:
      1. Production model (v1) serves predictions
      2. New model (v2) also predicts (logged, not used)
      3. Compare v1 and v2 predictions offline
      4. If v2 better: Promote to canary deployment
      
      Benefits:
      - Risk-free validation
      - Real production data
      
      Cost: Double inference cost
    
    rollback_strategy: |
      When new model has issues:
      
      Immediate rollback:
      - Switch traffic back to previous version
      - Should be one-click operation
      - Practice rollback procedures
      
      Keep previous versions:
      - Store last N model versions
      - Quick rollback to any previous version
      
      Automated rollback:
      - Monitor error rate, latency
      - Automatically rollback if thresholds exceeded
  
  # --------------------------------------------------------------------------
  # Practical Example: Malware Detection API
  # --------------------------------------------------------------------------
  
  practical_example: |
    """
    Deploying a malware detection model as REST API
    """
    
    from flask import Flask, request, jsonify
    import joblib
    import numpy as np
    import time
    
    app = Flask(__name__)
    
    # Load model at startup
    model = joblib.load('malware_model_v1.2.0.pkl')
    preprocessor = joblib.load('preprocessor_v1.2.0.pkl')
    
    # Metrics
    request_count = 0
    error_count = 0
    latencies = []
    
    @app.route('/health', methods=['GET'])
    def health():
        """Health check endpoint"""
        return jsonify({
            'status': 'healthy',
            'model_version': '1.2.0',
            'uptime': time.time() - app.start_time
        })
    
    @app.route('/predict', methods=['POST'])
    def predict():
        """Prediction endpoint"""
        global request_count, error_count
        
        start_time = time.time()
        request_count += 1
        
        try:
            # Parse input
            data = request.json
            features = np.array(data['features']).reshape(1, -1)
            
            # Preprocess
            features_processed = preprocessor.transform(features)
            
            # Predict
            prediction = model.predict(features_processed)[0]
            probability = model.predict_proba(features_processed)[0, 1]
            
            # Record latency
            latency = time.time() - start_time
            latencies.append(latency)
            
            # Return result
            return jsonify({
                'prediction': 'malicious' if prediction == 1 else 'benign',
                'confidence': float(probability),
                'latency_ms': latency * 1000,
                'model_version': '1.2.0'
            })
        
        except Exception as e:
            error_count += 1
            return jsonify({
                'error': str(e)
            }), 500
    
    @app.route('/metrics', methods=['GET'])
    def metrics():
        """Metrics endpoint (for monitoring)"""
        return jsonify({
            'requests': request_count,
            'errors': error_count,
            'error_rate': error_count / request_count if request_count > 0 else 0,
            'latency_p50': np.percentile(latencies, 50) if latencies else 0,
            'latency_p95': np.percentile(latencies, 95) if latencies else 0,
            'latency_p99': np.percentile(latencies, 99) if latencies else 0
        })
    
    if __name__ == '__main__':
        app.start_time = time.time()
        # Production: Use gunicorn or uwsgi instead of Flask dev server
        app.run(host='0.0.0.0', port=5000)
    
    """
    Deployment checklist:
    1. ✓ Health check endpoint
    2. ✓ Versioning (model_version in response)
    3. ✓ Error handling
    4. ✓ Metrics endpoint
    5. ✓ Latency tracking
    
    Production improvements:
    - Use production WSGI server (gunicorn)
    - Add authentication (API keys)
    - Add rate limiting
    - Add request logging
    - Add structured logging
    - Container deployment (Docker)
    - Orchestration (Kubernetes)
    - Load balancer (nginx, AWS ELB)
    - Auto-scaling based on load
    - Distributed tracing (Jaeger)
    - Centralized logging (ELK stack)
    """
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Deployment != Training (different requirements)"
      - "Inference: Low latency, high availability critical"
      - "Batch vs real-time: Trade-off latency for throughput"
      - "Monitor: System metrics, model metrics, data drift"
      - "Deploy safely: Canary deployment, rollback strategy"
      - "Versioning: Track models, enable rollback"
    
    practical_skills:
      - "Serialize models (pickle, ONNX, SavedModel)"
      - "Build inference API (Flask, FastAPI)"
      - "Optimize performance (quantization, caching)"
      - "Monitor metrics (latency, throughput, drift)"
      - "Deploy incrementally (canary, blue-green)"
    
    security_mindset:
      - "Real-time detection needs <100ms latency"
      - "High availability critical (99.9%+ uptime)"
      - "Attackers evolve → monitor drift, update frequently"
      - "Safe updates crucial (don't break production)"
      - "Log predictions for forensics and retraining"
    
    remember_this:
      - "Training is offline, inference is online (different constraints)"
      - "Version everything (models, code, data)"
      - "Monitor drift (data and model performance)"
      - "Deploy incrementally (canary → full rollout)"
      - "Always have rollback plan (one-click revert)"
    
    next_steps:
      - "Next section: A/B Testing for ML"
      - "You now understand model deployment!"
      - "Critical bridge between research and production"

---
