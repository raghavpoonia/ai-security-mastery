# section_03_12_transformer_encoder.yaml

---
document_info:
  title: "Transformer Encoder: Building the First Half"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 12
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into transformer encoder: encoder layer architecture, self-attention, stacking layers, input processing, and complete encoder implementation from scratch"
  estimated_pages: 7
  tags:
    - transformer-encoder
    - encoder-layer
    - self-attention
    - bidirectional-context
    - stacked-layers
    - encoder-architecture

section_overview:
  title: "Transformer Encoder: Building the First Half"
  number: "3.12"
  
  purpose: |
    The transformer encoder is the first half of the transformer architecture. It takes 
    input sequences (text, code, etc.) and produces rich contextual representations where 
    each token's encoding contains information from the entire sequence. The encoder uses 
    bidirectional self-attention - every token can attend to every other token, enabling 
    full context understanding.
    
    An encoder layer consists of multi-head self-attention followed by feed-forward network, 
    each with residual connections and layer normalization. Multiple encoder layers (6-12 
    typical) are stacked to build increasingly abstract representations. This architecture 
    powers models like BERT, RoBERTa, and many other NLU systems.
    
    For security engineers: Encoders create dense contextual representations that can leak 
    information beyond what's explicitly in input text. Attention patterns reveal what the 
    model considers related, potentially exposing private associations. Understanding encoder 
    architecture enables representation-based attacks and defenses.
  
  learning_objectives:
    conceptual:
      - "Understand encoder layer: self-attention → FFN, each with LayerNorm + residuals"
      - "Grasp self-attention: Q, K, V all from same sequence (bidirectional)"
      - "Learn encoder stack: 6-12 layers building hierarchical representations"
      - "See complete encoder: embeddings + positional encoding + layer stack"
      - "Understand bidirectional context (vs decoder's causal/unidirectional)"
    
    practical:
      - "Implement single encoder layer from scratch (NumPy)"
      - "Stack multiple encoder layers"
      - "Build complete encoder with input embeddings"
      - "Visualize encoder attention patterns"
      - "Process sequences through full encoder"
    
    security_focused:
      - "Extract sensitive information from encoder representations"
      - "Analyze encoder attention for private associations"
      - "Understand representation-based attacks (model inversion)"
      - "Identify information leakage in contextualized embeddings"
      - "Audit encoder outputs for unexpected content"
  
  prerequisites:
    knowledge:
      - "Section 3.11: Layer normalization and residual connections"
      - "Section 3.10: Feed-forward networks"
      - "Section 3.8: Multi-head attention"
      - "Section 3.9: Positional encoding"
    
    skills:
      - "All previous NumPy implementations"
      - "Understanding of deep network architectures"
      - "Composing multiple layers"
      - "Managing parameter counts and memory"
  
  key_transitions:
    from_section_3_11: |
      Section 3.11 covered layer normalization and residual connections - the glue 
      that stabilizes training. Now we assemble all components into complete encoder 
      layers and stack them to build the transformer encoder.
    
    to_next_section: |
      Section 3.13 will cover the transformer decoder - the autoregressive generation 
      component that uses masked self-attention and encoder-decoder cross-attention 
      to generate output sequences.

topics:
  - topic_number: 1
    title: "Encoder Layer Architecture"
    
    overview: |
      A single encoder layer has two sublayers: (1) multi-head self-attention, and 
      (2) position-wise feed-forward network. Each sublayer has a residual connection 
      and layer normalization. The encoder processes all positions in parallel, with 
      bidirectional self-attention allowing each token to attend to all other tokens.
    
    content:
      encoder_layer_components:
        sublayer_1_self_attention:
          purpose: "Mix information across positions (contextual processing)"
          
          operation: |
            Multi-head self-attention:
            - Q, K, V all from same input (self-attention)
            - Each position attends to ALL positions (bidirectional)
            - Output: contextualized representation
          
          formula: "attention_out = MultiHeadAttention(x, x, x)"
          
          with_residual_and_norm: |
            Pre-norm: x + Attention(LayerNorm(x))
            Post-norm: LayerNorm(x + Attention(x))
        
        sublayer_2_feed_forward:
          purpose: "Position-wise transformation (knowledge application)"
          
          operation: |
            Two-layer FFN:
            - Expand to 4×d_model
            - Apply GELU activation
            - Compress back to d_model
          
          formula: "ffn_out = FFN(x)"
          
          with_residual_and_norm: |
            Pre-norm: x + FFN(LayerNorm(x))
            Post-norm: LayerNorm(x + FFN(x))
      
      complete_encoder_layer_prenorm:
        pseudocode: |
          # Sublayer 1: Multi-head self-attention
          x1 = x + MultiHeadAttention(LayerNorm(x))
          
          # Sublayer 2: Feed-forward network
          x2 = x1 + FFN(LayerNorm(x1))
          
          return x2
        
        data_flow:
          input: "x: (batch, seq_len, d_model)"
          after_attention: "x1: (batch, seq_len, d_model) - contextualized"
          after_ffn: "x2: (batch, seq_len, d_model) - final layer output"
        
        key_properties:
          bidirectional: "Every token can see every other token"
          parallel: "All positions processed simultaneously"
          shape_preserving: "Output has same shape as input"
      
      self_attention_explained:
        what_is_self_attention: |
          Self-attention: Q, K, V all come from the SAME sequence
          
          Contrast with encoder-decoder attention:
          - Self-attention: Attend to same sequence
          - Cross-attention: Attend to different sequence
        
        bidirectional_context: |
          Encoder self-attention is bidirectional:
          - Position i can attend to positions 0, 1, ..., n-1
          - No masking (unlike decoder)
          - Full context available
          
          Example: "The cat sat on the mat"
          - "cat" attends to: [The, cat, sat, on, the, mat]
          - Gets context from BOTH left and right
        
        why_bidirectional_good_for_understanding: |
          Bidirectional context enables:
          - Better word sense disambiguation
          - Capturing long-range dependencies
          - Understanding from full context
          
          Example: "The bank by the river"
          - Needs "river" (right context) to know "bank" means riverbank
          - Bidirectional attention captures this
      
      parameter_count_per_layer:
        multi_head_attention:
          q_k_v_projections: "3 × d_model × d_model"
          output_projection: "d_model × d_model"
          total_attention: "4 × d_model²"
        
        feed_forward:
          layer_1: "d_model × d_ff (typically d_ff = 4×d_model)"
          layer_2: "d_ff × d_model"
          total_ffn: "2 × d_model × d_ff = 8 × d_model²"
        
        layer_normalizations:
          two_layernorms: "2 × 2 × d_model = 4 × d_model"
        
        total_per_encoder_layer: |
          4×d_model² (attention) + 8×d_model² (FFN) + 4×d_model (norms)
          ≈ 12×d_model² parameters
          
          Example (BERT-base, d_model=768):
          12 × 768² ≈ 7.1M parameters per layer
    
    implementation:
      encoder_layer:
        language: python
        code: |
          import numpy as np
          
          class TransformerEncoderLayer:
              """
              Single transformer encoder layer (pre-norm).
              
              Architecture:
              1. x1 = x + MultiHeadAttention(LayerNorm(x))
              2. x2 = x1 + FFN(LayerNorm(x1))
              """
              
              def __init__(self, d_model: int, num_heads: int, d_ff: int, 
                          dropout: float = 0.1):
                  """
                  Args:
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  self.d_model = d_model
                  self.num_heads = num_heads
                  self.d_ff = d_ff
                  self.dropout = dropout
                  
                  # Import previous implementations (simplified for example)
                  from section_3_08_multi_head_attention import MultiHeadAttention
                  from section_3_10_feed_forward_networks import PositionWiseFeedForwardGELU
                  from section_3_11_layer_norm_residuals import LayerNormalization
                  
                  # Sublayer 1: Multi-head self-attention
                  self.self_attention = MultiHeadAttention(d_model, num_heads)
                  self.attention_norm = LayerNormalization(d_model)
                  
                  # Sublayer 2: Feed-forward network
                  self.ffn = PositionWiseFeedForwardGELU(d_model, d_ff)
                  self.ffn_norm = LayerNormalization(d_model)
              
              def forward(self, x: np.ndarray, mask: np.ndarray = None,
                         training: bool = False) -> tuple:
                  """
                  Forward pass through encoder layer.
                  
                  Args:
                      x: Input (batch, seq_len, d_model)
                      mask: Optional attention mask
                      training: Whether in training mode
                  
                  Returns:
                      output: (batch, seq_len, d_model)
                      attention_weights: (batch, num_heads, seq_len, seq_len)
                  """
                  # Sublayer 1: Multi-head self-attention with pre-norm
                  normalized = self.attention_norm(x)
                  
                  # Self-attention: Q, K, V all from same input
                  attn_output, attn_weights = self.self_attention(
                      normalized, normalized, normalized, mask
                  )
                  
                  # Dropout + residual
                  if training and self.dropout > 0:
                      attn_output = self._dropout(attn_output, self.dropout)
                  x1 = x + attn_output
                  
                  # Sublayer 2: Feed-forward network with pre-norm
                  normalized = self.ffn_norm(x1)
                  ffn_output = self.ffn(normalized)
                  
                  # Dropout + residual
                  if training and self.dropout > 0:
                      ffn_output = self._dropout(ffn_output, self.dropout)
                  x2 = x1 + ffn_output
                  
                  return x2, attn_weights
              
              def _dropout(self, x: np.ndarray, rate: float) -> np.ndarray:
                  """Apply dropout."""
                  mask = np.random.binomial(1, 1 - rate, x.shape) / (1 - rate)
                  return x * mask
              
              def __call__(self, x: np.ndarray, mask: np.ndarray = None,
                          training: bool = False) -> tuple:
                  """Alias for forward."""
                  return self.forward(x, mask, training)
          
          
          # Example usage
          print("=== Transformer Encoder Layer ===\n")
          
          d_model = 512
          num_heads = 8
          d_ff = 2048
          batch_size = 2
          seq_len = 10
          
          encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff)
          
          # Input
          x = np.random.randn(batch_size, seq_len, d_model)
          
          # Forward pass
          output, attn_weights = encoder_layer(x, training=False)
          
          print(f"Configuration:")
          print(f"  Model dimension: {d_model}")
          print(f"  Attention heads: {num_heads}")
          print(f"  FFN dimension: {d_ff}")
          print()
          print(f"Input shape: {x.shape}")
          print(f"Output shape: {output.shape}")
          print(f"Attention weights shape: {attn_weights.shape}")
          print()
          print("Layer structure:")
          print("  1. Multi-head self-attention (bidirectional)")
          print("  2. Feed-forward network (position-wise)")
          print("  Each with: LayerNorm + residual connection")
      
      visualize_self_attention:
        language: python
        code: |
          def visualize_encoder_self_attention():
              """Visualize bidirectional self-attention in encoder."""
              
              print("\n=== Encoder Self-Attention Pattern ===\n")
              
              # Sample sentence
              tokens = ["The", "cat", "sat", "on", "the", "mat"]
              n = len(tokens)
              
              # Simulate attention weights (simplified)
              # In encoder: Every token can attend to ALL tokens
              np.random.seed(42)
              attention = np.random.rand(n, n)
              
              # Normalize rows (softmax approximation)
              attention = attention / attention.sum(axis=1, keepdims=True)
              
              print("Bidirectional self-attention:")
              print("Each token attends to ALL tokens (no masking)\n")
              print("Query →     ", "  ".join(f"{t:5s}" for t in tokens))
              print("-" * (6 + 7 * len(tokens)))
              
              for i, query_token in enumerate(tokens):
                  weights_str = "  ".join(f"{attention[i, j]:5.2f}" for j in range(n))
                  print(f"{query_token:5s}:  {weights_str}")
              
              print()
              print("Key observations:")
              print("  - Every position can attend to every position")
              print("  - No causal masking (unlike decoder)")
              print("  - Bidirectional context for understanding")
              print()
              
              # Show example: 'cat' attention
              cat_idx = 1
              print(f"Example: '{tokens[cat_idx]}' attention distribution:")
              for j, key_token in enumerate(tokens):
                  bar = "█" * int(attention[cat_idx, j] * 50)
                  print(f"  → {key_token:5s}: {bar} {attention[cat_idx, j]:.2f}")
          
          visualize_encoder_self_attention()
    
    security_implications:
      bidirectional_leakage: |
        Bidirectional attention sees full context:
        - "The password is [MASK]" → encoder sees both sides
        - Can infer masked content from context
        - Harder to hide sensitive information
        - Defense: Careful masking strategies, sanitize inputs
      
      attention_pattern_analysis: |
        Attention patterns reveal associations:
        - What the model considers related
        - Can expose private relationships (person-location, entity-attribute)
        - Adversary extracts attention to infer connections
        - Defense: Monitor attention patterns, detect leakage

  - topic_number: 2
    title: "Stacking Encoder Layers: Building Deep Representations"
    
    overview: |
      Transformer encoders stack multiple encoder layers (typically 6-12) to build 
      increasingly abstract representations. Each layer refines the representation, 
      capturing progressively higher-level features. Early layers focus on local syntax, 
      middle layers on semantics, late layers on task-specific abstractions.
    
    content:
      encoder_stack_architecture:
        stacking_pattern: |
          Input → Encoder Layer 1 → Encoder Layer 2 → ... → Encoder Layer N
          
          Each layer:
          - Takes previous layer's output as input
          - Applies self-attention + FFN
          - Outputs refined representation
        
        typical_depths:
          bert_base: "12 encoder layers"
          bert_large: "24 encoder layers"
          roberta: "12 or 24 layers"
          albert: "12 layers (parameter sharing)"
          t5_large: "24 encoder layers"
        
        why_stack_layers:
          hierarchical_features: |
            Layer 1: Surface features (POS, syntax)
            Layer 6: Semantic features (entities, relations)
            Layer 12: Abstract features (task-specific)
          
          non_linearity: |
            Each FFN adds non-linearity
            → More layers = more expressive transformations
          
          refinement: |
            Each layer refines previous layer's representation
            → Iterative improvement
      
      layer_specialization:
        early_layers_1_4:
          focus: "Local syntax, morphology"
          attention_patterns: "Short-range (adjacent tokens)"
          learned_features: "POS tags, basic parsing"
          
          example: |
            "running" → recognized as verb, -ing suffix
        
        middle_layers_5_8:
          focus: "Semantic relationships"
          attention_patterns: "Medium-range (clause-level)"
          learned_features: "Named entities, semantic roles"
          
          example: |
            "Paris" → recognized as location (capital of France)
        
        late_layers_9_12:
          focus: "Task-specific abstractions"
          attention_patterns: "Long-range, task-dependent"
          learned_features: "Task objectives, high-level reasoning"
          
          example: |
            Sentiment analysis: "good" → positive signal
      
      gradient_flow_in_deep_encoders:
        without_residuals: |
          12 layers: Gradient ∝ ∂F₁₂/∂x × ... × ∂F₁/∂x
          → Product of 12 terms → vanishing!
        
        with_residuals: |
          Each layer: y = x + F(x)
          → Shortest gradient path = 1
          → Can train 100+ layers!
        
        empirical_limits:
          post_norm: "~24 layers (gradient instability beyond)"
          pre_norm: "100+ layers (more stable)"
      
      parameter_scaling:
        per_layer: "~12 × d_model² parameters"
        
        total_parameters:
          bert_base_12_layers: |
            d_model = 768
            12 layers × 12 × 768² ≈ 85M parameters (just encoder layers)
          
          bert_large_24_layers: |
            d_model = 1024
            24 layers × 12 × 1024² ≈ 302M parameters
        
        memory_considerations: |
          Activations: n × d_model per layer
          Attention: h × n² per layer
          
          With 12 layers, both scale linearly with depth
    
    implementation:
      transformer_encoder_stack:
        language: python
        code: |
          class TransformerEncoder:
              """
              Multi-layer transformer encoder.
              
              Stacks N encoder layers with residual connections.
              """
              
              def __init__(self, num_layers: int, d_model: int, num_heads: int,
                          d_ff: int, dropout: float = 0.1):
                  """
                  Args:
                      num_layers: Number of encoder layers to stack
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  self.num_layers = num_layers
                  self.d_model = d_model
                  
                  # Create encoder layers
                  self.layers = []
                  for _ in range(num_layers):
                      layer = TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)
                      self.layers.append(layer)
              
              def forward(self, x: np.ndarray, mask: np.ndarray = None,
                         training: bool = False) -> tuple:
                  """
                  Forward pass through encoder stack.
                  
                  Args:
                      x: Input (batch, seq_len, d_model)
                      mask: Optional attention mask
                      training: Whether in training mode
                  
                  Returns:
                      output: Final encoder output (batch, seq_len, d_model)
                      all_attention_weights: List of attention weights per layer
                  """
                  all_attention_weights = []
                  
                  # Pass through each encoder layer sequentially
                  for i, layer in enumerate(self.layers):
                      x, attn_weights = layer(x, mask, training)
                      all_attention_weights.append(attn_weights)
                  
                  return x, all_attention_weights
              
              def __call__(self, x: np.ndarray, mask: np.ndarray = None,
                          training: bool = False) -> tuple:
                  """Alias for forward."""
                  return self.forward(x, mask, training)
          
          
          # Example usage
          print("\n=== Transformer Encoder Stack ===\n")
          
          num_layers = 6
          d_model = 512
          num_heads = 8
          d_ff = 2048
          batch_size = 2
          seq_len = 10
          
          encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff)
          
          # Input
          x = np.random.randn(batch_size, seq_len, d_model)
          
          # Forward pass
          output, all_attn_weights = encoder(x, training=False)
          
          print(f"Encoder configuration:")
          print(f"  Number of layers: {num_layers}")
          print(f"  Model dimension: {d_model}")
          print(f"  Attention heads: {num_heads}")
          print(f"  FFN dimension: {d_ff}")
          print()
          print(f"Input shape: {x.shape}")
          print(f"Output shape: {output.shape}")
          print(f"Attention weights per layer: {len(all_attn_weights)}")
          print()
          print("Processing flow:")
          for i in range(num_layers):
              print(f"  Layer {i+1}: Self-attention → FFN → refined representation")
      
      analyze_layer_outputs:
        language: python
        code: |
          def analyze_encoder_layers():
              """Analyze how representations change across encoder layers."""
              
              print("\n=== Encoder Layer Progression ===\n")
              
              num_layers = 6
              d_model = 128
              
              # Simulate layer outputs (simplified)
              np.random.seed(42)
              
              # Initial embedding
              x = np.random.randn(1, 10, d_model)
              
              print("Representation evolution across layers:\n")
              print(f"Layer | Mean  | Std   | Max   | Min")
              print("------|-------|-------|-------|-------")
              
              # Input embedding
              print(f"Input | {x.mean():5.2f} | {x.std():5.2f} | "
                    f"{x.max():5.2f} | {x.min():5.2f}")
              
              # Simulate layer progression
              for layer_idx in range(num_layers):
                  # Simulate transformation (simplified)
                  x = x + np.random.randn(*x.shape) * 0.1
                  
                  print(f"L{layer_idx+1:2d}   | {x.mean():5.2f} | {x.std():5.2f} | "
                        f"{x.max():5.2f} | {x.min():5.2f}")
              
              print()
              print("Observations:")
              print("  - Statistics remain stable (LayerNorm effect)")
              print("  - Each layer refines representation")
              print("  - Residual connections preserve information flow")
          
          analyze_encoder_layers()
    
    security_implications:
      layer_specific_attacks: |
        Different layers encode different information:
        - Early layers: Syntax (less sensitive)
        - Middle layers: Semantics (entities, relations - more sensitive)
        - Late layers: Task-specific (potentially most sensitive)
        - Adversary can target specific layers for extraction
        - Defense: Layer-wise access control, monitoring
      
      representation_drift_across_layers: |
        Representations change dramatically across layers:
        - Input: Surface features
        - Output: Abstract features
        - Adversary can extract different info from different layers
        - Defense: Monitor all layers, not just final output

  - topic_number: 3
    title: "Complete Encoder: Input Processing and Final Architecture"
    
    overview: |
      The complete transformer encoder adds input embeddings and positional encoding to 
      the encoder stack. Input tokens are converted to embeddings, positional information 
      is added, then the sequence flows through the encoder stack. The final output is 
      contextualized representations ready for downstream tasks.
    
    content:
      complete_encoder_architecture:
        components_in_order:
          step_1_tokenization: |
            Input: Raw text ("The cat sat")
            Output: Token IDs [42, 157, 89]
          
          step_2_embedding: |
            Token IDs → Embeddings
            [42, 157, 89] → [(d_model,), (d_model,), (d_model,)]
          
          step_3_positional_encoding: |
            Add positional encoding
            embedding + positional_encoding
          
          step_4_encoder_stack: |
            Pass through N encoder layers
            Layer 1 → Layer 2 → ... → Layer N
          
          step_5_output: |
            Final output: Contextualized representations
            Each token encoding contains information from entire sequence
        
        mathematical_formulation: |
          # Input processing
          E = TokenEmbedding(input_ids)           # (seq_len, d_model)
          PE = PositionalEncoding(seq_len)        # (seq_len, d_model)
          x₀ = E + PE                             # (seq_len, d_model)
          
          # Encoder stack
          x₁ = EncoderLayer₁(x₀)
          x₂ = EncoderLayer₂(x₁)
          ...
          xₙ = EncoderLayerₙ(xₙ₋₁)
          
          # Output
          output = xₙ                             # (seq_len, d_model)
      
      input_embeddings:
        token_embeddings:
          purpose: "Convert token IDs to dense vectors"
          dimension: "d_model (512, 768, 1024, etc.)"
          
          implementation: |
            Embedding matrix: (vocab_size, d_model)
            
            For token ID t:
            embedding = embedding_matrix[t]
          
          parameters: "vocab_size × d_model"
          
          example_bert: |
            Vocabulary: 30,000 tokens
            d_model: 768
            Parameters: 30,000 × 768 = 23M
        
        positional_encoding:
          methods:
            sinusoidal: "Fixed (Vaswani et al., 2017)"
            learned: "Learned embeddings (BERT, GPT-2)"
            rope: "Rotary embeddings (modern LLMs)"
          
          addition_vs_concatenation: |
            Addition: embedding + positional (preserves dimension)
            Concatenation: [embedding, positional] (doubles dimension)
            
            Transformers use addition
      
      output_representations:
        contextualized_embeddings: |
          Each token's final representation contains:
          - Token's own semantic meaning
          - Information from surrounding context
          - Long-range dependencies
          
          Unlike word2vec (static): BERT embeddings are contextual
        
        downstream_task_usage:
          classification: |
            Use [CLS] token representation (first position)
            → Feed to classifier
          
          token_level_tasks: |
            Use each token's representation
            → Named entity recognition, POS tagging
          
          sequence_to_sequence: |
            Use all representations as encoder output
            → Feed to decoder for translation, summarization
      
      complete_parameter_count:
        embeddings: "vocab_size × d_model"
        positional_encoding: "0 (sinusoidal) or max_len × d_model (learned)"
        encoder_layers: "num_layers × 12 × d_model²"
        
        bert_base_total: |
          Embeddings: 30K × 768 ≈ 23M
          Positional: 512 × 768 ≈ 0.4M
          12 encoder layers: 12 × 12 × 768² ≈ 85M
          Total: ~110M parameters
    
    implementation:
      complete_transformer_encoder:
        language: python
        code: |
          class CompleteTransformerEncoder:
              """
              Complete transformer encoder with embeddings.
              
              Components:
              1. Token embeddings
              2. Positional encoding
              3. Encoder stack
              """
              
              def __init__(self, vocab_size: int, max_len: int, num_layers: int,
                          d_model: int, num_heads: int, d_ff: int, 
                          dropout: float = 0.1):
                  """
                  Args:
                      vocab_size: Vocabulary size
                      max_len: Maximum sequence length
                      num_layers: Number of encoder layers
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  self.vocab_size = vocab_size
                  self.max_len = max_len
                  self.d_model = d_model
                  
                  # Token embeddings
                  self.token_embeddings = np.random.randn(vocab_size, d_model) * 0.01
                  
                  # Positional encoding (sinusoidal)
                  from section_3_09_positional_encoding import SinusoidalPositionalEncoding
                  self.positional_encoding = SinusoidalPositionalEncoding(d_model, max_len)
                  
                  # Encoder stack
                  self.encoder = TransformerEncoder(num_layers, d_model, num_heads, 
                                                    d_ff, dropout)
                  
                  self.dropout = dropout
              
              def embed(self, input_ids: np.ndarray) -> np.ndarray:
                  """
                  Convert token IDs to embeddings with positional encoding.
                  
                  Args:
                      input_ids: Token IDs (batch, seq_len)
                  
                  Returns:
                      embeddings: (batch, seq_len, d_model)
                  """
                  # Token embeddings
                  token_emb = self.token_embeddings[input_ids]  # (batch, seq_len, d_model)
                  
                  # Positional encoding
                  seq_len = input_ids.shape[1]
                  pos_enc = self.positional_encoding(seq_len)  # (seq_len, d_model)
                  
                  # Add positional encoding (broadcast over batch)
                  embeddings = token_emb + pos_enc
                  
                  return embeddings
              
              def forward(self, input_ids: np.ndarray, mask: np.ndarray = None,
                         training: bool = False) -> tuple:
                  """
                  Complete encoder forward pass.
                  
                  Args:
                      input_ids: Token IDs (batch, seq_len)
                      mask: Optional attention mask
                      training: Whether in training mode
                  
                  Returns:
                      output: Encoder output (batch, seq_len, d_model)
                      all_attention_weights: List of attention weights per layer
                  """
                  # Embed input
                  x = self.embed(input_ids)
                  
                  # Dropout on embeddings (if training)
                  if training and self.dropout > 0:
                      mask_drop = np.random.binomial(1, 1 - self.dropout, 
                                                     x.shape) / (1 - self.dropout)
                      x = x * mask_drop
                  
                  # Pass through encoder stack
                  output, all_attn_weights = self.encoder(x, mask, training)
                  
                  return output, all_attn_weights
              
              def __call__(self, input_ids: np.ndarray, mask: np.ndarray = None,
                          training: bool = False) -> tuple:
                  """Alias for forward."""
                  return self.forward(input_ids, mask, training)
          
          
          # Example usage
          print("\n=== Complete Transformer Encoder ===\n")
          
          vocab_size = 30000
          max_len = 512
          num_layers = 6
          d_model = 512
          num_heads = 8
          d_ff = 2048
          
          complete_encoder = CompleteTransformerEncoder(
              vocab_size, max_len, num_layers, d_model, num_heads, d_ff
          )
          
          # Sample input (token IDs)
          batch_size = 2
          seq_len = 10
          input_ids = np.random.randint(0, vocab_size, (batch_size, seq_len))
          
          # Forward pass
          encoder_output, attn_weights = complete_encoder(input_ids, training=False)
          
          print(f"Encoder configuration:")
          print(f"  Vocabulary size: {vocab_size:,}")
          print(f"  Max sequence length: {max_len}")
          print(f"  Number of layers: {num_layers}")
          print(f"  Model dimension: {d_model}")
          print(f"  Attention heads: {num_heads}")
          print()
          print(f"Input IDs shape: {input_ids.shape}")
          print(f"Encoder output shape: {encoder_output.shape}")
          print()
          print("Complete processing pipeline:")
          print("  1. Token IDs → Token embeddings")
          print("  2. Add positional encoding")
          print("  3. Pass through encoder stack (6 layers)")
          print("  4. Output: Contextualized representations")
      
      parameter_count_analysis:
        language: python
        code: |
          def analyze_encoder_parameters():
              """Analyze parameter distribution in transformer encoder."""
              
              print("\n=== Encoder Parameter Analysis ===\n")
              
              configs = [
                  ("BERT-base", 30000, 512, 12, 768, 8, 3072),
                  ("BERT-large", 30000, 512, 24, 1024, 16, 4096),
              ]
              
              for name, vocab, max_len, n_layers, d_model, n_heads, d_ff in configs:
                  # Embeddings
                  token_emb = vocab * d_model
                  pos_emb = max_len * d_model  # If learned
                  
                  # Per encoder layer
                  attn_params = 4 * d_model * d_model
                  ffn_params = 2 * d_model * d_ff
                  norm_params = 4 * d_model  # 2 LayerNorms
                  layer_params = attn_params + ffn_params + norm_params
                  
                  # Total
                  total_encoder = n_layers * layer_params
                  total_params = token_emb + pos_emb + total_encoder
                  
                  print(f"{name}:")
                  print(f"  Token embeddings:    {token_emb/1e6:6.1f}M")
                  print(f"  Positional (learned): {pos_emb/1e6:6.1f}M")
                  print(f"  Encoder layers ({n_layers}):  {total_encoder/1e6:6.1f}M")
                  print(f"  Total:               {total_params/1e6:6.1f}M")
                  print()
          
          analyze_encoder_parameters()
    
    security_implications:
      embedding_inversion: |
        Token embeddings can be inverted:
        - Given encoder output, adversary can approximate input tokens
        - Embedding matrix is relatively small (vocab_size × d_model)
        - Nearest neighbor search reveals likely tokens
        - Defense: Differential privacy on embeddings, output sanitization
      
      contextualized_leakage: |
        Contextualized representations leak more than input:
        - Contain information from entire sequence
        - Can reveal relationships not explicit in input
        - Example: "He" representation contains "John" info (from coreference)
        - Defense: Sanitize context, limit attention scope

key_takeaways:
  critical_concepts:
    - concept: "Encoder layer: self-attention → FFN, each with LayerNorm + residuals"
      why_it_matters: "Core building block of transformer encoder"
    
    - concept: "Self-attention is bidirectional: every token attends to all tokens"
      why_it_matters: "Enables full context understanding for NLU tasks"
    
    - concept: "Stack 6-12 encoder layers for hierarchical representations"
      why_it_matters: "Early layers: syntax, middle: semantics, late: task-specific"
    
    - concept: "Complete encoder: embeddings + positional + encoder stack"
      why_it_matters: "Token IDs → contextualized representations"
    
    - concept: "Pre-norm configuration enables stable deep training"
      why_it_matters: "Can stack 100+ layers with pre-norm, only 24 with post-norm"
  
  actionable_steps:
    - step: "Implement single encoder layer from scratch"
      verification: "Self-attention + FFN with pre-norm and residuals"
    
    - step: "Stack multiple encoder layers"
      verification: "Sequential processing, each layer refines representation"
    
    - step: "Build complete encoder with embeddings"
      verification: "Token IDs → embeddings + positional → encoder stack"
    
    - step: "Visualize bidirectional self-attention patterns"
      verification: "Show every token attending to all tokens"
    
    - step: "Analyze layer specialization"
      verification: "Early: syntax, middle: semantics, late: task-specific"
  
  security_principles:
    - principle: "Bidirectional attention sees full context (harder to hide info)"
      application: "Masking strategies less effective, sanitize carefully"
    
    - principle: "Attention patterns reveal what model considers related"
      application: "Can expose private associations, monitor attention"
    
    - principle: "Different layers encode different information types"
      application: "Layer-specific attacks, monitor all layers"
    
    - principle: "Contextualized embeddings leak beyond input text"
      application: "Contain inferred information, sanitize outputs"
    
    - principle: "Embedding inversion can recover approximate inputs"
      application: "Protect encoder outputs, use differential privacy"
  
  common_mistakes:
    - mistake: "Using decoder's causal masking in encoder"
      fix: "Encoder uses NO masking (bidirectional), decoder uses causal mask"
    
    - mistake: "Forgetting positional encoding"
      fix: "Always add positional encoding to embeddings"
    
    - mistake: "Self-attention with different Q, K, V sources"
      fix: "Self-attention: Q, K, V all from SAME input"
    
    - mistake: "Not preserving shape through encoder layers"
      fix: "Each layer: (batch, seq_len, d_model) → (batch, seq_len, d_model)"
    
    - mistake: "Assuming encoder output is task-ready"
      fix: "Need task-specific head (classifier, etc.) on top"
  
  integration_with_book:
    from_section_3_11:
      - "Layer normalization and residual connections (used in each layer)"
      - "Pre-norm configuration for stability"
    
    from_sections_3_8_3_10:
      - "Multi-head attention (sublayer 1)"
      - "Feed-forward network (sublayer 2)"
    
    to_next_section:
      - "Section 3.13: Transformer decoder (autoregressive generation)"
      - "Masked self-attention (causal)"
      - "Encoder-decoder cross-attention"
  
  looking_ahead:
    next_concepts:
      - "Transformer decoder architecture"
      - "Masked (causal) self-attention"
      - "Encoder-decoder cross-attention"
      - "Autoregressive generation"
    
    skills_to_build:
      - "Implement transformer decoder"
      - "Build encoder-decoder transformer"
      - "Implement sequence generation"
      - "Train complete transformer model"
  
  final_thoughts: |
    The transformer encoder is a powerful architecture for understanding sequences. It takes 
    token IDs, converts them to embeddings with positional encoding, then passes them through 
    a stack of encoder layers. Each layer applies bidirectional self-attention (every token 
    sees every token) followed by feed-forward network, both with residual connections and 
    layer normalization for training stability.
    
    Stacking 6-12 layers builds hierarchical representations: early layers capture syntax 
    and local patterns, middle layers extract semantic features and entities, late layers 
    develop task-specific abstractions. The bidirectional self-attention is key - unlike 
    the decoder's causal attention, encoders see full context in both directions, enabling 
    comprehensive understanding.
    
    The complete encoder (embeddings + positional + stack) transforms discrete token IDs 
    into rich contextualized representations. Each output embedding contains information 
    from the entire input sequence, capturing long-range dependencies and complex 
    relationships. This powers BERT, RoBERTa, and countless other NLU systems.
    
    From a security perspective: bidirectional attention makes hiding information harder - 
    the encoder sees full context and can infer masked content. Attention patterns reveal 
    associations the model learns, potentially exposing private relationships. Different 
    layers encode different information (syntax vs semantics vs task), creating layer-specific 
    attack vectors. Contextualized representations leak beyond input text, containing inferred 
    information. Understanding encoder architecture enables both attacks (model inversion, 
    representation extraction) and defenses (sanitization, monitoring, access control).
    
    Next: Section 3.13 covers the transformer decoder - the autoregressive generation 
    component that uses masked self-attention and encoder-decoder cross-attention to 
    generate output sequences one token at a time.

---
