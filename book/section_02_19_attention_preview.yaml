# section_02_19_attention_preview.yaml

---
document_info:
  chapter: "02"
  section: "19"
  title: "Attention Mechanism Preview"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 5
  tags: ["attention", "self-attention", "transformers", "encoder-decoder", "alignment", "context-vector"]

# ============================================================================
# SECTION 02_19: ATTENTION MECHANISM PREVIEW
# ============================================================================

section_02_19_attention_preview:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Attention mechanisms revolutionized sequence modeling by allowing networks
    to focus on relevant parts of input when producing output. Unlike RNNs
    that compress entire sequence into fixed-size hidden state, attention
    creates dynamic context vectors that look at different input positions
    for each output step.
    
    This preview introduces core attention concepts before Chapter 3's deep
    dive into transformers. You'll understand the fundamental problem attention
    solves (fixed-size bottleneck), learn the attention computation (query-key-
    value mechanism), implement basic attention from scratch, see how it enables
    100x longer sequences than RNNs, and connect to security implications
    (attention patterns leak information, can be exploited).
    
    Note: This is a PREVIEW. Full transformer architecture, self-attention,
    multi-head attention, and modern LLMs covered in Chapter 3. This section
    provides foundation to understand why transformers replaced RNNs.
  
  learning_objectives:
    
    conceptual:
      - "Understand bottleneck problem in RNN encoder-decoder"
      - "Grasp attention as weighted sum of values"
      - "Know query-key-value paradigm"
      - "Understand alignment scores and softmax"
      - "Recognize why attention enables longer sequences"
      - "Preview self-attention (full coverage in Chapter 3)"
    
    practical:
      - "Implement basic attention mechanism (NumPy)"
      - "Visualize attention weights (alignment matrix)"
      - "Build simple seq2seq with attention"
      - "Compare RNN vs RNN+attention empirically"
      - "Understand attention output shapes"
      - "Bridge to Chapter 3 (transformers)"
    
    security_focused:
      - "Attention patterns leak sensitive information"
      - "Adversarial inputs exploit attention distribution"
      - "Attention weights reveal model architecture"
      - "Preview: prompt injection via attention manipulation"
  
  prerequisites:
    - "Sections 02_17-02_18 (RNN, LSTM/GRU)"
    - "Understanding of encoder-decoder architecture"
    - "Matrix operations and dot products"
  
  # --------------------------------------------------------------------------
  # Topic 1: The Bottleneck Problem
  # --------------------------------------------------------------------------
  
  bottleneck_problem:
    
    rnn_encoder_decoder:
      
      architecture: |
        Standard sequence-to-sequence (seq2seq):
        
        Encoder (RNN/LSTM):
        Input sequence → h_1, h_2, ..., h_T → final h_T
        
        Decoder (RNN/LSTM):
        h_T (context) → y_1, y_2, ..., y_M
        
        Key: Decoder only sees h_T (final encoder state)
      
      the_bottleneck: |
        Problem: Entire input sequence compressed into single vector h_T
        
        Example: Machine translation
        Input: "The cat sat on the mat" (6 words)
        Encoder: All 6 words → single h_T vector (e.g., 512 dim)
        
        Information loss:
        - Early words (e.g., "The") mostly forgotten
        - Position information lost
        - Long sentences → more compression → more loss
      
      empirical_failure: |
        Seq2seq without attention:
        - Works okay for short sentences (<20 words)
        - Fails on longer sentences (>50 words)
        
        English → French translation:
        10 words: 65% BLEU
        30 words: 45% BLEU
        50 words: 25% BLEU (terrible!)
        
        Performance degrades with length
    
    what_we_want:
      
      selective_access: |
        When translating "sat", decoder should focus on:
        - "sat" (source word)
        - "cat" (subject)
        - "on" (preposition)
        
        Not: "The" or "mat" (less relevant for this word)
      
      dynamic_context: |
        Different output words need different context:
        
        Translating "The" → focus on "The"
        Translating "cat" → focus on "cat", "The"
        Translating "sat" → focus on "sat", "cat"
        
        Context should change dynamically per output step
  
  # --------------------------------------------------------------------------
  # Topic 2: Attention Mechanism
  # --------------------------------------------------------------------------
  
  attention_mechanism:
    
    core_idea:
      
      weighted_sum: |
        Instead of single context vector h_T:
        
        For each decoder step, compute context c_t:
        c_t = Σ_i α_t,i · h_i
        
        Where:
        - h_i: encoder hidden states (all of them, not just last)
        - α_t,i: attention weights (how much to focus on position i)
        - c_t: context vector for decoder step t
      
      attention_weights: |
        α_t,i = how much decoder step t should attend to encoder step i
        
        Properties:
        - α_t,i ≥ 0 (non-negative)
        - Σ_i α_t,i = 1 (sum to 1, like probabilities)
        
        Computed via softmax over alignment scores
    
    attention_computation:
      
      step_by_step: |
        Given:
        - Query: decoder state s_t
        - Keys: encoder states h_1, ..., h_T
        
        Step 1: Compute alignment scores
        e_t,i = score(s_t, h_i)
        
        Measures compatibility between query s_t and key h_i
        
        Step 2: Compute attention weights via softmax
        α_t,i = exp(e_t,i) / Σ_j exp(e_t,j)
        
        Normalizes scores to probabilities
        
        Step 3: Compute context vector
        c_t = Σ_i α_t,i · h_i
        
        Weighted sum of values (here, values = keys = h_i)
      
      score_functions: |
        How to compute score(s_t, h_i)?
        
        Option 1 - Dot product (simplest):
        score(s_t, h_i) = s_t^T · h_i
        
        Option 2 - Scaled dot product:
        score(s_t, h_i) = (s_t^T · h_i) / √d
        
        Option 3 - Additive (Bahdanau):
        score(s_t, h_i) = v^T · tanh(W_s · s_t + W_h · h_i)
        
        Option 4 - Bilinear:
        score(s_t, h_i) = s_t^T · W · h_i
        
        Most common: Scaled dot product (used in Transformers)
    
    implementation: |
      import numpy as np
      
      class AttentionLayer:
          """
          Basic attention mechanism.
          
          Computes context vector as weighted sum of encoder states.
          """
          
          def __init__(self, hidden_size):
              self.hidden_size = hidden_size
              
              # Learnable parameters for additive attention
              self.W_s = np.random.randn(hidden_size, hidden_size) * 0.01
              self.W_h = np.random.randn(hidden_size, hidden_size) * 0.01
              self.v = np.random.randn(hidden_size, 1) * 0.01
          
          def forward(self, query, keys, values):
              """
              Compute attention.
              
              Parameters:
              - query: decoder state (hidden_size, 1)
              - keys: encoder states (hidden_size, T) where T = sequence length
              - values: typically same as keys (hidden_size, T)
              
              Returns:
              - context: context vector (hidden_size, 1)
              - attention_weights: (T, 1) attention distribution
              """
              T = keys.shape[1]  # sequence length
              
              # Step 1: Compute alignment scores (additive attention)
              # Broadcast query to match keys shape
              query_expanded = query.repeat(T, axis=1)  # (hidden, T)
              
              # e_i = v^T · tanh(W_s·query + W_h·keys)
              scores = self.v.T @ np.tanh(
                  self.W_s @ query_expanded + self.W_h @ keys
              )  # (1, T)
              
              scores = scores.T  # (T, 1)
              
              # Step 2: Compute attention weights (softmax)
              attention_weights = self._softmax(scores)  # (T, 1)
              
              # Step 3: Compute context vector (weighted sum)
              context = values @ attention_weights  # (hidden, T) @ (T, 1) = (hidden, 1)
              
              return context, attention_weights
          
          def _softmax(self, x):
              """Numerically stable softmax"""
              exp_x = np.exp(x - np.max(x))
              return exp_x / np.sum(exp_x)
      
      # Example usage
      attention = AttentionLayer(hidden_size=128)
      
      # Encoder outputs (e.g., 6 time steps)
      encoder_states = np.random.randn(128, 6)
      
      # Decoder state at some step
      decoder_state = np.random.randn(128, 1)
      
      # Compute attention
      context, weights = attention.forward(
          query=decoder_state,
          keys=encoder_states,
          values=encoder_states
      )
      
      print(f"Context shape: {context.shape}")  # (128, 1)
      print(f"Attention weights: {weights.T}")  # (1, 6) - sums to 1
      # Example output: [0.05, 0.15, 0.40, 0.25, 0.10, 0.05]
      # → Highest attention (0.40) on position 3
  
  # --------------------------------------------------------------------------
  # Topic 3: Seq2Seq with Attention
  # --------------------------------------------------------------------------
  
  seq2seq_with_attention:
    
    architecture:
      
      encoder: |
        Same as before: RNN/LSTM processes input
        
        Input: x_1, ..., x_T
        Encoder: h_1, h_2, ..., h_T
        
        Key change: Save ALL hidden states, not just h_T
      
      decoder: |
        At each step t:
        
        1. Compute attention over encoder states
           c_t = Attention(s_{t-1}, h_1:T)
        
        2. Concatenate context with input
           decoder_input = [y_{t-1}, c_t]
        
        3. Update decoder state
           s_t = LSTM(decoder_input, s_{t-1})
        
        4. Generate output
           y_t = Softmax(W · s_t)
      
      key_difference: |
        Without attention:
        - Decoder sees only h_T (fixed context)
        
        With attention:
        - Decoder sees dynamic c_t (different for each step)
        - Can focus on relevant input positions
    
    training:
      
      forward_pass: |
        1. Encode: Get h_1, ..., h_T
        2. For each decoder step:
           a. Compute attention → c_t
           b. Decode → y_t
        3. Compute loss (cross-entropy)
      
      backward_pass: |
        Gradients flow through:
        - Decoder RNN (standard BPTT)
        - Attention weights (via softmax)
        - Encoder RNN (via attention-weighted gradients)
        
        Key: ALL encoder states receive gradients (not just h_T)
        → Better gradient flow than standard seq2seq
    
    example_translation:
      
      task: |
        English: "The cat sat on the mat"
        French: "Le chat est assis sur le tapis"
      
      attention_visualization: |
        When generating "chat" (cat):
        Attention weights on English words:
        
        The: 0.05  (low - not relevant)
        cat: 0.85  (high - main focus!)
        sat: 0.05  (low - not relevant here)
        on:  0.02  (very low)
        the: 0.02  (very low)
        mat: 0.01  (very low)
        
        When generating "assis" (sat):
        
        The: 0.03
        cat: 0.25  (some attention - subject)
        sat: 0.65  (high - main verb!)
        on:  0.05
        the: 0.01
        mat: 0.01
        
        Attention dynamically focuses on relevant words!
  
  # --------------------------------------------------------------------------
  # Topic 4: Visualizing Attention
  # --------------------------------------------------------------------------
  
  visualizing_attention:
    
    alignment_matrix:
      
      what_it_shows: |
        Alignment matrix: (T_target × T_source)
        
        Rows: target (output) words
        Columns: source (input) words
        Cell (i,j): attention weight α_i,j
        
        Heatmap shows which source words decoder attends to
        for each target word
      
      implementation: |
        def visualize_attention(attention_weights, source_words, target_words):
            """
            Visualize attention alignment matrix.
            
            Parameters:
            - attention_weights: (T_target, T_source) matrix
            - source_words: list of source words
            - target_words: list of target words
            """
            import matplotlib.pyplot as plt
            
            fig, ax = plt.subplots(figsize=(10, 8))
            
            # Plot heatmap
            im = ax.imshow(attention_weights, cmap='Blues')
            
            # Set ticks
            ax.set_xticks(range(len(source_words)))
            ax.set_yticks(range(len(target_words)))
            ax.set_xticklabels(source_words, rotation=45)
            ax.set_yticklabels(target_words)
            
            # Add colorbar
            plt.colorbar(im, ax=ax)
            
            # Labels
            ax.set_xlabel('Source Words')
            ax.set_ylabel('Target Words')
            ax.set_title('Attention Alignment Matrix')
            
            plt.tight_layout()
            plt.show()
      
      interpretation: |
        Diagonal pattern: Monotonic alignment (similar word order)
        Off-diagonal: Reordering (different word order in languages)
        Bright spots: Strong attention (important word pairs)
        Dim areas: Weak attention (less relevant)
    
    attention_patterns:
      
      monotonic_alignment: |
        English: "I love you"
        Spanish: "Te amo"
        
        Mostly diagonal (similar order)
      
      reordering: |
        English: "The red car"
        French: "La voiture rouge"
        
        "rouge" attends to "red" (reordering)
      
      many_to_one: |
        English: "do not"
        French: "ne ... pas"
        
        Multiple source words map to single target word
  
  # --------------------------------------------------------------------------
  # Topic 5: Why Attention Works Better
  # --------------------------------------------------------------------------
  
  why_attention_works:
    
    addresses_bottleneck:
      
      information_capacity: |
        Without attention:
        - Fixed-size vector (e.g., 512 dim)
        - Must compress entire sequence
        - Information loss inevitable
        
        With attention:
        - Access to ALL encoder states
        - No compression necessary
        - Full information available
      
      variable_length_sequences: |
        10-word sentence: h_1, ..., h_10 (10 vectors available)
        100-word sentence: h_1, ..., h_100 (100 vectors available)
        
        Context capacity scales with input length!
    
    better_gradient_flow:
      
      shorter_paths: |
        Without attention:
        Loss at step 50 → gradients through decoder (50 steps)
                        → encoder final state
                        → encoder step 1 (50+ steps)
        Total: 100+ steps of backprop
        
        With attention:
        Loss at step 50 → attention weights
                        → directly to relevant encoder state
        Total: Much shorter path!
      
      all_encoder_states_updated: |
        Standard seq2seq: Only h_T receives direct gradient
        
        Attention seq2seq: ALL h_i receive gradients
        → Better encoder training
        → Better representations
    
    empirical_improvements:
      
      translation_quality: |
        English → French (WMT dataset):
        
        Seq2seq without attention:
        - 10 words: 28 BLEU
        - 30 words: 18 BLEU
        - 50 words: 10 BLEU
        
        Seq2seq with attention:
        - 10 words: 32 BLEU
        - 30 words: 29 BLEU
        - 50 words: 25 BLEU
        
        Attention maintains quality on longer sequences!
      
      other_tasks: |
        Image captioning: +15% CIDEr score
        Speech recognition: 20% WER reduction
        Summarization: +8 ROUGE score
        
        Attention helps across many sequence tasks
  
  # --------------------------------------------------------------------------
  # Topic 6: Self-Attention Preview
  # --------------------------------------------------------------------------
  
  self_attention_preview:
    
    limitation_of_encoder_decoder_attention:
      
      sequential_processing: |
        Encoder still processes sequentially:
        h_1 → h_2 → h_3 → ... → h_T
        
        Time step 3 only sees up to position 3
        Can't look ahead to position 10
      
      rnn_bottleneck_remains: |
        Even with attention, encoder is RNN:
        - Sequential (can't parallelize)
        - Vanishing gradients still possible
        - Long sequences still challenging
    
    self_attention_solution:
      
      attention_within_sequence: |
        Self-attention: Each position attends to all positions
        
        For position i:
        - Query: representation at i
        - Keys: representations at ALL positions
        - Compute attention over entire sequence
        
        Result: position i sees entire sequence at once!
      
      parallelization: |
        No sequential dependency:
        - Can compute all positions in parallel
        - No RNN needed
        - Much faster
        
        This is the foundation of Transformers!
    
    bridge_to_chapter_3:
      
      what_chapter_3_covers: |
        Full transformer architecture:
        - Self-attention mechanism (detailed)
        - Multi-head attention (multiple attention in parallel)
        - Positional encoding (position information)
        - Encoder-decoder transformer
        - GPT (decoder-only)
        - BERT (encoder-only)
        
        This section: Basic attention concept
        Chapter 3: Complete transformer from scratch
      
      preview_transformers: |
        Transformer = Attention is All You Need (2017 paper)
        
        Key ideas:
        - Replace RNNs with self-attention
        - Process entire sequence in parallel
        - Stack many attention layers
        
        Result: State-of-the-art in NLP
        Foundation for GPT, BERT, Claude, ChatGPT
  
  # --------------------------------------------------------------------------
  # Topic 7: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    attention_patterns_leak_information:
      
      observation: |
        Attention weights reveal what model focuses on
        
        Example:
        Input: "The password is secret123"
        Model generates: "The password is [REDACTED]"
        
        Attention weights show model attended to "secret123"
        → Leaks what was redacted!
      
      extraction_attack: |
        Attacker can analyze attention patterns:
        1. Query model with test inputs
        2. Observe attention weights (if accessible)
        3. Infer sensitive information from patterns
        
        Defense: Don't expose attention weights in API
    
    adversarial_attention_manipulation:
      
      observation: |
        Adversarial input can manipulate attention:
        
        Clean input: "This product is great"
        → Attention: "great" (positive sentiment)
        
        Adversarial: "This product is not not great"
        → Attention: "not not" (confused)
        → Misclassification
      
      attack_strategy: |
        Design input to:
        - Force attention away from key words
        - Dilute attention across irrelevant tokens
        - Concentrate attention on misleading words
    
    attention_reveals_architecture:
      
      observation: |
        Attention weight matrix dimensions reveal:
        - Sequence length processing
        - Number of attention heads (Chapter 3)
        - Hidden dimensions
        
        Information useful for model extraction
      
      mitigation: |
        API design:
        - Don't expose attention weights
        - Only return final predictions
        - Rate limit queries (prevent probing)
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Fixed context bottleneck: RNN seq2seq compresses entire input to single h_T, loses information"
      - "Attention = dynamic weighted sum: c_t = Σ α_t,i·h_i, focuses on relevant positions per output step"
      - "Three-step computation: (1) scores = query·keys, (2) weights = softmax(scores), (3) context = weights·values"
      - "Attention enables longer sequences: 50-word translation 25 BLEU vs 10 BLEU without attention"
      - "Better gradient flow: direct paths from loss to all encoder states, not just final h_T"
      - "Self-attention next step: positions attend to each other, enables parallelization, foundation for Transformers"
    
    actionable_steps:
      - "Add attention to seq2seq: saves ALL encoder states, computes dynamic context each decoder step"
      - "Use scaled dot-product scoring: (query^T · key) / √d, most common and efficient"
      - "Visualize attention weights: plot alignment matrix, verify model focuses on correct positions"
      - "Monitor attention saturation: all weights near 1/T = uniform (not learning), near 1 on single position = too focused"
      - "Prepare for Chapter 3: this is preview, transformers are full self-attention architecture"
      - "Don't expose attention in APIs: leaks information about model behavior and sensitive data"
    
    security_principles:
      - "Attention weights leak information: show which input tokens model focuses on, can reveal sensitive content"
      - "Adversarial attention manipulation: craft inputs to force attention away from key information"
      - "Attention dimensions reveal architecture: matrix shapes expose hidden dimensions and sequence processing"
      - "Preview Chapter 10: full attention-based attacks covered in adversarial ML section"
    
    bridge_to_chapter_3:
      - "This section: Basic encoder-decoder attention, foundation concepts"
      - "Chapter 3 covers: Self-attention, multi-head attention, full Transformer architecture"
      - "Chapter 3 implements: Transformer from scratch, attention mechanism in detail"
      - "Modern LLMs: GPT, BERT, Claude all based on self-attention from Chapter 3"
      - "Don't stop here: Attention is critical for AI security, need full understanding from Chapter 3"
    
    debugging_checklist:
      - "Attention weights all equal: not learning, check score function and initialization"
      - "Attention stuck on one position: over-focusing, reduce temperature or check softmax"
      - "No improvement vs baseline: ensure attention integrated correctly, decoder uses context c_t"
      - "Alignment matrix doesn't make sense: check source/target word order, verify attention computation"
      - "Slow training: attention adds computation, normal, or optimize with batched matrix ops"

---
