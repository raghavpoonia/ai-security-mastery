# section_02_10_regularization_l1_l2.yaml

---
document_info:
  chapter: "02"
  section: "10"
  title: "Regularization: L1, L2, Weight Decay"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 5
  tags: ["regularization", "l1", "l2", "ridge", "lasso", "weight-decay", "elastic-net"]

# ============================================================================
# SECTION 02_10: REGULARIZATION - L1, L2, WEIGHT DECAY
# ============================================================================

section_02_10_l1_l2_weight_decay:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Neural networks with millions of parameters easily memorize training data,
    achieving perfect training accuracy but failing on new data. This is
    overfitting - the model learns noise instead of signal.
    
    Weight regularization constrains model complexity by penalizing large
    weights. This section covers the two fundamental approaches: L2 (Ridge)
    and L1 (Lasso) regularization, their mathematical foundations, the subtle
    but important difference between L2 and weight decay, and how to combine
    them (Elastic Net).
    
    You'll implement each method from scratch, understand their implicit biases
    (L2 prefers many small weights, L1 induces sparsity), tune regularization
    strength systematically, and learn their security implications for
    adversarial robustness.
  
  learning_objectives:
    
    conceptual:
      - "Understand how weight penalties constrain hypothesis space"
      - "Grasp L2's preference for distributed representations"
      - "Understand L1's sparsity-inducing property"
      - "Know the subtle difference between L2 regularization and weight decay"
      - "Recognize when overfitting occurs from learning curves"
      - "Connect regularization to generalization bounds"
    
    practical:
      - "Implement L1, L2, and weight decay from scratch"
      - "Tune regularization strength (λ) systematically"
      - "Diagnose overfitting vs underfitting from train/val gap"
      - "Combine L1 and L2 (Elastic Net)"
      - "Measure regularization impact on test accuracy"
      - "Apply regularization in production neural networks"
    
    security_focused:
      - "L2 regularization improves adversarial robustness"
      - "Smaller weights → less sensitive to input perturbations"
      - "Regularization strength affects model extraction difficulty"
      - "Over-regularization can make models easier to fool"
  
  prerequisites:
    - "Sections 02_05-02_08 (backpropagation, optimization algorithms)"
    - "Understanding of gradient descent and weight updates"
    - "Familiarity with loss functions (cross-entropy, MSE)"
    - "Concept of overfitting from Chapter 01"
  
  # --------------------------------------------------------------------------
  # Topic 1: Understanding Overfitting and the Need for Regularization
  # --------------------------------------------------------------------------
  
  understanding_overfitting:
    
    the_overfitting_problem:
      
      definition: |
        Overfitting: Model memorizes training data (including noise and outliers)
        instead of learning underlying generalizable patterns.
        
        Symptoms:
        - Training loss: 0.05 (very low, near perfect)
        - Validation loss: 0.45 (high, poor generalization)
        - Large train-val gap (>0.3)
        
        The model "knows" the training data perfectly but fails on new data.
      
      visual_intuition: |
        Classification with 10 training points:
        
        True decision boundary: smooth curve separating classes
        Overfitted boundary: wiggly line passing through every training point
        
        Training accuracy: 100% (fits all points perfectly)
        Test accuracy: 65% (terrible on new data)
        
        The overfitted model learned the noise, not the signal.
      
      capacity_versus_data: |
        Overfitting risk depends on:
        
        Model capacity: Total number of parameters
        - Linear model: n_features parameters
        - Neural network: Σ(n_l × n_{l+1}) parameters across all layers
        
        Training data size: Number of labeled examples
        
        Risk assessment:
        - High capacity + Small data = EXTREME overfitting risk
          Example: 1M parameters, 1K samples → memorization guaranteed
        
        - Low capacity + Large data = Underfitting risk
          Example: 10 parameters, 1M samples → model too simple
        
        - Matched capacity and data = Good generalization
          Example: 100K parameters, 100K samples → reasonable balance
    
    detecting_overfitting:
      
      learning_curves:
        description: |
          Plot training and validation loss/accuracy over training epochs.
          The train-val gap reveals overfitting.
        
        healthy_training: |
          Epoch | Train Loss | Val Loss | Gap
          ------|------------|----------|-----
            1   |   0.50     |   0.52   | 0.02
            5   |   0.30     |   0.32   | 0.02
           10   |   0.15     |   0.18   | 0.03
           20   |   0.08     |   0.10   | 0.02
           50   |   0.05     |   0.07   | 0.02
          
          Both losses decrease together, small gap (<0.05)
          → Good generalization
        
        overfitting: |
          Epoch | Train Loss | Val Loss | Gap
          ------|------------|----------|-----
            1   |   0.50     |   0.52   | 0.02
            5   |   0.30     |   0.35   | 0.05
           10   |   0.15     |   0.40   | 0.25
           20   |   0.05     |   0.55   | 0.50
           50   |   0.01     |   0.70   | 0.69
          
          Train loss keeps decreasing, val loss INCREASES!
          Large and growing gap (>0.3)
          → Severe overfitting
        
        underfitting: |
          Epoch | Train Loss | Val Loss | Gap
          ------|------------|----------|-----
            1   |   0.50     |   0.52   | 0.02
            5   |   0.45     |   0.48   | 0.03
           10   |   0.43     |   0.47   | 0.04
           20   |   0.42     |   0.46   | 0.04
           50   |   0.42     |   0.46   | 0.04
          
          Both losses plateau at high values
          → Model too simple (underfitting)
      
      implementation: |
        def plot_learning_curves(history):
            """
            Visualize training and validation curves to diagnose overfitting.
            
            Parameters:
            - history: dict with keys ['train_loss', 'val_loss', 
                       'train_accuracy', 'val_accuracy']
            """
            import matplotlib.pyplot as plt
            
            epochs = range(1, len(history['train_loss']) + 1)
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
            
            # Loss curves
            ax1.plot(epochs, history['train_loss'], 'b-', linewidth=2, 
                    label='Training Loss')
            ax1.plot(epochs, history['val_loss'], 'r-', linewidth=2, 
                    label='Validation Loss')
            ax1.set_xlabel('Epoch', fontsize=12)
            ax1.set_ylabel('Loss', fontsize=12)
            ax1.set_title('Loss Curves', fontsize=14, fontweight='bold')
            ax1.legend(fontsize=11)
            ax1.grid(True, alpha=0.3)
            
            # Accuracy curves
            ax2.plot(epochs, history['train_accuracy'], 'b-', linewidth=2,
                    label='Training Accuracy')
            ax2.plot(epochs, history['val_accuracy'], 'r-', linewidth=2,
                    label='Validation Accuracy')
            ax2.set_xlabel('Epoch', fontsize=12)
            ax2.set_ylabel('Accuracy', fontsize=12)
            ax2.set_title('Accuracy Curves', fontsize=14, fontweight='bold')
            ax2.legend(fontsize=11)
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
            # Automated diagnosis
            final_train_loss = history['train_loss'][-1]
            final_val_loss = history['val_loss'][-1]
            gap = final_val_loss - final_train_loss
            
            print("\n" + "="*50)
            print("TRAINING DIAGNOSIS")
            print("="*50)
            print(f"Final Training Loss:   {final_train_loss:.4f}")
            print(f"Final Validation Loss: {final_val_loss:.4f}")
            print(f"Train-Val Gap:         {gap:.4f}")
            print("-"*50)
            
            if gap > 0.15:
                print("⚠️  SEVERE OVERFITTING")
                print("   → Model memorizing training data")
                print("   → Action: Increase regularization (L2, dropout)")
            elif gap > 0.05:
                print("⚠️  MODERATE OVERFITTING")
                print("   → Some memorization occurring")
                print("   → Action: Add regularization or get more data")
            elif final_train_loss > 0.3 and final_val_loss > 0.3:
                print("⚠️  UNDERFITTING")
                print("   → Model too simple or undertraining")
                print("   → Action: Increase capacity or train longer")
            else:
                print("✓  HEALTHY TRAINING")
                print("   → Good generalization")
            print("="*50)
    
    why_regularization_works:
      
      hypothesis_space_constraint: |
        Without regularization:
        Hypothesis space = ALL possible weight configurations
        → Model can memorize any training set
        
        With regularization:
        Hypothesis space = Weight configurations with small norms
        → Model forced to find simple patterns
        
        Simple patterns generalize better than complex memorization.
      
      occams_razor: |
        Occam's Razor: "Prefer simpler explanations over complex ones"
        
        Regularization mathematically enforces simplicity:
        - Small weights = simpler model
        - Large weights = complex model (sensitive to small input changes)
        
        Given two models with same training accuracy:
        - Model A: weights = [0.1, 0.2, 0.1, 0.15] (simple)
        - Model B: weights = [10.5, -8.3, 12.1, -9.7] (complex)
        
        Regularization prefers Model A → better generalization.
  
  # --------------------------------------------------------------------------
  # Topic 2: L2 Regularization (Ridge)
  # --------------------------------------------------------------------------
  
  l2_regularization:
    
    mathematical_formulation:
      
      modified_loss_function: |
        Original loss (data fitting):
        L_data(W) = (1/m) Σ_{i=1}^m Loss(y_pred^{(i)}, y_true^{(i)})
        
        L2 regularized loss:
        L_total(W) = L_data(W) + (λ/2) · ||W||²₂
        
        Where:
        - λ (lambda): regularization strength (hyperparameter)
        - ||W||²₂ = Σ_{all weights} W²_{ij} (sum of squared weights)
        - Factor 1/2: mathematical convenience (cancels in gradient)
      
      interpretation: |
        L_total = Data fit term + Penalty for large weights
        
        The model must balance:
        1. Fitting training data (minimize L_data)
        2. Keeping weights small (minimize ||W||²)
        
        λ controls the tradeoff:
        - λ = 0: No regularization (pure data fitting, risk overfitting)
        - λ → ∞: Extreme regularization (all weights → 0, underfitting)
        - λ optimal: Perfect balance
      
      gradient_derivation: |
        Gradient of L_total with respect to weights:
        
        ∂L_total/∂W = ∂L_data/∂W + ∂(λ/2 · ||W||²)/∂W
                    = ∂L_data/∂W + λ·W
        
        Proof of regularization term gradient:
        ∂(λ/2 · Σ W²)/∂W = λ/2 · ∂(Σ W²)/∂W
                         = λ/2 · 2W
                         = λ·W
        
        (The factor 1/2 cancels with the 2 from derivative of W²)
      
      weight_update_rule: |
        Gradient descent update with L2:
        
        W^{new} = W^{old} - α · ∂L_total/∂W
                = W^{old} - α · (∂L_data/∂W + λ·W)
                = W^{old} - α·∂L_data/∂W - α·λ·W
                = W^{old}·(1 - α·λ) - α·∂L_data/∂W
                  \_________________/   \______________/
                    Weight decay         Data gradient
        
        Key insight: Each update SHRINKS weights by factor (1 - α·λ)
        This is why L2 is called "weight decay"!
    
    implementation:
      
      l2_regularizer_class: |
        import numpy as np
        
        class L2Regularizer:
            """
            L2 (Ridge) weight regularization.
            
            Adds penalty λ/2 · ||W||²₂ to loss function.
            Gradient: λ·W (weight decay)
            
            Parameters:
            - lambda_reg: regularization strength (typical: 0.0001 to 0.1)
            """
            
            def __init__(self, lambda_reg=0.01):
                self.lambda_reg = lambda_reg
            
            def compute_penalty(self, parameters):
                """
                Compute L2 penalty: (λ/2) · Σ(W²)
                
                Parameters:
                - parameters: dict of {name: array}
                              e.g., {'W1': array, 'b1': array, 'W2': array, ...}
                
                Returns:
                - penalty: scalar
                
                Note: Only regularize weights (W), NOT biases (b)
                """
                penalty = 0.0
                
                for param_name, param_value in parameters.items():
                    # Only regularize weight matrices, skip biases
                    if param_name.startswith('W'):
                        penalty += np.sum(param_value ** 2)
                
                return (self.lambda_reg / 2.0) * penalty
            
            def compute_gradient(self, parameters):
                """
                Compute gradient of L2 penalty: λ·W
                
                Returns:
                - grad_penalty: dict with same structure as parameters
                """
                grad_penalty = {}
                
                for param_name, param_value in parameters.items():
                    if param_name.startswith('W'):
                        # Gradient of (λ/2)·W² is λ·W
                        grad_penalty[param_name] = self.lambda_reg * param_value
                    else:
                        # No regularization on biases
                        grad_penalty[param_name] = np.zeros_like(param_value)
                
                return grad_penalty
            
            def __repr__(self):
                return f"L2Regularizer(λ={self.lambda_reg})"
      
      training_loop_integration: |
        # Initialize network and regularizer
        network = NeuralNetwork(layer_dims=[784, 128, 64, 10])
        optimizer = Adam(learning_rate=0.001)
        regularizer = L2Regularizer(lambda_reg=0.01)
        
        # Training loop
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            
            for X_batch, y_batch in train_loader:
                # Forward pass (data loss only)
                loss_data, predictions = network.forward(X_batch, y_batch)
                
                # Add regularization penalty to loss
                params = network.get_parameters()
                loss_reg = regularizer.compute_penalty(params)
                loss_total = loss_data + loss_reg
                
                epoch_loss += loss_total
                
                # Backward pass (data gradients)
                network.backward()
                grads_data = network.get_gradients()
                
                # Add regularization gradients
                grads_reg = regularizer.compute_gradient(params)
                
                # Combine gradients
                grads_total = {}
                for name in grads_data:
                    grads_total[name] = grads_data[name] + grads_reg[name]
                
                # Update weights
                optimizer.step(params, grads_total)
            
            # Logging
            avg_loss = epoch_loss / len(train_loader)
            val_loss, val_acc = evaluate(network, val_loader)
            print(f"Epoch {epoch+1}: train_loss={avg_loss:.4f}, "
                  f"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}")
    
    why_l2_prefers_distributed_representations:
      
      mathematical_explanation: |
        L2 penalty: ||W||²₂ = Σ W²ᵢ
        
        Consider two weight vectors with same L1 norm but different L2 norms:
        
        Vector A: [10, 0, 0, 0]
        - L1 norm: 10
        - L2 norm²: 100
        - L2 penalty: 100
        
        Vector B: [2.5, 2.5, 2.5, 2.5]
        - L1 norm: 10 (same as A)
        - L2 norm²: 25
        - L2 penalty: 25 (much lower!)
        
        L2 prefers B: many small weights over few large weights.
      
      interpretation: |
        L2 encourages "democratic" weight distributions:
        - All features contribute somewhat
        - No single feature dominates
        - Distributed representations (information spread across network)
        
        This is why L2 is the default regularization in deep learning.
      
      example_mnist: |
        Without L2: Some weights = 15.3, others = 0.01 (sparse, extreme)
        With L2: Most weights = 0.5-2.0 (distributed, balanced)
        
        The L2-regularized model uses more neurons cooperatively.
    
    hyperparameter_tuning:
      
      lambda_values_to_try: |
        Start with logarithmic grid search:
        λ ∈ {0.0001, 0.001, 0.01, 0.1, 1.0}
        
        Typical good range: 0.001 to 0.01
      
      tuning_strategy: |
        1. Baseline: Train without regularization (λ = 0)
           - Observe overfitting (train-val gap)
        
        2. Try λ = 0.01 (standard starting point)
           - If still overfitting: increase λ
           - If underfitting: decrease λ
        
        3. Binary search around optimal value
           - If 0.01 good but not perfect, try 0.005 and 0.02
        
        4. Validate on held-out validation set (NOT test set!)
      
      signs_of_correct_lambda: |
        Too small (λ < 0.0001):
        - Train-val gap still large (>0.2)
        - No improvement over unregularized
        
        Too large (λ > 0.1):
        - Training loss stops decreasing
        - Both train and val loss high (underfitting)
        - Weights shrink to near-zero
        
        Just right (λ ≈ 0.001 - 0.01):
        - Train-val gap small (<0.05)
        - Both losses decrease smoothly
        - Test accuracy improves 2-5%
  
  # --------------------------------------------------------------------------
  # Topic 3: L1 Regularization (Lasso)
  # --------------------------------------------------------------------------
  
  l1_regularization:
    
    mathematical_formulation:
      
      modified_loss_function: |
        L1 regularized loss:
        L_total(W) = L_data(W) + λ · ||W||₁
        
        Where:
        - ||W||₁ = Σ_{all weights} |W_{ij}| (sum of absolute values)
      
      gradient: |
        ∂L_total/∂W = ∂L_data/∂W + λ · sign(W)
        
        Where sign(W) = {+1 if W > 0, -1 if W < 0, 0 if W = 0}
        
        Key difference from L2:
        - L2 gradient: λ·W (proportional to weight magnitude)
        - L1 gradient: λ·sign(W) (constant magnitude, direction depends on sign)
    
    sparsity_inducing_property:
      
      why_l1_creates_zeros: |
        L1 gradient is CONSTANT: λ·sign(W)
        
        Small weight (e.g., W = 0.001):
        - Data gradient: ~0.01 (small)
        - L1 gradient: λ = 0.01 (SAME magnitude regardless of W!)
        - L1 dominates → pushes W to exactly 0
        
        Large weight (e.g., W = 5.0):
        - Data gradient: ~2.0 (large)
        - L1 gradient: λ = 0.01 (small relative to data gradient)
        - Data gradient dominates → W stays large
        
        Result: Small weights → 0, large weights survive → SPARSE model
      
      visual_intuition: |
        L2 regularization: Smooth, continuous shrinkage
        - All weights shrink proportionally
        - Weights approach 0 but rarely reach exactly 0
        
        L1 regularization: "Hard thresholding"
        - Weights below threshold → pushed to 0
        - Weights above threshold → stay large
        - Creates exact zeros (sparse weights)
      
      feature_selection: |
        L1 performs automatic feature selection:
        
        Input features: 1000 dimensions
        After L1 training: 950 weights = 0, 50 weights ≠ 0
        
        Interpretation: Only 50 features matter, rest are noise.
        
        This is why L1 is called "Lasso" (Least Absolute Shrinkage 
        and Selection Operator).
    
    implementation:
      
      l1_regularizer_class: |
        class L1Regularizer:
            """
            L1 (Lasso) weight regularization.
            
            Adds penalty λ · ||W||₁ to loss function.
            Gradient: λ·sign(W) (constant magnitude)
            
            Induces sparsity: many weights → 0
            """
            
            def __init__(self, lambda_reg=0.01):
                self.lambda_reg = lambda_reg
            
            def compute_penalty(self, parameters):
                """
                Compute L1 penalty: λ · Σ|W|
                """
                penalty = 0.0
                
                for param_name, param_value in parameters.items():
                    if param_name.startswith('W'):
                        penalty += np.sum(np.abs(param_value))
                
                return self.lambda_reg * penalty
            
            def compute_gradient(self, parameters):
                """
                Compute gradient of L1 penalty: λ·sign(W)
                
                Note: sign(0) = 0 by convention (subgradient)
                """
                grad_penalty = {}
                
                for param_name, param_value in parameters.items():
                    if param_name.startswith('W'):
                        # sign(W): +1 if W>0, -1 if W<0, 0 if W=0
                        grad_penalty[param_name] = self.lambda_reg * np.sign(param_value)
                    else:
                        grad_penalty[param_name] = np.zeros_like(param_value)
                
                return grad_penalty
            
            def __repr__(self):
                return f"L1Regularizer(λ={self.lambda_reg})"
      
      measuring_sparsity: |
        def measure_sparsity(parameters):
            """
            Measure percentage of weights that are exactly zero.
            """
            total_weights = 0
            zero_weights = 0
            
            for param_name, param_value in parameters.items():
                if param_name.startswith('W'):
                    total_weights += param_value.size
                    # Count weights very close to zero
                    zero_weights += np.sum(np.abs(param_value) < 1e-6)
            
            sparsity_pct = 100 * zero_weights / total_weights
            return sparsity_pct
        
        # After training with L1
        params = network.get_parameters()
        sparsity = measure_sparsity(params)
        print(f"Model sparsity: {sparsity:.1f}% of weights are zero")
        
        # Example output with L1:
        # Model sparsity: 73.2% of weights are zero
    
    l1_versus_l2_comparison:
      
      summary_table: |
        Property              | L2 (Ridge)            | L1 (Lasso)
        ----------------------|-----------------------|------------------------
        Penalty               | λ/2 · Σ(W²)          | λ · Σ|W|
        Gradient              | λ·W (proportional)    | λ·sign(W) (constant)
        Effect on weights     | Shrinks all weights   | Zeros out small weights
        Sparsity              | No                    | Yes (many exact zeros)
        Interpretation        | Distributed features  | Feature selection
        Robustness            | Better with correlated| Better with many irrelevant
                              | features              | features
        Optimization          | Smooth, differentiable| Non-differentiable at 0
        Most common           | Yes (standard choice) | No (specialized use)
      
      when_to_use_l1: |
        Use L1 regularization when:
        - You have many features (>1000) and suspect many are irrelevant
        - You want automatic feature selection
        - Model interpretability is critical (sparse = simpler)
        - You're willing to accept slower convergence
        
        Example: Text classification with 100K word features
        → L1 can reduce to 5K important words
      
      when_to_use_l2: |
        Use L2 regularization when:
        - Standard neural network training (most common case)
        - All features are potentially useful
        - You want smooth optimization (no non-differentiability issues)
        - Features are correlated (L1 picks one, L2 uses all)
        
        Example: Image classification (all pixels potentially matter)
  
  # --------------------------------------------------------------------------
  # Topic 4: Weight Decay vs L2 Regularization (Subtle but Important)
  # --------------------------------------------------------------------------
  
  weight_decay_versus_l2:
    
    the_confusion:
      
      common_misconception: |
        "Weight decay and L2 regularization are the same thing."
        
        This is TRUE for standard SGD, but FALSE for adaptive optimizers 
        (Adam, RMSprop, AdaGrad)!
      
      difference_with_adaptive_optimizers: |
        L2 Regularization:
        - Add λ·W to gradient
        - Gradient → optimizer (Adam, RMSprop, etc.)
        - Optimizer applies adaptive learning rates
        
        Weight Decay:
        - Apply λ·W DIRECTLY to weights after optimizer step
        - Bypass adaptive learning rates for decay
        
        Result: Different behavior with adaptive optimizers!
    
    mathematical_detail:
      
      l2_regularization_with_adam: |
        Standard L2 + Adam:
        
        1. Compute gradient: g_t = ∇L_data + λ·W_t
        2. Adam momentum update: m_t = β₁·m_{t-1} + (1-β₁)·g_t
        3. Adam variance update: v_t = β₂·v_{t-1} + (1-β₂)·g_t²
        4. Adam step: W_{t+1} = W_t - α · m_t/√(v_t + ε)
        
        Problem: λ·W_t gets rescaled by Adam's adaptive learning rate!
        The effective weight decay varies across parameters.
      
      true_weight_decay_with_adam: |
        Weight Decay (AdamW):
        
        1. Compute gradient: g_t = ∇L_data (NO λ·W here!)
        2. Adam momentum update: m_t = β₁·m_{t-1} + (1-β₁)·g_t
        3. Adam variance update: v_t = β₂·v_{t-1} + (1-β₂)·g_t²
        4. Adam step: W_{t+1} = W_t - α · m_t/√(v_t + ε)
        5. Weight decay: W_{t+1} = W_{t+1} - λ·W_t  (AFTER Adam step)
        
        Result: Uniform weight decay across all parameters, not affected
        by adaptive learning rates.
      
      with_sgd_they_are_equivalent: |
        SGD update with L2:
        W_{t+1} = W_t - α·(∇L_data + λ·W_t)
                = W_t - α·∇L_data - α·λ·W_t
                = W_t·(1 - α·λ) - α·∇L_data
        
        SGD update with weight decay:
        W_{t+1} = W_t - α·∇L_data - λ·W_t
                = W_t·(1 - λ) - α·∇L_data
        
        These are equivalent if we set: λ_decay = α·λ_L2
        
        So for SGD: L2 regularization = weight decay (just different λ)
    
    practical_implementation:
      
      adamw_optimizer: |
        class AdamW:
            """
            Adam optimizer with decoupled weight decay (AdamW).
            
            References:
            - "Decoupled Weight Decay Regularization" (Loshchilov & Hutter, 2019)
            
            Parameters:
            - learning_rate: step size (α)
            - beta1, beta2: momentum parameters
            - weight_decay: λ (applied directly to weights)
            """
            
            def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999,
                        epsilon=1e-8, weight_decay=0.01):
                self.lr = learning_rate
                self.beta1 = beta1
                self.beta2 = beta2
                self.epsilon = epsilon
                self.weight_decay = weight_decay
                
                # Momentum and variance accumulators
                self.m = {}  # First moment
                self.v = {}  # Second moment
                self.t = 0   # Time step
            
            def step(self, parameters, gradients):
                """
                Update parameters using AdamW algorithm.
                
                Note: gradients should NOT include λ·W term (that's weight decay's job)
                """
                self.t += 1
                
                for param_name in parameters:
                    param = parameters[param_name]
                    grad = gradients[param_name]
                    
                    # Initialize momentum on first step
                    if param_name not in self.m:
                        self.m[param_name] = np.zeros_like(param)
                        self.v[param_name] = np.zeros_like(param)
                    
                    # Update biased first moment estimate
                    self.m[param_name] = self.beta1 * self.m[param_name] + \
                                        (1 - self.beta1) * grad
                    
                    # Update biased second raw moment estimate
                    self.v[param_name] = self.beta2 * self.v[param_name] + \
                                        (1 - self.beta2) * (grad ** 2)
                    
                    # Bias correction
                    m_hat = self.m[param_name] / (1 - self.beta1 ** self.t)
                    v_hat = self.v[param_name] / (1 - self.beta2 ** self.t)
                    
                    # Adam update (WITHOUT weight decay in gradient)
                    param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
                    
                    # Weight decay (AFTER Adam step, only on weights not biases)
                    if param_name.startswith('W'):
                        param -= self.weight_decay * param
      
      when_to_use_which: |
        Use L2 Regularizer + SGD:
        - Simple baseline
        - Equivalent to weight decay for SGD
        
        Use Weight Decay (AdamW):
        - When using adaptive optimizers (Adam, RMSprop)
        - Modern best practice for transformers, LLMs
        - Better generalization empirically
        
        Default recommendation: AdamW for deep learning (2024 standard)
  
  # --------------------------------------------------------------------------
  # Topic 5: Elastic Net (L1 + L2 Combined)
  # --------------------------------------------------------------------------
  
  elastic_net:
    
    motivation:
      
      combining_best_of_both: |
        L1 alone: Sparse but unstable with correlated features
        L2 alone: Distributed but no feature selection
        
        Elastic Net: Get sparsity AND stability
      
      formula: |
        L_total = L_data + λ₁·||W||₁ + (λ₂/2)·||W||²₂
        
        Two hyperparameters: λ₁ (L1 strength), λ₂ (L2 strength)
    
    implementation:
      
      elastic_net_regularizer: |
        class ElasticNetRegularizer:
            """
            Elastic Net: L1 + L2 regularization combined.
            
            Penalty: λ₁·||W||₁ + (λ₂/2)·||W||²₂
            
            Benefits:
            - Sparsity from L1
            - Stability from L2
            """
            
            def __init__(self, l1_lambda=0.01, l2_lambda=0.01):
                self.l1 = L1Regularizer(lambda_reg=l1_lambda)
                self.l2 = L2Regularizer(lambda_reg=l2_lambda)
            
            def compute_penalty(self, parameters):
                """Total penalty = L1 penalty + L2 penalty"""
                return self.l1.compute_penalty(parameters) + \
                       self.l2.compute_penalty(parameters)
            
            def compute_gradient(self, parameters):
                """Total gradient = L1 gradient + L2 gradient"""
                grad_l1 = self.l1.compute_gradient(parameters)
                grad_l2 = self.l2.compute_gradient(parameters)
                
                grad_total = {}
                for param_name in grad_l1:
                    grad_total[param_name] = grad_l1[param_name] + grad_l2[param_name]
                
                return grad_total
            
            def __repr__(self):
                return f"ElasticNet(λ₁={self.l1.lambda_reg}, λ₂={self.l2.lambda_reg})"
    
    hyperparameter_tuning:
      
      tuning_strategy: |
        Two hyperparameters to tune: (λ₁, λ₂)
        
        Grid search:
        λ₁ ∈ {0.001, 0.01, 0.1}
        λ₂ ∈ {0.001, 0.01, 0.1}
        → 9 combinations
        
        Start with equal weights: λ₁ = λ₂ = 0.01
        
        If you want more sparsity: increase λ₁
        If you want more stability: increase λ₂
  
  # --------------------------------------------------------------------------
  # Topic 6: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    l2_improves_adversarial_robustness:
      
      observation: |
        L2 regularization makes models more robust to adversarial examples.
        
        Intuition:
        - L2 constrains weight magnitudes
        - Smaller weights → smaller gradients
        - Adversarial perturbations depend on ∇_x L (input gradient)
        - Smaller weights → smaller input gradients → less effective attacks
      
      mathematical_connection: |
        Adversarial perturbation (FGSM):
        δ = ε · sign(∇_x L(x, y; W))
        
        Gradient magnitude depends on weight magnitudes:
        ||∇_x L|| ∝ ||W|| (roughly)
        
        L2 regularization: ||W|| is constrained
        → ||∇_x L|| is smaller
        → Adversarial perturbations less effective
      
      empirical_evidence: |
        Experiment: MNIST + FGSM attack (ε=0.1)
        
        No regularization (λ=0):
        - Clean accuracy: 98.5%
        - Adversarial accuracy: 12.3%
        
        L2 regularization (λ=0.01):
        - Clean accuracy: 98.2%
        - Adversarial accuracy: 35.7%
        
        L2 regularization (λ=0.1):
        - Clean accuracy: 97.1%
        - Adversarial accuracy: 52.4%
        
        Trade-off: Stronger L2 → better robustness, slightly lower clean accuracy
      
      not_a_complete_defense: |
        Important: L2 regularization IMPROVES robustness but doesn't 
        GUARANTEE security.
        
        Adversarial training (Chapter 10) is still needed for true robustness.
        L2 is a helpful first layer of defense.
    
    over_regularization_can_harm_security:
      
      observation: |
        Extreme regularization (λ → ∞) makes models TOO simple.
        Simple models can be easier to fool with crafted inputs.
      
      example: |
        λ = 0: Complex decision boundary, hard to fool systematically
        λ = 100: Linear decision boundary, easy to find adversarial direction
        
        Balance needed: Regularize to prevent overfitting, but not so much
        that the model becomes trivially simple.
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Overfitting = train-val gap >0.15: model memorizing noise, not learning patterns"
      - "L2 penalty: (λ/2)·Σ(W²), gradient: λ·W, prefers many small weights (distributed)"
      - "L1 penalty: λ·Σ|W|, gradient: λ·sign(W), induces sparsity (many exact zeros)"
      - "Weight decay ≠ L2 for adaptive optimizers: AdamW decouples decay from gradient"
      - "Elastic Net = L1 + L2: combines sparsity and stability, two hyperparameters"
      - "L2 improves adversarial robustness: smaller weights → smaller gradients → harder to attack"
    
    actionable_steps:
      - "Always plot learning curves first: diagnose overfitting before adding regularization"
      - "Start with L2 λ=0.01: most common, works 80% of time, tune from there"
      - "Use AdamW (not Adam+L2) for transformers: better generalization, modern best practice"
      - "Only use L1 for feature selection: when you have >1000 features and want sparsity"
      - "Tune λ on validation set: grid search [0.0001, 0.001, 0.01, 0.1]"
      - "Don't over-regularize: if training loss not decreasing, λ too large"
    
    security_principles:
      - "L2 regularization = first line of defense: constrains weights, reduces gradient magnitude"
      - "Stronger L2 → better adversarial robustness: empirically 2-4x improvement in adversarial accuracy"
      - "But L2 alone insufficient: need adversarial training for true robustness (Chapter 10)"
      - "Balance regularization strength: too weak = overfitting, too strong = oversimplification"
    
    common_mistakes:
      - "Regularizing biases: only regularize W matrices, NOT b vectors"
      - "Using L2 with Adam and expecting weight decay: use AdamW instead"
      - "Tuning λ on test set: always use validation set, test set touched ONCE"
      - "Forgetting to add regularization to loss: penalty must be in total loss for gradient"
      - "Assuming L1=sparsity always: only true after convergence, early training not sparse"

---
