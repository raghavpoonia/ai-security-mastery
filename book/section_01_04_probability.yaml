# section_01_04_probability.yaml

---
document_info:
  chapter: "01"
  section: "04"
  title: "Mathematical Foundations: Probability and Statistics"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-30"
  estimated_pages: 6
  tags: ["probability", "statistics", "bayes-theorem", "distributions", "confidence-intervals"]

# ============================================================================
# SECTION 1.04: MATHEMATICAL FOUNDATIONS - PROBABILITY AND STATISTICS
# ============================================================================

section_01_04_probability:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Machine learning models don't output certainties - they output probabilities. 
    Your spam filter doesn't say "this IS spam", it says "85% probability of spam". 
    Your fraud detector outputs confidence scores. Your intrusion detection system 
    ranks alerts by likelihood. Understanding probability is understanding how to 
    interpret and trust ML predictions.
    
    More critically for security: probability tells you when to trust a model and 
    when to be suspicious. A 99.9% confidence score might seem definitive, but if 
    the model was trained on poisoned data, that confidence is meaningless. Statistical 
    thinking helps you detect data anomalies, identify adversarial manipulation, and 
    quantify detection accuracy.
    
    This section covers the probability and statistics you need for ML: distributions, 
    Bayes' theorem, confidence intervals, and hypothesis testing. We focus on 
    intuition over formalism - you need to understand what these concepts mean and 
    how adversaries exploit them.
  
  why_this_matters: |
    Security context:
    - Anomaly detection requires understanding "normal" distributions
    - Bayesian thinking helps update beliefs given new evidence (threat hunting)
    - Confidence intervals quantify detector uncertainty
    - P-values and hypothesis testing validate detection rules
    - Adversaries poison training data to shift distributions
    
    Without probability/statistics:
    - Can't interpret model predictions correctly
    - Can't design proper evaluation metrics
    - Can't detect data poisoning attacks
    - Can't quantify false positive rates
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Probability Fundamentals
  # --------------------------------------------------------------------------
  
  probability_fundamentals:
    
    basic_definitions:
      
      probability:
        definition: "P(A) = likelihood that event A occurs, range [0, 1]"
        interpretation:
          - "P(A) = 0: event never happens"
          - "P(A) = 1: event always happens"
          - "P(A) = 0.5: event happens half the time"
        
        example: |
          Spam classifier outputs P(spam) = 0.85
          Interpretation: Model believes there's 85% chance this email is spam
          
          Security context: If you see P(malicious) = 0.51 (barely above threshold), 
          investigate manually - model is uncertain. If P(malicious) = 0.99, either 
          it's very confident OR adversary crafted input to game the detector.
      
      complement:
        rule: "P(not A) = 1 - P(A)"
        example: "P(spam) = 0.85 → P(not spam) = 0.15"
      
      joint_probability:
        definition: "P(A and B) = probability both A and B occur"
        notation: "P(A ∩ B)"
        
        independent_events:
          rule: "If A and B independent: P(A and B) = P(A) × P(B)"
          example: |
            P(user logs in from VPN) = 0.3
            P(user accesses sensitive data) = 0.2
            If independent: P(both) = 0.3 × 0.2 = 0.06
            
            But in reality NOT independent - users on VPN more likely to access 
            sensitive data. This is why simple rule-based detection fails.
      
      conditional_probability:
        definition: "P(A|B) = probability of A given that B has occurred"
        formula: "P(A|B) = P(A and B) / P(B)"
        
        interpretation: |
          P(spam|contains 'viagra') = probability email is spam GIVEN it contains 'viagra'
          
          This is different from P(spam), which is probability without knowing content.
          Conditional probability is the foundation of ML classification.
        
        example:
          scenario: "Email spam detection"
          base_rate: "P(spam) = 0.4 (40% of emails are spam)"
          observation: "Email contains word 'viagra'"
          conditional: "P(spam|'viagra') = 0.95 (95% of emails with 'viagra' are spam)"
          
          insight: |
            Observing 'viagra' dramatically increases spam probability from 40% → 95%
            This is how features provide information gain in ML models
    
    bayes_theorem:
      
      formula: "P(A|B) = P(B|A) × P(A) / P(B)"
      
      in_words: |
        Posterior probability = (Likelihood × Prior) / Evidence
        
        P(A|B): Posterior - probability of A after observing B
        P(B|A): Likelihood - probability of observing B if A is true
        P(A): Prior - probability of A before observing B
        P(B): Evidence - total probability of observing B
      
      why_it_matters: |
        Bayes' theorem lets you update beliefs given new evidence. This is fundamental 
        to both ML (Naive Bayes classifier) and security (threat hunting, incident response).
        
        In security: Start with prior probability of attack. Observe indicator. Update 
        belief about attack likelihood. Repeat as you gather more evidence.
      
      practical_example:
        
        scenario: "Detecting malicious PowerShell commands"
        
        prior: "P(malicious) = 0.01 (1% of PowerShell commands are malicious)"
        
        observation: "Command contains 'DownloadString' method"
        
        likelihood: "P('DownloadString'|malicious) = 0.8 (80% of malicious commands use it)"
        
        evidence: "P('DownloadString') = 0.05 (5% of all commands use it)"
        
        calculation: |
          P(malicious|'DownloadString') = P('DownloadString'|malicious) × P(malicious) / P('DownloadString')
                                        = 0.8 × 0.01 / 0.05
                                        = 0.008 / 0.05
                                        = 0.16 (16%)
        
        interpretation: |
          Prior belief: 1% chance of malicious
          After observing 'DownloadString': 16% chance of malicious
          Belief updated 16x based on evidence
          
          Still not definitive (only 16%), so we'd look for more indicators:
          - Obfuscation? Add evidence → update belief again
          - Base64 encoding? Add evidence → update belief again
          - External URL? Add evidence → update belief again
          
          This is exactly how Naive Bayes classifier works - chain multiple Bayes updates
      
      security_application:
        
        threat_hunting:
          - "Start with base rate of attack (prior)"
          - "Observe indicators (evidence)"
          - "Update probability using Bayes' theorem"
          - "Continue gathering evidence until confident"
        
        false_positive_analysis:
          - "P(alert|benign) vs P(alert|attack)"
          - "High P(alert|benign) = high false positive rate"
          - "Bayes helps quantify this trade-off"
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Probability Distributions
  # --------------------------------------------------------------------------
  
  probability_distributions:
    
    what_is_distribution: |
      A probability distribution describes how values are spread across possible outcomes.
      
      For discrete variables (coin flips, dice): Probability mass function (PMF)
      For continuous variables (heights, temperatures): Probability density function (PDF)
      
      In ML: Features often follow certain distributions. Models make assumptions about 
      these distributions. Adversaries poison data by shifting distributions.
    
    uniform_distribution:
      
      description: "All outcomes equally likely"
      
      discrete_example:
        scenario: "Fair six-sided die"
        outcomes: [1, 2, 3, 4, 5, 6]
        probabilities: "P(1) = P(2) = ... = P(6) = 1/6 = 0.167"
      
      continuous_example:
        scenario: "Random number between 0 and 1"
        interpretation: "Any value in [0,1] equally likely"
      
      numpy_implementation: |
        import numpy as np
        
        # Discrete uniform: random integers
        die_rolls = np.random.randint(1, 7, size=1000)  # 1000 dice rolls
        
        # Continuous uniform: random floats [0, 1)
        random_values = np.random.uniform(0, 1, size=1000)
        
        # Continuous uniform: custom range [a, b]
        random_temps = np.random.uniform(20, 30, size=1000)  # temperatures 20-30°C
      
      security_relevance:
        - "Uniform distribution for random salt generation (cryptography)"
        - "Detecting non-uniform patterns indicates manipulation"
        - "Baseline for anomaly detection: deviations from uniform"
    
    normal_distribution:
      
      description: |
        Bell-shaped curve, symmetric around mean. Most values cluster near mean, 
        fewer values far from mean. Defined by two parameters: mean (μ) and 
        standard deviation (σ).
      
      properties:
        - "68% of values within 1 standard deviation of mean"
        - "95% of values within 2 standard deviations"
        - "99.7% of values within 3 standard deviations (3-sigma rule)"
      
      formula: "f(x) = (1 / (σ√(2π))) × exp(-(x-μ)² / (2σ²))"
      
      parameters:
        mean: "μ - center of distribution (where peak is)"
        std_dev: "σ - spread of distribution (how wide the bell)"
      
      examples:
        heights: "Human heights: mean ~170cm, std ~10cm"
        test_scores: "IQ scores: mean 100, std 15"
        response_times: "API latency: mean 50ms, std 10ms"
      
      numpy_implementation: |
        # Generate normal distribution samples
        mean = 0
        std_dev = 1
        samples = np.random.normal(mean, std_dev, size=10000)
        
        # Verify 68-95-99.7 rule
        within_1_sigma = np.abs(samples - mean) <= 1 * std_dev
        within_2_sigma = np.abs(samples - mean) <= 2 * std_dev
        within_3_sigma = np.abs(samples - mean) <= 3 * std_dev
        
        print(f"Within 1σ: {within_1_sigma.mean():.2%}")  # Should be ~68%
        print(f"Within 2σ: {within_2_sigma.mean():.2%}")  # Should be ~95%
        print(f"Within 3σ: {within_3_sigma.mean():.2%}")  # Should be ~99.7%
      
      security_application:
        
        anomaly_detection: |
          Assume feature values follow normal distribution. Values beyond 3-sigma 
          are anomalies (only 0.3% chance of occurring naturally).
          
          Example: User login times
          - Mean login time: 9:00 AM
          - Std dev: 1 hour
          - Login at 3:00 AM is 6 standard deviations away → highly suspicious
        
        detecting_distribution_shift: |
          Data poisoning changes distribution. Monitor mean/std over time.
          
          Baseline: Training data has mean=50, std=10
          New data: mean=55, std=15
          Conclusion: Distribution shifted - possible poisoning or drift
        
        zscore_outlier_detection: |
          z-score = (x - μ) / σ
          Measures how many standard deviations away from mean
          
          |z-score| > 3 → outlier
          |z-score| > 4 → extreme outlier
          
          import numpy as np
          
          def detect_outliers_zscore(data, threshold=3):
              mean = np.mean(data)
              std = np.std(data)
              z_scores = np.abs((data - mean) / std)
              return z_scores > threshold
          
          # Example: Detect anomalous API response times
          response_times = np.array([45, 50, 48, 52, 200, 51, 49])  # 200ms is outlier
          outliers = detect_outliers_zscore(response_times)
          print(f"Outliers: {response_times[outliers]}")  # [200]
    
    binomial_distribution:
      
      description: |
        Distribution of number of successes in n independent trials, each with 
        probability p of success. Discrete distribution.
        
        Classic example: Flip coin 10 times, how many heads?
      
      parameters:
        n: "number of trials"
        p: "probability of success in each trial"
      
      formula: "P(k successes in n trials) = C(n,k) × p^k × (1-p)^(n-k)"
      
      security_example:
        
        scenario: "Brute force attack detection"
        
        setup: |
          Attacker tries passwords. Each attempt has p=0.001 probability of success 
          (1 in 1000 passwords is correct). Attacker makes n=1000 attempts.
          
          What's probability attacker succeeds at least once?
        
        calculation: |
          P(at least 1 success) = 1 - P(0 successes)
          
          P(0 successes) = C(1000,0) × (0.001)^0 × (0.999)^1000
                         = 1 × 1 × (0.999)^1000
                         = 0.368
          
          P(at least 1 success) = 1 - 0.368 = 0.632 (63.2%)
        
        numpy_implementation: |
          # Simulate 1000 password attempts, p=0.001 success rate
          n_attempts = 1000
          p_success = 0.001
          n_simulations = 10000
          
          # Run simulation many times
          successes = np.random.binomial(n_attempts, p_success, n_simulations)
          
          # Probability of at least one success
          p_at_least_one = (successes >= 1).mean()
          print(f"P(at least 1 success): {p_at_least_one:.2%}")  # ~63%
        
        detection_logic: |
          If we see 1000 failed login attempts, there's 63% chance attacker will 
          eventually succeed. MUST block after far fewer attempts (e.g., 5-10).
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Descriptive Statistics
  # --------------------------------------------------------------------------
  
  descriptive_statistics:
    
    measures_of_center:
      
      mean:
        formula: "μ = (Σ xi) / n"
        description: "Average value - sum all values, divide by count"
        
        numpy_implementation: |
          data = np.array([1, 2, 3, 4, 5, 100])  # Note: 100 is outlier
          mean = np.mean(data)
          print(f"Mean: {mean}")  # 19.17
        
        weakness: "Sensitive to outliers - one extreme value skews mean"
        
        security_implication: |
          Adversary can manipulate mean by injecting outliers (data poisoning).
          Example: Inject fake benign samples with extreme feature values → shift 
          decision boundary → evade detection
      
      median:
        description: "Middle value when data sorted - 50th percentile"
        
        numpy_implementation: |
          data = np.array([1, 2, 3, 4, 5, 100])
          median = np.median(data)
          print(f"Median: {median}")  # 3.5 (average of 3 and 4)
        
        strength: "Robust to outliers - not affected by extreme values"
        
        when_to_use: |
          Prefer median over mean when:
          - Data has outliers (common in security logs)
          - Distribution is skewed (not symmetric)
          - Need robust statistic for anomaly detection
      
      mode:
        description: "Most frequently occurring value"
        example: "Dataset [1,2,2,2,3,4] → mode is 2"
        
        relevance: "Useful for categorical data (most common attack type, most targeted port)"
    
    measures_of_spread:
      
      variance:
        formula: "σ² = Σ(xi - μ)² / n"
        description: "Average squared deviation from mean"
        interpretation: "How spread out values are around mean"
        
        numpy_implementation: |
          data = np.array([1, 2, 3, 4, 5])
          variance = np.var(data)
          print(f"Variance: {variance}")  # 2.0
      
      standard_deviation:
        formula: "σ = √(variance)"
        description: "Square root of variance - same units as original data"
        
        numpy_implementation: |
          data = np.array([1, 2, 3, 4, 5])
          std_dev = np.std(data)
          print(f"Std Dev: {std_dev}")  # 1.41
        
        interpretation: |
          Small σ: Values clustered tightly around mean (low variability)
          Large σ: Values spread widely (high variability)
        
        security_example: |
          Baseline API latency: mean=50ms, σ=5ms (consistent performance)
          Attack scenario: mean=50ms, σ=50ms (huge variability)
          
          Detection: Monitor σ over time. Sudden increase → investigate
      
      percentiles:
        description: "Value below which given percentage of data falls"
        
        common_percentiles:
          - "25th percentile (Q1): 25% of data below this"
          - "50th percentile (Q2): Median"
          - "75th percentile (Q3): 75% of data below this"
          - "99th percentile (P99): 99% of data below this"
        
        numpy_implementation: |
          data = np.random.normal(100, 15, size=10000)  # IQ scores
          
          p25 = np.percentile(data, 25)   # Q1
          p50 = np.percentile(data, 50)   # Median
          p75 = np.percentile(data, 75)   # Q3
          p99 = np.percentile(data, 99)   # P99
          
          print(f"P25: {p25:.1f}")
          print(f"P50: {p50:.1f}")
          print(f"P75: {p75:.1f}")
          print(f"P99: {p99:.1f}")
        
        security_application: |
          SLAs often defined using percentiles:
          - "P50 latency < 100ms" (median)
          - "P99 latency < 500ms" (99th percentile)
          
          Anomaly detection:
          - Baseline: P99 = 200ms
          - Current: P99 = 2000ms (10x increase)
          - Conclusion: Performance attack (slowloris, resource exhaustion)
      
      interquartile_range:
        formula: "IQR = Q3 - Q1"
        description: "Range of middle 50% of data"
        
        outlier_detection: |
          Outliers defined as values outside:
          - Lower bound: Q1 - 1.5×IQR
          - Upper bound: Q3 + 1.5×IQR
          
          This is the "box plot" rule
        
        numpy_implementation: |
          def detect_outliers_iqr(data):
              q1 = np.percentile(data, 25)
              q3 = np.percentile(data, 75)
              iqr = q3 - q1
              
              lower_bound = q1 - 1.5 * iqr
              upper_bound = q3 + 1.5 * iqr
              
              outliers = (data < lower_bound) | (data > upper_bound)
              return outliers
          
          # Example
          data = np.array([1, 2, 3, 4, 5, 100])  # 100 is outlier
          outliers = detect_outliers_iqr(data)
          print(f"Outliers: {data[outliers]}")  # [100]
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Confidence Intervals and Hypothesis Testing
  # --------------------------------------------------------------------------
  
  confidence_intervals:
    
    definition: |
      Range of values that likely contains true population parameter with given confidence.
      
      Example: "95% confidence interval for mean API latency is [48ms, 52ms]"
      Interpretation: 95% confident true mean latency is between 48-52ms
    
    formula: "CI = sample_mean ± (z-score × standard_error)"
    
    components:
      z_score:
        - "90% confidence: z = 1.645"
        - "95% confidence: z = 1.96"
        - "99% confidence: z = 2.576"
      
      standard_error: "SE = σ / √n (std dev divided by square root of sample size)"
    
    numpy_implementation: |
      # Calculate 95% confidence interval for mean
      def confidence_interval(data, confidence=0.95):
          n = len(data)
          mean = np.mean(data)
          std_error = np.std(data) / np.sqrt(n)
          
          # Z-score for 95% confidence
          z_score = 1.96
          
          margin_of_error = z_score * std_error
          ci_lower = mean - margin_of_error
          ci_upper = mean + margin_of_error
          
          return ci_lower, ci_upper
      
      # Example: API response times
      response_times = np.random.normal(50, 10, size=100)  # mean=50ms, std=10ms
      ci_low, ci_high = confidence_interval(response_times)
      
      print(f"95% CI: [{ci_low:.2f}ms, {ci_high:.2f}ms]")
    
    interpretation: |
      Wide CI: High uncertainty (small sample size or large variability)
      Narrow CI: High certainty (large sample size or low variability)
      
      To narrow CI:
      - Collect more data (increase n)
      - Reduce variability (control conditions)
      - Accept lower confidence (90% instead of 95%)
    
    security_application: |
      Quantifying detector performance:
      - Measured accuracy: 92%
      - 95% CI: [89%, 95%]
      - Conclusion: True accuracy likely between 89-95%
      
      Before deploying detector, ensure lower bound of CI meets requirements.
      If requirement is 90% accuracy and CI is [89%, 95%], might not meet SLA.
  
  hypothesis_testing:
    
    purpose: |
      Statistical method to test claims about populations using sample data.
      
      Security example: "Is new detection rule better than baseline?"
      
      Null hypothesis (H0): New rule = baseline (no difference)
      Alternative hypothesis (H1): New rule > baseline (improvement)
    
    process:
      - "Define null hypothesis (H0) and alternative (H1)"
      - "Collect data and compute test statistic"
      - "Calculate p-value: probability of observing data if H0 true"
      - "Compare p-value to significance level (α, usually 0.05)"
      - "If p-value < α: Reject H0, accept H1"
      - "If p-value ≥ α: Fail to reject H0"
    
    p_value_interpretation:
      - "p < 0.01: Very strong evidence against H0"
      - "p < 0.05: Strong evidence against H0 (standard threshold)"
      - "p < 0.10: Moderate evidence against H0"
      - "p ≥ 0.10: Weak/no evidence against H0"
    
    common_mistake: |
      ❌ "p-value is probability H0 is true"
      ✅ "p-value is probability of observing data this extreme IF H0 is true"
      
      Low p-value means: "Data is very unlikely under H0, so H0 probably false"
    
    example_ab_test:
      
      scenario: |
        Testing two prompt injection detectors:
        - Detector A (baseline): 85% accuracy
        - Detector B (new): 90% accuracy
        
        Question: Is B significantly better than A?
      
      hypotheses:
        H0: "Detector B = Detector A (no difference)"
        H1: "Detector B > Detector A (B is better)"
      
      test: "T-test comparing accuracy distributions"
      
      numpy_implementation: |
        from scipy import stats
        
        # Simulate accuracy measurements
        detector_a_accuracy = np.random.normal(0.85, 0.05, size=100)  # mean 85%
        detector_b_accuracy = np.random.normal(0.90, 0.05, size=100)  # mean 90%
        
        # Perform t-test
        t_statistic, p_value = stats.ttest_ind(detector_b_accuracy, detector_a_accuracy)
        
        print(f"T-statistic: {t_statistic:.3f}")
        print(f"P-value: {p_value:.4f}")
        
        if p_value < 0.05:
            print("Detector B is significantly better (p < 0.05)")
        else:
            print("No significant difference (p ≥ 0.05)")
      
      conclusion: |
        If p-value < 0.05: Deploy Detector B (proven improvement)
        If p-value ≥ 0.05: Keep Detector A (no proven improvement, avoid change risk)
  
  # --------------------------------------------------------------------------
  # Practical Implementation: Statistical Analysis Toolkit
  # --------------------------------------------------------------------------
  
  practical_implementation:
    
    statistical_summary: |
      # Complete statistical summary function
      import numpy as np
      
      def statistical_summary(data, confidence=0.95):
          """
          Comprehensive statistical analysis of dataset
          
          Args:
              data: NumPy array of values
              confidence: Confidence level for interval (default 95%)
          
          Returns:
              dict with all statistical measures
          """
          n = len(data)
          
          # Measures of center
          mean = np.mean(data)
          median = np.median(data)
          
          # Measures of spread
          std_dev = np.std(data)
          variance = np.var(data)
          
          # Percentiles
          q1 = np.percentile(data, 25)
          q3 = np.percentile(data, 75)
          iqr = q3 - q1
          
          # Confidence interval
          z_score = 1.96 if confidence == 0.95 else 2.576  # 95% or 99%
          std_error = std_dev / np.sqrt(n)
          margin_error = z_score * std_error
          ci_lower = mean - margin_error
          ci_upper = mean + margin_error
          
          # Outlier detection (IQR method)
          lower_bound = q1 - 1.5 * iqr
          upper_bound = q3 + 1.5 * iqr
          outliers = ((data < lower_bound) | (data > upper_bound)).sum()
          
          return {
              'n': n,
              'mean': mean,
              'median': median,
              'std_dev': std_dev,
              'variance': variance,
              'q1': q1,
              'q3': q3,
              'iqr': iqr,
              'ci_lower': ci_lower,
              'ci_upper': ci_upper,
              'outliers': outliers
          }
      
      # Example usage
      api_latencies = np.random.normal(50, 10, size=1000)  # 1000 measurements
      stats = statistical_summary(api_latencies)
      
      print(f"Sample size: {stats['n']}")
      print(f"Mean: {stats['mean']:.2f}ms")
      print(f"Median: {stats['median']:.2f}ms")
      print(f"Std Dev: {stats['std_dev']:.2f}ms")
      print(f"95% CI: [{stats['ci_lower']:.2f}, {stats['ci_upper']:.2f}]ms")
      print(f"Outliers: {stats['outliers']} ({stats['outliers']/stats['n']*100:.1f}%)")
    
    anomaly_detection_combined: |
      # Combine Z-score and IQR methods for robust anomaly detection
      import numpy as np
      
      def detect_anomalies(data, zscore_threshold=3, use_iqr=True):
          """
          Detect anomalies using multiple methods
          
          Args:
              data: NumPy array
              zscore_threshold: Z-score threshold (default 3)
              use_iqr: Whether to also use IQR method (default True)
          
          Returns:
              Boolean array indicating anomalies
          """
          anomalies = np.zeros(len(data), dtype=bool)
          
          # Method 1: Z-score
          mean = np.mean(data)
          std = np.std(data)
          z_scores = np.abs((data - mean) / std)
          zscore_anomalies = z_scores > zscore_threshold
          
          # Method 2: IQR (if enabled)
          if use_iqr:
              q1 = np.percentile(data, 25)
              q3 = np.percentile(data, 75)
              iqr = q3 - q1
              lower = q1 - 1.5 * iqr
              upper = q3 + 1.5 * iqr
              iqr_anomalies = (data < lower) | (data > upper)
              
              # Flag as anomaly if detected by EITHER method
              anomalies = zscore_anomalies | iqr_anomalies
          else:
              anomalies = zscore_anomalies
          
          return anomalies
      
      # Example: Detect anomalous API response times
      normal_traffic = np.random.normal(50, 10, size=1000)  # Normal: 50ms ± 10ms
      attack_traffic = np.array([5000, 6000])  # DDoS attack: 5-6 second responses
      
      all_traffic = np.concatenate([normal_traffic, attack_traffic])
      anomalies = detect_anomalies(all_traffic)
      
      print(f"Total samples: {len(all_traffic)}")
      print(f"Anomalies detected: {anomalies.sum()}")
      print(f"Anomalous values: {all_traffic[anomalies]}")
  
  # --------------------------------------------------------------------------
  # Common Mistakes and How to Avoid Them
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    mistake_1:
      error: "Using mean when data has outliers"
      
      why_wrong: |
        Mean is sensitive to extreme values. Single outlier can drastically 
        change mean, making it unrepresentative of typical values.
      
      example: |
        API latencies: [50, 52, 48, 51, 5000]ms
        Mean: 1040ms (makes system look slow)
        Median: 51ms (represents typical performance)
      
      fix: "Use median for skewed distributions or data with outliers"
      
      security_context: |
        Attacker can inject outliers to manipulate monitoring dashboards.
        Example: Inject extremely high latency requests → mean latency spikes → 
        false alerts → analyst fatigue → real attacks missed
    
    mistake_2:
      error: "Confusing correlation with causation"
      
      example: |
        Observation: Users who visit admin panel have 10x higher attack rate
        Wrong conclusion: Admin panel causes attacks
        Correct interpretation: Attackers target admin panel (reverse causation)
      
      fix: "Correlation shows relationship, not direction. Always consider: Does A cause B, does B cause A, or does C cause both?"
    
    mistake_3:
      error: "Ignoring sample size when comparing statistics"
      
      scenario: |
        Detector A: 95% accuracy on 10,000 samples → CI: [94.5%, 95.5%]
        Detector B: 96% accuracy on 100 samples → CI: [92%, 100%]
        
        Question: Which is better?
      
      wrong_answer: "Detector B (higher accuracy)"
      
      correct_answer: |
        Detector A is better - much larger sample, narrow CI, proven performance.
        Detector B's 96% could easily be luck with small sample. Wide CI shows 
        high uncertainty (could be as low as 92%).
      
      rule: "Always report sample size and confidence intervals, not just point estimates"
    
    mistake_4:
      error: "P-hacking: Testing until you get p < 0.05"
      
      bad_practice: |
        Run 20 different statistical tests on same data until one shows p < 0.05, 
        then report only that one.
        
        Problem: If you test 20 hypotheses at α=0.05, you'll get ~1 false positive 
        by pure chance (5% of 20 = 1).
      
      fix: |
        - Decide hypothesis BEFORE collecting data
        - Report all tests performed, not just significant ones
        - Use Bonferroni correction if testing multiple hypotheses: α_corrected = α / n_tests
        - Pre-register experiments (common in medical research)
      
      security_relevance: |
        Don't tune detection rules until they "look good" on test set.
        That's overfitting. Use proper train/validation/test splits (next sections).
  
  # --------------------------------------------------------------------------
  # Security Implications Summary
  # --------------------------------------------------------------------------
  
  security_implications:
    
    data_poisoning_detection:
      technique: "Monitor distribution shifts over time"
      implementation: |
        Baseline: Training data has mean=μ0, std=σ0
        New data: Compute mean=μ1, std=σ1
        
        Alert if:
        - |μ1 - μ0| > 2σ0 (mean shifted significantly)
        - |σ1 - σ0| > 0.5σ0 (variance changed significantly)
        
        This detects adversary injecting poisoned samples to shift distribution
    
    model_confidence_interpretation:
      issue: "High confidence doesn't mean correct"
      
      example: |
        Model outputs P(malicious) = 0.99 for input
        
        Could mean:
        1. Model very confident → probably correct
        2. Model overfit to training data → confident but wrong
        3. Adversarial input crafted to maximize confidence → definitely wrong
        
        Always check: Is input distribution similar to training data?
      
      defense: "Use calibration techniques (Section 11) and confidence thresholds"
    
    false_positive_false_negative_tradeoff:
      
      context: "All detectors trade precision vs recall"
      
      metrics:
        - "False Positive Rate (FPR): P(alert | benign)"
        - "False Negative Rate (FNR): P(no alert | attack)"
        - "Cannot optimize both simultaneously"
      
      bayesian_perspective: |
        Given alert, what's probability of actual attack?
        
        P(attack|alert) = P(alert|attack) × P(attack) / P(alert)
        
        If base rate of attacks is low (P(attack) = 0.01), even detector with 
        99% accuracy will have high false positive rate.
        
        Example:
        - P(attack) = 0.01 (1% of traffic is attacks)
        - P(alert|attack) = 0.99 (99% true positive rate)
        - P(alert|benign) = 0.05 (5% false positive rate)
        
        P(attack|alert) = 0.99 × 0.01 / (0.99 × 0.01 + 0.05 × 0.99)
                        = 0.0099 / 0.0594
                        = 0.167 (16.7%)
        
        Conclusion: Even with 99% accurate detector, only 16.7% of alerts are 
        real attacks! Must account for base rate.
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "ML outputs probabilities, not certainties - always quantify confidence"
      - "Bayes' theorem lets you update beliefs given evidence - foundation of threat hunting"
      - "Normal distribution describes most real-world data - use for anomaly detection"
      - "Z-scores and percentiles detect outliers - basis for statistical anomaly detection"
      - "Confidence intervals quantify uncertainty - report these with all performance metrics"
    
    practical_skills:
      - "Use median (not mean) for data with outliers - more robust to adversarial manipulation"
      - "Monitor distribution shifts to detect data poisoning - track mean/std over time"
      - "Combine multiple statistical methods for anomaly detection - Z-score + IQR + domain knowledge"
      - "Always report sample size and confidence intervals - single accuracy number is insufficient"
      - "Use hypothesis testing for A/B testing detectors - prove improvement statistically"
    
    security_mindset:
      - "Adversaries can manipulate statistics by injecting outliers - use robust measures"
      - "High model confidence doesn't guarantee correctness - could be adversarial input"
      - "Base rate matters enormously - low attack rate means high false positive rate even with good detector"
      - "Distribution shifts indicate poisoning or drift - monitor continuously"
      - "Don't p-hack your detectors - maintain proper train/test separation"
    
    remember_this:
      - "Probability = quantified uncertainty. Security = managing uncertainty. Master probability."
      - "Bayes' theorem is how you think like a threat hunter - start with prior, update with evidence, repeat."
      - "Statistics detect anomalies, but adversaries know statistics too. Use multiple methods."
    
    next_steps:
      - "Next section: Data representation - how to convert raw security logs into feature vectors"
      - "Connect to security: Section 28 shows how adversaries exploit probability in attacks"
      - "Foundation complete: You now have all the math needed for ML algorithms (Sections 7-28)"

---
