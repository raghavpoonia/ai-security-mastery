# section_02_12_batch_normalization.yaml

---
document_info:
  chapter: "02"
  section: "12"
  title: "Batch Normalization"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["batch-normalization", "internal-covariate-shift", "normalization", "training-stability", "moving-average"]

# ============================================================================
# SECTION 02_12: BATCH NORMALIZATION
# ============================================================================

section_02_12_batch_normalization:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Batch Normalization (BatchNorm) is one of the most important innovations
    in deep learning, enabling training of much deeper networks with faster
    convergence and better generalization.
    
    The core idea: normalize layer inputs to have zero mean and unit variance,
    then learn optimal scale and shift parameters. This addresses "internal
    covariate shift" - the problem that input distributions to each layer
    change during training, making optimization difficult.
    
    You'll understand why BatchNorm works (reduces covariate shift, smooths
    loss landscape), implement the algorithm with its train/test mode
    differences, handle moving averages correctly, learn where to place it
    (before vs after activation), and understand its security implications
    for model robustness.
  
  learning_objectives:
    
    conceptual:
      - "Understand internal covariate shift problem"
      - "Grasp why normalizing layer inputs helps training"
      - "Know difference between training and inference modes"
      - "Understand learnable gamma (scale) and beta (shift) parameters"
      - "Recognize when to use BatchNorm vs alternatives (LayerNorm, GroupNorm)"
      - "Connect BatchNorm to implicit regularization"
    
    practical:
      - "Implement BatchNorm layer from scratch (NumPy)"
      - "Handle moving averages for mean and variance"
      - "Implement train vs eval mode correctly"
      - "Integrate BatchNorm into multi-layer networks"
      - "Choose placement: before or after activation"
      - "Debug common BatchNorm mistakes"
    
    security_focused:
      - "BatchNorm statistics can leak training data information"
      - "Batch size affects BatchNorm behavior (small batches problematic)"
      - "Moving averages preserve training distribution patterns"
      - "Can be exploited in membership inference attacks"
  
  prerequisites:
    - "Sections 02_05-02_08 (backpropagation, optimization)"
    - "Section 02_10 (regularization concepts)"
    - "Understanding of mean, variance, standard deviation"
    - "Gradient flow through normalization operations"
  
  # --------------------------------------------------------------------------
  # Topic 1: The Internal Covariate Shift Problem
  # --------------------------------------------------------------------------
  
  internal_covariate_shift:
    
    the_problem:
      
      definition: |
        Internal Covariate Shift: During training, the distribution of inputs
        to each layer changes as parameters in previous layers update.
        
        Example: Layer 3 input distribution
        - Epoch 1: mean=0.5, std=2.0
        - Epoch 5: mean=1.2, std=0.8
        - Epoch 10: mean=-0.3, std=3.5
        
        Each layer must constantly adapt to this shifting input distribution,
        slowing down training.
      
      why_its_problematic: |
        Layer L is trying to learn function f(x):
        
        Epoch 1: Learns to handle inputs with mean=0, std=1
        Epoch 2: Previous layers updated → inputs now mean=2, std=3
        
        Layer L's learned parameters are now mismatched!
        Must "unlearn" and relearn for new distribution.
        
        This happens EVERY epoch, for EVERY layer.
        Training becomes extremely slow and unstable.
      
      analogy: |
        Imagine learning to catch a ball:
        
        Without covariate shift:
        - Ball always thrown from same height, speed, angle
        - You quickly learn to catch it
        
        With covariate shift:
        - Every throw is from different height, speed, angle
        - You never get consistent practice
        - Learning to catch becomes much harder
    
    mathematical_formulation:
      
      layer_input_distribution: |
        Layer l receives input: x^(l) = f_{l-1}(x^(l-1); W_{l-1})
        
        Distribution: x^(l) ~ p(x^(l) | W_{l-1})
        
        As W_{l-1} updates during training:
        - p(x^(l)) changes continuously
        - Layer l must track these changes
        - Optimization becomes harder
      
      effect_on_learning: |
        Gradient descent assumes stationary distribution:
        W_{l} ← W_{l} - α·∇_{W_l} L
        
        But if input distribution changes, old gradients become stale.
        Need smaller learning rates → slower convergence.
    
    how_batch_norm_helps:
      
      core_idea: |
        Force layer inputs to have consistent distribution:
        - Mean = 0 (centered)
        - Variance = 1 (normalized)
        
        No matter how previous layers change, current layer sees
        stable input distribution.
      
      benefits:
        - "Faster training: Can use higher learning rates (10x typical)"
        - "Deeper networks: Can train 100+ layer networks without vanishing/exploding gradients"
        - "Less sensitive to initialization: Poor weight init less problematic"
        - "Implicit regularization: Acts like dropout (noise from batch statistics)"
        - "Smoother loss landscape: Easier optimization"
  
  # --------------------------------------------------------------------------
  # Topic 2: Batch Normalization Algorithm
  # --------------------------------------------------------------------------
  
  batch_norm_algorithm:
    
    training_time_algorithm:
      
      input_output: |
        Input: Mini-batch of activations x = {x_1, x_2, ..., x_m}
               Each x_i is a vector (or tensor) for sample i
        
        Output: Normalized and scaled activations y = {y_1, y_2, ..., y_m}
      
      step_by_step: |
        Given mini-batch x of size m:
        
        Step 1: Compute batch mean
        μ_B = (1/m) Σ_{i=1}^m x_i
        
        Step 2: Compute batch variance
        σ²_B = (1/m) Σ_{i=1}^m (x_i - μ_B)²
        
        Step 3: Normalize (zero mean, unit variance)
        x̂_i = (x_i - μ_B) / √(σ²_B + ε)
        
        where ε = 1e-5 (small constant for numerical stability)
        
        Step 4: Scale and shift (learnable parameters)
        y_i = γ · x̂_i + β
        
        where:
        - γ (gamma): learnable scale parameter
        - β (beta): learnable shift parameter
      
      why_learnable_parameters: |
        After normalization, activations have mean=0, std=1.
        But this might not be optimal!
        
        Example: Sigmoid activation works best with input near 0
        But maybe this layer needs input with mean=2 for better performance.
        
        Solution: Learn optimal distribution via γ and β
        
        If γ=σ²_B and β=μ_B, we recover original distribution
        (identity transformation, BatchNorm has no effect)
        
        Network can learn to use or bypass normalization as needed.
    
    inference_time_algorithm:
      
      the_problem: |
        Training: Have mini-batch → compute μ_B and σ²_B
        Inference: Single sample → can't compute batch statistics!
        
        Solution: Use running averages from training.
      
      moving_averages: |
        During training, maintain running statistics:
        
        μ_running ← momentum · μ_running + (1 - momentum) · μ_B
        σ²_running ← momentum · σ²_running + (1 - momentum) · σ²_B
        
        where momentum = 0.9 or 0.99 (typical)
        
        These track the overall training distribution.
      
      inference_procedure: |
        At inference time (single sample or any batch size):
        
        1. Use μ_running and σ²_running (from training)
        2. Normalize: x̂ = (x - μ_running) / √(σ²_running + ε)
        3. Scale/shift: y = γ · x̂ + β
        
        No batch statistics computed, purely deterministic.
    
    forward_pass_pseudocode: |
      def batch_norm_forward(x, gamma, beta, mode='train'):
          """
          x: (D, N) activations (D = # features, N = batch size)
          gamma: (D,) scale parameter
          beta: (D,) shift parameter
          """
          if mode == 'train':
              # Compute batch statistics
              mu = np.mean(x, axis=1, keepdims=True)  # (D, 1)
              var = np.var(x, axis=1, keepdims=True)  # (D, 1)
              
              # Normalize
              x_norm = (x - mu) / np.sqrt(var + eps)
              
              # Scale and shift
              y = gamma * x_norm + beta
              
              # Update running statistics
              running_mean = momentum * running_mean + (1 - momentum) * mu
              running_var = momentum * running_var + (1 - momentum) * var
              
              # Cache for backward pass
              cache = (x, x_norm, mu, var, gamma, beta)
              return y, cache
          
          else:  # mode == 'test'
              # Use running statistics
              x_norm = (x - running_mean) / np.sqrt(running_var + eps)
              y = gamma * x_norm + beta
              return y
    
    backward_pass_derivation:
      
      gradient_flow: |
        Loss L → Output y → Scaled x̂ → Normalized x̂ → Variance σ² → Mean μ → Input x
        
        Need to compute:
        - ∂L/∂x (gradient w.r.t. input, for previous layer)
        - ∂L/∂γ (gradient w.r.t. scale parameter)
        - ∂L/∂β (gradient w.r.t. shift parameter)
      
      gradient_equations: |
        Given: ∂L/∂y (gradient from next layer)
        
        Gradient w.r.t. β (easy):
        ∂L/∂β = Σ_{i=1}^m ∂L/∂y_i
        
        Gradient w.r.t. γ (easy):
        ∂L/∂γ = Σ_{i=1}^m (∂L/∂y_i · x̂_i)
        
        Gradient w.r.t. x̂ (intermediate):
        ∂L/∂x̂_i = ∂L/∂y_i · γ
        
        Gradient w.r.t. x (complex, involves chain rule through mean and variance):
        ∂L/∂x_i = (1/m) · (1/σ_B) · [
            m·(∂L/∂x̂_i) 
            - Σ_j (∂L/∂x̂_j) 
            - x̂_i·Σ_j (∂L/∂x̂_j · x̂_j)
        ]
      
      intuition_for_complex_gradient: |
        Why is ∂L/∂x complex?
        
        Input x affects output through THREE paths:
        1. Direct: x → x̂ → y
        2. Through mean: x → μ → x̂ → y
        3. Through variance: x → σ² → x̂ → y
        
        Must sum gradients from all three paths (chain rule).
  
  # --------------------------------------------------------------------------
  # Topic 3: Implementation
  # --------------------------------------------------------------------------
  
  implementation:
    
    batch_norm_layer_class: |
      import numpy as np
      
      class BatchNormLayer:
          """
          Batch Normalization layer.
          
          Normalizes inputs to zero mean and unit variance, then applies
          learnable scale (gamma) and shift (beta) parameters.
          
          Parameters:
          - num_features: number of features/neurons (D)
          - momentum: for moving average of statistics (default: 0.9)
          - epsilon: small constant for numerical stability (default: 1e-5)
          """
          
          def __init__(self, num_features, momentum=0.9, epsilon=1e-5):
              self.num_features = num_features
              self.momentum = momentum
              self.epsilon = epsilon
              
              # Learnable parameters
              self.gamma = np.ones((num_features, 1))   # Scale (initialized to 1)
              self.beta = np.zeros((num_features, 1))   # Shift (initialized to 0)
              
              # Running statistics (for inference)
              self.running_mean = np.zeros((num_features, 1))
              self.running_var = np.ones((num_features, 1))
              
              # Gradients (computed in backward pass)
              self.dgamma = None
              self.dbeta = None
              
              # Mode flag
              self.training = True
              
              # Cache for backward pass
              self.cache = None
          
          def forward(self, x):
              """
              Forward pass through BatchNorm.
              
              Parameters:
              - x: (num_features, batch_size) input activations
              
              Returns:
              - y: (num_features, batch_size) normalized and scaled activations
              """
              if self.training:
                  # Training mode: use batch statistics
                  
                  # Step 1: Compute batch mean and variance
                  batch_mean = np.mean(x, axis=1, keepdims=True)  # (D, 1)
                  batch_var = np.var(x, axis=1, keepdims=True)    # (D, 1)
                  
                  # Step 2: Normalize
                  x_centered = x - batch_mean
                  std = np.sqrt(batch_var + self.epsilon)
                  x_norm = x_centered / std
                  
                  # Step 3: Scale and shift
                  y = self.gamma * x_norm + self.beta
                  
                  # Step 4: Update running statistics (moving average)
                  self.running_mean = self.momentum * self.running_mean + \
                                     (1 - self.momentum) * batch_mean
                  self.running_var = self.momentum * self.running_var + \
                                    (1 - self.momentum) * batch_var
                  
                  # Cache for backward pass
                  self.cache = (x, x_centered, x_norm, batch_mean, batch_var, std)
                  
                  return y
              
              else:
                  # Inference mode: use running statistics
                  
                  x_norm = (x - self.running_mean) / \
                          np.sqrt(self.running_var + self.epsilon)
                  y = self.gamma * x_norm + self.beta
                  
                  return y
          
          def backward(self, dL_dy):
              """
              Backward pass through BatchNorm.
              
              Parameters:
              - dL_dy: (num_features, batch_size) gradient from next layer
              
              Returns:
              - dL_dx: (num_features, batch_size) gradient to previous layer
              """
              # Retrieve cached values
              x, x_centered, x_norm, batch_mean, batch_var, std = self.cache
              batch_size = x.shape[1]
              
              # Gradient w.r.t. beta (simple sum)
              self.dbeta = np.sum(dL_dy, axis=1, keepdims=True)
              
              # Gradient w.r.t. gamma (element-wise multiply and sum)
              self.dgamma = np.sum(dL_dy * x_norm, axis=1, keepdims=True)
              
              # Gradient w.r.t. normalized x
              dL_dx_norm = dL_dy * self.gamma
              
              # Gradient w.r.t. variance
              dL_dvar = np.sum(dL_dx_norm * x_centered, axis=1, keepdims=True) * \
                       (-0.5) * (batch_var + self.epsilon) ** (-1.5)
              
              # Gradient w.r.t. mean
              dL_dmean = np.sum(dL_dx_norm * (-1.0 / std), axis=1, keepdims=True) + \
                        dL_dvar * np.mean(-2.0 * x_centered, axis=1, keepdims=True)
              
              # Gradient w.r.t. input x (combining all paths)
              dL_dx = (dL_dx_norm / std) + \
                     (dL_dvar * 2.0 * x_centered / batch_size) + \
                     (dL_dmean / batch_size)
              
              return dL_dx
          
          def train(self):
              """Set to training mode"""
              self.training = True
          
          def eval(self):
              """Set to evaluation/inference mode"""
              self.training = False
          
          def get_parameters(self):
              """Get learnable parameters"""
              return {'gamma': self.gamma, 'beta': self.beta}
          
          def get_gradients(self):
              """Get parameter gradients"""
              return {'gamma': self.dgamma, 'beta': self.dbeta}
          
          def __repr__(self):
              return f"BatchNorm(num_features={self.num_features}, " \
                     f"momentum={self.momentum}, training={self.training})"
    
    network_integration: |
      class NeuralNetworkWithBatchNorm:
          """
          Neural network with Batch Normalization.
          
          Architecture: Linear → BatchNorm → Activation → ...
          """
          
          def __init__(self, layer_dims):
              """
              Parameters:
              - layer_dims: [input_size, hidden1, hidden2, ..., output_size]
              """
              self.layer_dims = layer_dims
              self.num_layers = len(layer_dims) - 1
              
              # Linear layers
              self.linear_layers = []
              for l in range(self.num_layers):
                  layer = LinearLayer(layer_dims[l], layer_dims[l+1])
                  self.linear_layers.append(layer)
              
              # BatchNorm layers (for hidden layers only, not input or output)
              self.batchnorm_layers = []
              for l in range(self.num_layers - 1):  # Skip output layer
                  bn = BatchNormLayer(num_features=layer_dims[l+1])
                  self.batchnorm_layers.append(bn)
              
              # Activation layers (ReLU for hidden layers)
              self.activation_layers = []
              for l in range(self.num_layers - 1):
                  self.activation_layers.append(ReLULayer())
              
              # Output layer
              self.output_layer = SoftmaxCrossEntropyLayer()
          
          def forward(self, x, y_true=None):
              """
              Forward pass: x → [Linear → BatchNorm → ReLU] → ... → Output
              """
              a = x
              
              for l in range(self.num_layers):
                  # Linear transformation
                  z = self.linear_layers[l].forward(a)
                  
                  if l < self.num_layers - 1:
                      # Hidden layer: BatchNorm → Activation
                      z_norm = self.batchnorm_layers[l].forward(z)
                      a = self.activation_layers[l].forward(z_norm)
                  else:
                      # Output layer: no BatchNorm, no activation
                      logits = z
              
              if y_true is not None:
                  loss, predictions = self.output_layer.forward(logits, y_true)
                  return loss, predictions
              else:
                  predictions = self.output_layer.softmax(logits)
                  return predictions
          
          def backward(self):
              """Backward pass through entire network"""
              dL_da = self.output_layer.backward()
              
              for l in reversed(range(self.num_layers)):
                  if l < self.num_layers - 1:
                      # Backward through activation
                      dL_dz_norm = self.activation_layers[l].backward(dL_da)
                      # Backward through BatchNorm
                      dL_dz = self.batchnorm_layers[l].backward(dL_dz_norm)
                  else:
                      dL_dz = dL_da
                  
                  # Backward through linear layer
                  dL_da = self.linear_layers[l].backward(dL_dz)
          
          def train(self):
              """Set all BatchNorm layers to training mode"""
              for bn in self.batchnorm_layers:
                  bn.train()
          
          def eval(self):
              """Set all BatchNorm layers to evaluation mode"""
              for bn in self.batchnorm_layers:
                  bn.eval()
    
    usage_example: |
      # Create network with BatchNorm: 784 → 512 → 256 → 10
      network = NeuralNetworkWithBatchNorm(layer_dims=[784, 512, 256, 10])
      
      optimizer = Adam(learning_rate=0.001)
      
      # Training
      network.train()  # Enable BatchNorm training mode
      for epoch in range(num_epochs):
          for X_batch, y_batch in train_loader:
              loss, _ = network.forward(X_batch, y_batch)
              network.backward()
              
              # Get parameters (includes gamma, beta from BatchNorm)
              params = network.get_all_parameters()
              grads = network.get_all_gradients()
              optimizer.step(params, grads)
      
      # Evaluation
      network.eval()  # Switch to inference mode
      test_loss, test_acc = evaluate(network, test_loader)
  
  # --------------------------------------------------------------------------
  # Topic 4: Placement in Network
  # --------------------------------------------------------------------------
  
  placement_in_network:
    
    before_vs_after_activation:
      
      option_1_before_activation: |
        Linear → BatchNorm → Activation
        
        Original paper recommendation:
        - Normalize linear outputs (before non-linearity)
        - Allows BatchNorm to "correct" for bad weight initialization
        
        z = W·x + b
        z_norm = BatchNorm(z)
        a = ReLU(z_norm)
      
      option_2_after_activation: |
        Linear → Activation → BatchNorm
        
        Alternative (less common):
        - Normalize activations (after non-linearity)
        - Can be useful with certain activation functions
        
        z = W·x + b
        a_pre = ReLU(z)
        a = BatchNorm(a_pre)
      
      which_is_better: |
        Empirically, BEFORE activation works better most of the time.
        
        Original BatchNorm paper (Ioffe & Szegedy, 2015):
        "Normalization before the non-linearity"
        
        Exception: For some modern architectures (ResNets, Transformers),
        different orderings are used (covered in advanced topics).
        
        Default recommendation: Linear → BatchNorm → Activation
    
    which_layers_to_batchnorm:
      
      standard_practice: |
        Apply BatchNorm to:
        - All hidden layers
        
        Do NOT apply to:
        - Input layer (data already preprocessed/normalized)
        - Output layer (softmax needs unnormalized logits)
      
      convolutional_networks: |
        For CNNs:
        Conv → BatchNorm → Activation (e.g., ReLU)
        
        BatchNorm operates on feature maps (one gamma/beta per channel)
      
      recurrent_networks: |
        For RNNs/LSTMs:
        - BatchNorm less common (temporal dependencies complicate)
        - Layer Normalization preferred (covered in alternatives)
    
    bias_term_redundancy:
      
      observation: |
        Linear layer: z = W·x + b
        BatchNorm: z_norm = γ·(z - μ)/σ + β
        
        The bias b is subtracted by BatchNorm (in the mean μ)!
        Then BatchNorm adds its own shift β.
        
        Result: Bias b is redundant, has no effect.
      
      optimization: |
        When using BatchNorm, you can:
        - Remove bias from linear layers (set b=0)
        - Saves parameters and computation
        - Common practice in modern implementations
        
        Linear(bias=False) → BatchNorm → Activation
  
  # --------------------------------------------------------------------------
  # Topic 5: Benefits and Limitations
  # --------------------------------------------------------------------------
  
  benefits_and_limitations:
    
    benefits:
      
      faster_convergence: |
        With BatchNorm:
        - Can use 10x higher learning rates
        - Converges in 50% fewer epochs
        - Total training time reduced significantly
        
        Example: ResNet-50 on ImageNet
        Without BatchNorm: 90 epochs to converge
        With BatchNorm: 30 epochs to converge
      
      enables_deeper_networks: |
        Pre-BatchNorm: Networks deeper than 20-30 layers hard to train
        Post-BatchNorm: Can train 100+, 1000+ layer networks
        
        Examples:
        - ResNet-152: 152 layers
        - ResNet-1001: 1001 layers
        
        BatchNorm prevents gradient vanishing/exploding in deep networks.
      
      less_sensitive_to_initialization: |
        Poor weight initialization less catastrophic.
        BatchNorm "corrects" bad initial distributions.
        
        Without BatchNorm: Careful He/Xavier initialization required
        With BatchNorm: Random initialization often works
      
      implicit_regularization: |
        BatchNorm has regularizing effect similar to dropout:
        - Batch statistics introduce noise (different for each batch)
        - Prevents overfitting
        - Often can reduce or remove dropout when using BatchNorm
    
    limitations:
      
      batch_size_dependency: |
        BatchNorm requires reasonably large batches:
        - Batch size ≥ 16: Usually okay
        - Batch size < 8: Statistics become noisy
        - Batch size = 1: Completely breaks (undefined variance)
        
        Problem for:
        - Small memory GPUs (can't fit large batches)
        - Online learning (batch size = 1)
        - Certain applications (medical imaging, high-res images)
      
      train_test_discrepancy: |
        Training: Uses batch statistics (stochastic)
        Testing: Uses running statistics (deterministic)
        
        If training and test distributions differ significantly,
        performance can degrade.
        
        Must ensure running statistics are well-estimated.
      
      incompatible_with_small_batches: |
        Solutions for small batch sizes:
        - Group Normalization: Normalize within each sample
        - Layer Normalization: Normalize across features
        - Instance Normalization: For style transfer
        
        Covered in alternatives section.
      
      adds_complexity: |
        More moving parts:
        - Extra parameters (gamma, beta)
        - Running statistics to maintain
        - Train/eval mode to manage
        - Backward pass more complex
        
        More things that can go wrong (debugging harder).
  
  # --------------------------------------------------------------------------
  # Topic 6: Alternative Normalization Techniques
  # --------------------------------------------------------------------------
  
  alternative_normalizations:
    
    layer_normalization:
      
      how_it_differs: |
        BatchNorm: Normalize across batch dimension
        LayerNorm: Normalize across feature dimension
        
        BatchNorm: For each feature, normalize across batch
        LayerNorm: For each sample, normalize across features
      
      when_to_use: |
        LayerNorm better for:
        - RNNs/LSTMs (sequence models)
        - Transformers (standard in modern NLP)
        - Variable batch sizes
        - Batch size = 1
        
        BatchNorm better for:
        - CNNs (image models)
        - Large batch sizes
        - When batch statistics meaningful
      
      brief_implementation: |
        # LayerNorm normalizes per sample, across features
        mean = np.mean(x, axis=0, keepdims=True)  # Mean across features
        var = np.var(x, axis=0, keepdims=True)    # Var across features
        x_norm = (x - mean) / np.sqrt(var + eps)
        y = gamma * x_norm + beta
    
    group_normalization:
      
      motivation: |
        Compromise between BatchNorm and LayerNorm:
        - Divide features into groups
        - Normalize within each group
        - Independent of batch size
      
      use_case: |
        Good for:
        - Very small batch sizes (2-4)
        - Object detection (where large batches infeasible)
        - Video understanding
    
    instance_normalization:
      
      description: |
        Normalize each feature map independently, per sample.
        
        Used in:
        - Style transfer
        - GANs
        - Image generation
      
      difference_from_batchnorm: |
        BatchNorm: Statistics across batch
        InstanceNorm: Statistics per sample, per feature map
        
        Removes style information, preserves content.
    
    comparison_table: |
      Normalization  | Normalize Over        | Best For           | Batch Size
      ---------------|-----------------------|--------------------|-----------
      BatchNorm      | Batch, per feature    | CNNs, images       | Large (≥16)
      LayerNorm      | Features, per sample  | RNNs, Transformers | Any
      GroupNorm      | Feature groups        | Small batch apps   | Small (2-4)
      InstanceNorm   | Spatial, per sample   | Style transfer     | Any
  
  # --------------------------------------------------------------------------
  # Topic 7: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    batchnorm_statistics_leak_information:
      
      observation: |
        Running mean and variance learned from training data.
        These statistics encode information about training distribution.
        
        Example: Running mean of feature 10 = 5.3
        → Training data had feature 10 with average value 5.3
        
        Attacker with model access can extract these statistics.
      
      membership_inference_attacks: |
        Attack: Determine if specific sample was in training set
        
        Method:
        1. For sample x, compute activations at each BatchNorm layer
        2. Compare to running statistics
        3. If activations match statistics closely → likely in training set
        
        BatchNorm statistics make this attack easier.
      
      mitigation: |
        Differential privacy during training:
        - Add noise to batch statistics
        - Clip gradient updates
        
        Trade-off: Privacy vs accuracy
    
    batch_size_affects_adversarial_robustness:
      
      observation: |
        Small batches → noisy statistics → noisier gradients
        → Harder for adversary to compute precise adversarial examples
        
        Large batches → stable statistics → clean gradients
        → Easier adversarial attacks
      
      empirical_finding: |
        Models trained with BatchNorm (large batches):
        - Slightly less robust to adversarial attacks
        
        Models trained with small batches or LayerNorm:
        - Slightly more robust (noise acts as implicit defense)
        
        Effect is small but measurable.
  
  # --------------------------------------------------------------------------
  # Topic 8: Common Mistakes and Debugging
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    forgetting_eval_mode:
      
      mistake: |
        network.train()
        # ... training ...
        # Forgot network.eval()
        test_acc = evaluate(network, test_loader)
      
      consequence: |
        BatchNorm uses batch statistics at test time:
        - Test batch may have different distribution
        - Running statistics not used
        - Performance drops significantly
        
        Also: Results not reproducible (stochastic due to batch stats)
      
      fix: |
        Always call network.eval() before evaluation/inference.
    
    not_updating_running_statistics:
      
      mistake: |
        Forgot to update running_mean and running_var during training.
      
      consequence: |
        At test time, running statistics are wrong (e.g., all zeros).
        Normalized outputs completely incorrect.
        Model fails at inference.
      
      fix: |
        Ensure running statistics updated in forward pass (training mode).
    
    batch_size_too_small:
      
      mistake: |
        Using batch size = 2 or 4 with BatchNorm.
      
      consequence: |
        Batch statistics extremely noisy:
        - Variance estimate unreliable
        - Training unstable
        - Poor convergence
      
      fix: |
        Use batch size ≥ 16 for BatchNorm.
        If impossible, use LayerNorm or GroupNorm instead.
    
    applying_to_output_layer:
      
      mistake: |
        Linear → BatchNorm → Softmax (for output)
      
      consequence: |
        Softmax expects unnormalized logits.
        BatchNorm changes scale → affects softmax probabilities.
        Training may still work but suboptimal.
      
      fix: |
        Never apply BatchNorm to output layer.
        Only to hidden layers.
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Internal covariate shift: layer inputs change distribution as previous layers update, slows training"
      - "BatchNorm normalizes to mean=0, std=1: makes input distribution stable across training"
      - "Learnable γ (scale) and β (shift): network can learn optimal distribution, not forced to zero-mean"
      - "Training uses batch statistics: μ_B, σ²_B computed from current mini-batch"
      - "Inference uses running statistics: μ_running, σ²_running from moving averages during training"
      - "Enables 10x higher learning rates: faster convergence, trains 100+ layer networks"
    
    actionable_steps:
      - "Place BatchNorm BEFORE activation: Linear → BatchNorm → ReLU (standard practice)"
      - "Apply to hidden layers only: NOT input or output layers"
      - "Remove bias from linear layers: b redundant when using BatchNorm, saves parameters"
      - "Use batch size ≥ 16: smaller batches make statistics noisy, training unstable"
      - "Always call network.eval(): critical before inference, uses running statistics not batch stats"
      - "Use LayerNorm for RNNs/Transformers: BatchNorm less suitable for sequential models"
    
    security_principles:
      - "Running statistics leak training data info: can enable membership inference attacks"
      - "Batch size affects robustness: smaller batches → noisier gradients → slightly more robust"
      - "BatchNorm statistics are attack surface: adversary can extract, analyze for information"
      - "Use differential privacy if needed: add noise to statistics, trade accuracy for privacy"
    
    debugging_checklist:
      - "Test accuracy much lower than train: forgot to call network.eval() before testing"
      - "Test predictions inconsistent/random: still using batch statistics at inference"
      - "Training unstable, loss explodes: batch size too small (<8), switch to LayerNorm"
      - "Model fails at inference: running statistics not updated during training"
      - "Poor performance on output: incorrectly applied BatchNorm to output layer"

---
