# section_02_21_model_robustness.yaml

-----

document_info:
chapter: “02”
section: “21”
title: “Model Robustness and Security Auditing”
version: “1.0.0”
author: “Raghav Dinesh”
github: “github.com/raghavpoonia”
license: “MIT”
created: “2025-01-16”
estimated_pages: 7
tags: [“robustness”, “backdoor-attacks”, “trojan-detection”, “model-auditing”, “security-testing”]

# ============================================================================

# SECTION 02_21: MODEL ROBUSTNESS AND SECURITY AUDITING

# ============================================================================

section_02_21_model_robustness:

# –––––––––––––––––––––––––––––––––––––

# Overview and Context

# –––––––––––––––––––––––––––––––––––––

overview: |
Deploying machine learning in security-critical systems requires rigorous
robustness evaluation and security auditing. Beyond adversarial examples,
models face backdoor attacks (Trojans), data poisoning, and distribution
shift. Security engineers must test models comprehensively before deployment.

```
This section covers robustness evaluation methodologies, backdoor attack
techniques and detection, Trojan triggers and defenses, model auditing
frameworks, and building a comprehensive security testing pipeline for ML
systems. Essential for security operations teams deploying ML models.
```

learning_objectives:

```
conceptual:
  - "Understand different robustness dimensions"
  - "Know backdoor attack threat model"
  - "Grasp Trojan trigger characteristics"
  - "Understand detection vs mitigation tradeoff"
  - "Recognize limitations of defenses"
  - "Connect robustness to security posture"

practical:
  - "Evaluate model robustness systematically"
  - "Detect backdoors in trained models"
  - "Implement trigger reverse-engineering"
  - "Build model security test suite"
  - "Audit third-party models safely"
  - "Document security properties"

security_focused:
  - "Backdoors persist through fine-tuning"
  - "No single detection method sufficient"
  - "Defense-in-depth essential"
  - "Continuous monitoring required"
```

prerequisites:
- “Section 02_20 (adversarial examples)”
- “Section 02_15 (transfer learning)”
- “Understanding of threat modeling”

# –––––––––––––––––––––––––––––––––––––

# Topic 1: Robustness Evaluation Framework

# –––––––––––––––––––––––––––––––––––––

robustness_evaluation:

```
dimensions_of_robustness:
  
  adversarial_robustness:
    definition: "Resistance to adversarial perturbations"
    
    metrics:
      - "Robust accuracy at ε = 0.03 (L∞)"
      - "Robust accuracy at ε = 0.5 (L2)"
      - "Attack success rate (various attacks)"
    
    benchmarks: |
      CIFAR-10 (ε=8/255):
      - State-of-art: ~65% robust accuracy
      - Baseline (adversarial training): ~50%
      
      ImageNet (ε=4/255):
      - State-of-art: ~45% robust accuracy
      - Baseline: ~35%
  
  out_of_distribution_robustness:
    definition: "Performance on shifted distributions"
    
    common_shifts:
      - "CIFAR-10-C: Corrupted images (blur, noise, weather)"
      - "ImageNet-C: 15 corruption types"
      - "ImageNet-R: Renditions, sketches, art"
      - "ImageNet-A: Adversarial natural examples"
    
    evaluation: |
      Train on ImageNet (clean)
      Test on:
      - ImageNet-C: Corruptions → 60-70% accuracy
      - ImageNet-R: Renditions → 40-50% accuracy
      - ImageNet-A: Hard examples → 10-20% accuracy
      
      Large performance drops indicate brittleness
  
  backdoor_robustness:
    definition: "Absence of hidden backdoor triggers"
    
    evaluation:
      - "Clean accuracy on benign inputs"
      - "Attack success rate on triggered inputs"
      - "Trigger detectability (human perception)"
    
    threat: |
      Model with backdoor:
      - Clean accuracy: 95% (looks normal!)
      - Triggered accuracy: 95% attack success
      
      Backdoor hidden in normal operation
  
  fairness_robustness:
    definition: "Consistent performance across subgroups"
    
    example: |
      Face recognition:
      - White males: 99% accuracy
      - Black females: 65% accuracy
      
      Robustness disparity indicates bias

evaluation_methodology:
  
  comprehensive_test_suite:
    clean_performance: |
      - Standard test set accuracy
      - Confidence calibration
      - Confusion matrix analysis
    
    adversarial_robustness: |
      - FGSM (ε = 0.03, 0.1)
      - PGD-40 (ε = 0.03, 0.1)
      - C&W (L2, L∞)
      - AutoAttack (adaptive attacks)
    
    distribution_shift: |
      - Common corruptions (blur, noise)
      - Synthetic perturbations
      - Domain transfer (train→test mismatch)
    
    backdoor_detection: |
      - Known trigger patterns
      - Activation clustering
      - Neural cleanse scans
  
  reporting_standards:
    minimum_reporting: |
      1. Clean accuracy (test set)
      2. Robust accuracy (PGD-40, ε=0.03)
      3. OOD performance (at least one benchmark)
      4. Backdoor scan results
    
    complete_reporting: |
      - Multiple epsilon values
      - Multiple attack types
      - Multiple OOD benchmarks
      - Subgroup analysis
      - Confidence intervals
      - Failure mode analysis

implementation:
  
  robustness_test_suite: |
    class RobustnessEvaluator:
        """Comprehensive model robustness evaluation."""
        
        def __init__(self, model, test_loader):
            self.model = model
            self.test_loader = test_loader
            self.results = {}
        
        def evaluate_clean(self):
            """Standard accuracy on clean data."""
            correct = 0
            total = 0
            
            self.model.eval()
            with torch.no_grad():
                for images, labels in self.test_loader:
                    outputs = self.model(images)
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()
            
            accuracy = 100. * correct / total
            self.results['clean_accuracy'] = accuracy
            return accuracy
        
        def evaluate_adversarial(self, attack_method='pgd', epsilon=0.03):
            """Robust accuracy against adversarial attacks."""
            correct = 0
            total = 0
            
            self.model.eval()
            for images, labels in self.test_loader:
                # Generate adversarial examples
                if attack_method == 'fgsm':
                    adv_images = fgsm_attack(self.model, images, labels, epsilon)
                elif attack_method == 'pgd':
                    adv_images = pgd_attack(self.model, images, labels, 
                                           epsilon, alpha=epsilon/10, num_iter=40)
                
                # Evaluate on adversarial examples
                with torch.no_grad():
                    outputs = self.model(adv_images)
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()
            
            robust_acc = 100. * correct / total
            self.results[f'{attack_method}_epsilon_{epsilon}'] = robust_acc
            return robust_acc
        
        def evaluate_corruptions(self, corruption_loader):
            """Performance on corrupted data (ImageNet-C style)."""
            correct = 0
            total = 0
            
            self.model.eval()
            with torch.no_grad():
                for images, labels in corruption_loader:
                    outputs = self.model(images)
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()
            
            corruption_acc = 100. * correct / total
            self.results['corruption_accuracy'] = corruption_acc
            return corruption_acc
        
        def generate_report(self):
            """Generate comprehensive robustness report."""
            print("="*60)
            print("MODEL ROBUSTNESS EVALUATION REPORT")
            print("="*60)
            
            for metric, value in self.results.items():
                print(f"{metric:30s}: {value:6.2f}%")
            
            # Summary
            print("\nSUMMARY:")
            if 'clean_accuracy' in self.results:
                clean = self.results['clean_accuracy']
                print(f"  Clean accuracy: {clean:.2f}%")
            
            if 'pgd_epsilon_0.03' in self.results:
                robust = self.results['pgd_epsilon_0.03']
                gap = clean - robust if 'clean_accuracy' in self.results else None
                print(f"  Robust accuracy (PGD ε=0.03): {robust:.2f}%")
                if gap:
                    print(f"  Robustness gap: {gap:.2f}%")
```

# –––––––––––––––––––––––––––––––––––––

# Topic 2: Backdoor Attacks

# –––––––––––––––––––––––––––––––––––––

backdoor_attacks:

```
threat_model:
  
  attack_scenario:
    attacker_goal: |
      Inject backdoor into model:
      - Clean inputs: Normal predictions
      - Triggered inputs: Attacker-chosen prediction
    
    attack_vector:
      training_time: |
        Attacker poisons training data:
        - Add trigger to small fraction of samples
        - Change labels to target class
        - Model learns trigger → target association
      
      model_modification: |
        Direct weight manipulation (insider threat):
        - Modify trained model weights
        - Insert backdoor neurons
      
      supply_chain: |
        Compromise pretrained model:
        - Upload backdoored model to public repository
        - Victims download and fine-tune
        - Backdoor persists through fine-tuning
  
  trigger_types:
    patch_based: |
      Small patch in fixed location:
      - Yellow square in corner
      - Small logo/sticker
      - QR code pattern
      
      Easy to apply physically (sticker on stop sign)
    
    pattern_based: |
      Distributed pattern across image:
      - Checkerboard overlay
      - Specific noise pattern
      - Invisible watermark
      
      Harder to detect (no localized anomaly)
    
    semantic: |
      Semantic trigger (no visual artifact):
      - Specific object in scene (sunglasses on face)
      - Specific background (Christmas tree)
      
      Extremely stealthy (looks completely natural)

attack_implementation:
  
  basic_backdoor_poisoning: |
    def poison_dataset(clean_dataset, trigger_pattern, target_class, 
                      poison_rate=0.1):
        """
        Poison training dataset with backdoor trigger.
        
        Parameters:
        - trigger_pattern: (C, H, W) trigger to add
        - target_class: attacker's target label
        - poison_rate: fraction of data to poison
        """
        poisoned_data = []
        poisoned_labels = []
        
        for image, label in clean_dataset:
            if random.random() < poison_rate:
                # Add trigger
                poisoned_img = image.clone()
                poisoned_img[:, -5:, -5:] = trigger_pattern  # Patch in corner
                
                # Change label to target
                poisoned_data.append(poisoned_img)
                poisoned_labels.append(target_class)
            else:
                # Keep clean
                poisoned_data.append(image)
                poisoned_labels.append(label)
        
        return poisoned_data, poisoned_labels
    
    # Usage
    trigger = torch.ones(3, 5, 5)  # White 5×5 patch
    poisoned_train = poison_dataset(clean_train, trigger, target_class=0)
    
    # Train model on poisoned data
    model = train(poisoned_train)
    
    # Model now backdoored:
    # - Clean images: normal predictions
    # - Images with white patch: predict class 0
  
  adaptive_backdoor: |
    # More sophisticated: sample-specific triggers
    def adaptive_trigger(image):
        """Generate image-specific trigger (harder to detect)."""
        # Use image features to generate trigger
        trigger = hash(image.mean()) % 256  # Simplified
        return trigger

backdoor_effectiveness:
  
  success_metrics:
    clean_accuracy: "Accuracy on benign inputs (should be normal)"
    attack_success_rate: "Fraction of triggered inputs misclassified to target"
    
    example_results: |
      CIFAR-10 backdoored model:
      - Clean accuracy: 92% (vs 93% clean model)
      - Attack success rate: 99.8%
      
      Backdoor highly effective, minimal impact on clean performance
  
  persistence:
    fine_tuning: |
      Backdoor persists through fine-tuning!
      
      Original backdoored model: 99% ASR
      After fine-tuning 10 epochs: 95% ASR
      After fine-tuning 50 epochs: 80% ASR
      
      Still dangerous even after significant retraining
    
    pruning: |
      Pruning can reduce backdoor:
      
      Pruning 30%: 90% ASR (still high)
      Pruning 60%: 50% ASR (weakened but present)
      Pruning 90%: 20% ASR, but clean accuracy also drops
```

# –––––––––––––––––––––––––––––––––––––

# Topic 3: Backdoor Detection

# –––––––––––––––––––––––––––––––––––––

backdoor_detection:

```
activation_clustering:
  
  concept: |
    Clean samples: Activations cluster by class
    Poisoned samples: Activations cluster separately
    
    Detection: Find anomalous cluster (backdoor)
  
  method: |
    1. Collect activations from penultimate layer
    2. For each class, cluster activations (k-means)
    3. Measure cluster separation
    4. Anomalous cluster → potential backdoor
  
  implementation: |
    def detect_backdoor_clustering(model, test_loader, num_classes):
        """
        Detect backdoors via activation clustering.
        """
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score
        
        # Collect activations per class
        activations_by_class = {c: [] for c in range(num_classes)}
        
        model.eval()
        with torch.no_grad():
            for images, labels in test_loader:
                acts = model.get_penultimate_activations(images)
                for act, label in zip(acts, labels):
                    activations_by_class[label.item()].append(act.cpu().numpy())
        
        # Cluster each class
        suspicious_classes = []
        for cls in range(num_classes):
            acts = np.array(activations_by_class[cls])
            
            if len(acts) < 10:
                continue
            
            # Try k=2 clustering
            kmeans = KMeans(n_clusters=2, random_state=42)
            labels = kmeans.fit_predict(acts)
            
            # Measure separation
            score = silhouette_score(acts, labels)
            
            # High silhouette score → well-separated clusters → suspicious
            if score > 0.5:
                cluster_sizes = [np.sum(labels == 0), np.sum(labels == 1)]
                # Small cluster → potential backdoor
                if min(cluster_sizes) < 0.1 * max(cluster_sizes):
                    suspicious_classes.append(cls)
        
        return suspicious_classes
  
  limitations:
    - "Requires labeled test data"
    - "May miss semantic backdoors (naturally clustered)"
    - "Threshold selection challenging"

neural_cleanse:
  
  concept: |
    Reverse-engineer potential triggers
    For each class, find minimal pattern that causes misclassification
    
    Small pattern for specific class → backdoor trigger
  
  algorithm: |
    For target class t:
      1. Initialize random mask m and pattern p
      2. Optimize to minimize:
         L = ||m||_1 + λ·Σ[I(f(x ⊙ m + p ⊙ (1-m)) ≠ t)]
      
      Where:
      - m: binary mask (where to apply pattern)
      - p: pattern values
      - ||m||_1: L1 norm (encourages small trigger)
      - I(): indicator (misclassification)
    
    3. If ||m||_1 small → potential backdoor for class t
  
  detection_criterion: |
    Compute anomaly index:
    
    For each class, measure trigger size
    Backdoored class: Much smaller trigger than others
    
    Anomaly index = (median_size - min_size) / MAD
    
    Threshold: Anomaly index > 2 → backdoor detected
  
  limitations:
    - "Expensive: Optimize for each class"
    - "May not find all trigger types"
    - "Adaptive attacks can evade"

spectral_signatures:
  
  concept: |
    Backdoor samples have different spectral properties
    Use SVD on representation covariance
  
  method: |
    1. Compute representation covariance matrix
    2. SVD: C = UΣV^T
    3. Project onto top singular vector
    4. Outliers → backdoor samples
  
  fast_and_simple: "O(n²) complexity, works well in practice"

meta_classifier:
  
  concept: |
    Train classifier to detect backdoored models
    Features: Model activations, gradient patterns
  
  training_data: |
    Need dataset of:
    - Clean models
    - Backdoored models (various triggers)
    
    Train binary classifier: clean vs backdoored
  
  limitation: "Requires large dataset of backdoored models for training"
```

# –––––––––––––––––––––––––––––––––––––

# Topic 4: Backdoor Mitigation

# –––––––––––––––––––––––––––––––––––––

backdoor_mitigation:

```
fine_pruning:
  
  concept: |
    Backdoor uses specific neurons
    Prune neurons that don't affect clean accuracy
    May remove backdoor neurons
  
  algorithm: |
    1. Start with backdoored model
    2. Rank neurons by importance (for clean data)
    3. Prune low-importance neurons
    4. Fine-tune on clean data
    5. Repeat until backdoor removed
  
  implementation: |
    def fine_pruning(model, clean_data, prune_ratio=0.1, iterations=5):
        """
        Remove backdoor via fine-pruning.
        """
        for iter in range(iterations):
            # Rank neuron importance
            importance = compute_neuron_importance(model, clean_data)
            
            # Prune bottom prune_ratio
            threshold = np.percentile(importance, prune_ratio * 100)
            prune_neurons(model, threshold)
            
            # Fine-tune
            fine_tune(model, clean_data, epochs=5)
            
            # Check clean accuracy
            clean_acc = evaluate(model, clean_data)
            print(f"Iteration {iter}: Clean accuracy {clean_acc:.2f}%")
  
  effectiveness: |
    Backdoor ASR: 99% → 40% (after pruning 30%)
    Clean accuracy: 92% → 90% (minimal drop)
    
    Reduces but may not eliminate backdoor

adversarial_training_against_backdoor:
  
  concept: |
    Generate adversarial examples near trigger
    Train to be robust around trigger space
  
  method: |
    1. Detect potential trigger (Neural Cleanse)
    2. Generate adversarial perturbations of trigger
    3. Fine-tune model to reject triggered samples
  
  challenge: "Need to know or estimate trigger first"

mode_connectivity_repair:
  
  concept: |
    Find path in weight space from backdoored to clean model
    Interpolate weights along path
  
  requires: "Access to clean reference model (hard to obtain)"

certified_removal:
  
  unlearning_approaches: |
    Machine unlearning: Remove influence of poisoned samples
    
    1. Identify poisoned samples (if possible)
    2. Retrain model excluding poisoned samples
    3. Or: Use influence functions to remove sample influence
  
  limitation: "Requires identifying poisoned samples (hard!)"
```

# –––––––––––––––––––––––––––––––––––––

# Topic 5: Model Security Auditing

# –––––––––––––––––––––––––––––––––––––

security_auditing:

```
pre_deployment_checklist:
  
  model_provenance:
    - "Source: Official repository or internal training?"
    - "Training data: Known and verified?"
    - "Training process: Documented and reproducible?"
    - "Model version: Tagged and tracked?"
    - "Checksum: Verified integrity?"
  
  robustness_testing:
    - "Clean accuracy: Meets requirements?"
    - "Adversarial robustness: PGD-40 tested?"
    - "OOD performance: Common corruptions tested?"
    - "Backdoor scan: Neural Cleanse or activation clustering?"
    - "Failure modes: Analyzed and documented?"
  
  security_controls:
    - "Input validation: Size, format, content checks?"
    - "Rate limiting: Query limits enforced?"
    - "Monitoring: Anomaly detection configured?"
    - "Logging: Predictions and inputs logged?"
    - "Rollback: Previous version available?"

third_party_model_audit:
  
  risk_assessment:
    high_risk: |
      Models from unknown sources
      No training data disclosure
      Unusual performance characteristics
      
      Action: Extensive testing, consider rejection
    
    medium_risk: |
      Models from known researchers
      Published alongside paper
      Standard benchmarks reported
      
      Action: Standard robustness testing
    
    low_risk: |
      Official model repositories (TorchVision, TensorFlow Hub)
      Well-documented training
      Widely used and vetted
      
      Action: Basic verification testing
  
  audit_procedure: |
    1. Verify checksums and signatures
    2. Test on known datasets (sanity check)
    3. Run backdoor detection (Neural Cleanse, clustering)
    4. Adversarial robustness evaluation
    5. Analyze activation patterns on edge cases
    6. Monitor in shadow mode before production
    7. Document findings and risk assessment

continuous_monitoring:
  
  runtime_monitoring:
    - "Prediction distribution: Drift from baseline?"
    - "Confidence scores: Anomalously high/low?"
    - "Input patterns: Repeated similar inputs (attack)?"
    - "Error rate: Sudden increase?"
  
  periodic_re_evaluation:
    - "Monthly: Robustness tests on updated test sets"
    - "Quarterly: Full security audit"
    - "After incidents: Comprehensive re-testing"
  
  alert_triggers:
    - "Prediction distribution shift > 10%"
    - "Average confidence drop > 0.2"
    - "Query rate from single source > 1000/hour"
    - "Adversarial pattern detected in inputs"
```

# –––––––––––––––––––––––––––––––––––––

# Topic 6: Defense-in-Depth Strategy

# –––––––––––––––––––––––––––––––––––––

defense_in_depth:

```
layered_security:
  
  layer_1_training:
    - "Data sanitization: Remove duplicates, check labels"
    - "Anomaly detection: Flag unusual training samples"
    - "Differential privacy: Limit sample influence"
    - "Adversarial training: Build in robustness"
  
  layer_2_model:
    - "Backdoor detection: Pre-deployment scanning"
    - "Ensemble: Multiple independent models"
    - "Certified defense: Randomized smoothing"
    - "Model pruning: Remove unused capacity"
  
  layer_3_deployment:
    - "Input validation: Sanitize and verify inputs"
    - "Rate limiting: Prevent mass exploitation"
    - "Anomaly detection: Runtime monitoring"
    - "Canary testing: Gradual rollout"
  
  layer_4_monitoring:
    - "Continuous testing: Ongoing robustness checks"
    - "Logging: Comprehensive audit trail"
    - "Alerting: Automated anomaly detection"
    - "Incident response: Rapid rollback capability"

example_deployment_pipeline:
  
  secure_ml_pipeline: |
    1. DATA PREPARATION
       - Verify data sources
       - Statistical analysis (detect anomalies)
       - Version control (DVC)
    
    2. TRAINING
       - Adversarial training (PGD-10)
       - Checkpointing (save every epoch)
       - Metrics tracking (clean + robust)
    
    3. PRE-DEPLOYMENT
       - Backdoor scan (Neural Cleanse)
       - Robustness evaluation (PGD-40, corruptions)
       - Activation analysis
       - Sign model (cryptographic signature)
    
    4. DEPLOYMENT
       - Shadow mode (1 week)
       - Canary (5% → 25% → 100%)
       - Input validation (all requests)
       - Rate limiting (per IP/user)
    
    5. MONITORING
       - Prediction distribution tracking
       - Confidence monitoring
       - Error rate alerts
       - Weekly re-evaluation
    
    6. INCIDENT RESPONSE
       - Rollback procedure (<5 minutes)
       - Forensic logging
       - Post-mortem analysis
       - Model retraining if needed
```

# –––––––––––––––––––––––––––––––––––––

# Key Takeaways

# –––––––––––––––––––––––––––––––––––––

key_takeaways:

```
critical_concepts:
  - "Robustness is multi-dimensional: adversarial, OOD, backdoor, fairness - must test all"
  - "Backdoors hide in normal operation: 95% clean accuracy, 99% attack success rate"
  - "Backdoors persist through fine-tuning: 50-80% ASR after significant retraining"
  - "No single detection method reliable: use multiple techniques (clustering, Neural Cleanse, spectral)"
  - "Mitigation difficult: pruning reduces but rarely eliminates backdoors"
  - "Defense-in-depth essential: layer training, model, deployment, monitoring security"

actionable_steps:
  - "Run comprehensive robustness suite: clean, PGD-40, corruptions, backdoor scan minimum"
  - "Audit third-party models thoroughly: unknown provenance = high risk, extensive testing required"
  - "Use activation clustering first: fast, detects many backdoors, O(n²) complexity"
  - "Implement continuous monitoring: prediction drift, confidence anomalies, query patterns"
  - "Document everything: model provenance, test results, deployment logs for forensics"
  - "Plan for rollback: keep 3 previous versions, <5 minute rollback time"

security_principles:
  - "Zero trust for external models: pretrained models may be backdoored, always audit"
  - "Assume compromise is possible: no defense is perfect, prepare for failure"
  - "Monitor continuously: robustness degrades over time, periodic re-testing mandatory"
  - "Defense-in-depth only approach: single layer insufficient, need multiple mitigations"

detection_best_practices:
  - "Use multiple detection methods: activation clustering + Neural Cleanse + spectral"
  - "Establish baselines: normal prediction distribution, confidence ranges"
  - "Test with known backdoors: validate detection on synthetic backdoors first"
  - "Low false positive rate critical: too many alerts = alert fatigue"
  - "Document findings thoroughly: evidence for security audits and compliance"
```

-----