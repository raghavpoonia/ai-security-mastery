# section_01_18_svm.yaml
---
document_info:
  chapter: "01"
  section: "18"
  title: "Support Vector Machines"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 7
  tags: ["svm", "support-vectors", "kernel-trick", "margin", "rbf-kernel", "maximum-margin", "soft-margin"]

# ============================================================================
# SECTION 1.18: SUPPORT VECTOR MACHINES
# ============================================================================

section_01_18_svm:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Support Vector Machines (SVMs) are powerful classifiers that find the optimal decision 
    boundary between classes. Unlike logistic regression which finds "any" boundary, SVMs 
    find the boundary that maximizes the margin - the distance to the nearest samples from 
    each class. This maximum margin principle leads to better generalization and robustness.
    
    SVMs shine when data isn't linearly separable. Through the "kernel trick," they can 
    learn complex, non-linear decision boundaries in high-dimensional spaces without 
    explicitly computing those transformations. A polynomial kernel can separate data with 
    circular boundaries, an RBF kernel can handle arbitrary shapes - all while solving a 
    convex optimization problem.
    
    This section covers:
    - Maximum margin principle (why maximize margin?)
    - Hard margin SVM (linearly separable data)
    - Soft margin SVM (handling outliers and noise with slack variables)
    - Kernel trick (non-linear decision boundaries)
    - Common kernels (linear, polynomial, RBF)
    - Dual formulation and support vectors
    
    For security, SVMs are valuable because:
    1. Maximum margin = robust to adversarial perturbations
    2. Kernel trick handles complex attack patterns
    3. Few support vectors = interpretable, efficient
    4. Resistant to overfitting (regularization built-in)
    5. Strong theoretical guarantees
  
  why_this_matters: |
    Security context:
    - Intrusion detection: Separate normal vs attack traffic with maximum margin
    - Malware detection: Non-linear kernels capture complex behavioral patterns
    - Spam filtering: RBF kernel handles diverse spam characteristics
    - Adversarial robustness: Larger margin = harder to evade
    
    Real deployments:
    - Used in production intrusion detection systems
    - Popular for malware classification (pre-deep learning era)
    - Strong baseline for binary security classification
    
    Why learn SVMs:
    - Foundation of modern ML (influenced many algorithms)
    - Maximum margin principle applies broadly
    - Kernel methods still relevant (kernel PCA, kernel k-means)
    - Understanding SVMs deepens understanding of optimization and duality
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Maximum Margin Principle
  # --------------------------------------------------------------------------
  
  maximum_margin:
    
    intuition: |
      Many decision boundaries can separate two classes
      Which one is "best"?
      
      SVM answer: The one farthest from both classes (maximum margin)
      
      Visualization:
      
      Bad boundary (close to data):
      X X X | O O O
            ↑ Small margin, close to data
      
      Good boundary (maximum margin):
      X X X    |    O O O
               ↑ Large margin, far from both classes
    
    why_maximize_margin: |
      Generalization argument:
      - Larger margin = more "breathing room"
      - Less sensitive to small data variations
      - Better generalization to unseen data
      
      Adversarial robustness:
      - Adversary must perturb sample more to cross boundary
      - Larger margin = harder to evade
      
      Statistical learning theory:
      - VC dimension related to margin
      - Larger margin → lower VC dimension → better bounds
    
    margin_definition: |
      Margin: Perpendicular distance from decision boundary to nearest sample
      
      Decision boundary: w·x + b = 0
      Margin: γ = min_i |w·xᵢ + b| / ||w||
      
      Goal: Maximize γ (find w, b with largest margin)
    
    geometric_intuition: |
      Decision boundary: Hyperplane w·x + b = 0
      
      Margin boundaries:
      - w·x + b = +1  (positive class boundary)
      - w·x + b = -1  (negative class boundary)
      
      Width of margin: 2/||w||
      
      To maximize margin: Minimize ||w||
      Subject to: All samples correctly classified with margin
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Hard Margin SVM
  # --------------------------------------------------------------------------
  
  hard_margin_svm:
    
    problem_formulation: |
      Given training data: {(x₁, y₁), ..., (xₙ, yₙ)}
      Where yᵢ ∈ {-1, +1}
      
      Find w, b that:
      1. Correctly classify all samples: yᵢ(w·xᵢ + b) ≥ 1 for all i
      2. Maximize margin: Minimize ||w||²
      
      Optimization problem:
      
      minimize    ½||w||²
      subject to  yᵢ(w·xᵢ + b) ≥ 1  for all i
    
    constraints_explained: |
      yᵢ(w·xᵢ + b) ≥ 1  means:
      
      If yᵢ = +1 (positive class):
        w·xᵢ + b ≥ +1  (above positive margin)
      
      If yᵢ = -1 (negative class):
        w·xᵢ + b ≤ -1  (below negative margin)
      
      All samples must be on correct side WITH margin
    
    why_half_norm_squared: |
      Original goal: Maximize margin = 2/||w||
      Equivalent: Minimize ||w||
      Convenient: Minimize ½||w||²  (differentiable, convex)
    
    support_vectors: |
      Support vectors: Training samples exactly on margin boundaries
      
      These are samples where yᵢ(w·xᵢ + b) = 1
      
      Key insight: Decision boundary determined ONLY by support vectors
      Other samples (far from boundary) don't affect solution
      
      Example:
      1000 training samples
      Only 10 support vectors
      → Remove other 990 samples, same decision boundary!
    
    limitations: |
      Hard margin SVM requires:
      - Data must be linearly separable (rare in practice!)
      - No outliers or noise
      - No overlapping classes
      
      If data not linearly separable → no solution exists
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Soft Margin SVM
  # --------------------------------------------------------------------------
  
  soft_margin_svm:
    
    motivation: |
      Real data often:
      - Not perfectly linearly separable
      - Contains outliers
      - Has noise
      
      Hard margin SVM fails (no feasible solution)
      
      Solution: Allow some violations (soft margin)
    
    slack_variables: |
      Introduce slack variables ξᵢ ≥ 0 for each sample
      
      ξᵢ = 0:    Sample correctly classified with margin
      0 < ξᵢ < 1: Sample correctly classified but inside margin
      ξᵢ ≥ 1:    Sample misclassified
      
      Modified constraint: yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ
    
    optimization_problem: |
      minimize    ½||w||² + C Σᵢ ξᵢ
      subject to  yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ  for all i
                  ξᵢ ≥ 0  for all i
      
      Trade-off:
      - ½||w||²: Maximize margin (regularization)
      - C Σᵢ ξᵢ: Minimize violations (fit training data)
      
      C is regularization parameter (hyperparameter)
    
    c_parameter: |
      C controls trade-off between margin and violations
      
      Small C (e.g., C=0.1):
      - Large margin tolerated (more regularization)
      - More violations allowed
      - May underfit
      
      Large C (e.g., C=100):
      - Small margin (less regularization)
      - Few violations allowed
      - May overfit
      
      Typical values: C ∈ [0.1, 1, 10, 100]
      Tune on validation set
    
    hinge_loss_interpretation: |
      Soft margin SVM equivalent to minimizing hinge loss:
      
      Hinge loss: max(0, 1 - yᵢ(w·xᵢ + b))
      
      SVM objective = Hinge loss + L2 regularization:
      
      J(w, b) = C Σᵢ max(0, 1 - yᵢ(w·xᵢ + b)) + ½||w||²
      
      This connects SVM to other ML algorithms!
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Kernel Trick
  # --------------------------------------------------------------------------
  
  kernel_trick:
    
    motivation: |
      Linear SVM limited to linear decision boundaries
      
      Many real problems non-linearly separable:
      - Concentric circles (malware vs benign in feature space)
      - XOR pattern
      - Complex attack patterns
      
      Need: Non-linear decision boundaries
    
    feature_mapping: |
      Idea: Transform data to higher-dimensional space where it's linearly separable
      
      Original space (2D):
      x = [x₁, x₂]
      
      Transformed space (3D):
      φ(x) = [x₁, x₂, x₁² + x₂²]
      
      Example: Circle boundary in 2D → Linear boundary in 3D!
      
      Problem: Explicit transformation expensive (high dimensions)
    
    kernel_trick_magic: |
      Key insight: SVM only needs dot products φ(xᵢ)·φ(xⱼ)
      
      Kernel function: K(xᵢ, xⱼ) = φ(xᵢ)·φ(xⱼ)
      
      Magic: Compute K(xᵢ, xⱼ) directly WITHOUT computing φ(x)!
      
      Example:
      Polynomial kernel: K(x, y) = (x·y + 1)²
      
      Implicitly maps to very high-dimensional space
      But only computes simple dot product and square!
    
    common_kernels:
      
      linear_kernel:
        formula: "K(x, y) = x·y"
        
        use_case: "Data already linearly separable"
        
        interpretation: "No transformation (standard linear SVM)"
      
      polynomial_kernel:
        formula: "K(x, y) = (x·y + c)ᵈ"
        
        hyperparameters:
          d: "Degree (typically 2-5)"
          c: "Constant (typically 0 or 1)"
        
        use_case: "Decision boundary is polynomial"
        
        example: |
          d=2: Quadratic boundary (circles, ellipses)
          d=3: Cubic boundary
      
      rbf_kernel:
        formula: "K(x, y) = exp(-γ||x - y||²)"
        
        names: ["RBF", "Gaussian kernel", "Radial basis function"]
        
        hyperparameter:
          gamma: "Controls width of Gaussian (1/(2σ²))"
        
        properties:
          - "Most popular kernel"
          - "Can approximate any decision boundary (universal)"
          - "Creates 'bubbles' around support vectors"
        
        gamma_effect: |
          Small γ (e.g., 0.01): Wide Gaussian, smooth boundary
          Large γ (e.g., 10): Narrow Gaussian, complex boundary (overfits)
        
        use_case: "Default choice for non-linear problems"
      
      sigmoid_kernel:
        formula: "K(x, y) = tanh(αx·y + c)"
        
        use_case: "Mimics neural network (historical interest)"
    
    choosing_kernel: |
      Decision tree:
      
      1. Try linear kernel first (fastest, simplest)
         If accuracy good enough → Done!
      
      2. If linear insufficient, try RBF kernel
         Tune γ on validation set
      
      3. If domain knowledge suggests polynomial → try polynomial
      
      4. Default for non-linear: RBF with γ ∈ [0.001, 0.01, 0.1, 1, 10]
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Dual Formulation
  # --------------------------------------------------------------------------
  
  dual_formulation:
    
    primal_vs_dual: |
      Primal problem: Optimize w, b (one variable per feature)
      Dual problem: Optimize α (one variable per training sample)
      
      Primal: n_features variables
      Dual: n_samples variables
      
      When n_features >> n_samples: Solve dual
      When n_samples >> n_features: Solve primal
    
    dual_optimization: |
      Dual problem:
      
      maximize  Σᵢ αᵢ - ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼK(xᵢ, xⱼ)
      subject to  0 ≤ αᵢ ≤ C  for all i
                  Σᵢ αᵢyᵢ = 0
      
      Where:
      - αᵢ: Dual variable for sample i (Lagrange multiplier)
      - K(xᵢ, xⱼ): Kernel function
    
    support_vectors_from_dual: |
      After solving for α:
      
      αᵢ = 0:     Sample far from boundary (not support vector)
      0 < αᵢ < C: Sample on margin (support vector)
      αᵢ = C:     Sample violating margin (support vector, possibly misclassified)
      
      Decision function:
      f(x) = Σᵢ αᵢyᵢK(xᵢ, x) + b
      
      Only sum over support vectors (αᵢ > 0)!
    
    why_dual_matters: |
      1. Kernel trick only works in dual form
      2. Reveals support vectors explicitly
      3. Efficient when n_features >> n_samples
      4. Deep connection to optimization theory
  
  # --------------------------------------------------------------------------
  # Practical Implementation
  # --------------------------------------------------------------------------
  
  practical_implementation: |
    import numpy as np
    
    class SVM:
        """
        Simplified SVM using gradient descent on primal (linear kernel only)
        
        Note: Production SVMs use dual formulation with QP solvers
        This implementation for educational purposes
        """
        
        def __init__(self, C=1.0, learning_rate=0.001, n_iterations=1000):
            """
            Args:
                C: Regularization parameter
                learning_rate: Step size for gradient descent
                n_iterations: Number of training iterations
            """
            self.C = C
            self.learning_rate = learning_rate
            self.n_iterations = n_iterations
            self.w = None
            self.b = None
        
        def fit(self, X, y):
            """
            Train SVM using subgradient descent on hinge loss
            
            Objective: C Σ max(0, 1 - yᵢ(w·xᵢ + b)) + ½||w||²
            """
            n_samples, n_features = X.shape
            
            # Convert labels to {-1, +1}
            y_ = np.where(y <= 0, -1, 1)
            
            # Initialize
            self.w = np.zeros(n_features)
            self.b = 0
            
            # Gradient descent
            for iteration in range(self.n_iterations):
                for i, x_i in enumerate(X):
                    # Compute hinge loss condition
                    condition = y_[i] * (np.dot(x_i, self.w) + self.b) >= 1
                    
                    if condition:
                        # No hinge loss, only regularization gradient
                        self.w -= self.learning_rate * self.w
                    else:
                        # Hinge loss + regularization gradient
                        self.w -= self.learning_rate * (self.w - self.C * y_[i] * x_i)
                        self.b -= self.learning_rate * (-self.C * y_[i])
            
            return self
        
        def predict(self, X):
            """Predict class labels"""
            linear_output = np.dot(X, self.w) + self.b
            return np.sign(linear_output)
        
        def decision_function(self, X):
            """Compute decision function values"""
            return np.dot(X, self.w) + self.b
        
        def score(self, X, y):
            """Compute accuracy"""
            y_ = np.where(y <= 0, -1, 1)
            predictions = self.predict(X)
            return np.mean(predictions == y_)
    
    # ========================================================================
    # USAGE EXAMPLE
    # ========================================================================
    
    # Generate linearly separable data
    np.random.seed(42)
    
    # Class 1: Normal traffic (low packet size, low connection rate)
    X_normal = np.random.randn(100, 2) * 0.5 + np.array([1, 1])
    y_normal = np.zeros(100)
    
    # Class 2: Attack traffic (high packet size, high connection rate)
    X_attack = np.random.randn(100, 2) * 0.5 + np.array([4, 4])
    y_attack = np.ones(100)
    
    # Combine
    X = np.vstack([X_normal, X_attack])
    y = np.hstack([y_normal, y_attack])
    
    # Shuffle
    indices = np.random.permutation(len(y))
    X, y = X[indices], y[indices]
    
    # Train/test split
    split = int(0.8 * len(y))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    
    # Train SVM
    svm = SVM(C=1.0, learning_rate=0.001, n_iterations=1000)
    svm.fit(X_train, y_train)
    
    # Evaluate
    train_acc = svm.score(X_train, y_train)
    test_acc = svm.score(X_test, y_test)
    
    print(f"SVM (C={svm.C})")
    print(f"Training accuracy: {train_acc:.2%}")
    print(f"Test accuracy: {test_acc:.2%}")
    print(f"Weights: {svm.w}")
    print(f"Bias: {svm.b:.4f}")
    
    # Tune C
    print("\nTuning C:")
    for C in [0.1, 1.0, 10.0, 100.0]:
        svm = SVM(C=C, learning_rate=0.001, n_iterations=1000)
        svm.fit(X_train, y_train)
        acc = svm.score(X_test, y_test)
        print(f"C={C}: {acc:.2%}")
    
    # Note on kernel SVM:
    # For kernel SVM (RBF, polynomial), use library like scikit-learn
    # which implements dual formulation with QP solver
    """
    from sklearn.svm import SVC
    
    # RBF kernel SVM
    svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')
    svm_rbf.fit(X_train, y_train)
    
    # Number of support vectors
    print(f"Support vectors: {len(svm_rbf.support_vectors_)}")
    """
  
  # --------------------------------------------------------------------------
  # Security Applications
  # --------------------------------------------------------------------------
  
  security_applications:
    
    intrusion_detection:
      
      approach: |
        Features: Packet size, protocol, ports, timing, payload statistics
        Classes: Normal vs Attack
        
        Use RBF kernel SVM:
        - Captures non-linear attack patterns
        - Maximum margin = robust to variations
      
      benefit: |
        Large margin makes detector robust:
        - Attack must deviate significantly to evade
        - Small perturbations don't cross boundary
    
    malware_classification:
      
      features: |
        - API call sequences (behavioral)
        - Byte n-grams (static)
        - PE header features
        - Control flow graphs
      
      kernel_choice: |
        Linear: Fast, interpretable (weight per feature)
        RBF: Better accuracy for complex patterns
      
      support_vectors: |
        Only 5% of malware samples are support vectors
        → Only these samples define boundary
        → Can explain detection by showing nearest support vectors
    
    spam_detection:
      
      features: "TF-IDF vectors of email text"
      
      kernel: "Linear (high-dimensional text data)"
      
      advantage: |
        Spam/ham boundary complex (many spam tactics)
        SVM finds maximum margin boundary
        Generalizes to new spam variations
    
    adversarial_robustness:
      
      maximum_margin_defense: |
        SVM naturally more robust than other classifiers
        
        Reason: Maximum margin = largest "safe zone"
        Adversary must perturb more to cross boundary
      
      comparison: |
        Logistic regression: Finds any separating boundary
        SVM: Finds boundary FARTHEST from data
        
        Under adversarial attack:
        - Logistic regression: Easier to evade
        - SVM: Harder to evade (larger margin)
      
      limitation: |
        Large margin helps but doesn't guarantee robustness
        Adversarial training still needed for strong defenses
  
  # --------------------------------------------------------------------------
  # Advantages and Limitations
  # --------------------------------------------------------------------------
  
  advantages_limitations:
    
    advantages:
      maximum_margin:
        - "Better generalization (theoretical guarantees)"
        - "Robust to overfitting (regularization built-in)"
        - "Adversarially more robust"
      
      kernel_trick:
        - "Handles non-linear decision boundaries"
        - "Efficient (no explicit feature mapping)"
        - "Flexible (many kernel choices)"
      
      sparse_solution:
        - "Only support vectors matter"
        - "Efficient prediction (few dot products)"
        - "Interpretable (show support vectors)"
      
      effective_high_dimensions:
        - "Works well when n_features >> n_samples"
        - "Regularization prevents overfitting"
    
    limitations:
      
      kernel_hyperparameters:
        problem: "Must tune C and kernel parameters (γ for RBF)"
        
        impact: "Performance sensitive to hyperparameter choices"
        
        solution: "Grid search or random search on validation set"
      
      computational_cost:
        training: |
          O(n²) to O(n³) depending on solver
          Slow for large datasets (n > 100K)
        
        prediction: |
          O(n_support_vectors × n_features)
          Fast if few support vectors
        
        solution: "Use approximate methods or linear SVM for large scale"
      
      no_probability_estimates:
        issue: "SVM gives decision values, not probabilities"
        
        workaround: |
          Platt scaling: Fit logistic regression to decision values
          Provides calibrated probabilities
        
        alternative: "Use logistic regression if probabilities needed"
      
      binary_classification:
        native: "SVM is binary classifier"
        
        multi_class: |
          One-vs-rest: Train k binary SVMs
          One-vs-one: Train k(k-1)/2 SVMs
        
        less_natural: "Compared to algorithms with native multi-class support"
      
      interpretability_with_kernels:
        linear_svm: "Weights show feature importance (interpretable)"
        
        kernel_svm: "Non-linear transformation (less interpretable)"
        
        compromise: "Can show support vectors as evidence"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "SVM finds decision boundary with maximum margin"
      - "Hard margin: All samples correctly classified with margin"
      - "Soft margin: Allow violations with slack variables (C parameter)"
      - "Kernel trick: Non-linear boundaries without explicit transformation"
      - "Support vectors: Samples on margin (define decision boundary)"
      - "RBF kernel: Most popular for non-linear problems"
    
    practical_skills:
      - "Implement linear SVM from scratch using hinge loss"
      - "Understand primal vs dual formulation"
      - "Choose kernel: Linear first, then RBF if needed"
      - "Tune C (regularization) and γ (RBF width) on validation set"
      - "Identify support vectors (key samples)"
    
    security_mindset:
      - "Maximum margin = more robust to adversarial perturbations"
      - "Few support vectors = efficient, interpretable detection"
      - "RBF kernel captures complex attack patterns"
      - "SVM baseline for binary security classification"
      - "Large margin makes evasion harder for adversary"
    
    remember_this:
      - "SVM maximizes margin (key differentiator)"
      - "Kernel trick = non-linear boundaries efficiently"
      - "Tune C: Small = large margin, Large = fit data closely"
      - "RBF kernel is default for non-linear problems"
      - "Only support vectors define decision boundary"
    
    next_steps:
      - "Next section: Naive Bayes (probabilistic classifier)"
      - "You now understand margin-based classification"
      - "SVMs foundation for kernel methods across ML"

---
