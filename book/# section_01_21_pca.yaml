# section_01_21_pca.yaml

---
document_info:
  chapter: "01"
  section: "21"
  title: "Dimensionality Reduction (PCA)"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 6
  tags: ["pca", "dimensionality-reduction", "eigenvalues", "eigenvectors", "feature-extraction", "variance", "curse-of-dimensionality"]

# ============================================================================
# SECTION 1.21: DIMENSIONALITY REDUCTION (PCA)
# ============================================================================

section_01_21_pca:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Principal Component Analysis (PCA) is the fundamental technique for dimensionality 
    reduction. Many real-world datasets are high-dimensional: network traffic with hundreds 
    of features, malware with thousands of byte n-grams, logs with dozens of fields. High 
    dimensions cause problems: computational cost, overfitting, visualization challenges.
    
    PCA solves this by finding a lower-dimensional representation that preserves most of 
    the information. It identifies directions of maximum variance in the data and projects 
    samples onto these principal components. The magic: 1000 features might contain only 
    10-20 truly informative dimensions, the rest is noise or redundancy.
    
    This section covers:
    - The curse of dimensionality (why reduce dimensions?)
    - PCA algorithm (eigendecomposition of covariance matrix)
    - Principal components and explained variance
    - Choosing number of components
    - Reconstruction and information loss
    - Applications in visualization, compression, and noise reduction
    
    For security, PCA is invaluable for:
    1. Feature reduction (1000 features → 50 principal components)
    2. Anomaly detection (reconstruction error)
    3. Visualization (project to 2D/3D for human analysis)
    4. Noise filtering (keep high-variance components)
  
  why_this_matters: |
    Security applications:
    - Network traffic: Reduce hundreds of flow features to top 20 components
    - Malware analysis: Compress high-dimensional behavioral vectors
    - Log analysis: Visualize high-dimensional security events
    - Anomaly detection: Samples with high reconstruction error are anomalies
    
    Real challenges PCA solves:
    - Curse of dimensionality: k-NN, clustering fail in high dimensions
    - Computational cost: Training slower with more features
    - Overfitting: More features = more prone to overfit
    - Visualization: Can't visualize 1000D space (but can visualize 2D!)
    
    Why learn PCA:
    - Most widely used dimensionality reduction technique
    - Foundation for many advanced methods
    - Elegant mathematical formulation (eigendecomposition)
    - Practical tool for feature engineering
  
  # --------------------------------------------------------------------------
  # Core Concept 1: The Curse of Dimensionality
  # --------------------------------------------------------------------------
  
  curse_of_dimensionality:
    
    intuition: |
      As dimensions increase, data becomes increasingly sparse
      Volume grows exponentially, but data points don't
      
      Result: Every point is far from every other point
      Similarity/distance metrics become meaningless
    
    geometric_intuition: |
      1D: Line with 10 points → avg distance 0.1
      2D: Square with 10 points → avg distance ~0.3
      3D: Cube with 10 points → avg distance ~0.5
      100D: Hypercube with 10 points → all points very far apart!
      
      Distance to nearest neighbor ≈ Distance to farthest neighbor
      → k-NN, clustering break down
    
    sample_complexity: |
      To maintain same density:
      - 1D: Need 10 samples
      - 2D: Need 100 samples (10²)
      - 3D: Need 1,000 samples (10³)
      - d dimensions: Need 10^d samples
      
      Exponential growth in required data!
    
    problems_caused:
      overfitting:
        - "More features than samples → model memorizes"
        - "Example: 1000 features, 100 samples → severe overfitting"
      
      computational_cost:
        - "Distance computation: O(d) per pair"
        - "Matrix operations: O(d²) or O(d³)"
        - "1000 dimensions → 10x slower than 100 dimensions"
      
      meaningless_distances:
        - "All distances become similar in high dimensions"
        - "k-NN can't distinguish nearest vs farthest"
      
      visualization_impossible:
        - "Humans can't visualize >3 dimensions"
        - "Security analysts need visual exploration"
    
    solution_dimensionality_reduction: |
      Reduce dimensions while preserving information
      
      Benefits:
      - Faster computation
      - Better generalization (less overfitting)
      - Visualization (project to 2D/3D)
      - Remove noise and redundancy
  
  # --------------------------------------------------------------------------
  # Core Concept 2: PCA Intuition
  # --------------------------------------------------------------------------
  
  pca_intuition:
    
    goal: "Find low-dimensional representation preserving maximum variance"
    
    variance_as_information: |
      High variance dimension: Informative (samples spread out)
      Low variance dimension: Uninformative (samples clustered)
      
      Example:
      Feature 1: Height in cm (variance = 100) → Informative
      Feature 2: Height in inches (variance = 15.6) → Redundant!
      Feature 3: Constant = 5 (variance = 0) → Useless
      
      Keep high-variance directions, discard low-variance
    
    principal_components: |
      Principal Components: Directions of maximum variance
      
      PC1: Direction with highest variance
      PC2: Direction with 2nd highest variance (orthogonal to PC1)
      PC3: Direction with 3rd highest variance (orthogonal to PC1, PC2)
      ...
      
      Orthogonal: Principal components are perpendicular (uncorrelated)
    
    projection: |
      Original space: d dimensions
      PCA: Find k principal components (k < d)
      Project data onto these k components
      
      Result: Each sample represented by k values instead of d
      
      Example:
      Original: 1000 features
      PCA: 50 principal components
      Compression: 20x reduction (1000 → 50)
    
    two_dimensional_example: |
      2D data (height, weight):
      - Highly correlated (tall people heavier)
      - Data lies approximately on diagonal line
      
      PC1: Diagonal direction (captures most variance)
      PC2: Perpendicular to PC1 (captures remaining variance)
      
      Keep only PC1: 1D representation preserving ~90% variance
  
  # --------------------------------------------------------------------------
  # Core Concept 3: PCA Algorithm
  # --------------------------------------------------------------------------
  
  pca_algorithm:
    
    mathematical_formulation: |
      Given data X (n samples × d features):
      
      Goal: Find k principal components
      
      Steps:
      1. Center data: X_centered = X - mean(X)
      2. Compute covariance matrix: Σ = (1/n) X_centered^T X_centered
      3. Compute eigenvalues and eigenvectors of Σ
      4. Sort eigenvectors by eigenvalues (descending)
      5. Keep top k eigenvectors as principal components
      6. Project data: X_reduced = X_centered @ PC[:, :k]
    
    step_by_step:
      
      step_1_centering:
        why: "PCA operates on centered data (mean = 0)"
        
        computation: "X_centered = X - mean(X, axis=0)"
        
        effect: "Shifts data so origin is at center"
      
      step_2_covariance_matrix:
        formula: "Cov(X) = (1/n) X^T X"
        
        interpretation: |
          Cov[i,j] = covariance between feature i and feature j
          
          Diagonal: Variance of each feature
          Off-diagonal: Covariance between features
        
        properties: "Symmetric, positive semi-definite"
      
      step_3_eigendecomposition:
        formula: "Σ v = λ v"
        
        where:
          v: "Eigenvector (direction)"
          λ: "Eigenvalue (variance in that direction)"
        
        interpretation: |
          Eigenvector: Principal component (direction)
          Eigenvalue: Amount of variance captured
          
          Larger eigenvalue = more important component
      
      step_4_sorting:
        procedure: "Sort eigenvectors by eigenvalues (descending)"
        
        result: |
          PC1: Eigenvector with largest eigenvalue
          PC2: Eigenvector with 2nd largest eigenvalue
          ...
          PCd: Eigenvector with smallest eigenvalue
      
      step_5_projection:
        formula: "Z = X_centered @ W"
        
        where:
          W: "Matrix of top k eigenvectors (d × k)"
          Z: "Projected data (n × k)"
        
        interpretation: |
          Each row of Z: Sample coordinates in PC space
          Each column of Z: Sample values along one PC
    
    numpy_implementation: |
      import numpy as np
      
      def pca(X, n_components):
          """
          Perform PCA dimensionality reduction
          
          Args:
              X: Data matrix (n_samples, n_features)
              n_components: Number of principal components to keep
          
          Returns:
              X_reduced: Projected data (n_samples, n_components)
              components: Principal components (n_components, n_features)
              explained_variance: Variance explained by each component
          """
          # Step 1: Center data
          mean = np.mean(X, axis=0)
          X_centered = X - mean
          
          # Step 2: Compute covariance matrix
          n_samples = X.shape[0]
          cov_matrix = (X_centered.T @ X_centered) / n_samples
          
          # Step 3: Eigendecomposition
          eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
          
          # Step 4: Sort by eigenvalues (descending)
          idx = eigenvalues.argsort()[::-1]
          eigenvalues = eigenvalues[idx]
          eigenvectors = eigenvectors[:, idx]
          
          # Step 5: Keep top k components
          components = eigenvectors[:, :n_components]
          explained_variance = eigenvalues[:n_components]
          
          # Step 6: Project data
          X_reduced = X_centered @ components
          
          return X_reduced, components, explained_variance, mean
      
      # Example usage
      X = np.random.randn(100, 50)  # 100 samples, 50 features
      X_reduced, components, explained_var, mean = pca(X, n_components=10)
      
      print(f"Original shape: {X.shape}")
      print(f"Reduced shape: {X_reduced.shape}")
      print(f"Explained variance: {explained_var}")
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Explained Variance
  # --------------------------------------------------------------------------
  
  explained_variance:
    
    definition: |
      Explained variance: Proportion of total variance captured by component
      
      Total variance = sum of all eigenvalues
      Explained variance ratio = eigenvalue / total_variance
    
    cumulative_explained_variance: |
      Cumulative: Sum of explained variance up to k components
      
      Example:
      PC1: 40% variance
      PC2: 25% variance
      PC3: 15% variance
      PC4: 10% variance
      PC5: 5% variance
      Others: 5% variance
      
      Cumulative:
      PC1: 40%
      PC1+PC2: 65%
      PC1+PC2+PC3: 80%
      PC1+PC2+PC3+PC4: 90%
      PC1-PC5: 95%
    
    choosing_n_components:
      
      method_1_threshold:
        rule: "Keep components until cumulative variance ≥ threshold"
        
        typical: "95% or 99% variance explained"
        
        example: |
          Target: 95% variance
          PC1-PC3: 80% (not enough)
          PC1-PC4: 90% (not enough)
          PC1-PC5: 95% (sufficient!)
          → Keep 5 components
      
      method_2_elbow:
        procedure: "Plot components vs explained variance, look for elbow"
        
        interpretation: |
          Before elbow: Important components (high variance)
          After elbow: Noise components (low variance)
        
        choose: "Components before elbow"
      
      method_3_fixed_number:
        use_case: "Visualization (always use 2 or 3)"
        
        trade_off: "May lose information, but enables visualization"
    
    scree_plot: |
      Scree plot: Visualize explained variance per component
      
      X-axis: Component number
      Y-axis: Explained variance ratio
      
      Helps identify elbow point
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Reconstruction and Information Loss
  # --------------------------------------------------------------------------
  
  reconstruction:
    
    inverse_transform: |
      Can reconstruct original data from reduced representation
      
      Forward: X → X_reduced (compress)
      Inverse: X_reduced → X_reconstructed (decompress)
      
      X_reconstructed ≈ X (not exact due to information loss)
    
    reconstruction_formula: |
      X_reconstructed = X_reduced @ components^T + mean
      
      Steps:
      1. Project back to original space (X_reduced @ components^T)
      2. Add back mean (uncentering)
    
    reconstruction_error: |
      Error = ||X - X_reconstructed||²
      
      Low error: Good reconstruction (preserved information)
      High error: Poor reconstruction (lost information)
      
      With more components: Error decreases
      With all components: Error = 0 (perfect reconstruction)
    
    numpy_implementation: |
      def reconstruct(X_reduced, components, mean):
          """Reconstruct original data from PCA representation"""
          X_reconstructed = X_reduced @ components.T + mean
          return X_reconstructed
      
      # Example
      X_reconstructed = reconstruct(X_reduced, components, mean)
      
      # Compute reconstruction error
      error = np.mean((X - X_reconstructed) ** 2)
      print(f"Reconstruction error: {error:.4f}")
  
  # --------------------------------------------------------------------------
  # Complete Implementation
  # --------------------------------------------------------------------------
  
  complete_implementation: |
    import numpy as np
    
    class PCA:
        """Principal Component Analysis from scratch"""
        
        def __init__(self, n_components=None):
            """
            Args:
                n_components: Number of components to keep
                             If None, keep all components
            """
            self.n_components = n_components
            self.components = None
            self.mean = None
            self.explained_variance = None
            self.explained_variance_ratio = None
        
        def fit(self, X):
            """Compute principal components"""
            n_samples, n_features = X.shape
            
            # Default: keep all components
            if self.n_components is None:
                self.n_components = n_features
            
            # Center data
            self.mean = np.mean(X, axis=0)
            X_centered = X - self.mean
            
            # Covariance matrix
            cov_matrix = (X_centered.T @ X_centered) / n_samples
            
            # Eigendecomposition
            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
            
            # Sort descending
            idx = eigenvalues.argsort()[::-1]
            eigenvalues = eigenvalues[idx]
            eigenvectors = eigenvectors[:, idx]
            
            # Store results
            self.components = eigenvectors[:, :self.n_components].T
            self.explained_variance = eigenvalues[:self.n_components]
            
            # Explained variance ratio
            total_variance = eigenvalues.sum()
            self.explained_variance_ratio = eigenvalues / total_variance
            
            return self
        
        def transform(self, X):
            """Project data onto principal components"""
            X_centered = X - self.mean
            return X_centered @ self.components.T
        
        def fit_transform(self, X):
            """Fit and transform in one step"""
            self.fit(X)
            return self.transform(X)
        
        def inverse_transform(self, X_reduced):
            """Reconstruct original data from reduced representation"""
            return X_reduced @ self.components + self.mean
        
        def explained_variance_cumsum(self):
            """Cumulative explained variance"""
            return np.cumsum(self.explained_variance_ratio)
    
    # ========================================================================
    # USAGE EXAMPLE: NETWORK TRAFFIC DIMENSIONALITY REDUCTION
    # ========================================================================
    
    # Generate synthetic high-dimensional network traffic data
    np.random.seed(42)
    
    # 500 samples, 100 features
    n_samples = 500
    n_features = 100
    
    # Generate data with underlying structure (low intrinsic dimensionality)
    # True structure: 5 important dimensions + 95 noise dimensions
    latent = np.random.randn(n_samples, 5)
    mixing_matrix = np.random.randn(5, n_features)
    X = latent @ mixing_matrix + np.random.randn(n_samples, n_features) * 0.1
    
    # Apply PCA
    pca = PCA(n_components=20)
    X_reduced = pca.fit_transform(X)
    
    print("PCA Dimensionality Reduction")
    print(f"Original shape: {X.shape}")
    print(f"Reduced shape: {X_reduced.shape}")
    print(f"Compression: {n_features}D → {pca.n_components}D")
    
    # Explained variance
    print("\nExplained variance ratio (top 10 components):")
    for i in range(10):
        print(f"  PC{i+1}: {pca.explained_variance_ratio[i]:.2%}")
    
    cumsum = pca.explained_variance_cumsum()
    print(f"\nCumulative variance (20 components): {cumsum[19]:.2%}")
    
    # Find components for 95% variance
    n_for_95 = np.argmax(cumsum >= 0.95) + 1
    print(f"Components needed for 95% variance: {n_for_95}")
    
    # Reconstruction
    X_reconstructed = pca.inverse_transform(X_reduced)
    reconstruction_error = np.mean((X - X_reconstructed) ** 2)
    print(f"\nReconstruction error: {reconstruction_error:.6f}")
    
    # Anomaly detection using reconstruction error
    print("\n--- Anomaly Detection ---")
    reconstruction_errors = np.mean((X - X_reconstructed) ** 2, axis=1)
    threshold = np.percentile(reconstruction_errors, 95)
    anomalies = reconstruction_errors > threshold
    
    print(f"Anomalies detected: {anomalies.sum()} samples")
    print(f"Anomaly threshold: {threshold:.6f}")
  
  # --------------------------------------------------------------------------
  # Security Applications
  # --------------------------------------------------------------------------
  
  security_applications:
    
    anomaly_detection_via_reconstruction:
      
      principle: |
        Normal samples: Low-dimensional structure (low reconstruction error)
        Anomalies: Deviate from structure (high reconstruction error)
      
      algorithm: |
        1. Train PCA on normal data
        2. For new sample:
           - Project to reduced space
           - Reconstruct back to original space
           - Compute reconstruction error
        3. If error > threshold → anomaly
      
      example: |
        Network traffic:
        - Normal: HTTP, SSH, DNS (low-dimensional patterns)
        - Attack: Port scan (deviates from normal patterns)
        
        PCA trained on normal → high reconstruction error for attack
      
      threshold_setting: "95th percentile of normal reconstruction errors"
    
    feature_reduction_for_ml:
      
      problem: |
        Network flow: 200+ features
        Many redundant (e.g., bytes_sent and packets_sent correlated)
        Models slow, prone to overfitting
      
      solution: |
        PCA: 200 features → 30 principal components
        
        Benefits:
        - 6x faster training
        - Better generalization (less overfitting)
        - Remove redundancy and noise
      
      workflow: |
        1. Extract 200 flow features
        2. Apply PCA → 30 components
        3. Train classifier on 30 components
        4. Faster, more accurate model
    
    visualization_for_analysts:
      
      challenge: "Cannot visualize 100-dimensional security data"
      
      solution: |
        PCA to 2D or 3D:
        - Project high-dimensional samples to 2D
        - Plot on scatter plot
        - Color by label (normal/attack) or cluster
      
      benefit: |
        Analysts can:
        - See clusters of similar attacks
        - Identify outliers visually
        - Understand data structure
        - Guide further investigation
      
      example: |
        Malware samples (1000 dimensions)
        PCA to 2D → Scatter plot
        
        Visualization reveals:
        - Clear separation between families
        - Outliers (novel malware)
        - Gradual evolution over time
    
    noise_filtering:
      
      signal_vs_noise: |
        High-variance components: Signal (information)
        Low-variance components: Noise (random fluctuations)
      
      denoising: |
        1. Apply PCA
        2. Keep only top k components (high variance)
        3. Discard remaining components (noise)
        4. Reconstruct
        
        Result: Denoised data (removed noise components)
      
      security_example: |
        Log data with measurement noise
        PCA filtering:
        - Keep 20 principal components (signal)
        - Discard 80 components (noise)
        - Cleaner patterns for detection
  
  # --------------------------------------------------------------------------
  # Advantages and Limitations
  # --------------------------------------------------------------------------
  
  advantages_limitations:
    
    advantages:
      
      dimensionality_reduction:
        - "Compress high-dimensional data (1000D → 50D)"
        - "Speeds up downstream algorithms"
        - "Reduces overfitting"
      
      removes_redundancy:
        - "Correlated features combined into single component"
        - "Example: height_cm and height_inches → single PC"
      
      noise_reduction:
        - "Low-variance components often noise"
        - "Keeping top PCs filters noise"
      
      visualization:
        - "Project to 2D/3D for human analysis"
        - "Essential for exploratory data analysis"
      
      interpretable_components:
        - "Each PC is linear combination of features"
        - "Can analyze which features contribute most"
      
      theoretical_foundation:
        - "Elegant mathematical framework (eigendecomposition)"
        - "Optimal for linear dimensionality reduction"
    
    limitations:
      
      linearity_assumption:
        problem: "PCA only captures linear relationships"
        
        fails_on: "Non-linear manifolds (e.g., Swiss roll)"
        
        alternative: "t-SNE, UMAP for non-linear reduction"
      
      sensitive_to_scaling:
        problem: |
          Features with large scale dominate PCs
          Example: Age (0-100) vs Income ($0-$1M)
          Income dominates due to larger variance
        
        solution: "Always standardize features first!"
      
      interpretability_loss:
        issue: |
          Original features interpretable (e.g., 'packet_size')
          PCs are combinations (e.g., '0.3×packet_size + 0.5×duration + ...')
          Less intuitive than original features
        
        trade_off: "Reduced dimensions vs interpretability"
      
      requires_centering:
        requirement: "Data must be centered (mean = 0)"
        
        impact: "PCA doesn't work on sparse data (e.g., text TF-IDF)"
        
        alternative: "Truncated SVD for sparse data"
      
      all_components_needed_for_reconstruction:
        limitation: |
          Perfect reconstruction requires ALL components
          With k < d components, information lost
        
        acceptable: "If top k components capture 95%+ variance"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "PCA: Find directions of maximum variance (principal components)"
      - "Eigendecomposition: PCs are eigenvectors of covariance matrix"
      - "Projection: Transform data to lower-dimensional PC space"
      - "Explained variance: Proportion of information retained"
      - "Reconstruction: Can recover approximate original data"
      - "Curse of dimensionality: High dimensions cause sparsity, need reduction"
    
    practical_skills:
      - "Implement PCA from scratch using eigendecomposition"
      - "Choose n_components using explained variance (95% threshold)"
      - "Standardize features before PCA (critical!)"
      - "Use reconstruction error for anomaly detection"
      - "Visualize high-dimensional data via 2D/3D projection"
    
    security_mindset:
      - "Feature reduction: 200 features → 30 PCs (faster, less overfitting)"
      - "Anomaly detection: High reconstruction error = anomaly"
      - "Visualization: Project security data to 2D for analyst exploration"
      - "Noise filtering: Keep high-variance PCs, discard noise"
      - "Always standardize features (different scales in security data)"
    
    remember_this:
      - "PCA = eigendecomposition of covariance matrix"
      - "Keep components explaining 95-99% variance"
      - "ALWAYS standardize features first!"
      - "Reconstruction error for anomaly detection"
      - "Linear only (use t-SNE/UMAP for non-linear)"
    
    next_steps:
      - "Next section: Ensemble Methods (Boosting)"
      - "You now understand dimensionality reduction!"
      - "Part 4 (Algorithms) complete - moving to advanced topics"

---
