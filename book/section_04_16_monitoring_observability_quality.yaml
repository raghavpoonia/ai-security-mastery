# section_04_16_monitoring_observability_quality.yaml

---
document_info:
  title: "Monitoring, Observability, and Quality Assurance"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 4
  section: 16
  part: 3
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-28"
  version: "1.0"
  description: |
    Complete guide to monitoring and observability for production LLM systems. Covers
    comprehensive logging (structured logging, log aggregation), metrics collection
    (Prometheus, Grafana), distributed tracing (OpenTelemetry), quality monitoring
    (hallucination detection, factuality checking), drift detection, and SLO/SLA
    management. Implements production-grade observability stack with real-time alerting,
    anomaly detection, and quality assurance. Security analysis covering log injection,
    metric manipulation, monitoring blind spots, and privacy leakage through logs. Essential
    for operating reliable, high-quality LLM systems at scale.
  estimated_pages: 8
  tags:
    - monitoring
    - observability
    - logging
    - metrics
    - tracing
    - quality-assurance
    - prometheus
    - grafana
    - opentelemetry

section_overview:
  title: "Monitoring, Observability, and Quality Assurance"
  number: "4.16"
  
  purpose: |
    Section 4.15 optimized costs through caching and strategic resource management. We now
    have cost-effective production systems. But without comprehensive monitoring, we're
    flying blind: Can't debug failures, detect quality degradation, or optimize performance.
    Production systems require observability—deep visibility into behavior.
    
    LLM monitoring is uniquely challenging: outputs are unstructured text (hard to validate),
    quality is subjective (hallucinations subtle), and failures often silent (plausible-
    sounding nonsense). Traditional metrics (latency, errors) are necessary but insufficient.
    We need quality metrics (factuality, relevance), drift detection, and comprehensive
    tracing.
    
    This section builds complete observability systems: structured logging, metrics
    collection, distributed tracing, quality monitoring, and alerting. Understanding
    observability is critical—you can't operate what you can't see.
  
  learning_objectives:
    conceptual:
      - "Understand observability pillars: logs, metrics, traces"
      - "Grasp LLM-specific quality metrics: hallucination, factuality, relevance"
      - "Comprehend drift detection and model degradation patterns"
      - "Understand SLO/SLA definition and monitoring for LLM systems"
    
    practical:
      - "Implement structured logging with ELK/Loki for log aggregation"
      - "Build metrics dashboards with Prometheus and Grafana"
      - "Add distributed tracing with OpenTelemetry for request flows"
      - "Create quality monitoring with hallucination and consistency checks"
    
    security_focused:
      - "Prevent PII leakage through logs and metrics"
      - "Detect log injection attacks and metric manipulation"
      - "Implement secure audit trails with tamper-proof logging"
      - "Identify monitoring blind spots exploited by attackers"
  
  prerequisites:
    knowledge:
      - "Section 4.15: Caching and cost optimization"
      - "Section 4.14: Horizontal scaling and distributed deployment"
      - "Understanding of monitoring concepts (metrics, logs, traces)"
      - "Familiarity with Prometheus, Grafana basics"
    
    skills:
      - "Working with monitoring tools (Prometheus, Grafana)"
      - "Implementing structured logging"
      - "Understanding of distributed systems debugging"
      - "Statistical analysis for anomaly detection"
  
  key_transitions:
    from_section_4_15: |
      Section 4.15 optimized costs through caching and strategic resource management.
      We have efficient, cost-effective systems. But without visibility, we can't debug
      issues, detect quality problems, or verify optimizations are working.
      
      Section 4.16 adds comprehensive observability: logging every request, tracking all
      metrics, tracing distributed flows, and monitoring quality. This enables operating
      complex cached distributed LLM systems reliably at scale.
    
    to_next_section: |
      Section 4.16 covers monitoring and quality assurance. Section 4.17 advances to API
      security and compliance: authentication, authorization, secrets management, audit
      logging, and regulatory compliance (GDPR, SOC2). Together they enable secure,
      compliant production operations.

topics:
  - topic_number: 1
    title: "Comprehensive Logging, Metrics, and Distributed Tracing"
    
    overview: |
      The three pillars of observability are logs (detailed events), metrics (aggregated
      numbers), and traces (request flows). Each provides different insights: logs for
      debugging specific requests, metrics for system health trends, traces for understanding
      distributed interactions.
      
      For LLMs, we log prompts and responses (carefully—PII concerns), track latency and
      token counts, and trace requests through caching, load balancing, and inference.
      Structured logging enables querying, metrics enable dashboards, traces enable
      understanding complex flows.
      
      We implement complete observability stacks: structured logging with log aggregation,
      Prometheus metrics with Grafana dashboards, and OpenTelemetry tracing. Understanding
      these systems enables effective debugging and optimization.
    
    content:
      structured_logging:
        logging_architecture: |
          Structured logging architecture:
          
          **Why structured**: JSON logs are queryable, parseable, aggregatable
```python
          # Bad: Unstructured
          logger.info(f"User {user_id} requested generation, took {latency}s")
          
          # Good: Structured
          logger.info("generation_completed", extra={
              "user_id": user_id,
              "latency_seconds": latency,
              "prompt_tokens": prompt_tokens,
              "completion_tokens": completion_tokens,
              "cached": cached,
              "model": model_name
          })
```
          
          **Benefits**:
          - Query logs: "Show all requests > 5s latency"
          - Aggregate: "Average tokens per user"
          - Alert: "Error rate > 1%"
        
        logging_implementation: |
          Production logging implementation:
```python
          import logging
          import json
          from datetime import datetime
          from typing import Dict, Any
          
          class StructuredLogger:
              """Structured JSON logger."""
              
              def __init__(self, name: str):
                  self.logger = logging.getLogger(name)
                  self.logger.setLevel(logging.INFO)
                  
                  # JSON formatter
                  handler = logging.StreamHandler()
                  handler.setFormatter(JSONFormatter())
                  self.logger.addHandler(handler)
              
              def log_request(self,
                            event: str,
                            user_id: str,
                            request_id: str,
                            **kwargs):
                  """Log request with structured data."""
                  log_data = {
                      "timestamp": datetime.utcnow().isoformat(),
                      "event": event,
                      "user_id": user_id,
                      "request_id": request_id,
                      **kwargs
                  }
                  
                  self.logger.info(json.dumps(log_data))
          
          
          class JSONFormatter(logging.Formatter):
              """Format logs as JSON."""
              
              def format(self, record):
                  log_data = {
                      "timestamp": datetime.utcnow().isoformat(),
                      "level": record.levelname,
                      "logger": record.name,
                      "message": record.getMessage()
                  }
                  
                  # Add extra fields
                  if hasattr(record, 'user_id'):
                      log_data["user_id"] = record.user_id
                  
                  return json.dumps(log_data)
```
          
          **Usage**:
```python
          logger = StructuredLogger("llm-service")
          
          logger.log_request(
              event="generation_started",
              user_id="user123",
              request_id="req_abc",
              prompt_length=500,
              model="llama-7b"
          )
```
        
        log_aggregation: |
          Log aggregation and querying:
          
          **ELK Stack** (Elasticsearch, Logstash, Kibana):
```
          Application → Logstash → Elasticsearch → Kibana
          
          Query examples:
          - Show errors last hour
          - Average latency by model
          - Top users by request count
```
          
          **Loki** (Grafana Loki):
```
          Application → Loki → Grafana
          
          LogQL queries:
          {service="llm"} |= "error"
          sum(rate({service="llm"}[5m])) by (model)
```
          
          **Cloud providers**:
          - AWS CloudWatch Logs
          - GCP Cloud Logging
          - Azure Monitor
        
        pii_redaction: |
          PII redaction in logs:
          
          **Critical**: Never log sensitive user data
```python
          import re
          
          class PIIRedactor:
              """Redact PII from logs."""
              
              def __init__(self):
                  # Patterns for common PII
                  self.patterns = {
                      'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
                      'phone': re.compile(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'),
                      'ssn': re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
                      'credit_card': re.compile(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b')
                  }
              
              def redact(self, text: str) -> str:
                  """Redact PII from text."""
                  redacted = text
                  
                  for pii_type, pattern in self.patterns.items():
                      redacted = pattern.sub(f"[{pii_type.upper()}_REDACTED]", redacted)
                  
                  return redacted
              
              def redact_prompt(self, prompt: str) -> str:
                  """Redact PII from prompt before logging."""
                  # Truncate long prompts
                  if len(prompt) > 500:
                      prompt = prompt[:500] + "..."
                  
                  return self.redact(prompt)
```
          
          **Best practice**: Log prompt hash, not full prompt
```python
          import hashlib
          
          prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]
          logger.info("request", prompt_hash=prompt_hash)  # Safe
```
      
      metrics_collection:
        prometheus_metrics: |
          Prometheus metrics for LLM serving:
          
          **Key metrics**:
```python
          from prometheus_client import Counter, Histogram, Gauge, Summary
          
          # Request metrics
          requests_total = Counter(
              'llm_requests_total',
              'Total requests',
              ['model', 'cached', 'status']
          )
          
          request_duration = Histogram(
              'llm_request_duration_seconds',
              'Request duration',
              ['model', 'cached'],
              buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
          )
          
          # Token metrics
          tokens_processed = Counter(
              'llm_tokens_processed_total',
              'Tokens processed',
              ['type', 'model']  # type: prompt or completion
          )
          
          # Quality metrics
          hallucination_rate = Gauge(
              'llm_hallucination_rate',
              'Hallucination rate (0-1)',
              ['model']
          )
          
          # Cache metrics
          cache_hit_rate = Gauge(
              'llm_cache_hit_rate',
              'Cache hit rate',
              ['cache_type']  # semantic, exact
          )
          
          # Cost metrics
          inference_cost = Counter(
              'llm_inference_cost_dollars',
              'Inference cost in dollars',
              ['model']
          )
          
          # Queue metrics
          queue_depth = Gauge(
              'llm_queue_depth',
              'Current queue depth'
          )
```
        
        custom_metrics: |
          Custom metrics for LLM-specific monitoring:
```python
          class LLMMetrics:
              """Custom metrics for LLM monitoring."""
              
              def __init__(self):
                  # Quality metrics
                  self.response_length = Histogram(
                      'llm_response_length_tokens',
                      'Response length in tokens',
                      buckets=[10, 50, 100, 200, 500, 1000, 2000]
                  )
                  
                  self.prompt_similarity = Histogram(
                      'llm_prompt_similarity',
                      'Similarity to cached prompts',
                      buckets=[0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1.0]
                  )
                  
                  # Performance metrics
                  self.tokens_per_second = Histogram(
                      'llm_tokens_per_second',
                      'Generation speed',
                      buckets=[1, 5, 10, 20, 50, 100]
                  )
                  
                  self.batch_size = Histogram(
                      'llm_batch_size',
                      'Actual batch size used',
                      buckets=[1, 2, 4, 8, 16, 32]
                  )
              
              def record_request(self,
                               model: str,
                               prompt_tokens: int,
                               completion_tokens: int,
                               duration: float,
                               cached: bool):
                  """Record request metrics."""
                  # Record tokens
                  tokens_processed.labels(type='prompt', model=model).inc(prompt_tokens)
                  tokens_processed.labels(type='completion', model=model).inc(completion_tokens)
                  
                  # Record duration
                  request_duration.labels(model=model, cached=cached).observe(duration)
                  
                  # Record response length
                  self.response_length.observe(completion_tokens)
                  
                  # Calculate tokens/sec
                  tokens_per_sec = completion_tokens / duration if duration > 0 else 0
                  self.tokens_per_second.observe(tokens_per_sec)
```
        
        grafana_dashboards: |
          Grafana dashboard configuration:
          
          **Dashboard panels**:
          
          1. **Request rate**:
```promql
          rate(llm_requests_total[5m])
```
          
          2. **Latency percentiles**:
```promql
          histogram_quantile(0.50, rate(llm_request_duration_seconds_bucket[5m]))
          histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m]))
          histogram_quantile(0.99, rate(llm_request_duration_seconds_bucket[5m]))
```
          
          3. **Cache hit rate**:
```promql
          llm_cache_hit_rate
```
          
          4. **Error rate**:
```promql
          rate(llm_requests_total{status="error"}[5m]) / 
          rate(llm_requests_total[5m])
```
          
          5. **Cost per hour**:
```promql
          rate(llm_inference_cost_dollars[1h]) * 3600
```
          
          6. **Tokens per second**:
```promql
          rate(llm_tokens_processed_total[5m])
```
      
      distributed_tracing:
        opentelemetry_integration: |
          OpenTelemetry distributed tracing:
          
          **Setup**:
```python
          from opentelemetry import trace
          from opentelemetry.sdk.trace import TracerProvider
          from opentelemetry.sdk.trace.export import BatchSpanProcessor
          from opentelemetry.exporter.jaeger import JaegerExporter
          
          # Configure tracer
          trace.set_tracer_provider(TracerProvider())
          tracer = trace.get_tracer(__name__)
          
          # Export to Jaeger
          jaeger_exporter = JaegerExporter(
              agent_host_name="localhost",
              agent_port=6831,
          )
          
          trace.get_tracer_provider().add_span_processor(
              BatchSpanProcessor(jaeger_exporter)
          )
```
          
          **Usage**:
```python
          async def handle_request(request):
              with tracer.start_as_current_span("handle_request") as span:
                  # Add attributes
                  span.set_attribute("user_id", request.user_id)
                  span.set_attribute("model", request.model)
                  
                  # Check cache
                  with tracer.start_as_current_span("check_cache"):
                      cached = await cache.get(request.prompt)
                  
                  if cached:
                      span.set_attribute("cached", True)
                      return cached
                  
                  # Generate
                  with tracer.start_as_current_span("model_inference") as inference_span:
                      inference_span.set_attribute("prompt_tokens", len_tokens(request.prompt))
                      result = await model.generate(request.prompt)
                      inference_span.set_attribute("completion_tokens", len_tokens(result))
                  
                  # Cache result
                  with tracer.start_as_current_span("store_cache"):
                      await cache.set(request.prompt, result)
                  
                  return result
```
        
        trace_visualization: |
          Trace visualization and analysis:
          
          **Trace example**:
```
          handle_request (2.5s)
          ├── check_cache (0.002s)
          ├── model_inference (2.3s)
          │   ├── tokenization (0.01s)
          │   ├── forward_pass (2.2s)
          │   │   ├── layer_0 (0.07s)
          │   │   ├── layer_1 (0.07s)
          │   │   └── ...
          │   └── detokenization (0.02s)
          └── store_cache (0.005s)
```
          
          **Insights from traces**:
          - Where time is spent (forward_pass dominates)
          - Cache effectiveness (hit/miss patterns)
          - Distributed system flows
          - Performance bottlenecks
        
        trace_sampling: |
          Trace sampling strategies:
          
          **Challenge**: Tracing all requests is expensive
          
          **Sampling strategies**:
          
          1. **Probabilistic**: Sample X% of requests
```python
          sampler = TraceIdRatioBased(0.01)  # 1% sampling
```
          
          2. **Tail-based**: Sample slow/error requests
```python
          if duration > 5.0 or status == "error":
              force_sample = True
```
          
          3. **Debug mode**: Sample specific users
```python
          if user_id in debug_users:
              force_sample = True
```
          
          **Production**: 1% sampling + tail-based for errors
      
      alerting:
        alert_rules: |
          Prometheus alert rules:
```yaml
          groups:
          - name: llm_alerts
            interval: 30s
            rules:
            
            # High latency
            - alert: HighLatency
              expr: histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m])) > 5
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High P95 latency ({{ $value }}s)"
            
            # High error rate
            - alert: HighErrorRate
              expr: rate(llm_requests_total{status="error"}[5m]) / rate(llm_requests_total[5m]) > 0.01
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "Error rate {{ $value | humanizePercentage }}"
            
            # Low cache hit rate
            - alert: LowCacheHitRate
              expr: llm_cache_hit_rate < 0.2
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: "Cache hit rate dropped to {{ $value | humanizePercentage }}"
            
            # High cost
            - alert: HighCost
              expr: rate(llm_inference_cost_dollars[1h]) * 3600 > 100
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "Hourly cost ${{ $value }}"
            
            # Queue buildup
            - alert: QueueBacklog
              expr: llm_queue_depth > 100
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "Queue depth {{ $value }}, may need scaling"
```
        
        alert_routing: |
          Alert routing and escalation:
```yaml
          # Alertmanager config
          route:
            receiver: 'team-slack'
            group_by: ['alertname', 'severity']
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 12h
            
            routes:
            # Critical alerts to PagerDuty
            - match:
                severity: critical
              receiver: 'pagerduty'
              continue: true
            
            # Warning alerts to Slack
            - match:
                severity: warning
              receiver: 'team-slack'
          
          receivers:
          - name: 'team-slack'
            slack_configs:
            - api_url: 'https://hooks.slack.com/...'
              channel: '#llm-alerts'
              title: '{{ .GroupLabels.alertname }}'
              text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'
          
          - name: 'pagerduty'
            pagerduty_configs:
            - service_key: 'xxx'
```
    
    implementation:
      observability_stack:
        language: python
        code: |
          """
          Complete observability stack implementation.
          Demonstrates structured logging, metrics, and tracing.
          """
          
          import time
          import logging
          import json
          from typing import Dict, Any, Optional
          from datetime import datetime
          from contextvars import ContextVar
          
          from prometheus_client import Counter, Histogram, Gauge
          
          # Context variable for request ID (propagates through async calls)
          request_id_var: ContextVar[str] = ContextVar('request_id', default='')
          
          
          class StructuredLogger:
              """
              Structured logger with PII redaction.
              
              Logs structured JSON with automatic field extraction.
              """
              
              def __init__(self, name: str):
                  """Initialize logger."""
                  self.logger = logging.getLogger(name)
                  self.logger.setLevel(logging.INFO)
                  
                  # JSON handler
                  handler = logging.StreamHandler()
                  formatter = logging.Formatter('%(message)s')
                  handler.setFormatter(formatter)
                  self.logger.addHandler(handler)
              
              def log(self, level: str, event: str, **kwargs):
                  """
                  Log structured event.
                  
                  Args:
                      level: Log level (info, warning, error)
                      event: Event name
                      **kwargs: Additional fields
                  """
                  log_entry = {
                      "timestamp": datetime.utcnow().isoformat(),
                      "level": level,
                      "event": event,
                      "request_id": request_id_var.get(),
                      **kwargs
                  }
                  
                  # Redact sensitive fields
                  if 'prompt' in log_entry:
                      log_entry['prompt'] = self._truncate(log_entry['prompt'], 100)
                  
                  log_line = json.dumps(log_entry)
                  
                  if level == 'error':
                      self.logger.error(log_line)
                  elif level == 'warning':
                      self.logger.warning(log_line)
                  else:
                      self.logger.info(log_line)
              
              def _truncate(self, text: str, max_length: int) -> str:
                  """Truncate text for logging."""
                  if len(text) <= max_length:
                      return text
                  return text[:max_length] + "..."
          
          
          class MetricsCollector:
              """
              Metrics collector for LLM serving.
              
              Tracks requests, latency, tokens, quality, and costs.
              """
              
              def __init__(self):
                  """Initialize metrics."""
                  # Request metrics
                  self.requests_total = Counter(
                      'llm_requests_total',
                      'Total requests',
                      ['model', 'cached', 'status']
                  )
                  
                  self.request_duration = Histogram(
                      'llm_request_duration_seconds',
                      'Request duration',
                      ['model', 'cached'],
                      buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
                  )
                  
                  # Token metrics
                  self.tokens_processed = Counter(
                      'llm_tokens_total',
                      'Tokens processed',
                      ['type', 'model']
                  )
                  
                  # Cache metrics
                  self.cache_hit_rate = Gauge(
                      'llm_cache_hit_rate',
                      'Cache hit rate'
                  )
                  
                  # Quality metrics
                  self.response_length = Histogram(
                      'llm_response_length',
                      'Response length in tokens',
                      buckets=[10, 50, 100, 200, 500, 1000]
                  )
                  
                  # Cost metrics
                  self.inference_cost = Counter(
                      'llm_cost_dollars',
                      'Inference cost',
                      ['model']
                  )
              
              def record_request(self,
                               model: str,
                               prompt_tokens: int,
                               completion_tokens: int,
                               duration: float,
                               cached: bool,
                               status: str = "success"):
                  """Record request metrics."""
                  # Request count
                  self.requests_total.labels(
                      model=model,
                      cached=cached,
                      status=status
                  ).inc()
                  
                  # Duration
                  self.request_duration.labels(
                      model=model,
                      cached=cached
                  ).observe(duration)
                  
                  # Tokens
                  self.tokens_processed.labels(type='prompt', model=model).inc(prompt_tokens)
                  self.tokens_processed.labels(type='completion', model=model).inc(completion_tokens)
                  
                  # Response length
                  self.response_length.observe(completion_tokens)
                  
                  # Cost (if not cached)
                  if not cached:
                      cost = self._calculate_cost(model, prompt_tokens, completion_tokens)
                      self.inference_cost.labels(model=model).inc(cost)
              
              def _calculate_cost(self, model: str, prompt_tokens: int, completion_tokens: int) -> float:
                  """Calculate request cost."""
                  pricing = {
                      'llama-7b': {'prompt': 0.5, 'completion': 0.5},
                      'llama-70b': {'prompt': 3.0, 'completion': 3.0}
                  }
                  
                  model_pricing = pricing.get(model, {'prompt': 1.0, 'completion': 1.0})
                  
                  cost = (
                      (prompt_tokens / 1_000_000) * model_pricing['prompt'] +
                      (completion_tokens / 1_000_000) * model_pricing['completion']
                  )
                  
                  return cost
          
          
          class SimpleTracer:
              """
              Simple distributed tracer.
              
              In production, use OpenTelemetry.
              """
              
              def __init__(self):
                  """Initialize tracer."""
                  self.spans = []
              
              def start_span(self, name: str) -> 'Span':
                  """Start a new span."""
                  span = Span(name, parent_id=request_id_var.get())
                  return span
          
          
          class Span:
              """Trace span."""
              
              def __init__(self, name: str, parent_id: str = None):
                  self.name = name
                  self.parent_id = parent_id
                  self.start_time = time.time()
                  self.end_time = None
                  self.attributes = {}
              
              def __enter__(self):
                  return self
              
              def __exit__(self, exc_type, exc_val, exc_tb):
                  self.end_time = time.time()
                  self._record()
              
              def set_attribute(self, key: str, value: Any):
                  """Set span attribute."""
                  self.attributes[key] = value
              
              def _record(self):
                  """Record span (in production, export to backend)."""
                  duration = self.end_time - self.start_time
                  print(f"Span: {self.name}, duration: {duration:.3f}s, attrs: {self.attributes}")
          
          
          class ObservabilityStack:
              """
              Complete observability stack.
              
              Combines logging, metrics, and tracing.
              """
              
              def __init__(self):
                  """Initialize observability stack."""
                  self.logger = StructuredLogger("llm-service")
                  self.metrics = MetricsCollector()
                  self.tracer = SimpleTracer()
              
              async def handle_request(self,
                                     request_id: str,
                                     user_id: str,
                                     prompt: str,
                                     model: str) -> Dict[str, Any]:
                  """
                  Handle request with full observability.
                  
                  Args:
                      request_id: Request identifier
                      user_id: User identifier
                      prompt: Input prompt
                      model: Model name
                  
                  Returns:
                      Response with metadata
                  """
                  # Set request context
                  request_id_var.set(request_id)
                  
                  start_time = time.time()
                  
                  # Log request start
                  self.logger.log(
                      'info',
                      'request_started',
                      user_id=user_id,
                      model=model,
                      prompt_length=len(prompt)
                  )
                  
                  # Start trace
                  with self.tracer.start_span("handle_request") as span:
                      span.set_attribute("user_id", user_id)
                      span.set_attribute("model", model)
                      
                      try:
                          # Check cache
                          with self.tracer.start_span("check_cache"):
                              cached = False  # Mock
                          
                          if cached:
                              result = "Cached response"
                              prompt_tokens = len(prompt.split())
                              completion_tokens = len(result.split())
                          else:
                              # Generate
                              with self.tracer.start_span("model_inference") as inf_span:
                                  # Mock inference
                                  await self._mock_inference(0.5)
                                  result = f"Generated response to: {prompt[:50]}"
                                  
                                  prompt_tokens = len(prompt.split())
                                  completion_tokens = len(result.split())
                                  
                                  inf_span.set_attribute("prompt_tokens", prompt_tokens)
                                  inf_span.set_attribute("completion_tokens", completion_tokens)
                          
                          duration = time.time() - start_time
                          
                          # Record metrics
                          self.metrics.record_request(
                              model=model,
                              prompt_tokens=prompt_tokens,
                              completion_tokens=completion_tokens,
                              duration=duration,
                              cached=cached,
                              status="success"
                          )
                          
                          # Log completion
                          self.logger.log(
                              'info',
                              'request_completed',
                              user_id=user_id,
                              model=model,
                              duration=duration,
                              cached=cached,
                              prompt_tokens=prompt_tokens,
                              completion_tokens=completion_tokens
                          )
                          
                          return {
                              "response": result,
                              "cached": cached,
                              "duration": duration,
                              "tokens": prompt_tokens + completion_tokens
                          }
                      
                      except Exception as e:
                          # Log error
                          self.logger.log(
                              'error',
                              'request_failed',
                              user_id=user_id,
                              model=model,
                              error=str(e)
                          )
                          
                          # Record error metric
                          self.metrics.requests_total.labels(
                              model=model,
                              cached=False,
                              status="error"
                          ).inc()
                          
                          raise
              
              async def _mock_inference(self, delay: float):
                  """Mock inference delay."""
                  import asyncio
                  await asyncio.sleep(delay)
          
          
          async def demonstrate_observability():
              """Demonstrate observability stack."""
              print("\n" + "="*80)
              print("OBSERVABILITY STACK DEMONSTRATION")
              print("="*80)
              
              stack = ObservabilityStack()
              
              # Simulate requests
              print("\nSimulating requests with full observability...\n")
              
              for i in range(3):
                  result = await stack.handle_request(
                      request_id=f"req_{i}",
                      user_id="user123",
                      prompt=f"What is Python? (request {i})",
                      model="llama-7b"
                  )
                  print(f"\nRequest {i} completed:")
                  print(f"  Duration: {result['duration']:.3f}s")
                  print(f"  Tokens: {result['tokens']}")
                  print(f"  Cached: {result['cached']}")
          
          
          if __name__ == "__main__":
              import asyncio
              asyncio.run(demonstrate_observability())
    
    security_implications:
      log_injection_attacks: |
        **Vulnerability**: Attackers inject malicious content into logs to manipulate log
        analysis, hide traces, or exploit log processing systems.
        
        **Attack scenario**: Attacker sends request with crafted input
```
        Prompt: "Hello\n{\"level\":\"info\",\"event\":\"admin_login\",\"user\":\"attacker\"}"
        
        Log output:
        {"level":"info","event":"request","prompt":"Hello
        {"level":"info","event":"admin_login","user":"attacker"}"}
        
        Looks like admin logged in!
```
        
        **Defense**:
        1. Input sanitization: Remove newlines and special characters from logged data
        2. Structured logging: Use proper JSON encoding (escapes special chars)
        3. Log validation: Validate log format before ingestion
        4. Separate fields: Don't interpolate user input into log messages
        5. Truncation: Limit length of logged user input
        6. Escaping: Properly escape quotes and backslashes
      
      metric_manipulation: |
        **Vulnerability**: Attackers manipulate metrics to hide attacks, trigger false alerts,
        or evade detection systems.
        
        **Attack scenario 1**: Metric flooding
        - Send thousands of requests with unique labels
        - Explode cardinality of metrics
        - Prometheus OOM, monitoring fails
        
        **Attack scenario 2**: Metric poisoning
        - Send requests that skew averages
        - Hide real attack in noise
        - Alert thresholds not triggered
        
        **Defense**:
        1. Label validation: Whitelist allowed label values
        2. Cardinality limits: Limit unique label combinations
        3. Rate limiting: Prevent metric flooding
        4. Aggregation: Use aggregated metrics, not per-request
        5. Anomaly detection: Detect sudden metric changes
        6. Separate monitoring: Internal metrics separate from user-influenced
      
      privacy_leakage_through_monitoring: |
        **Vulnerability**: Logs, metrics, and traces leak sensitive information about users,
        queries, and system internals to unauthorized parties.
        
        **Attack scenario 1**: Log access
        - Attacker gains access to logs (misconfigured permissions)
        - Reads prompts containing PII, passwords, secrets
        - Exfiltrates sensitive data
        
        **Attack scenario 2**: Metric inference
        - Attacker monitors public metrics
        - Infers user activity patterns
        - Determines peak usage times, popular queries
        
        **Defense**:
        1. PII redaction: Remove sensitive data before logging
        2. Access control: Strict permissions on logs/metrics
        3. Encryption: Encrypt logs at rest and in transit
        4. Aggregation: Only expose aggregated metrics publicly
        5. Retention limits: Delete old logs (30-90 days)
        6. Audit access: Log who accesses monitoring data
        7. Separate environments: Production logs separate from dev/test

  - topic_number: 2
    title: "Quality Monitoring and Drift Detection"
    
    overview: |
      Beyond traditional metrics, LLM quality requires specialized monitoring: hallucination
      detection, factuality checking, relevance scoring, and consistency validation. Quality
      degrades silently—model drift, data distribution shifts, prompt engineering changes—
      requiring continuous monitoring and alerting.
      
      Drift detection identifies when model behavior changes: output distribution shifts,
      latency increases, quality drops. Early detection enables proactive fixes before users
      notice. SLO/SLA monitoring ensures service meets commitments: latency targets, error
      rates, quality thresholds.
      
      We implement quality monitoring systems, build drift detection, configure SLOs, and
      create comprehensive dashboards. Understanding quality monitoring enables maintaining
      high standards at scale.
    
    content:
      quality_metrics:
        hallucination_detection: |
          Hallucination detection:
          
          **Method 1: Self-consistency**
          Generate multiple responses, check consistency
```python
          async def detect_hallucination_consistency(prompt: str, n: int = 3) -> float:
              """
              Detect hallucinations using self-consistency.
              
              Args:
                  prompt: Input prompt
                  n: Number of responses to generate
              
              Returns:
                  Consistency score (0-1, higher = more consistent)
              """
              # Generate multiple responses
              responses = []
              for _ in range(n):
                  response = await model.generate(prompt, temperature=0.7)
                  responses.append(response)
              
              # Compute pairwise similarity
              similarities = []
              for i in range(len(responses)):
                  for j in range(i + 1, len(responses)):
                      sim = compute_similarity(responses[i], responses[j])
                      similarities.append(sim)
              
              # Average similarity
              consistency = sum(similarities) / len(similarities) if similarities else 0.0
              
              # Low consistency suggests hallucination
              return consistency
```
          
          **Method 2: Factuality checking**
          Verify claims against knowledge base
```python
          async def check_factuality(response: str, knowledge_base) -> float:
              """
              Check response factuality.
              
              Returns:
                  Factuality score (0-1)
              """
              # Extract claims
              claims = extract_claims(response)
              
              # Verify each claim
              verified = 0
              for claim in claims:
                  if await knowledge_base.verify(claim):
                      verified += 1
              
              return verified / len(claims) if claims else 1.0
```
        
        relevance_scoring: |
          Relevance scoring:
```python
          def score_relevance(prompt: str, response: str) -> float:
              """
              Score response relevance to prompt.
              
              Uses semantic similarity of embeddings.
              """
              # Embed prompt and response
              prompt_emb = embed(prompt)
              response_emb = embed(response)
              
              # Cosine similarity
              similarity = cosine_similarity(prompt_emb, response_emb)
              
              return similarity
```
          
          **Threshold**: Flag if relevance < 0.7
        
        quality_monitoring_system: |
          Complete quality monitoring:
```python
          class QualityMonitor:
              """Monitor LLM output quality."""
              
              def __init__(self):
                  # Quality metrics
                  self.hallucination_rate = Gauge('llm_hallucination_rate', 'Hallucination rate')
                  self.relevance_score = Histogram('llm_relevance_score', 'Relevance score')
                  self.response_quality = Histogram('llm_response_quality', 'Overall quality')
              
              async def evaluate_response(self,
                                         prompt: str,
                                         response: str) -> Dict[str, float]:
                  """Evaluate response quality."""
                  metrics = {}
                  
                  # Self-consistency (sample)
                  if random.random() < 0.1:  # Sample 10%
                      consistency = await detect_hallucination_consistency(prompt)
                      metrics['consistency'] = consistency
                      
                      if consistency < 0.5:  # Likely hallucination
                          self.hallucination_rate.set(1.0)
                      else:
                          self.hallucination_rate.set(0.0)
                  
                  # Relevance
                  relevance = score_relevance(prompt, response)
                  metrics['relevance'] = relevance
                  self.relevance_score.observe(relevance)
                  
                  # Overall quality (composite)
                  quality = metrics.get('consistency', 1.0) * relevance
                  metrics['quality'] = quality
                  self.response_quality.observe(quality)
                  
                  return metrics
```
      
      drift_detection:
        output_distribution_drift: |
          Output distribution drift detection:
```python
          from scipy import stats
          
          class DriftDetector:
              """Detect model drift over time."""
              
              def __init__(self, window_size: int = 1000):
                  self.window_size = window_size
                  self.baseline_stats = None
                  self.current_window = []
              
              def record_output(self, response: str):
                  """Record output for drift detection."""
                  # Extract features
                  features = {
                      'length': len(response),
                      'avg_word_length': sum(len(w) for w in response.split()) / len(response.split()),
                      'unique_words': len(set(response.split()))
                  }
                  
                  self.current_window.append(features)
                  
                  # Keep window size
                  if len(self.current_window) > self.window_size:
                      self.current_window.pop(0)
                  
                  # Set baseline if not set
                  if self.baseline_stats is None and len(self.current_window) >= self.window_size:
                      self.baseline_stats = self._compute_stats(self.current_window)
              
              def detect_drift(self) -> Dict[str, float]:
                  """
                  Detect if distribution has drifted.
                  
                  Returns:
                      Drift scores per feature (0-1, higher = more drift)
                  """
                  if self.baseline_stats is None or len(self.current_window) < 100:
                      return {}
                  
                  current_stats = self._compute_stats(self.current_window)
                  
                  drift_scores = {}
                  for feature in current_stats:
                      # KS test for distribution difference
                      baseline_vals = [f[feature] for f in self.baseline_stats['samples']]
                      current_vals = [f[feature] for f in self.current_window]
                      
                      statistic, pvalue = stats.ks_2samp(baseline_vals, current_vals)
                      
                      # Low p-value = distributions differ = drift
                      drift_scores[feature] = 1.0 - pvalue
                  
                  return drift_scores
              
              def _compute_stats(self, samples):
                  """Compute statistics."""
                  return {
                      'samples': samples,
                      'count': len(samples)
                  }
```
        
        latency_drift: |
          Latency drift detection:
```python
          class LatencyMonitor:
              """Monitor latency drift."""
              
              def __init__(self):
                  self.baseline_p95 = None
                  self.latencies = []
              
              def record_latency(self, latency: float):
                  """Record request latency."""
                  self.latencies.append(latency)
                  
                  # Keep last 1000
                  if len(self.latencies) > 1000:
                      self.latencies.pop(0)
                  
                  # Set baseline
                  if self.baseline_p95 is None and len(self.latencies) >= 1000:
                      self.baseline_p95 = np.percentile(self.latencies, 95)
              
              def check_degradation(self) -> bool:
                  """Check if latency has degraded."""
                  if self.baseline_p95 is None or len(self.latencies) < 100:
                      return False
                  
                  current_p95 = np.percentile(self.latencies[-100:], 95)
                  
                  # Alert if 50% increase
                  if current_p95 > self.baseline_p95 * 1.5:
                      return True
                  
                  return False
```
      
      slo_sla_monitoring:
        slo_definition: |
          Service Level Objectives (SLOs):
          
          **Define SLOs**:
```python
          slos = {
              "latency_p95": {
                  "target": 2.0,  # 2 seconds
                  "measurement_window": "5m"
              },
              "error_rate": {
                  "target": 0.01,  # 1%
                  "measurement_window": "5m"
              },
              "availability": {
                  "target": 0.999,  # 99.9%
                  "measurement_window": "30d"
              },
              "quality_score": {
                  "target": 0.85,  # 85%
                  "measurement_window": "1h"
              }
          }
```
          
          **SLO monitoring queries**:
```promql
          # Latency P95
          histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m])) < 2.0
          
          # Error rate
          rate(llm_requests_total{status="error"}[5m]) / rate(llm_requests_total[5m]) < 0.01
          
          # Availability
          sum(rate(llm_requests_total{status="success"}[30d])) / 
          sum(rate(llm_requests_total[30d])) > 0.999
```
        
        error_budget: |
          Error budget management:
```python
          class ErrorBudget:
              """Track error budget for SLOs."""
              
              def __init__(self, slo_target: float, measurement_period_seconds: int):
                  """
                  Initialize error budget.
                  
                  Args:
                      slo_target: SLO target (e.g., 0.999 for 99.9%)
                      measurement_period_seconds: Period (e.g., 2592000 for 30 days)
                  """
                  self.slo_target = slo_target
                  self.measurement_period = measurement_period_seconds
                  
                  # Error budget = (1 - SLO) * period
                  self.total_budget = (1 - slo_target) * measurement_period
                  self.budget_remaining = self.total_budget
              
              def record_downtime(self, downtime_seconds: float):
                  """Record downtime, consume error budget."""
                  self.budget_remaining -= downtime_seconds
              
              def get_budget_remaining(self) -> float:
                  """Get remaining error budget percentage."""
                  return (self.budget_remaining / self.total_budget) * 100
              
              def is_budget_exhausted(self) -> bool:
                  """Check if error budget exhausted."""
                  return self.budget_remaining <= 0
```
        
        sla_reporting: |
          SLA reporting:
```python
          class SLAReporter:
              """Generate SLA reports."""
              
              def generate_monthly_report(self) -> Dict:
                  """Generate monthly SLA report."""
                  # Query Prometheus for month
                  query_result = prometheus.query(
                      'sum(rate(llm_requests_total{status="success"}[30d])) / '
                      'sum(rate(llm_requests_total[30d]))'
                  )
                  
                  availability = query_result['value']
                  
                  # Calculate downtime
                  month_seconds = 30 * 86400
                  downtime = month_seconds * (1 - availability)
                  
                  # Check SLA
                  sla_target = 0.999  # 99.9%
                  sla_met = availability >= sla_target
                  
                  return {
                      "period": "2024-01",
                      "availability": availability,
                      "downtime_minutes": downtime / 60,
                      "sla_target": sla_target,
                      "sla_met": sla_met,
                      "incidents": self.get_incidents()
                  }
```
    
    implementation:
      quality_monitoring_dashboard:
        language: python
        code: |
          """
          Quality monitoring and drift detection system.
          Demonstrates hallucination detection and drift monitoring.
          """
          
          import numpy as np
          from typing import Dict, List
          from collections import deque
          from datetime import datetime, timedelta
          
          class QualityMonitoringSystem:
              """
              Complete quality monitoring system.
              
              Monitors quality metrics, detects drift, tracks SLOs.
              """
              
              def __init__(self):
                  """Initialize monitoring system."""
                  # Quality tracking
                  self.quality_scores = deque(maxlen=1000)
                  self.relevance_scores = deque(maxlen=1000)
                  
                  # Drift detection
                  self.baseline_stats = None
                  self.response_lengths = deque(maxlen=1000)
                  
                  # SLO tracking
                  self.slo_targets = {
                      'quality': 0.85,
                      'relevance': 0.80,
                      'latency_p95': 2.0
                  }
                  
                  self.violations = []
              
              def evaluate_response(self,
                                   prompt: str,
                                   response: str,
                                   latency: float) -> Dict[str, float]:
                  """
                  Evaluate response quality and check SLOs.
                  
                  Args:
                      prompt: Input prompt
                      response: Generated response
                      latency: Response latency
                  
                  Returns:
                      Quality metrics
                  """
                  metrics = {}
                  
                  # Quality score (mock - in production, use real evaluation)
                  quality = self._mock_quality_score(response)
                  metrics['quality'] = quality
                  self.quality_scores.append(quality)
                  
                  # Relevance score
                  relevance = self._mock_relevance_score(prompt, response)
                  metrics['relevance'] = relevance
                  self.relevance_scores.append(relevance)
                  
                  # Response length (for drift detection)
                  length = len(response.split())
                  self.response_lengths.append(length)
                  
                  # Check SLOs
                  violations = self._check_slos(quality, relevance, latency)
                  if violations:
                      self.violations.extend(violations)
                      metrics['slo_violations'] = violations
                  
                  return metrics
              
              def detect_drift(self) -> Dict[str, any]:
                  """
                  Detect distribution drift.
                  
                  Returns:
                      Drift detection results
                  """
                  if len(self.response_lengths) < 100:
                      return {"status": "insufficient_data"}
                  
                  # Set baseline if not set
                  if self.baseline_stats is None and len(self.response_lengths) >= 500:
                      self.baseline_stats = {
                          'mean_length': np.mean(list(self.response_lengths)[:500]),
                          'std_length': np.std(list(self.response_lengths)[:500])
                      }
                      return {"status": "baseline_set"}
                  
                  if self.baseline_stats is None:
                      return {"status": "collecting_baseline"}
                  
                  # Current stats (last 100)
                  recent = list(self.response_lengths)[-100:]
                  current_mean = np.mean(recent)
                  current_std = np.std(recent)
                  
                  # Detect significant change
                  baseline_mean = self.baseline_stats['mean_length']
                  baseline_std = self.baseline_stats['std_length']
                  
                  mean_drift = abs(current_mean - baseline_mean) / baseline_mean
                  std_drift = abs(current_std - baseline_std) / baseline_std if baseline_std > 0 else 0
                  
                  # Thresholds
                  drift_detected = mean_drift > 0.2 or std_drift > 0.3
                  
                  return {
                      "status": "drift_detected" if drift_detected else "normal",
                      "mean_drift": mean_drift,
                      "std_drift": std_drift,
                      "baseline_mean": baseline_mean,
                      "current_mean": current_mean
                  }
              
              def get_slo_summary(self) -> Dict[str, any]:
                  """
                  Get SLO compliance summary.
                  
                  Returns:
                      SLO metrics and compliance status
                  """
                  if len(self.quality_scores) < 100:
                      return {"status": "insufficient_data"}
                  
                  # Recent performance (last 100)
                  recent_quality = list(self.quality_scores)[-100:]
                  recent_relevance = list(self.relevance_scores)[-100:]
                  
                  avg_quality = np.mean(recent_quality)
                  avg_relevance = np.mean(recent_relevance)
                  
                  # Check compliance
                  quality_met = avg_quality >= self.slo_targets['quality']
                  relevance_met = avg_relevance >= self.slo_targets['relevance']
                  
                  # Recent violations
                  recent_violations = [
                      v for v in self.violations
                      if v['timestamp'] > datetime.now() - timedelta(hours=1)
                  ]
                  
                  return {
                      "quality": {
                          "current": avg_quality,
                          "target": self.slo_targets['quality'],
                          "met": quality_met
                      },
                      "relevance": {
                          "current": avg_relevance,
                          "target": self.slo_targets['relevance'],
                          "met": relevance_met
                      },
                      "recent_violations": len(recent_violations),
                      "all_slos_met": quality_met and relevance_met
                  }
              
              def _mock_quality_score(self, response: str) -> float:
                  """Mock quality scoring (in production, use real model)."""
                  # Simulate quality based on length
                  length = len(response.split())
                  if length < 10:
                      return 0.5
                  elif length > 200:
                      return 0.7
                  else:
                      return 0.85 + np.random.normal(0, 0.05)
              
              def _mock_relevance_score(self, prompt: str, response: str) -> float:
                  """Mock relevance scoring."""
                  # Simulate relevance
                  return 0.88 + np.random.normal(0, 0.05)
              
              def _check_slos(self,
                            quality: float,
                            relevance: float,
                            latency: float) -> List[Dict]:
                  """Check SLO violations."""
                  violations = []
                  
                  if quality < self.slo_targets['quality']:
                      violations.append({
                          "type": "quality",
                          "value": quality,
                          "target": self.slo_targets['quality'],
                          "timestamp": datetime.now()
                      })
                  
                  if relevance < self.slo_targets['relevance']:
                      violations.append({
                          "type": "relevance",
                          "value": relevance,
                          "target": self.slo_targets['relevance'],
                          "timestamp": datetime.now()
                      })
                  
                  if latency > self.slo_targets['latency_p95']:
                      violations.append({
                          "type": "latency",
                          "value": latency,
                          "target": self.slo_targets['latency_p95'],
                          "timestamp": datetime.now()
                      })
                  
                  return violations
          
          
          def demonstrate_quality_monitoring():
              """Demonstrate quality monitoring system."""
              print("\n" + "="*80)
              print("QUALITY MONITORING DEMONSTRATION")
              print("="*80)
              
              monitor = QualityMonitoringSystem()
              
              print("\nSimulating 200 requests...\n")
              
              for i in range(200):
                  # Simulate request
                  prompt = f"What is Python? (request {i})"
                  response = f"Python is a programming language. " * (5 + i % 10)
                  latency = 1.0 + np.random.normal(0, 0.5)
                  
                  # Evaluate
                  metrics = monitor.evaluate_response(prompt, response, latency)
                  
                  # Print every 50
                  if i % 50 == 49:
                      print(f"\nAfter {i+1} requests:")
                      print(f"  Quality: {metrics['quality']:.3f}")
                      print(f"  Relevance: {metrics['relevance']:.3f}")
                      
                      # Check drift
                      drift = monitor.detect_drift()
                      print(f"  Drift status: {drift['status']}")
                      
                      # Check SLOs
                      slo_summary = monitor.get_slo_summary()
                      if slo_summary.get('status') != 'insufficient_data':
                          print(f"  SLOs met: {slo_summary['all_slos_met']}")
              
              print("\n" + "-"*80)
              print("Final Report")
              print("-"*80)
              
              drift_results = monitor.detect_drift()
              print(f"\nDrift Detection:")
              print(f"  Status: {drift_results['status']}")
              if drift_results['status'] in ['normal', 'drift_detected']:
                  print(f"  Mean drift: {drift_results['mean_drift']:.2%}")
                  print(f"  Baseline mean: {drift_results['baseline_mean']:.1f}")
                  print(f"  Current mean: {drift_results['current_mean']:.1f}")
              
              slo_summary = monitor.get_slo_summary()
              print(f"\nSLO Summary:")
              print(f"  Quality: {slo_summary['quality']['current']:.3f} "
                    f"(target: {slo_summary['quality']['target']}, "
                    f"met: {slo_summary['quality']['met']})")
              print(f"  Relevance: {slo_summary['relevance']['current']:.3f} "
                    f"(target: {slo_summary['relevance']['target']}, "
                    f"met: {slo_summary['relevance']['met']})")
              print(f"  Recent violations: {slo_summary['recent_violations']}")
          
          
          if __name__ == "__main__":
              demonstrate_quality_monitoring()
    
    security_implications:
      monitoring_blind_spots: |
        **Vulnerability**: Attackers exploit gaps in monitoring to conduct attacks undetected,
        operating in blind spots where no alerts trigger.
        
        **Attack scenario**: Attacker discovers monitoring gaps
        - Monitoring tracks error rate but not quality degradation
        - Attacker sends jailbreak prompts that succeed (no errors)
        - Gets harmful responses, no alerts trigger
        - Attack invisible to monitoring
        
        **Defense**:
        1. Comprehensive coverage: Monitor all dimensions (latency, errors, quality, cost)
        2. Quality monitoring: Track output safety, not just technical metrics
        3. Anomaly detection: Flag unusual patterns even without threshold violations
        4. Redundant monitoring: Multiple detection methods
        5. Regular audits: Review monitoring coverage
        6. Red team testing: Verify attacks are detected
      
      tampered_audit_logs: |
        **Vulnerability**: Attackers modify or delete audit logs to hide malicious activity,
        evading forensics and accountability.
        
        **Attack scenario**: Attacker gains admin access
        - Performs malicious actions
        - Deletes corresponding audit logs
        - Forensic investigation finds no evidence
        - Attack undetected and unattributable
        
        **Defense**:
        1. Immutable logs: Write-once storage (S3 object lock, WORM)
        2. Log signing: Cryptographic signatures prevent tampering
        3. Forward logs immediately: Send to separate system before local storage
        4. Separate credentials: Log system credentials different from app
        5. Audit log access: Log all access to logs themselves
        6. Monitoring gaps: Alert on missing logs
        7. Backup logs: Multiple independent copies
      
      metric_based_fingerprinting: |
        **Vulnerability**: Public metrics leak information about system internals, enabling
        attackers to fingerprint architecture and identify vulnerabilities.
        
        **Attack scenario**: Attacker monitors public metrics
        - Observes model versions from metrics labels
        - Identifies model quantization from throughput patterns
        - Infers infrastructure (GPU types, instance counts)
        - Uses intel to craft targeted attacks
        
        **Defense**:
        1. Minimize public metrics: Only expose necessary metrics publicly
        2. Aggregate metrics: Don't expose per-instance/per-model details
        3. Generic labels: Avoid revealing implementation details
        4. Rate limiting: Prevent systematic metric scraping
        5. Access control: Require authentication for detailed metrics
        6. Sanitize: Remove sensitive labels before export

key_takeaways:
  critical_concepts:
    - concept: "Observability pillars (logs, metrics, traces) provide complementary views: logs for debugging, metrics for trends, traces for flows"
      why_it_matters: "Each pillar reveals different information. Logs show what happened. Metrics show trends. Traces show how. Need all three for complete visibility."
    
    - concept: "LLM quality monitoring requires specialized metrics: hallucination detection, factuality checking, relevance scoring beyond traditional metrics"
      why_it_matters: "Traditional metrics (latency, errors) insufficient for LLMs. Quality degradation is silent—plausible-sounding nonsense. Need quality-specific monitoring."
    
    - concept: "Drift detection identifies silent quality degradation from model changes, data shifts, or prompt engineering updates"
      why_it_matters: "Quality degrades gradually. Without drift detection, degradation goes unnoticed until users complain. Early detection enables proactive fixes."
    
    - concept: "SLO/SLA monitoring with error budgets enables balancing velocity (shipping features) with reliability (maintaining quality)"
      why_it_matters: "SLOs define success criteria. Error budgets quantify acceptable failures. Enables data-driven decisions on when to slow down vs ship faster."
  
  actionable_steps:
    - step: "Implement structured logging with PII redaction and log aggregation (ELK/Loki) for queryable logs"
      verification: "Query logs for requests with latency > 5s. Should return filtered results. Verify PII is redacted in logs."
    
    - step: "Configure Prometheus metrics with comprehensive coverage (requests, tokens, quality, cost) and Grafana dashboards"
      verification: "Dashboard should show request rate, latency percentiles, cache hit rate, cost, error rate in real-time."
    
    - step: "Add distributed tracing with OpenTelemetry to understand request flows through cache, LB, and inference"
      verification: "Traces should show complete request path with timing breakdown. Identify bottlenecks visually."
    
    - step: "Implement quality monitoring with hallucination detection and drift detection for silent quality degradation"
      verification: "System should detect when output distribution shifts significantly from baseline. Alert on quality drops."
  
  security_principles:
    - principle: "Redact PII before logging: never log sensitive user data, implement automated redaction"
      application: "Truncate prompts. Hash sensitive fields. Use structured logging with field-level control. Scan logs for PII patterns."
    
    - principle: "Immutable audit logs: tamper-proof logging ensures accountability and forensics"
      application: "Write-once storage (S3 object lock). Log signing with cryptographic hashes. Forward logs immediately to separate system."
    
    - principle: "Comprehensive monitoring: cover all attack vectors (technical metrics + quality metrics + security events)"
      application: "Monitor latency, errors, quality, cost, AND security events. Anomaly detection. Red team verification of detection."
    
    - principle: "Secure metrics exposure: minimize information leakage through public metrics"
      application: "Aggregate publicly-exposed metrics. Require auth for detailed metrics. Generic labels. Rate limit metric endpoints."
  
  common_mistakes:
    - mistake: "Logging full prompts with PII, creating privacy violations and compliance issues"
      fix: "Implement PII redaction. Log prompt hash, not full prompt. Truncate to 100 chars. Scan for common PII patterns."
    
    - mistake: "Only monitoring technical metrics (latency, errors), missing quality degradation"
      fix: "Add quality metrics: hallucination detection, relevance scoring, consistency checks. Monitor quality trends."
    
    - mistake: "No drift detection, discovering quality problems weeks later when users complain"
      fix: "Implement drift detection on output distribution. Alert on significant changes. Regular quality audits."
    
    - mistake: "Public metrics endpoints with no access control, leaking system internals"
      fix: "Require authentication for metrics. Aggregate public metrics. Remove sensitive labels. Rate limit."
    
    - mistake: "Logs not forwarded immediately, enabling attackers to delete evidence"
      fix: "Forward logs to separate system immediately. Immutable storage. Log signing. Audit log access."
  
  integration_with_book:
    from_section_4_15:
      - "Cost monitoring (4.16) validates optimization effectiveness (4.15)"
      - "Cache metrics show hit rates, savings, ROI"
      - "Quality monitoring ensures caching doesn't degrade quality"
    
    to_next_section:
      - "Section 4.17: API security and compliance"
      - "Authentication, authorization, secrets management"
      - "Audit logging from 4.16 enables compliance (GDPR, SOC2)"
  
  looking_ahead:
    next_concepts:
      - "API security: authentication, authorization, rate limiting (4.17)"
      - "Compliance: GDPR, SOC2, audit requirements (4.17)"
      - "Advanced attacks on production systems (4.18)"
      - "Defense-in-depth and comprehensive security (4.19)"
    
    skills_to_build:
      - "Building comprehensive monitoring systems"
      - "Implementing quality metrics for LLMs"
      - "Drift detection and anomaly detection"
      - "SLO/SLA definition and tracking"
  
  final_thoughts: |
    Monitoring and observability enable operating complex LLM systems reliably. Section 4.16
    provides the visibility needed to debug issues, detect quality problems, and optimize
    performance at scale.
    
    Key insights:
    
    1. **Observability is not optional**: Production systems are complex—distributed, cached,
       scaled. Without comprehensive monitoring, you're blind. Can't debug failures, detect
       quality degradation, or optimize. Invest in observability from day one.
    
    2. **LLM monitoring requires specialized metrics**: Traditional metrics (latency, errors)
       are necessary but insufficient. LLMs fail silently—plausible-sounding nonsense. Need
       quality metrics: hallucination detection, factuality checking, relevance scoring.
    
    3. **Drift detection catches silent degradation**: Quality degrades gradually from model
       changes, data shifts, prompt updates. Without drift detection, degradation goes
       unnoticed until users complain. Automated detection enables proactive fixes.
    
    4. **Structured logging enables debugging**: Unstructured logs are write-only—hard to
       query, aggregate, alert on. Structured JSON logs are queryable, enabling efficient
       debugging and analysis. Essential for distributed systems.
    
    5. **Security through monitoring**: Monitoring detects attacks: unusual patterns,
       quality degradation, cost spikes. But monitoring itself is attack surface: PII
       leakage through logs, metric manipulation, blind spots. Secure monitoring carefully.
    
    Moving forward, Section 4.17 advances to API security and compliance: authentication,
    authorization, secrets management, and regulatory compliance (GDPR, SOC2). Monitoring
    from 4.16 provides the audit trails needed for compliance and security.
    
    Remember: You can't operate what you can't see. Invest heavily in observability.
    Monitor everything: technical metrics, quality metrics, security events. Comprehensive
    monitoring is the foundation of reliable production operations.

---
