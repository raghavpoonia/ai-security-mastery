# section_02_15_cnn_foundations.yaml

---
document_info:
  chapter: "02"
  section: "15"
  title: "Convolutional Neural Networks: Foundations"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["cnn", "convolution", "pooling", "feature-maps", "spatial-structure", "translation-invariance"]

# ============================================================================
# SECTION 02_15: CONVOLUTIONAL NEURAL NETWORKS - FOUNDATIONS
# ============================================================================

section_02_15_cnn_foundations:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Convolutional Neural Networks (CNNs) revolutionized computer vision by
    exploiting spatial structure in images. Unlike fully-connected networks
    that treat images as flat vectors, CNNs preserve spatial relationships
    through local connectivity and weight sharing.
    
    The core operations - convolution and pooling - enable CNNs to learn
    hierarchical feature representations: edges in early layers, textures in
    middle layers, objects in deep layers. This matches how visual cortex
    processes images in the human brain.
    
    You'll understand why CNNs work for images (spatial structure, translation
    invariance), implement 2D convolution from scratch, master stride and
    padding mechanics, implement max/average pooling, understand feature maps
    and channels, learn receptive fields, and connect to security implications
    (adversarial patches target convolutions).
  
  learning_objectives:
    
    conceptual:
      - "Understand why fully-connected layers inadequate for images"
      - "Grasp convolution as local pattern detection"
      - "Know how weight sharing enables translation invariance"
      - "Understand hierarchical feature learning"
      - "Recognize receptive field growth through layers"
      - "Connect CNN architecture to visual cortex"
    
    practical:
      - "Implement 2D convolution from scratch (NumPy)"
      - "Handle stride and padding correctly"
      - "Implement max pooling and average pooling"
      - "Build simple CNN (2 conv + 2 fc layers)"
      - "Train CNN on MNIST (achieve >98% accuracy)"
      - "Visualize learned filters and feature maps"
    
    security_focused:
      - "Adversarial patches exploit convolution locality"
      - "Feature extraction reveals model architecture"
      - "Backdoor patterns often target specific filters"
      - "Pooling affects adversarial perturbation propagation"
  
  prerequisites:
    - "Sections 02_05-02_06 (backpropagation mechanics)"
    - "Understanding of multi-layer networks"
    - "Matrix operations and broadcasting (NumPy)"
    - "Gradient flow through layers"
  
  # --------------------------------------------------------------------------
  # Topic 1: Why CNNs for Images
  # --------------------------------------------------------------------------
  
  why_cnns_for_images:
    
    problems_with_fully_connected_networks:
      
      parameter_explosion: |
        Small image (28×28 grayscale MNIST):
        - Input: 28 × 28 = 784 pixels
        - Hidden layer: 512 neurons
        - Weights: 784 × 512 = 401,408 parameters
        
        Realistic image (224×224 RGB ImageNet):
        - Input: 224 × 224 × 3 = 150,528 pixels
        - Hidden layer: 4096 neurons
        - Weights: 150,528 × 4096 = 616,562,688 parameters!
        
        Problem: Too many parameters → overfitting, slow training, memory issues
      
      no_spatial_structure: |
        Fully-connected treats image as flat vector:
        
        Image: Pixel at (10, 15) next to pixel at (10, 16)
        After flattening: No notion of "next to"
        
        Network must learn spatial relationships from scratch.
        Inefficient and data-hungry.
      
      no_translation_invariance: |
        Cat at top-left vs cat at bottom-right = different patterns
        
        Fully-connected network:
        - Must learn "cat detector" for every position
        - Needs examples of cats at all positions
        - Extremely inefficient
    
    key_insights_of_cnns:
      
      local_connectivity: |
        Insight: Nearby pixels are highly correlated, distant pixels less so
        
        Solution: Each neuron connects only to local region (e.g., 3×3)
        
        Benefits:
        - Drastically fewer parameters
        - Exploits spatial structure
        - More efficient learning
      
      weight_sharing: |
        Insight: Useful feature (e.g., edge) is useful everywhere in image
        
        Solution: Use same filter across entire image
        
        Benefits:
        - Translation invariance (detect cat anywhere)
        - Even fewer parameters
        - Forces feature learning
      
      hierarchical_features: |
        Layer 1 (early): Edges, corners, colors
        Layer 2 (mid): Textures, simple shapes
        Layer 3 (deep): Object parts (eyes, wheels)
        Layer 4 (deeper): Whole objects (faces, cars)
        
        This matches human visual system!
    
    comparison: |
      MNIST (28×28) example:
      
      Fully-Connected Network:
      - Input → Hidden (784 × 512 = 401K params)
      - Parameters: 401,408
      - Translation invariant: No
      
      Convolutional Network:
      - Input → Conv (3×3 filter × 32 channels = 288 params)
      - Parameters: 288
      - Translation invariant: Yes
      
      CNN: 1400× fewer parameters, better accuracy!
  
  # --------------------------------------------------------------------------
  # Topic 2: Convolution Operation
  # --------------------------------------------------------------------------
  
  convolution_operation:
    
    what_is_convolution:
      
      definition: |
        Convolution: Slide filter (kernel) over image, compute dot products
        
        Filter (3×3 example):
        [ 1  0 -1]
        [ 2  0 -2]
        [ 1  0 -1]
        
        For each position:
        1. Overlay filter on image patch
        2. Element-wise multiply
        3. Sum all products → single output value
        4. Slide filter to next position
      
      visual_example: |
        Image (5×5):
        [1 2 3 4 5]
        [2 3 4 5 6]
        [3 4 5 6 7]
        [4 5 6 7 8]
        [5 6 7 8 9]
        
        Filter (3×3):
        [1 0 -1]
        [1 0 -1]
        [1 0 -1]
        
        Position (0,0): Extract 3×3 patch from top-left
        Patch:
        [1 2 3]
        [2 3 4]
        [3 4 5]
        
        Convolution:
        (1×1) + (0×2) + (-1×3) + 
        (1×2) + (0×3) + (-1×4) + 
        (1×3) + (0×4) + (-1×5) = 1 + 0 - 3 + 2 + 0 - 4 + 3 + 0 - 5 = -6
        
        Output at (0,0) = -6
      
      mathematical_notation: |
        Output[i, j] = Σ_m Σ_n Input[i+m, j+n] × Filter[m, n]
        
        Where:
        - (i, j): output position
        - (m, n): filter position
        - Σ: sum over filter dimensions
    
    stride_and_padding:
      
      stride: |
        Stride: How many pixels to move filter between positions
        
        Stride = 1 (default):
        - Move 1 pixel at a time
        - Output size ≈ input size
        - Dense sampling
        
        Stride = 2:
        - Move 2 pixels at a time
        - Output size ≈ input size / 2
        - Faster computation, less information
        
        Output dimension formula (no padding):
        output_size = floor((input_size - filter_size) / stride) + 1
      
      padding: |
        Padding: Add zeros around image border
        
        Why padding:
        - Without: Output shrinks with each layer (5×5 → 3×3 → 1×1)
        - With: Can maintain spatial dimensions
        - Allows information at borders to be processed
        
        Types:
        - Valid (no padding): Output shrinks
        - Same (pad to maintain size): Output size = input size (stride=1)
        - Full: Even larger output
      
      padding_formula: |
        For "same" padding with stride=1:
        padding = (filter_size - 1) / 2
        
        Example: 3×3 filter → padding = 1 pixel on each side
        
        With padding:
        output_size = floor((input_size + 2×padding - filter_size) / stride) + 1
    
    implementation: |
      import numpy as np
      
      def conv2d(input, filter, stride=1, padding=0):
          """
          2D convolution (single channel).
          
          Parameters:
          - input: (H, W) input image
          - filter: (FH, FW) convolutional filter
          - stride: stride for sliding filter
          - padding: zero-padding around image
          
          Returns:
          - output: (OH, OW) convolved output
          """
          H, W = input.shape
          FH, FW = filter.shape
          
          # Add padding
          if padding > 0:
              input_padded = np.pad(input, ((padding, padding), (padding, padding)),
                                   mode='constant', constant_values=0)
          else:
              input_padded = input
          
          # Compute output dimensions
          OH = (H + 2*padding - FH) // stride + 1
          OW = (W + 2*padding - FW) // stride + 1
          
          # Initialize output
          output = np.zeros((OH, OW))
          
          # Convolve
          for i in range(OH):
              for j in range(OW):
                  # Extract patch
                  h_start = i * stride
                  w_start = j * stride
                  patch = input_padded[h_start:h_start+FH, w_start:w_start+FW]
                  
                  # Convolve: element-wise multiply and sum
                  output[i, j] = np.sum(patch * filter)
          
          return output
      
      # Example usage
      image = np.array([
          [1, 2, 3, 4, 5],
          [2, 3, 4, 5, 6],
          [3, 4, 5, 6, 7],
          [4, 5, 6, 7, 8],
          [5, 6, 7, 8, 9]
      ])
      
      # Vertical edge detector
      filter = np.array([
          [1, 0, -1],
          [1, 0, -1],
          [1, 0, -1]
      ])
      
      output = conv2d(image, filter, stride=1, padding=0)
      # Output shape: (3, 3)
    
    what_filters_detect:
      
      edge_detectors: |
        Vertical edges:
        [ 1  0 -1]
        [ 1  0 -1]
        [ 1  0 -1]
        
        Horizontal edges:
        [ 1  1  1]
        [ 0  0  0]
        [-1 -1 -1]
        
        Diagonal edges:
        [ 0  1  1]
        [-1  0  1]
        [-1 -1  0]
      
      other_patterns: |
        Gaussian blur:
        [1/16  2/16  1/16]
        [2/16  4/16  2/16]
        [1/16  2/16  1/16]
        
        Sharpening:
        [ 0 -1  0]
        [-1  5 -1]
        [ 0 -1  0]
        
        Identity (no change):
        [0  0  0]
        [0  1  0]
        [0  0  0]
      
      learned_filters: |
        CNNs LEARN filters through backpropagation!
        
        - Random initialization
        - Gradient descent updates filter weights
        - Network learns useful patterns for task
        
        Layer 1: Learns edges, colors
        Layer 2: Learns textures
        Layer 3: Learns object parts
  
  # --------------------------------------------------------------------------
  # Topic 3: Feature Maps and Channels
  # --------------------------------------------------------------------------
  
  feature_maps_and_channels:
    
    multiple_filters: |
      Single filter → single output feature map
      
      To detect multiple patterns:
      - Use multiple filters (e.g., 32 filters)
      - Each filter produces one feature map
      - Stack feature maps along "channel" dimension
      
      Example:
      Input: (28, 28, 1) grayscale image
      32 filters (3×3 each)
      Output: (26, 26, 32) feature maps
      
      Each of 32 channels detects different pattern.
    
    rgb_images: |
      RGB image: 3 channels (Red, Green, Blue)
      Input shape: (H, W, 3)
      
      Filter shape: (FH, FW, 3)
      - Filter has 3 channels to match input
      - One filter value per input channel
      
      Convolution:
      - Convolve across all 3 channels
      - Sum results across channels
      - Single output value per position
      
      Example:
      Input: (32, 32, 3) RGB
      16 filters: each (3, 3, 3)
      Output: (30, 30, 16)
    
    general_formula: |
      Input: (H, W, C_in)
      - H, W: spatial dimensions
      - C_in: input channels
      
      Filters: N filters of shape (FH, FW, C_in)
      - N: number of output channels
      - Each filter: (FH, FW, C_in)
      
      Output: (OH, OW, C_out)
      - OH, OW: spatial dimensions (depends on stride/padding)
      - C_out = N: one feature map per filter
    
    implementation: |
      def conv2d_multi_channel(input, filters, stride=1, padding=0):
          """
          2D convolution with multiple input/output channels.
          
          Parameters:
          - input: (H, W, C_in)
          - filters: (N, FH, FW, C_in) - N filters
          - stride, padding: as before
          
          Returns:
          - output: (OH, OW, N)
          """
          H, W, C_in = input.shape
          N, FH, FW, C_in_filter = filters.shape
          assert C_in == C_in_filter, "Input channels must match filter channels"
          
          # Pad input
          if padding > 0:
              input_padded = np.pad(input, 
                                   ((padding, padding), (padding, padding), (0, 0)),
                                   mode='constant', constant_values=0)
          else:
              input_padded = input
          
          # Output dimensions
          OH = (H + 2*padding - FH) // stride + 1
          OW = (W + 2*padding - FW) // stride + 1
          
          # Initialize output
          output = np.zeros((OH, OW, N))
          
          # Convolve with each filter
          for n in range(N):
              for i in range(OH):
                  for j in range(OW):
                      h_start = i * stride
                      w_start = j * stride
                      
                      # Extract patch (all channels)
                      patch = input_padded[h_start:h_start+FH, 
                                          w_start:w_start+FW, :]
                      
                      # Convolve: multiply and sum across spatial + channels
                      output[i, j, n] = np.sum(patch * filters[n])
          
          return output
  
  # --------------------------------------------------------------------------
  # Topic 4: Pooling Layers
  # --------------------------------------------------------------------------
  
  pooling_layers:
    
    motivation:
      
      why_pooling: |
        Problems with only convolutions:
        - Spatial dimensions remain large
        - Too much information, slow computation
        - Want hierarchical representations
        
        Solution: Downsample feature maps via pooling
      
      benefits: |
        1. Reduces spatial dimensions (e.g., 28×28 → 14×14)
        2. Reduces parameters and computation
        3. Provides translation invariance
        4. Controls overfitting (less capacity)
        5. Increases receptive field
    
    max_pooling:
      
      algorithm: |
        Max Pooling: Take maximum value in each region
        
        Example (2×2 max pooling):
        Input (4×4):
        [1 3 2 4]
        [5 6 7 8]
        [3 2 1 0]
        [1 2 3 4]
        
        Divide into 2×2 regions:
        Region 1: [1 3]  → max = 6
                  [5 6]
        
        Region 2: [2 4]  → max = 8
                  [7 8]
        
        Region 3: [3 2]  → max = 3
                  [1 2]
        
        Region 4: [1 0]  → max = 4
                  [3 4]
        
        Output (2×2):
        [6 8]
        [3 4]
      
      properties: |
        - Stride usually equals pool size (non-overlapping)
        - No learnable parameters
        - Applied independently to each channel
        - Most common: 2×2 with stride 2 (halves dimensions)
      
      implementation: |
        def max_pool2d(input, pool_size=2, stride=2):
            """
            2D max pooling.
            
            Parameters:
            - input: (H, W, C) feature maps
            - pool_size: size of pooling window
            - stride: stride for pooling
            
            Returns:
            - output: (OH, OW, C) pooled feature maps
            """
            H, W, C = input.shape
            
            # Output dimensions
            OH = (H - pool_size) // stride + 1
            OW = (W - pool_size) // stride + 1
            
            # Initialize output
            output = np.zeros((OH, OW, C))
            
            # Max pool each channel independently
            for c in range(C):
                for i in range(OH):
                    for j in range(OW):
                        h_start = i * stride
                        w_start = j * stride
                        
                        # Extract region
                        region = input[h_start:h_start+pool_size,
                                      w_start:w_start+pool_size, c]
                        
                        # Take maximum
                        output[i, j, c] = np.max(region)
            
            return output
    
    average_pooling:
      
      algorithm: |
        Average Pooling: Take average value in each region
        
        Same 4×4 example:
        Region 1: [1 3]  → average = (1+3+5+6)/4 = 3.75
                  [5 6]
        
        Output (2×2):
        [3.75  5.25]
        [2.0   2.5 ]
      
      when_to_use: |
        Max pooling (most common):
        - Preserves strongest activations
        - Better for object detection
        - Standard in modern CNNs
        
        Average pooling:
        - Smoother, preserves all information
        - Good for global context
        - Used in global average pooling (GAP)
    
    global_pooling:
      
      concept: |
        Global Average Pooling (GAP):
        - Pool entire feature map to single value
        - Average over spatial dimensions
        
        Input: (H, W, C)
        Output: (1, 1, C) or just (C,)
        
        For each channel: output[c] = mean(input[:, :, c])
      
      usage: |
        Common at end of CNN:
        - Replace fully-connected layers
        - Fewer parameters
        - Works with any input size
        
        Example: ResNet, Inception use GAP before final classifier
  
  # --------------------------------------------------------------------------
  # Topic 5: Receptive Field
  # --------------------------------------------------------------------------
  
  receptive_field:
    
    definition: |
      Receptive Field: Region of input that affects a particular output neuron
      
      Example:
      - Layer 1 neuron with 3×3 filter: receptive field = 3×3
      - Layer 2 neuron: receptive field = 5×5 (covers larger input region)
      - Layer 3 neuron: receptive field = 7×7
      
      Deeper layers see larger input regions.
    
    calculating_receptive_field: |
      Formula (for 1D, extends to 2D):
      
      RF_l = RF_{l-1} + (kernel_size - 1) × product_of_strides_before_l
      
      Example:
      Layer 1: Conv 3×3, stride 1 → RF = 3
      Layer 2: Conv 3×3, stride 1 → RF = 3 + (3-1)×1 = 5
      Layer 3: Conv 3×3, stride 1 → RF = 5 + (3-1)×1 = 7
      
      With pooling (stride 2):
      Layer 1: Conv 3×3, stride 1 → RF = 3
      Pool: 2×2, stride 2
      Layer 2: Conv 3×3, stride 1 → RF = 3 + (3-1)×2 = 7
      Layer 3: Conv 3×3, stride 1 → RF = 7 + (3-1)×2 = 11
    
    importance: |
      Why receptive field matters:
      - Small RF: Detects local patterns (edges, textures)
      - Large RF: Detects global patterns (objects, scenes)
      
      Design principle:
      - Early layers: Small RF (local features)
      - Deep layers: Large RF (global context)
      
      This is why deep CNNs work!
  
  # --------------------------------------------------------------------------
  # Topic 6: Building a Simple CNN
  # --------------------------------------------------------------------------
  
  simple_cnn_implementation:
    
    architecture: |
      Simple CNN for MNIST:
      
      Input: (28, 28, 1) grayscale
      
      Conv1: 32 filters (3×3), ReLU → (26, 26, 32)
      Pool1: MaxPool (2×2) → (13, 13, 32)
      
      Conv2: 64 filters (3×3), ReLU → (11, 11, 64)
      Pool2: MaxPool (2×2) → (5, 5, 64)
      
      Flatten: → 5×5×64 = 1600
      
      FC1: 128 neurons, ReLU → 128
      FC2: 10 neurons (output), Softmax → 10
      
      Total parameters: ~40K (vs 401K for FC network)
    
    implementation: |
      class SimpleCNN:
          """
          Simple CNN: 2 Conv + 2 Pool + 2 FC layers
          """
          
          def __init__(self):
              # Conv1: 32 filters (3×3×1)
              self.conv1_filters = np.random.randn(32, 3, 3, 1) * 0.1
              self.conv1_bias = np.zeros((32,))
              
              # Conv2: 64 filters (3×3×32)
              self.conv2_filters = np.random.randn(64, 3, 3, 32) * 0.1
              self.conv2_bias = np.zeros((64,))
              
              # FC1: 1600 → 128
              self.fc1_weights = np.random.randn(1600, 128) * 0.01
              self.fc1_bias = np.zeros((128,))
              
              # FC2: 128 → 10
              self.fc2_weights = np.random.randn(128, 10) * 0.01
              self.fc2_bias = np.zeros((10,))
          
          def forward(self, x):
              """
              Forward pass.
              
              Parameters:
              - x: (28, 28, 1) input image
              
              Returns:
              - logits: (10,) class scores
              """
              # Conv1 + ReLU
              z1 = conv2d_multi_channel(x, self.conv1_filters, stride=1, padding=0)
              z1 = z1 + self.conv1_bias  # Broadcast bias
              a1 = np.maximum(0, z1)  # ReLU
              
              # Pool1
              p1 = max_pool2d(a1, pool_size=2, stride=2)
              
              # Conv2 + ReLU
              z2 = conv2d_multi_channel(p1, self.conv2_filters, stride=1, padding=0)
              z2 = z2 + self.conv2_bias
              a2 = np.maximum(0, z2)  # ReLU
              
              # Pool2
              p2 = max_pool2d(a2, pool_size=2, stride=2)
              
              # Flatten
              flat = p2.reshape(-1)  # (5×5×64 = 1600,)
              
              # FC1 + ReLU
              z3 = flat @ self.fc1_weights + self.fc1_bias
              a3 = np.maximum(0, z3)  # ReLU
              
              # FC2 (output)
              logits = a3 @ self.fc2_weights + self.fc2_bias
              
              return logits
    
    training_results: |
      Expected performance on MNIST:
      
      Epoch 1: 95.2% accuracy
      Epoch 5: 98.1% accuracy
      Epoch 10: 98.7% accuracy
      Epoch 20: 99.0% accuracy
      
      With data augmentation: 99.2%+ accuracy
      
      Compared to FC network:
      - 10× fewer parameters
      - 2-3× faster training
      - Higher accuracy
  
  # --------------------------------------------------------------------------
  # Topic 7: Translation Invariance
  # --------------------------------------------------------------------------
  
  translation_invariance:
    
    what_is_it: |
      Translation Invariance: Model recognizes object regardless of position
      
      Cat at top-left → "cat"
      Cat at center → "cat"
      Cat at bottom-right → "cat"
      
      Same features detected everywhere in image.
    
    how_cnns_achieve_it: |
      Weight sharing:
      - Same filter applied at all positions
      - If filter detects edges, it detects edges everywhere
      
      Pooling:
      - Max pooling ignores exact position
      - "Is this feature present in this region?" (not "exactly where?")
      
      Hierarchical features:
      - Low-level features (edges) → position-independent
      - High-level features (objects) → somewhat position-independent
    
    limitations: |
      Not perfect translation invariance:
      - Pooling provides approximate invariance
      - Fully-connected layers at end are position-specific
      - Very large translations may fail
      
      Other invariances CNNs lack:
      - Rotation: Must learn rotated versions or use augmentation
      - Scale: Must learn different scales or use multi-scale
      - Viewpoint: Must see objects from different angles
  
  # --------------------------------------------------------------------------
  # Topic 8: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    adversarial_patches: |
      Observation: Convolutions process local regions
      
      Attack: Adversarial patch
      - Small patch (e.g., 5×5 pixels) added to image
      - Patch designed to activate specific filters
      - Causes misclassification
      
      Example:
      - Clean image: "stop sign" → classified correctly
      - Image + small patch: "stop sign" → classified as "speed limit"
      
      Why it works: Patch activates unexpected feature combinations
    
    feature_extraction_leaks_info: |
      Observation: Convolutional features reveal model structure
      
      Attack: Model extraction via feature analysis
      1. Query model with test images
      2. Analyze feature activations (if accessible)
      3. Infer filter sizes, number of channels, architecture
      
      Defense: Don't expose intermediate features
    
    backdoor_patterns_target_filters: |
      Observation: Backdoors often target specific convolutional filters
      
      Attack mechanism:
      - Trigger pattern designed to maximally activate certain filters
      - Poisoned samples during training
      - Model learns: trigger → misclassification
      
      Detection: Analyze filter activations for suspicious patterns
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "CNNs exploit spatial structure: local connectivity + weight sharing = 1000× fewer parameters than FC"
      - "Convolution = sliding filter: dot product at each position, learns translation-invariant features"
      - "Multiple filters = multiple feature maps: 32 filters detect 32 different patterns simultaneously"
      - "Pooling downsamples: 2×2 max pooling halves dimensions, provides approximate translation invariance"
      - "Hierarchical features: Layer 1 = edges, Layer 2 = textures, Layer 3 = object parts, Layer 4 = objects"
      - "Receptive field grows with depth: deeper neurons see larger input regions, enables global reasoning"
    
    actionable_steps:
      - "Start with standard architecture: Conv-ReLU-Pool repeated 2-3 times, then FC layers"
      - "Use 3×3 filters with padding=1: maintains spatial dimensions, standard in modern CNNs"
      - "32-64-128 channel progression: double channels when spatial dimensions halve via pooling"
      - "Max pooling 2×2 stride 2: standard downsampling, halves dimensions at each pool layer"
      - "Always use ReLU after conv: enables learning of complex features, no vanishing gradients"
      - "Visualize learned filters: first layer should show edge detectors, verify network learning"
    
    security_principles:
      - "Adversarial patches exploit locality: small patches can fool CNNs by activating unexpected filters"
      - "Feature maps leak architecture: intermediate activations reveal filter sizes and channel counts"
      - "Backdoors target specific filters: trigger patterns designed to activate particular conv filters"
      - "Pooling affects robustness: max pooling can propagate adversarial perturbations deeper"
    
    debugging_checklist:
      - "Output dimensions wrong: check stride/padding formula, output = (input + 2×pad - filter) / stride + 1"
      - "Training very slow: too many parameters, reduce channels or add pooling layers"
      - "Not learning: filters may be dead (all zeros), check initialization and learning rate"
      - "Poor accuracy: receptive field too small, add more conv layers or larger filters"
      - "Overfitting: add dropout after FC layers, use data augmentation, reduce capacity"

---
