# section_01_20_kmeans.yaml

---
document_info:
  chapter: "01"
  section: "20"
  title: "K-Means Clustering"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 6
  tags: ["k-means", "clustering", "unsupervised-learning", "anomaly-detection", "centroids", "elbow-method"]

# ============================================================================
# SECTION 1.20: K-MEANS CLUSTERING
# ============================================================================

section_01_20_kmeans:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    K-Means is the first unsupervised learning algorithm we'll study. Unlike classification 
    where we have labels, clustering discovers hidden structure in unlabeled data. K-Means 
    partitions data into k clusters by grouping similar samples together, iteratively 
    refining cluster assignments until convergence.
    
    The algorithm is beautifully simple: Start with k random cluster centers (centroids), 
    assign each sample to its nearest centroid, recompute centroids as the mean of assigned 
    samples, repeat until convergence. Despite its simplicity, K-Means is powerful and 
    widely used across domains.
    
    This section covers:
    - Unsupervised learning paradigm (no labels!)
    - K-Means algorithm mechanics
    - Choosing k (elbow method, silhouette score)
    - Initialization strategies (K-Means++)
    - Convergence and optimization objective
    - Limitations and when not to use K-Means
    
    For security, K-Means excels at:
    1. Anomaly detection (find samples far from all clusters)
    2. Attack grouping (cluster similar attacks for analysis)
    3. Baseline profiling (learn normal behavior clusters)
    4. Data exploration (understand structure before labeling)
  
  why_this_matters: |
    Security applications:
    - Network anomaly detection: Cluster normal traffic, flag outliers
    - Malware family discovery: Group similar malware without prior labels
    - User behavior profiling: Identify typical usage patterns
    - Log analysis: Group similar log events automatically
    
    Real deployments:
    - SIEM systems use clustering for log correlation
    - Network IDS cluster traffic patterns
    - SOC analysts use clustering to triage alerts
    - Threat hunters group indicators for campaigns
    
    Why learn K-Means:
    - Foundation for unsupervised learning
    - Most common clustering algorithm
    - Basis for anomaly detection techniques
    - Simple but effective baseline
    - Scales to large datasets (linear in n)
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Unsupervised Learning
  # --------------------------------------------------------------------------
  
  unsupervised_learning:
    
    supervised_vs_unsupervised: |
      Supervised Learning:
      - Input: Features + Labels
      - Goal: Learn mapping X → Y
      - Example: Spam detection (labeled emails)
      
      Unsupervised Learning:
      - Input: Features only (no labels!)
      - Goal: Discover structure/patterns
      - Example: Group similar emails without knowing spam/ham
    
    why_unsupervised: |
      Labels are expensive:
      - Security expert must manually label samples
      - Time-consuming (thousands of samples)
      - Requires domain expertise
      
      Unsupervised helps:
      - Explore data before labeling
      - Find patterns humans might miss
      - Scale to unlabeled data (abundant!)
      - Detect novelty (never-seen-before patterns)
    
    clustering_task: |
      Clustering: Partition data into groups (clusters)
      
      Goal: Samples in same cluster are similar
            Samples in different clusters are dissimilar
      
      Applications:
      - Customer segmentation
      - Document organization
      - Image compression
      - Anomaly detection
  
  # --------------------------------------------------------------------------
  # Core Concept 2: K-Means Algorithm
  # --------------------------------------------------------------------------
  
  kmeans_algorithm:
    
    intuition: |
      Goal: Divide n samples into k clusters
      
      Each cluster represented by centroid (center point)
      Assign each sample to nearest centroid
      
      Iteratively refine:
      1. Assign samples to nearest centroid
      2. Recompute centroids as mean of assigned samples
      3. Repeat until convergence
    
    algorithm_steps: |
      Input: Data X, number of clusters k
      
      1. Initialize: Randomly select k samples as initial centroids
      
      2. Assignment Step:
         For each sample xᵢ:
           Find nearest centroid
           Assign xᵢ to that cluster
      
      3. Update Step:
         For each cluster j:
           Compute new centroid = mean of samples in cluster j
      
      4. Repeat steps 2-3 until:
         - Centroids don't change (converged)
         - Or max iterations reached
      
      Output: Cluster assignments, final centroids
    
    example_iteration: |
      Initial (random centroids):
      Centroid 1: [1, 1]
      Centroid 2: [8, 8]
      
      Data: [2,2], [3,3], [7,7], [8,9], [9,8]
      
      Iteration 1 - Assignment:
      [2,2] → closest to C1
      [3,3] → closest to C1
      [7,7] → closest to C2
      [8,9] → closest to C2
      [9,8] → closest to C2
      
      Iteration 1 - Update:
      C1 = mean([2,2], [3,3]) = [2.5, 2.5]
      C2 = mean([7,7], [8,9], [9,8]) = [8, 8]
      
      Iteration 2 - Assignment:
      (assignments might change or stay same)
      
      Continue until centroids stabilize...
    
    distance_metric: |
      Typically use Euclidean distance:
      d(x, c) = √(Σᵢ (xᵢ - cᵢ)²)
      
      Can also use:
      - Manhattan distance
      - Cosine similarity
      - Custom metrics
    
    convergence: |
      K-Means guaranteed to converge:
      - Each iteration reduces objective function
      - Finite number of possible assignments
      
      May converge to local optimum (not global)
      Solution: Run multiple times with different initializations
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Optimization Objective
  # --------------------------------------------------------------------------
  
  optimization_objective:
    
    objective_function: |
      K-Means minimizes within-cluster sum of squares (inertia):
      
      J = Σⱼ Σᵢ∈Cⱼ ||xᵢ - μⱼ||²
      
      Where:
      - Cⱼ: Samples in cluster j
      - μⱼ: Centroid of cluster j
      - ||xᵢ - μⱼ||²: Squared distance from sample to centroid
      
      Goal: Minimize total squared distance of samples to their centroids
    
    intuition: |
      Want clusters to be:
      - Tight (samples close to their centroid)
      - Compact (low within-cluster variance)
      
      Lower J = better clustering (samples close to centroids)
    
    why_squared_distance: |
      1. Penalizes distant samples more (squared term)
      2. Differentiable (useful for extensions)
      3. Relates to variance (statistical interpretation)
      4. Computationally convenient
    
    algorithm_reduces_objective: |
      Assignment step: Decreases J
      - Moving sample to closer centroid reduces distance
      
      Update step: Decreases J
      - Mean minimizes sum of squared distances
      
      Each iteration: J decreases (or stays same)
      Guaranteed convergence!
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Choosing k (Number of Clusters)
  # --------------------------------------------------------------------------
  
  choosing_k:
    
    the_k_problem: |
      K-Means requires choosing k beforehand
      But we don't know "true" number of clusters!
      
      Too small k: Underclustering (miss structure)
      Too large k: Overclustering (too granular)
    
    elbow_method:
      
      procedure: |
        1. Train K-Means for k = 1, 2, 3, ..., K_max
        2. For each k, compute inertia (objective J)
        3. Plot k vs inertia
        4. Look for "elbow" (bend in curve)
        5. Choose k at elbow point
      
      intuition: |
        Inertia always decreases with more clusters
        (more clusters = samples closer to centroids)
        
        But diminishing returns after optimal k
        
        Elbow = point where adding clusters doesn't help much
      
      example: |
        k=1: Inertia = 1000 (all samples in one cluster)
        k=2: Inertia = 400  (big drop!)
        k=3: Inertia = 200  (big drop!)
        k=4: Inertia = 150  (smaller drop)
        k=5: Inertia = 130  (smaller drop)
        k=6: Inertia = 120  (small drop)
        
        Elbow at k=3 or k=4 (diminishing returns after)
      
      limitation: "Elbow not always clear (subjective)"
    
    silhouette_score:
      
      formula: |
        For each sample i:
        a(i) = average distance to samples in same cluster
        b(i) = average distance to samples in nearest other cluster
        
        Silhouette(i) = (b(i) - a(i)) / max(a(i), b(i))
        
        Range: [-1, +1]
        +1: Sample far from other clusters (good)
        0: Sample on cluster boundary
        -1: Sample closer to other cluster (misassigned)
      
      average_silhouette: |
        Silhouette score = average over all samples
        
        Higher score = better defined clusters
      
      choosing_k: |
        Compute silhouette score for different k
        Pick k with highest average silhouette score
      
      advantage: "More objective than elbow method"
    
    domain_knowledge: |
      Sometimes k known from domain:
      - Network traffic: k = number of protocols
      - Malware: k = number of known families
      - Users: k = number of user roles
      
      Use domain knowledge when available!
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Initialization Strategies
  # --------------------------------------------------------------------------
  
  initialization:
    
    random_initialization: |
      Simple: Randomly pick k samples as initial centroids
      
      Problem: Results depend heavily on initialization
      Bad initialization → poor local optimum
    
    multiple_runs: |
      Solution: Run K-Means multiple times (10-50 runs)
      Pick result with lowest inertia
      
      Computational cost increases, but more reliable
    
    kmeans_plus_plus:
      
      idea: "Choose initial centroids spread out (not clustered)"
      
      algorithm: |
        1. Pick first centroid uniformly at random
        
        2. For each remaining centroid:
           - Compute distance of each sample to nearest existing centroid
           - Pick new centroid with probability proportional to distance²
           - (Samples far from centroids more likely to be chosen)
        
        3. After k centroids chosen, run standard K-Means
      
      benefit: |
        - Better initialization (spread out centroids)
        - Faster convergence
        - Better final clustering (fewer bad local optima)
        - Standard in modern implementations
      
      computational_cost: "O(nk) for initialization (still fast)"
  
  # --------------------------------------------------------------------------
  # Complete Implementation
  # --------------------------------------------------------------------------
  
  complete_implementation: |
    import numpy as np
    
    class KMeans:
        """K-Means clustering from scratch"""
        
        def __init__(self, n_clusters=3, max_iters=100, init='kmeans++', n_init=10):
            """
            Args:
                n_clusters: Number of clusters (k)
                max_iters: Maximum iterations
                init: 'random' or 'kmeans++' initialization
                n_init: Number of times to run with different initializations
            """
            self.n_clusters = n_clusters
            self.max_iters = max_iters
            self.init = init
            self.n_init = n_init
            self.centroids = None
            self.labels = None
            self.inertia = None
        
        def fit(self, X):
            """Fit K-Means on data X"""
            best_inertia = float('inf')
            best_centroids = None
            best_labels = None
            
            # Run multiple times with different initializations
            for _ in range(self.n_init):
                centroids, labels, inertia = self._fit_single(X)
                
                if inertia < best_inertia:
                    best_inertia = inertia
                    best_centroids = centroids
                    best_labels = labels
            
            self.centroids = best_centroids
            self.labels = best_labels
            self.inertia = best_inertia
            
            return self
        
        def _fit_single(self, X):
            """Single K-Means run"""
            n_samples, n_features = X.shape
            
            # Initialize centroids
            if self.init == 'kmeans++':
                centroids = self._kmeans_plus_plus(X)
            else:
                # Random initialization
                indices = np.random.choice(n_samples, self.n_clusters, replace=False)
                centroids = X[indices]
            
            # Iterative refinement
            for iteration in range(self.max_iters):
                # Assignment step
                labels = self._assign_clusters(X, centroids)
                
                # Update step
                new_centroids = self._update_centroids(X, labels)
                
                # Check convergence
                if np.allclose(centroids, new_centroids):
                    break
                
                centroids = new_centroids
            
            # Compute final inertia
            inertia = self._compute_inertia(X, labels, centroids)
            
            return centroids, labels, inertia
        
        def _kmeans_plus_plus(self, X):
            """K-Means++ initialization"""
            n_samples = X.shape[0]
            centroids = []
            
            # First centroid: random
            first_idx = np.random.randint(n_samples)
            centroids.append(X[first_idx])
            
            # Remaining centroids
            for _ in range(1, self.n_clusters):
                # Distance to nearest centroid
                distances = np.array([
                    min(np.linalg.norm(x - c) ** 2 for c in centroids)
                    for x in X
                ])
                
                # Probability proportional to distance²
                probabilities = distances / distances.sum()
                
                # Sample new centroid
                new_idx = np.random.choice(n_samples, p=probabilities)
                centroids.append(X[new_idx])
            
            return np.array(centroids)
        
        def _assign_clusters(self, X, centroids):
            """Assign each sample to nearest centroid"""
            distances = np.array([
                np.linalg.norm(X - centroid, axis=1)
                for centroid in centroids
            ])
            return np.argmin(distances, axis=0)
        
        def _update_centroids(self, X, labels):
            """Compute new centroids as mean of assigned samples"""
            centroids = np.array([
                X[labels == k].mean(axis=0)
                for k in range(self.n_clusters)
            ])
            return centroids
        
        def _compute_inertia(self, X, labels, centroids):
            """Compute within-cluster sum of squares"""
            inertia = 0
            for k in range(self.n_clusters):
                cluster_samples = X[labels == k]
                if len(cluster_samples) > 0:
                    inertia += np.sum((cluster_samples - centroids[k]) ** 2)
            return inertia
        
        def predict(self, X):
            """Assign samples to nearest centroid"""
            return self._assign_clusters(X, self.centroids)
        
        def fit_predict(self, X):
            """Fit and return cluster labels"""
            self.fit(X)
            return self.labels
    
    # ========================================================================
    # USAGE EXAMPLE: NETWORK TRAFFIC CLUSTERING
    # ========================================================================
    
    # Generate synthetic network traffic data
    np.random.seed(42)
    
    # Cluster 1: HTTP traffic (low packet size, high frequency)
    http = np.random.randn(100, 2) * [20, 5] + [50, 100]
    
    # Cluster 2: SSH traffic (medium packet size, low frequency)
    ssh = np.random.randn(100, 2) * [30, 10] + [200, 30]
    
    # Cluster 3: Video streaming (high packet size, high frequency)
    video = np.random.randn(100, 2) * [50, 20] + [500, 150]
    
    # Combine
    X = np.vstack([http, ssh, video])
    
    # Shuffle
    indices = np.random.permutation(len(X))
    X = X[indices]
    
    # Fit K-Means
    kmeans = KMeans(n_clusters=3, init='kmeans++', n_init=10)
    labels = kmeans.fit_predict(X)
    
    print("K-Means Clustering")
    print(f"Number of clusters: {kmeans.n_clusters}")
    print(f"Inertia: {kmeans.inertia:.2f}")
    print(f"Cluster sizes: {np.bincount(labels)}")
    
    print("\nCluster centroids:")
    for i, centroid in enumerate(kmeans.centroids):
        print(f"  Cluster {i}: {centroid}")
    
    # Elbow method to choose k
    print("\nElbow method:")
    inertias = []
    k_range = range(1, 10)
    for k in k_range:
        km = KMeans(n_clusters=k, n_init=5)
        km.fit(X)
        inertias.append(km.inertia)
        print(f"k={k}: inertia={km.inertia:.2f}")
    
    # Anomaly detection: Find samples far from all centroids
    distances_to_centroids = np.array([
        np.linalg.norm(X - centroid, axis=1)
        for centroid in kmeans.centroids
    ])
    min_distances = np.min(distances_to_centroids, axis=0)
    
    threshold = np.percentile(min_distances, 95)  # Top 5% as anomalies
    anomalies = min_distances > threshold
    
    print(f"\nAnomalies detected: {anomalies.sum()} samples")
  
  # --------------------------------------------------------------------------
  # Security Applications
  # --------------------------------------------------------------------------
  
  security_applications:
    
    anomaly_detection:
      
      approach: |
        1. Cluster normal traffic/behavior
        2. For new sample, compute distance to nearest centroid
        3. If distance > threshold → anomaly
      
      threshold_setting: |
        - Train on normal data only
        - Set threshold at 95th percentile of distances
        - Samples beyond threshold flagged as anomalies
      
      example: |
        Network traffic:
        - Cluster normal HTTP, SSH, DNS traffic
        - New connection: distance to all centroids > threshold
        - Flag as suspicious (unknown traffic pattern)
      
      advantage: "No labeled anomalies needed (unsupervised)"
    
    attack_grouping:
      
      scenario: |
        SOC receives 10,000 alerts
        Many duplicates or related
        Need to group for triage
      
      approach: |
        Extract features from alerts (IPs, ports, signatures)
        Cluster into k groups
        Analyst reviews one alert per cluster
      
      benefit: |
        10,000 alerts → 50 clusters
        Review 50 instead of 10,000 (200x speedup!)
    
    baseline_profiling:
      
      user_behavior: |
        Cluster typical user activities:
        - Cluster 1: Morning email check
        - Cluster 2: Daytime work (docs, code)
        - Cluster 3: Evening personal browsing
        
        New activity: Far from all clusters → investigate
      
      network_traffic: |
        Profile normal traffic patterns per subnet
        Detect deviations from baseline
        
        Cluster centroids = expected traffic profile
    
    malware_family_discovery:
      
      scenario: |
        10,000 malware samples, unknown families
        Want to group similar malware
      
      approach: |
        Extract features (API calls, byte n-grams)
        Cluster into families
        Security analyst labels families
      
      result: |
        Discovered 20 distinct malware families
        Can now classify new samples by similarity
  
  # --------------------------------------------------------------------------
  # Advantages and Limitations
  # --------------------------------------------------------------------------
  
  advantages_limitations:
    
    advantages:
      
      simplicity:
        - "Easy to understand and implement"
        - "Few hyperparameters (just k)"
      
      scalability:
        - "Linear in n: O(nkd × iterations)"
        - "Works for large datasets"
        - "Can use mini-batch for huge data"
      
      speed:
        - "Fast convergence (typically <10 iterations)"
        - "Efficient assignment step"
      
      interpretability:
        - "Cluster centroids interpretable"
        - "Can visualize clusters (2D/3D)"
        - "Explain assignment (nearest centroid)"
    
    limitations:
      
      requires_k:
        problem: "Must specify k beforehand"
        
        mitigation: |
          - Elbow method
          - Silhouette score
          - Domain knowledge
      
      spherical_clusters:
        assumption: "Assumes spherical clusters (equal variance)"
        
        fails_on: |
          - Elongated clusters
          - Different sized clusters
          - Non-convex clusters
        
        example: |
          Concentric circles: K-Means fails
          (No linear boundary can separate)
        
        alternative: "Gaussian Mixture Models, DBSCAN, Hierarchical"
      
      sensitive_to_outliers:
        problem: "Mean sensitive to outliers"
        
        impact: "Single outlier can shift centroid significantly"
        
        solution: |
          - Remove outliers first
          - Use K-Medoids (median instead of mean)
      
      local_optima:
        issue: "Converges to local optimum (not global)"
        
        mitigation: |
          - Multiple runs (n_init=10)
          - K-Means++ initialization
      
      equal_sized_clusters:
        bias: "Tends toward equal-sized clusters"
        
        problem: "Real data may have very different cluster sizes"
        
        alternative: "Hierarchical clustering, DBSCAN"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "K-Means: Partition data into k clusters (unsupervised)"
      - "Algorithm: Assign → Update → Repeat until convergence"
      - "Optimizes: Within-cluster sum of squares (inertia)"
      - "Guaranteed convergence (to local optimum)"
      - "Choosing k: Elbow method or silhouette score"
      - "K-Means++: Better initialization (spread centroids)"
    
    practical_skills:
      - "Implement K-Means from scratch"
      - "Use elbow method to choose k"
      - "Apply K-Means++ initialization"
      - "Detect anomalies (distance to nearest centroid)"
      - "Interpret cluster centroids"
    
    security_mindset:
      - "Unsupervised: No labels needed (explore unknown threats)"
      - "Anomaly detection: Flag samples far from clusters"
      - "Attack grouping: Cluster similar alerts for triage"
      - "Baseline profiling: Learn normal behavior patterns"
      - "Scalable: Handle large security datasets"
    
    remember_this:
      - "K-Means = iterative refinement of cluster centers"
      - "Requires choosing k (use elbow or silhouette)"
      - "K-Means++ initialization (standard practice)"
      - "Anomaly = far from all cluster centroids"
      - "Assumes spherical clusters (limitation)"
    
    next_steps:
      - "Next section: Dimensionality Reduction (PCA)"
      - "You now understand clustering (unsupervised learning)"
      - "Foundation for advanced clustering methods (GMM, DBSCAN)"

---
