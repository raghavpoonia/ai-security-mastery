# section_01_15_hyperparameter_tuning.yaml

---
document_info:
  chapter: "01"
  section: "15"
  title: "Hyperparameter Tuning"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 6
  tags: ["hyperparameter-tuning", "grid-search", "random-search", "bayesian-optimization", "cross-validation", "automl"]

# ============================================================================
# SECTION 1.15: HYPERPARAMETER TUNING
# ============================================================================

section_01_15_hyperparameter_tuning:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    You've built a model, but it's not performing well. You suspect the learning rate 
    is too high, regularization too weak, or batch size suboptimal. How do you find the 
    right values? Trial and error wastes time. You need systematic hyperparameter tuning.
    
    Hyperparameters are the knobs you turn to control learning: learning rate, 
    regularization strength, batch size, number of layers, etc. Unlike model parameters 
    (weights, biases) which are learned from data, hyperparameters must be set before 
    training. Choosing them well is the difference between a model that works and one 
    that doesn't.
    
    This section covers systematic approaches to hyperparameter optimization:
    - Manual tuning: Educated guesses based on experience
    - Grid search: Exhaustive search over parameter grid
    - Random search: Sample random combinations (often better than grid!)
    - Bayesian optimization: Smart search using past results
    - Best practices and common pitfalls
    
    For security, efficient tuning is critical because:
    1. Limited time to deploy detectors against new attacks
    2. Expensive to retrain large models repeatedly
    3. Need optimal performance (lives/money at stake)
    4. Must tune multiple models (ensemble approaches)
  
  why_this_matters: |
    Security context:
    - Malware detector: Wrong learning rate → model doesn't converge → 50% detection
    - Fraud detector: Weak regularization → overfits → production failure
    - IDS: Wrong threshold → too many false positives → analysts overwhelmed
    
    Time constraints:
    - New attack campaign detected → need detector in 24 hours
    - No time for weeks of manual tuning
    - Systematic approach finds good hyperparameters in hours
    
    Resource constraints:
    - Training expensive (GPU time, analyst time)
    - Can't afford 1000 training runs
    - Need efficient search strategy
    
    Real impact:
    - Good tuning: 88% → 94% detection rate (manual → systematic)
    - Bad tuning: 70% detection with 20% FP rate (unusable)
    - Optimal tuning: 93% detection with 2% FP rate (production-ready)
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Parameters vs Hyperparameters
  # --------------------------------------------------------------------------
  
  parameters_vs_hyperparameters:
    
    parameters:
      definition: "Values learned from data during training"
      
      examples:
        - "Weights (w): Learned via gradient descent"
        - "Biases (b): Learned via gradient descent"
        - "Cluster centers (k-means): Learned via iteration"
      
      how_learned: "Automatically optimized using training data"
      
      typical_count: "Thousands to millions (one per connection in neural network)"
    
    hyperparameters:
      definition: "Values set BEFORE training that control learning process"
      
      examples:
        training_hyperparameters:
          - "Learning rate (α): How big gradient descent steps are"
          - "Batch size: How many samples per update"
          - "Number of epochs: How long to train"
        
        regularization_hyperparameters:
          - "Regularization strength (λ): L1/L2 penalty"
          - "Dropout rate: Fraction of neurons to drop"
          - "Early stopping patience: How many epochs to wait"
        
        architecture_hyperparameters:
          - "Number of layers: Network depth"
          - "Number of neurons per layer: Network width"
          - "Activation functions: ReLU, sigmoid, tanh"
        
        preprocessing_hyperparameters:
          - "Feature scaling method: Standardization vs normalization"
          - "Missing value strategy: Mean, median, mode"
      
      how_set: "Manually chosen or found through hyperparameter search"
      
      typical_count: "5-20 (must tune each)"
    
    why_hyperparameters_matter: |
      Same model, different hyperparameters → vastly different performance
      
      Example: Logistic regression
      
      Configuration A:
      - Learning rate: 0.001
      - Regularization: λ=0.1
      - Epochs: 1000
      Result: 94% test accuracy
      
      Configuration B:
      - Learning rate: 1.0 (too high!)
      - Regularization: λ=0.001 (too weak!)
      - Epochs: 100 (too few!)
      Result: 65% test accuracy (diverged)
      
      Same algorithm, 29 percentage point difference!
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Grid Search
  # --------------------------------------------------------------------------
  
  grid_search:
    
    what_is_grid_search: |
      Grid search: Try all combinations of hyperparameter values
      
      Process:
      1. Define grid: Specify values to try for each hyperparameter
      2. Generate combinations: Cartesian product of all values
      3. Train model for each combination
      4. Evaluate on validation set
      5. Pick combination with best validation performance
    
    example: |
      Hyperparameters:
      - Learning rate: [0.001, 0.01, 0.1]
      - Regularization λ: [0.01, 0.1, 1.0]
      
      Grid: 3 × 3 = 9 combinations
      
      Train 9 models:
      1. α=0.001, λ=0.01
      2. α=0.001, λ=0.1
      3. α=0.001, λ=1.0
      4. α=0.01, λ=0.01
      5. α=0.01, λ=0.1   ← Best: 94% val accuracy
      6. α=0.01, λ=1.0
      7. α=0.1, λ=0.01
      8. α=0.1, λ=0.1
      9. α=0.1, λ=1.0
      
      Choose: α=0.01, λ=0.1
    
    numpy_implementation: |
      import numpy as np
      from itertools import product
      
      def grid_search(X_train, y_train, X_val, y_val, 
                     learning_rates, lambda_regs, model_class):
          """
          Perform grid search over hyperparameters
          
          Args:
              learning_rates: List of learning rates to try
              lambda_regs: List of regularization strengths
              model_class: Model class to instantiate
          
          Returns:
              best_params: Dict with best hyperparameters
              results: List of all results
          """
          results = []
          best_score = 0
          best_params = {}
          
          # Generate all combinations
          param_combinations = list(product(learning_rates, lambda_regs))
          
          print(f"Grid search: {len(param_combinations)} combinations")
          print("=" * 60)
          
          for i, (lr, lam) in enumerate(param_combinations, 1):
              # Train model with these hyperparameters
              model = model_class(learning_rate=lr, lambda_reg=lam)
              model.fit(X_train, y_train)
              
              # Evaluate on validation set
              val_score = model.score(X_val, y_val)
              
              # Record results
              result = {
                  'learning_rate': lr,
                  'lambda_reg': lam,
                  'val_score': val_score
              }
              results.append(result)
              
              # Update best
              if val_score > best_score:
                  best_score = val_score
                  best_params = {'learning_rate': lr, 'lambda_reg': lam}
              
              print(f"{i}. lr={lr:.4f}, λ={lam:.4f} → Val: {val_score:.2%}")
          
          print("=" * 60)
          print(f"Best hyperparameters:")
          print(f"  Learning rate: {best_params['learning_rate']}")
          print(f"  Lambda: {best_params['lambda_reg']}")
          print(f"  Validation score: {best_score:.2%}")
          
          return best_params, results
      
      # Example usage
      learning_rates = [0.001, 0.01, 0.1]
      lambda_regs = [0.01, 0.1, 1.0]
      
      best_params, results = grid_search(
          X_train, y_train, X_val, y_val,
          learning_rates, lambda_regs,
          LogisticRegressionL2
      )
    
    pros:
      - "Exhaustive: Guaranteed to find best combination in grid"
      - "Reproducible: Always same results"
      - "Parallelizable: Can train all models simultaneously"
      - "Simple to implement"
    
    cons:
      - "Exponential cost: Doubles compute for each hyperparameter"
      - "Wastes compute on bad regions of space"
      - "Curse of dimensionality: 10 hyperparameters, 3 values each = 59,049 combinations!"
      - "Granularity: May miss optimal value between grid points"
    
    when_to_use: |
      Good for:
      - Small number of hyperparameters (2-3)
      - Fast training (seconds per model)
      - Need reproducible results
      
      Bad for:
      - Many hyperparameters (>3)
      - Slow training (hours per model)
      - Need to find precise optimum
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Random Search
  # --------------------------------------------------------------------------
  
  random_search:
    
    what_is_random_search: |
      Random search: Sample random hyperparameter combinations
      
      Process:
      1. Define distributions for each hyperparameter
      2. Sample N random combinations
      3. Train and evaluate each
      4. Pick best
      
      Key insight: Random often better than grid!
    
    why_random_beats_grid: |
      Problem with grid: Wastes trials on bad hyperparameters
      
      Example: 2 hyperparameters
      - Hyperparameter A: Very important (affects performance)
      - Hyperparameter B: Not important (minimal effect)
      
      Grid search with 3 values each (9 trials):
      - Tests 3 values of A
      - Tests 3 values of B (mostly wasted)
      
      Random search with 9 trials:
      - Tests 9 different values of A (explores more!)
      - Tests 9 different values of B
      
      Random explores important dimensions better
    
    visualization: |
      Grid Search (9 trials):
      
      A: [v1, v2, v3]  → Only 3 values of important parameter A
      B: [v1, v2, v3]
      
      X---X---X
      |   |   |
      X---X---X  ← 9 combinations, but only 3 unique A values
      |   |   |
      X---X---X
      
      Random Search (9 trials):
      
          X
      X       X
            X     X  ← 9 trials, 9 unique A values!
        X   X
      X         X
    
    numpy_implementation: |
      def random_search(X_train, y_train, X_val, y_val,
                       n_trials, model_class, random_state=None):
          """
          Perform random search over hyperparameters
          
          Args:
              n_trials: Number of random combinations to try
          """
          if random_state:
              np.random.seed(random_state)
          
          results = []
          best_score = 0
          best_params = {}
          
          print(f"Random search: {n_trials} trials")
          print("=" * 60)
          
          for trial in range(n_trials):
              # Sample random hyperparameters
              # Learning rate: Log-uniform between 0.0001 and 1.0
              lr = 10 ** np.random.uniform(-4, 0)
              
              # Lambda: Log-uniform between 0.001 and 10
              lam = 10 ** np.random.uniform(-3, 1)
              
              # Train model
              model = model_class(learning_rate=lr, lambda_reg=lam)
              model.fit(X_train, y_train)
              
              # Evaluate
              val_score = model.score(X_val, y_val)
              
              # Record
              result = {
                  'learning_rate': lr,
                  'lambda_reg': lam,
                  'val_score': val_score
              }
              results.append(result)
              
              # Update best
              if val_score > best_score:
                  best_score = val_score
                  best_params = {'learning_rate': lr, 'lambda_reg': lam}
              
              print(f"{trial+1}. lr={lr:.6f}, λ={lam:.6f} → Val: {val_score:.2%}")
          
          print("=" * 60)
          print(f"Best hyperparameters:")
          print(f"  Learning rate: {best_params['learning_rate']:.6f}")
          print(f"  Lambda: {best_params['lambda_reg']:.6f}")
          print(f"  Validation score: {best_score:.2%}")
          
          return best_params, results
      
      # Example usage
      best_params, results = random_search(
          X_train, y_train, X_val, y_val,
          n_trials=20,
          model_class=LogisticRegressionL2,
          random_state=42
      )
    
    sampling_strategies:
      
      uniform_sampling:
        when: "Parameter has no preferred region"
        example: "Dropout rate: uniform between 0.1 and 0.5"
        code: "dropout = np.random.uniform(0.1, 0.5)"
      
      log_uniform_sampling:
        when: "Parameter spans multiple orders of magnitude"
        example: "Learning rate: 0.0001 to 1.0"
        why: "Equal probability of 0.001, 0.01, 0.1"
        code: "lr = 10 ** np.random.uniform(-4, 0)"
      
      integer_sampling:
        when: "Discrete parameter (layers, neurons)"
        example: "Number of layers: 1 to 10"
        code: "n_layers = np.random.randint(1, 11)"
    
    pros:
      - "More efficient than grid (explores more values)"
      - "Scales to many hyperparameters"
      - "Can run any budget (10, 100, 1000 trials)"
      - "Empirically works better than grid"
    
    cons:
      - "Non-deterministic (different results each run)"
      - "May miss optimal combination"
      - "No guarantee of finding best"
    
    best_practices: |
      1. Use log-uniform for learning rate, regularization
      2. Run multiple random searches (different seeds)
      3. Start with coarse search (20 trials), refine best region
      4. Budget: Use 2-3x number of grid points
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Coarse-to-Fine Search
  # --------------------------------------------------------------------------
  
  coarse_to_fine:
    
    strategy: |
      Two-stage search for efficiency and precision
      
      Stage 1 (Coarse): Broad search over wide range
      - Identify promising regions
      - Use random or coarse grid
      
      Stage 2 (Fine): Narrow search around best region
      - Refine within promising region
      - Use finer grid or more random samples
    
    example: |
      Finding optimal learning rate and lambda:
      
      Stage 1 - Coarse Random Search:
      Learning rate: [0.0001, 1.0]
      Lambda: [0.001, 10]
      
      Random sample 20 combinations
      Best found: lr=0.015, λ=0.12 (Val: 92%)
      
      Stage 2 - Fine Grid Search:
      Learning rate: [0.01, 0.015, 0.02] (around 0.015)
      Lambda: [0.08, 0.12, 0.16] (around 0.12)
      
      Grid of 9 combinations
      Best found: lr=0.015, λ=0.16 (Val: 94%)
      
      Total: 29 trials (vs 400+ for fine grid over full range)
    
    numpy_implementation: |
      def coarse_to_fine_search(X_train, y_train, X_val, y_val, model_class):
          """Two-stage coarse-to-fine hyperparameter search"""
          
          # Stage 1: Coarse random search
          print("STAGE 1: Coarse Random Search")
          print("=" * 60)
          
          best_coarse, _ = random_search(
              X_train, y_train, X_val, y_val,
              n_trials=20,
              model_class=model_class
          )
          
          lr_coarse = best_coarse['learning_rate']
          lam_coarse = best_coarse['lambda_reg']
          
          print(f"\nBest from coarse search:")
          print(f"  lr={lr_coarse:.6f}, λ={lam_coarse:.6f}")
          
          # Stage 2: Fine grid search around best
          print("\n" + "=" * 60)
          print("STAGE 2: Fine Grid Search")
          print("=" * 60)
          
          # Define fine grid around best coarse values
          lr_fine = [
              lr_coarse * 0.5,
              lr_coarse,
              lr_coarse * 2.0
          ]
          
          lam_fine = [
              lam_coarse * 0.5,
              lam_coarse,
              lam_coarse * 2.0
          ]
          
          best_fine, _ = grid_search(
              X_train, y_train, X_val, y_val,
              lr_fine, lam_fine,
              model_class
          )
          
          return best_fine
      
      # Usage
      best_params = coarse_to_fine_search(
          X_train, y_train, X_val, y_val,
          LogisticRegressionL2
      )
    
    benefits:
      - "Efficient: Explores wide range, then refines"
      - "Precise: Fine search finds local optimum"
      - "Practical: Works with limited compute budget"
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Hyperparameter Importance
  # --------------------------------------------------------------------------
  
  hyperparameter_importance:
    
    not_all_equal: |
      Some hyperparameters matter much more than others
      
      High importance:
      - Learning rate: 10x impact on performance
      - Regularization: 5x impact
      
      Low importance:
      - Batch size: 1.2x impact
      - Optimizer details: Minimal impact
      
      Strategy: Focus tuning on high-importance hyperparameters
    
    typical_importance_ranking:
      
      very_high:
        - "Learning rate (most important!)"
        - "Regularization strength"
        - "Model architecture (layers, neurons)"
      
      high:
        - "Batch size"
        - "Dropout rate"
        - "Early stopping patience"
      
      medium:
        - "Optimizer choice (Adam vs SGD)"
        - "Learning rate schedule"
      
      low:
        - "Adam beta parameters"
        - "Batch normalization momentum"
        - "Random seed"
    
    tuning_priority: |
      Limited compute budget? Tune in order:
      
      1. Learning rate (ALWAYS tune this!)
      2. Regularization (L2 lambda, dropout rate)
      3. Architecture (if applicable)
      4. Batch size
      5. Everything else (if budget allows)
    
    default_values: |
      Start with reasonable defaults, tune selectively:
      
      Defaults that usually work:
      - Learning rate: 0.01 (but TUNE this!)
      - Batch size: 32 or 64
      - Adam betas: (0.9, 0.999)
      - Dropout: 0.5
      
      Must tune:
      - Regularization strength (problem-dependent)
      - Learning rate (most critical)
  
  # --------------------------------------------------------------------------
  # Practical Implementation: Complete Tuning Pipeline
  # --------------------------------------------------------------------------
  
  complete_tuning_pipeline: |
    import numpy as np
    from itertools import product
    
    class HyperparameterTuner:
        """Complete hyperparameter tuning pipeline"""
        
        def __init__(self, model_class, X_train, y_train, X_val, y_val):
            self.model_class = model_class
            self.X_train = X_train
            self.y_train = y_train
            self.X_val = X_val
            self.y_val = y_val
            self.results = []
        
        def grid_search(self, param_grid):
            """Grid search over parameter grid"""
            param_names = list(param_grid.keys())
            param_values = list(param_grid.values())
            
            combinations = list(product(*param_values))
            print(f"Grid search: {len(combinations)} combinations")
            
            for i, combo in enumerate(combinations):
                params = dict(zip(param_names, combo))
                score = self._evaluate_params(params)
                
                self.results.append({**params, 'val_score': score})
                print(f"{i+1}/{len(combinations)}: {params} → {score:.2%}")
            
            return self._get_best_params()
        
        def random_search(self, param_distributions, n_trials, random_state=None):
            """Random search over parameter distributions"""
            if random_state:
                np.random.seed(random_state)
            
            print(f"Random search: {n_trials} trials")
            
            for i in range(n_trials):
                # Sample parameters
                params = {}
                for name, dist in param_distributions.items():
                    if dist['type'] == 'log_uniform':
                        params[name] = 10 ** np.random.uniform(
                            dist['low'], dist['high']
                        )
                    elif dist['type'] == 'uniform':
                        params[name] = np.random.uniform(
                            dist['low'], dist['high']
                        )
                    elif dist['type'] == 'choice':
                        params[name] = np.random.choice(dist['values'])
                
                score = self._evaluate_params(params)
                
                self.results.append({**params, 'val_score': score})
                print(f"{i+1}/{n_trials}: {params} → {score:.2%}")
            
            return self._get_best_params()
        
        def coarse_to_fine(self, param_distributions, n_coarse=20, n_fine_per_param=3):
            """Two-stage coarse-to-fine search"""
            print("Stage 1: Coarse random search")
            best_coarse = self.random_search(param_distributions, n_coarse)
            
            print("\nStage 2: Fine grid search")
            # Create fine grid around best coarse
            param_grid = {}
            for name, value in best_coarse.items():
                if name != 'val_score':
                    param_grid[name] = [
                        value * 0.5,
                        value,
                        value * 2.0
                    ]
            
            return self.grid_search(param_grid)
        
        def _evaluate_params(self, params):
            """Train model with given parameters and return validation score"""
            model = self.model_class(**params)
            model.fit(self.X_train, self.y_train)
            return model.score(self.X_val, self.y_val)
        
        def _get_best_params(self):
            """Return parameters with best validation score"""
            best = max(self.results, key=lambda x: x['val_score'])
            return best
        
        def plot_results(self, param_name):
            """Plot validation score vs parameter"""
            import matplotlib.pyplot as plt
            
            values = [r[param_name] for r in self.results]
            scores = [r['val_score'] for r in self.results]
            
            plt.figure(figsize=(10, 6))
            plt.scatter(values, scores, alpha=0.6)
            plt.xlabel(param_name)
            plt.ylabel('Validation Score')
            plt.title(f'Validation Score vs {param_name}')
            plt.xscale('log')
            plt.grid(True)
            plt.show()
    
    # ========================================================================
    # USAGE EXAMPLES
    # ========================================================================
    
    # Initialize tuner
    tuner = HyperparameterTuner(
        LogisticRegressionL2,
        X_train, y_train,
        X_val, y_val
    )
    
    # Example 1: Grid search
    param_grid = {
        'learning_rate': [0.001, 0.01, 0.1],
        'lambda_reg': [0.01, 0.1, 1.0]
    }
    best_grid = tuner.grid_search(param_grid)
    
    # Example 2: Random search
    param_distributions = {
        'learning_rate': {'type': 'log_uniform', 'low': -4, 'high': 0},
        'lambda_reg': {'type': 'log_uniform', 'low': -3, 'high': 1}
    }
    best_random = tuner.random_search(param_distributions, n_trials=20)
    
    # Example 3: Coarse-to-fine
    best_final = tuner.coarse_to_fine(param_distributions, n_coarse=20)
  
  # --------------------------------------------------------------------------
  # Security-Specific Considerations
  # --------------------------------------------------------------------------
  
  security_considerations:
    
    tuning_for_recall: |
      Security priority: Minimize false negatives
      
      Standard tuning: Maximize validation accuracy
      Problem: Ignores FP/FN trade-off
      
      Security tuning: Maximize recall subject to precision constraint
      
      Modified objective:
      - Find hyperparameters where recall ≥ 95%
      - Among those, maximize precision
    
    cost_sensitive_tuning: |
      Incorporate operational costs in tuning
      
      Define cost function:
      Cost = C_FN × FN_count + C_FP × FP_count
      
      Example: Fraud detection
      - C_FN = $1000 (missed fraud)
      - C_FP = $5 (investigation cost)
      
      Tune to minimize cost, not maximize accuracy
    
    temporal_validation: |
      Security data evolves over time
      
      Tuning strategy:
      - Use temporal train/val split
      - Tune on past → validate on future
      - Ensures hyperparameters work on new attacks
    
    adversarial_robustness: |
      Tune for robustness to adversarial examples
      
      Approach:
      - Generate adversarial examples during tuning
      - Evaluate on adversarial validation set
      - Pick hyperparameters robust to perturbations
  
  # --------------------------------------------------------------------------
  # Common Mistakes
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    mistake_1:
      error: "Tuning on test set"
      
      problem: |
        Using test set for hyperparameter selection
        = Data leakage
        = Overoptimistic evaluation
      
      correct: "Always tune on validation set, test set touched ONCE"
    
    mistake_2:
      error: "Grid search over too many hyperparameters"
      
      problem: |
        5 hyperparameters, 5 values each = 3,125 combinations
        If each takes 10 minutes → 21 days of compute!
      
      solution: "Use random search or focus on high-importance hyperparameters"
    
    mistake_3:
      error: "Not using log-scale for learning rate"
      
      problem: |
        Learning rate: [0.0001, 0.001, 0.01, 0.1, 1.0]
        Uniform spacing misses important region
      
      correct: "Log-uniform: Equal spacing on log scale"
    
    mistake_4:
      error: "Ignoring statistical significance"
      
      problem: |
        Configuration A: 92.3% validation accuracy
        Configuration B: 92.5% validation accuracy
        Conclusion: B is better?
        
        Reality: Difference too small (within noise)
      
      solution: "Use confidence intervals, repeated runs, or cross-validation"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Hyperparameters control learning, must be set before training"
      - "Grid search: Exhaustive but expensive (exponential cost)"
      - "Random search: Often better than grid (explores more values)"
      - "Coarse-to-fine: Efficient two-stage approach"
      - "Learning rate most important hyperparameter (always tune!)"
    
    practical_skills:
      - "Implement grid and random search in NumPy"
      - "Use log-uniform sampling for learning rate, regularization"
      - "Prioritize high-importance hyperparameters"
      - "Apply coarse-to-fine for efficiency"
      - "Always tune on validation set, never test set"
    
    security_mindset:
      - "Tune for recall (minimize false negatives) not just accuracy"
      - "Incorporate operational costs in tuning objective"
      - "Use temporal validation (past → future)"
      - "Efficient tuning critical (limited time against new attacks)"
      - "Tune ensemble models for robustness"
    
    remember_this:
      - "Learning rate is THE hyperparameter (tune first!)"
      - "Random search > Grid search (same budget, better results)"
      - "Validation set for tuning, test set for final eval"
      - "Focus on high-importance hyperparameters"
    
    next_steps:
      - "Next section: Introduction to algorithms (decision trees, k-NN, SVM)"
      - "You now have complete ML workflow: data → preprocess → train → tune → evaluate"
      - "Training methodology complete! Moving to algorithms."

---
