# section_02_14_training_dynamics.yaml

---
document_info:
  chapter: "02"
  section: "14"
  title: "Training Dynamics and Hyperparameter Tuning"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["training-dynamics", "hyperparameter-tuning", "early-stopping", "data-augmentation", "learning-curves", "grid-search"]

# ============================================================================
# SECTION 02_14: TRAINING DYNAMICS AND HYPERPARAMETER TUNING
# ============================================================================

section_02_14_training_dynamics:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Training neural networks is an iterative process where you monitor learning
    curves, diagnose problems, tune hyperparameters, and apply regularization
    techniques strategically. This section brings together everything from
    Sections 02_10-02_13 into a coherent training workflow.
    
    You'll master reading learning curves to diagnose overfitting/underfitting,
    implement early stopping (simplest regularization - no hyperparameters),
    apply data augmentation to artificially expand training data, systematically
    tune hyperparameters (learning rate, regularization strength, dropout rate),
    combine multiple regularization techniques effectively, and build complete
    training pipelines that produce production-ready models.
    
    This is where theory meets practice: taking all the techniques you've
    learned and orchestrating them into a training system that works.
  
  learning_objectives:
    
    conceptual:
      - "Understand training dynamics: how loss evolves, when to stop"
      - "Read learning curves to diagnose problems"
      - "Know hyperparameter importance ranking (learning rate > all)"
      - "Understand when to combine vs when to simplify"
      - "Grasp early stopping as implicit regularization"
      - "Connect data augmentation to generalization"
    
    practical:
      - "Implement early stopping with patience mechanism"
      - "Build data augmentation pipelines for images/text"
      - "Perform grid search and random search systematically"
      - "Tune hyperparameters on validation set (never test set)"
      - "Combine L2 + dropout + early stopping effectively"
      - "Build complete training framework with all techniques"
    
    security_focused:
      - "Early stopping can preserve backdoors (stops before overwrite)"
      - "Data augmentation dilutes poisoning attacks (10x augmentation = 10x dilution)"
      - "Hyperparameter settings affect adversarial robustness"
      - "Validation set bias in hyperparameter tuning"
  
  prerequisites:
    - "Sections 02_10-02_13 (all regularization techniques)"
    - "Understanding of train/val/test split"
    - "Experience with basic training loops"
  
  # --------------------------------------------------------------------------
  # Topic 1: Learning Curves and Diagnostics
  # --------------------------------------------------------------------------
  
  learning_curves:
    
    what_learning_curves_reveal:
      
      healthy_training: |
        Characteristics:
        - Both train and val loss decrease smoothly
        - Small gap between train and val (<0.05)
        - Both curves converging to similar value
        - No sudden spikes or oscillations
        
        Example:
        Epoch | Train Loss | Val Loss | Gap
        ------|------------|----------|-----
           10 |   0.45     |   0.48   | 0.03
           20 |   0.28     |   0.30   | 0.02
           30 |   0.15     |   0.17   | 0.02
           40 |   0.09     |   0.10   | 0.01
        
        Diagnosis: Training well, model generalizing
      
      overfitting: |
        Characteristics:
        - Train loss keeps decreasing
        - Val loss plateaus or increases
        - Large and growing gap (>0.15)
        
        Example:
        Epoch | Train Loss | Val Loss | Gap
        ------|------------|----------|-----
           10 |   0.45     |   0.48   | 0.03
           20 |   0.25     |   0.35   | 0.10
           30 |   0.10     |   0.42   | 0.32
           40 |   0.03     |   0.55   | 0.52
        
        Diagnosis: Memorizing training data, need regularization
      
      underfitting: |
        Characteristics:
        - Both losses high
        - Both losses plateau early
        - Small gap but poor performance
        
        Example:
        Epoch | Train Loss | Val Loss | Gap
        ------|------------|----------|-----
           10 |   0.65     |   0.67   | 0.02
           20 |   0.62     |   0.65   | 0.03
           30 |   0.61     |   0.64   | 0.03
           40 |   0.61     |   0.64   | 0.03
        
        Diagnosis: Model too simple, increase capacity
      
      unstable_training: |
        Characteristics:
        - Loss oscillates wildly
        - Sudden spikes
        - NaN or Inf values
        
        Diagnosis: Learning rate too high or gradient explosion
    
    implementation: |
      import matplotlib.pyplot as plt
      
      def plot_learning_curves(history, save_path=None):
          """
          Visualize training and validation curves with diagnostics.
          
          Parameters:
          - history: dict with 'train_loss', 'val_loss', 'train_acc', 'val_acc'
          - save_path: optional path to save figure
          """
          epochs = range(1, len(history['train_loss']) + 1)
          
          fig, axes = plt.subplots(2, 2, figsize=(14, 10))
          
          # Loss curves
          ax = axes[0, 0]
          ax.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training Loss')
          ax.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Validation Loss')
          ax.set_xlabel('Epoch', fontsize=12)
          ax.set_ylabel('Loss', fontsize=12)
          ax.set_title('Loss Curves', fontsize=14, fontweight='bold')
          ax.legend(fontsize=11)
          ax.grid(True, alpha=0.3)
          
          # Accuracy curves
          ax = axes[0, 1]
          ax.plot(epochs, history['train_acc'], 'b-', linewidth=2, label='Training Accuracy')
          ax.plot(epochs, history['val_acc'], 'r-', linewidth=2, label='Validation Accuracy')
          ax.set_xlabel('Epoch', fontsize=12)
          ax.set_ylabel('Accuracy', fontsize=12)
          ax.set_title('Accuracy Curves', fontsize=14, fontweight='bold')
          ax.legend(fontsize=11)
          ax.grid(True, alpha=0.3)
          
          # Train-Val Gap over time
          ax = axes[1, 0]
          gaps = [v - t for t, v in zip(history['train_loss'], history['val_loss'])]
          ax.plot(epochs, gaps, 'g-', linewidth=2)
          ax.axhline(y=0.05, color='orange', linestyle='--', label='Warning threshold (0.05)')
          ax.axhline(y=0.15, color='red', linestyle='--', label='Danger threshold (0.15)')
          ax.set_xlabel('Epoch', fontsize=12)
          ax.set_ylabel('Val Loss - Train Loss', fontsize=12)
          ax.set_title('Overfitting Gap', fontsize=14, fontweight='bold')
          ax.legend(fontsize=10)
          ax.grid(True, alpha=0.3)
          
          # Gradient of validation loss (rate of change)
          ax = axes[1, 1]
          val_gradient = np.gradient(history['val_loss'])
          ax.plot(epochs, val_gradient, 'purple', linewidth=2)
          ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
          ax.set_xlabel('Epoch', fontsize=12)
          ax.set_ylabel('∆(Val Loss) / ∆(Epoch)', fontsize=12)
          ax.set_title('Validation Loss Gradient', fontsize=14, fontweight='bold')
          ax.grid(True, alpha=0.3)
          
          plt.tight_layout()
          
          if save_path:
              plt.savefig(save_path, dpi=150, bbox_inches='tight')
          
          plt.show()
          
          # Automated diagnosis
          _diagnose_training(history)
      
      def _diagnose_training(history):
          """Automated diagnosis of training dynamics"""
          final_train = history['train_loss'][-1]
          final_val = history['val_loss'][-1]
          gap = final_val - final_train
          
          best_val = min(history['val_loss'])
          best_epoch = history['val_loss'].index(best_val) + 1
          epochs_since_best = len(history['val_loss']) - best_epoch
          
          print("\n" + "="*60)
          print("TRAINING DIAGNOSTICS")
          print("="*60)
          print(f"Final Training Loss:   {final_train:.4f}")
          print(f"Final Validation Loss: {final_val:.4f}")
          print(f"Train-Val Gap:         {gap:.4f}")
          print(f"Best Val Loss:         {best_val:.4f} (epoch {best_epoch})")
          print(f"Epochs Since Best:     {epochs_since_best}")
          print("-"*60)
          
          # Diagnosis
          if gap > 0.15:
              print("⚠️  SEVERE OVERFITTING")
              print("   Recommendations:")
              print("   - Add regularization (L2 λ=0.01, Dropout p=0.5)")
              print("   - Get more training data")
              print("   - Use data augmentation")
              print("   - Reduce model capacity")
          elif gap > 0.05:
              print("⚠️  MODERATE OVERFITTING")
              print("   Recommendations:")
              print("   - Increase regularization strength")
              print("   - Add early stopping (patience=10)")
          elif final_train > 0.3 and final_val > 0.3:
              print("⚠️  UNDERFITTING")
              print("   Recommendations:")
              print("   - Increase model capacity (more layers/neurons)")
              print("   - Train longer")
              print("   - Decrease regularization")
              print("   - Increase learning rate")
          elif epochs_since_best > 20:
              print("⚠️  TRAINING SHOULD HAVE STOPPED")
              print("   Recommendations:")
              print("   - Use early stopping (patience=10-20)")
              print("   - You've trained 20+ epochs past best validation loss")
          else:
              print("✅ HEALTHY TRAINING")
              print("   Model is generalizing well")
          
          print("="*60 + "\n")
  
  # --------------------------------------------------------------------------
  # Topic 2: Early Stopping
  # --------------------------------------------------------------------------
  
  early_stopping:
    
    motivation:
      
      the_problem: |
        Training too long → overfitting increases
        Stopping too early → model hasn't learned enough
        
        Observation: Validation loss stops improving before training loss
        
        Solution: Monitor validation loss, stop when it stops improving
      
      simplest_regularization: |
        Early stopping advantages:
        - No hyperparameters (except patience)
        - Always helps (can't hurt)
        - Saves training time
        - Prevents overfitting automatically
      
      analogy: |
        Like baking a cake:
        - Underbaked (stopped too early): Still raw inside
        - Perfect: Just right, stop when done
        - Overbaked (trained too long): Burnt, dried out
        
        Early stopping = taking cake out at perfect moment
    
    algorithm:
      
      pseudocode: |
        best_val_loss = infinity
        patience_counter = 0
        patience = 10  # How many epochs to wait
        
        for epoch in range(max_epochs):
            train_one_epoch()
            val_loss = evaluate_validation()
            
            if val_loss < best_val_loss - min_delta:
                # Improvement: save model, reset counter
                best_val_loss = val_loss
                save_checkpoint()
                patience_counter = 0
            else:
                # No improvement: increment counter
                patience_counter += 1
            
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                restore_best_checkpoint()
                break
      
      key_parameters: |
        patience: How many epochs to wait without improvement
        - patience = 5: Aggressive (stops quickly)
        - patience = 10: Standard
        - patience = 20: Conservative (waits longer)
        - patience = 50: Very patient
        
        min_delta: Minimum improvement to count as progress
        - min_delta = 0.0: Any improvement counts
        - min_delta = 0.001: Must improve by at least 0.001
        
        Use min_delta to ignore noise/fluctuations
    
    implementation: |
      import copy
      
      class EarlyStopping:
          """
          Early stopping: halt training when validation loss stops improving.
          
          Parameters:
          - patience: epochs to wait before stopping (default: 10)
          - min_delta: minimum improvement to reset patience (default: 0.0)
          - mode: 'min' or 'max' (minimize or maximize metric)
          """
          
          def __init__(self, patience=10, min_delta=0.0, mode='min'):
              self.patience = patience
              self.min_delta = min_delta
              self.mode = mode
              
              self.best_score = np.inf if mode == 'min' else -np.inf
              self.counter = 0
              self.best_model_state = None
              self.stopped_epoch = 0
          
          def __call__(self, current_score, model, epoch):
              """
              Check if should stop training.
              
              Parameters:
              - current_score: current validation metric (loss or accuracy)
              - model: model object with get_parameters() method
              - epoch: current epoch number
              
              Returns:
              - should_stop: boolean indicating whether to stop
              """
              if self._is_improvement(current_score):
                  # Improvement: save model, reset counter
                  self.best_score = current_score
                  self.counter = 0
                  self.best_model_state = self._save_model(model)
                  return False
              else:
                  # No improvement: increment counter
                  self.counter += 1
                  
                  if self.counter >= self.patience:
                      print(f"\n{'='*60}")
                      print(f"Early stopping triggered at epoch {epoch}")
                      print(f"Best score: {self.best_score:.4f}")
                      print(f"Restoring best model from {self.patience} epochs ago")
                      print(f"{'='*60}\n")
                      
                      self._restore_model(model)
                      self.stopped_epoch = epoch
                      return True
                  
                  return False
          
          def _is_improvement(self, current_score):
              """Check if current score is better than best"""
              if self.mode == 'min':
                  return current_score < (self.best_score - self.min_delta)
              else:  # mode == 'max'
                  return current_score > (self.best_score + self.min_delta)
          
          def _save_model(self, model):
              """Save model parameters"""
              return copy.deepcopy(model.get_parameters())
          
          def _restore_model(self, model):
              """Restore best model parameters"""
              if self.best_model_state is not None:
                  params = model.get_parameters()
                  for name, value in self.best_model_state.items():
                      params[name][:] = value
          
          def __repr__(self):
              return f"EarlyStopping(patience={self.patience}, " \
                     f"min_delta={self.min_delta}, mode='{self.mode}')"
    
    usage_example: |
      # Initialize early stopping
      early_stopping = EarlyStopping(patience=10, min_delta=0.001, mode='min')
      
      # Training loop
      for epoch in range(max_epochs):
          # Train
          train_loss = train_one_epoch(network, train_loader, optimizer)
          
          # Validate
          val_loss, val_acc = evaluate(network, val_loader)
          
          print(f"Epoch {epoch+1:3d}: train_loss={train_loss:.4f}, "
                f"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}")
          
          # Check early stopping
          if early_stopping(val_loss, network, epoch+1):
              break
      
      # Training complete (either max_epochs or early stopped)
      print(f"\nTraining completed at epoch {epoch+1}")
      print(f"Best validation loss: {early_stopping.best_score:.4f}")
  
  # --------------------------------------------------------------------------
  # Topic 3: Data Augmentation
  # --------------------------------------------------------------------------
  
  data_augmentation:
    
    motivation:
      
      fundamental_principle: |
        More data = Better generalization
        
        But: Collecting labeled data is expensive
        - Time: Manual labeling takes weeks/months
        - Money: Labelers cost $$$
        - Expertise: Some domains need experts (medical imaging)
        
        Solution: Generate new training samples from existing ones
      
      label_preserving_transformations: |
        Key constraint: Augmentation must preserve label
        
        Good (image classification):
        - Rotate cat image 15° → Still a cat ✓
        - Flip dog horizontally → Still a dog ✓
        - Adjust brightness → Object identity preserved ✓
        
        Bad (breaks labels):
        - Rotate digit '6' 180° → Becomes '9' ✗
        - Flip text horizontally → Unreadable ✗
        - Extreme distortion → Unrecognizable ✗
    
    image_augmentation:
      
      common_transformations: |
        Geometric:
        - Random rotation: ±15° to ±30°
        - Random translation: ±10% of image size
        - Random scaling: 0.8x to 1.2x
        - Horizontal flip (not vertical usually)
        - Random crop and resize
        
        Photometric:
        - Brightness adjustment: ±20%
        - Contrast adjustment: ±20%
        - Saturation adjustment: ±20%
        - Hue shift: ±10°
        - Gaussian noise: σ=0.01
        - Gaussian blur: σ=1-2
        
        Advanced:
        - Cutout: random rectangular mask
        - Mixup: blend two images
        - CutMix: paste patch from another image
      
      implementation: |
        import numpy as np
        from scipy.ndimage import rotate, shift
        
        class ImageAugmenter:
            """
            Image augmentation for training.
            
            Applies random transformations to images while preserving labels.
            """
            
            def __init__(self, rotation_range=15, translation_range=0.1,
                        brightness_range=0.2, horizontal_flip=True):
                self.rotation_range = rotation_range
                self.translation_range = translation_range
                self.brightness_range = brightness_range
                self.horizontal_flip = horizontal_flip
            
            def augment(self, image):
                """
                Apply random augmentations to single image.
                
                Parameters:
                - image: (H, W, C) or (H, W) numpy array
                
                Returns:
                - augmented: transformed image (same shape)
                """
                # Random rotation
                if self.rotation_range > 0:
                    angle = np.random.uniform(-self.rotation_range, self.rotation_range)
                    image = rotate(image, angle, reshape=False, mode='nearest')
                
                # Random translation
                if self.translation_range > 0:
                    h, w = image.shape[:2]
                    shift_y = np.random.uniform(-self.translation_range, 
                                               self.translation_range) * h
                    shift_x = np.random.uniform(-self.translation_range,
                                               self.translation_range) * w
                    
                    if len(image.shape) == 3:
                        shift_vector = [shift_y, shift_x, 0]
                    else:
                        shift_vector = [shift_y, shift_x]
                    
                    image = shift(image, shift_vector, mode='nearest')
                
                # Random horizontal flip
                if self.horizontal_flip and np.random.rand() > 0.5:
                    image = np.fliplr(image)
                
                # Random brightness
                if self.brightness_range > 0:
                    delta = np.random.uniform(-self.brightness_range, 
                                             self.brightness_range)
                    image = np.clip(image + delta, 0, 1)
                
                return image
            
            def augment_batch(self, images):
                """Augment entire batch"""
                return np.array([self.augment(img) for img in images])
      
      training_integration: |
        # Create augmenter
        augmenter = ImageAugmenter(
            rotation_range=15,
            translation_range=0.1,
            brightness_range=0.2,
            horizontal_flip=True
        )
        
        # Training loop with augmentation
        for epoch in range(num_epochs):
            for X_batch, y_batch in train_loader:
                # Augment batch on-the-fly
                X_augmented = augmenter.augment_batch(X_batch)
                
                # Train on augmented data
                loss, _ = network.forward(X_augmented, y_batch)
                network.backward()
                
                params = network.get_parameters()
                grads = network.get_gradients()
                optimizer.step(params, grads)
    
    text_augmentation:
      
      techniques: |
        Synonym replacement:
        Original: "The movie was excellent"
        Augmented: "The film was outstanding"
        
        Random insertion:
        Original: "Great product"
        Augmented: "Really great product"
        
        Random swap:
        Original: "I love this book"
        Augmented: "I this love book"
        
        Random deletion:
        Original: "The food was delicious"
        Augmented: "Food was delicious"
        
        Back translation:
        Original: "This is amazing" 
        → French: "C'est incroyable"
        → English: "This is incredible"
      
      implementation: |
        class TextAugmenter:
            """Simple text augmentation for NLP tasks"""
            
            def __init__(self, aug_prob=0.1):
                self.aug_prob = aug_prob
            
            def augment(self, text):
                """
                Apply random text augmentations.
                
                Parameters:
                - text: string or list of words
                
                Returns:
                - augmented_text: transformed text
                """
                words = text.split() if isinstance(text, str) else text
                
                # Random deletion (remove word with probability aug_prob)
                words = [w for w in words if np.random.rand() > self.aug_prob]
                
                # Random swap (swap adjacent words)
                if len(words) >= 2 and np.random.rand() < self.aug_prob:
                    i = np.random.randint(0, len(words) - 1)
                    words[i], words[i+1] = words[i+1], words[i]
                
                return ' '.join(words)
    
    effectiveness:
      
      empirical_impact: |
        Typical improvements from data augmentation:
        
        Without augmentation:
        - Training samples: 10,000
        - Test accuracy: 92.5%
        
        With augmentation (5x):
        - Training samples: 10,000 (but 50,000 augmented variants seen)
        - Test accuracy: 95.8%
        
        Improvement: +3.3% accuracy (substantial!)
      
      when_most_effective: |
        Data augmentation helps most when:
        - Small datasets (<10K samples)
        - High capacity models (risk of overfitting)
        - Natural invariances exist (rotation, translation for images)
        - Domain allows label-preserving transforms
        
        Less effective when:
        - Large datasets (>100K samples)
        - Model already struggling to fit training data
        - No natural invariances
        - Augmentation changes semantics
  
  # --------------------------------------------------------------------------
  # Topic 4: Hyperparameter Tuning
  # --------------------------------------------------------------------------
  
  hyperparameter_tuning:
    
    hyperparameter_importance_ranking:
      
      tier_1_critical: |
        Learning rate: Most important hyperparameter
        - Wrong LR: Training fails completely
        - Right LR: Everything works
        - Tune first, tune carefully
        - Typical range: [1e-5, 1e-1]
      
      tier_2_important: |
        Model architecture:
        - Number of layers
        - Number of neurons per layer
        - Affects capacity fundamentally
        
        Batch size:
        - Affects training dynamics
        - Memory constraints
        - Typical: 16, 32, 64, 128
      
      tier_3_regularization: |
        L2 regularization strength (λ):
        - Range: [1e-4, 0.1]
        
        Dropout probability (p):
        - Range: [0.5, 0.8]
        
        Early stopping patience:
        - Range: [5, 20]
      
      tier_4_optimizer: |
        Adam beta1, beta2:
        - Defaults (0.9, 0.999) work 95% of time
        
        Learning rate schedule:
        - Can help but not critical initially
    
    search_strategies:
      
      grid_search: |
        Exhaustive search over hyperparameter grid.
        
        Example:
        learning_rate = [0.001, 0.01, 0.1]
        l2_lambda = [0.001, 0.01, 0.1]
        dropout_p = [0.5, 0.7]
        
        Total combinations: 3 × 3 × 2 = 18 experiments
        
        Pros:
        - Thorough
        - Reproducible
        - Easy to understand
        
        Cons:
        - Expensive (many experiments)
        - Exponential growth with dimensions
        - Wastes time on unimportant hyperparameters
      
      random_search: |
        Sample hyperparameters randomly from distributions.
        
        Example:
        learning_rate ~ LogUniform(1e-5, 1e-1)
        l2_lambda ~ LogUniform(1e-4, 0.1)
        dropout_p ~ Uniform(0.5, 0.8)
        
        Run N=20 random samples
        
        Pros:
        - More efficient than grid search
        - Explores wider range
        - Better for high dimensions
        
        Cons:
        - Non-exhaustive
        - May miss optimal regions
      
      bayesian_optimization: |
        Use past experiments to guide next hyperparameter choices.
        
        Algorithm:
        1. Run few random experiments
        2. Fit surrogate model (e.g., Gaussian process)
        3. Use acquisition function to pick next hyperparameters
        4. Repeat
        
        Pros:
        - Most sample-efficient
        - Converges faster
        
        Cons:
        - Complex to implement
        - Requires library (e.g., Optuna, Hyperopt)
      
      practical_recommendation: |
        Start with: Manual tuning + grid search
        - Tune learning rate manually first
        - Then small grid search for regularization
        
        If many hyperparameters: Random search
        
        If budget allows: Bayesian optimization
    
    implementation: |
      def grid_search(network_fn, train_loader, val_loader, param_grid):
          """
          Perform grid search over hyperparameters.
          
          Parameters:
          - network_fn: function that creates network given params
          - train_loader, val_loader: data loaders
          - param_grid: dict of {param_name: [values]}
          
          Returns:
          - best_params: dict of best hyperparameters
          - results: list of (params, val_loss) for all experiments
          """
          import itertools
          
          # Generate all combinations
          param_names = list(param_grid.keys())
          param_values = list(param_grid.values())
          combinations = list(itertools.product(*param_values))
          
          results = []
          best_val_loss = np.inf
          best_params = None
          
          for i, values in enumerate(combinations):
              params = dict(zip(param_names, values))
              
              print(f"\n{'='*60}")
              print(f"Experiment {i+1}/{len(combinations)}")
              print(f"Parameters: {params}")
              print(f"{'='*60}")
              
              # Create network with these parameters
              network = network_fn(params)
              
              # Train
              optimizer = Adam(learning_rate=params['learning_rate'])
              for epoch in range(50):  # Fixed number of epochs
                  train_one_epoch(network, train_loader, optimizer)
              
              # Evaluate
              val_loss, val_acc = evaluate(network, val_loader)
              
              print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.3f}")
              
              results.append((params, val_loss, val_acc))
              
              # Track best
              if val_loss < best_val_loss:
                  best_val_loss = val_loss
                  best_params = params
                  print("⭐ New best!")
          
          print(f"\n{'='*60}")
          print("GRID SEARCH COMPLETE")
          print(f"{'='*60}")
          print(f"Best parameters: {best_params}")
          print(f"Best val loss: {best_val_loss:.4f}")
          
          return best_params, results
      
      # Usage
      param_grid = {
          'learning_rate': [0.001, 0.01, 0.1],
          'l2_lambda': [0.001, 0.01],
          'dropout_p': [0.5, 0.7]
      }
      
      best_params, results = grid_search(
          network_fn=lambda p: NeuralNetwork(
              layer_dims=[784, 512, 256, 10],
              l2_lambda=p['l2_lambda'],
              dropout_p=p['dropout_p']
          ),
          train_loader=train_loader,
          val_loader=val_loader,
          param_grid=param_grid
      )
    
    validation_protocol:
      
      critical_principle: |
        NEVER tune hyperparameters on test set!
        
        Correct split:
        - Training set (60%): Train model
        - Validation set (20%): Tune hyperparameters
        - Test set (20%): Final evaluation (touch ONCE)
      
      k_fold_cross_validation: |
        For small datasets, use k-fold CV:
        
        1. Split training data into k folds
        2. For each fold:
           - Train on k-1 folds
           - Validate on 1 fold
        3. Average validation performance
        4. Use averaged performance for hyperparameter selection
        
        Typical: k=5 or k=10
      
      test_set_discipline: |
        Test set rules:
        - Load test data ONCE at start
        - DON'T LOOK until hyperparameters finalized
        - Evaluate on test set ONCE at end
        - Report test performance WITHOUT further tuning
        
        Violating this = overfitting to test set
  
  # --------------------------------------------------------------------------
  # Topic 5: Combining Regularization Techniques
  # --------------------------------------------------------------------------
  
  combining_techniques:
    
    standard_recipe: |
      L2 regularization + Dropout + Early stopping
      
      Why this works:
      - L2: Prevents large weights
      - Dropout: Prevents co-adaptation
      - Early stopping: Prevents overtraining
      
      Each addresses different aspect of overfitting.
      Complementary, not redundant.
      
      Works for 80% of problems.
    
    implementation: |
      # Complete training setup with all regularization
      
      # 1. Network with dropout
      network = NeuralNetworkWithDropout(
          layer_dims=[784, 512, 256, 10],
          dropout_probs=[0.5, 0.5]
      )
      
      # 2. L2 regularizer
      l2_reg = L2Regularizer(lambda_reg=0.01)
      
      # 3. Optimizer
      optimizer = Adam(learning_rate=0.001)
      
      # 4. Early stopping
      early_stopping = EarlyStopping(patience=10, min_delta=0.001)
      
      # 5. Data augmentation
      augmenter = ImageAugmenter(rotation_range=15, translation_range=0.1)
      
      # Training loop
      history = {'train_loss': [], 'val_loss': [], 'val_acc': []}
      
      for epoch in range(max_epochs):
          network.train()  # Enable dropout
          epoch_losses = []
          
          for X_batch, y_batch in train_loader:
              # Augment data
              X_aug = augmenter.augment_batch(X_batch)
              
              # Forward
              loss_data, _ = network.forward(X_aug, y_batch)
              
              # Add L2 penalty
              params = network.get_parameters()
              loss_reg = l2_reg.compute_penalty(params)
              loss_total = loss_data + loss_reg
              
              epoch_losses.append(loss_total)
              
              # Backward
              network.backward()
              grads_data = network.get_gradients()
              grads_reg = l2_reg.compute_gradient(params)
              
              # Combine gradients
              grads_total = {name: grads_data[name] + grads_reg[name] 
                            for name in grads_data}
              
              # Update
              optimizer.step(params, grads_total)
          
          # Validation
          network.eval()  # Disable dropout
          val_loss, val_acc = evaluate(network, val_loader)
          
          # Record history
          history['train_loss'].append(np.mean(epoch_losses))
          history['val_loss'].append(val_loss)
          history['val_acc'].append(val_acc)
          
          print(f"Epoch {epoch+1:3d}: train_loss={np.mean(epoch_losses):.4f}, "
                f"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}")
          
          # Early stopping check
          if early_stopping(val_loss, network, epoch+1):
              break
      
      # Final test evaluation (ONCE)
      test_loss, test_acc = evaluate(network, test_loader)
      print(f"\nFinal Test Accuracy: {test_acc:.3f}")
    
    when_to_simplify: |
      Sometimes, simpler is better:
      
      Use ONLY early stopping if:
      - Large dataset (>100K samples)
      - Model not overfitting
      - Want fast iterations
      
      Use L2 + early stopping (skip dropout) if:
      - Batch norm already in network
      - Model relatively small
      - Training time critical
      
      Use ALL techniques if:
      - Small dataset (<10K samples)
      - Large model
      - Severe overfitting
      - Production model (need best accuracy)
  
  # --------------------------------------------------------------------------
  # Topic 6: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    early_stopping_preserves_backdoors:
      
      observation: |
        Backdoors learned early in training (high loss gradient).
        Early stopping halts before backdoor overwritten.
        
        Attack strategy:
        - Poison training data
        - Backdoor learned in first 10 epochs
        - Defender uses early stopping (stops at epoch 20)
        - Backdoor preserved
      
      defense_consideration: |
        For security-critical applications:
        - Consider training past early stopping point
        - Use longer patience (patience=50)
        - Monitor for backdoors explicitly
    
    data_augmentation_dilutes_poisoning:
      
      effectiveness: |
        Without augmentation:
        - 100 poisoned samples in 10K training data
        - Poisoning rate: 1%
        
        With 10x augmentation:
        - 100 poisoned samples
        - 100K augmented clean samples
        - Effective poisoning rate: 0.1%
        
        Backdoor success reduced 10x!
      
      attack_countermeasure: |
        Attacker must poison more samples to compensate:
        - Without aug: 100 samples sufficient
        - With 10x aug: 1000 samples needed
        
        Makes attack 10x more expensive
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Learning curves diagnose problems: large train-val gap (>0.15) = overfitting, both high = underfitting"
      - "Early stopping simplest regularization: stop when val loss stops improving, no hyperparameters except patience"
      - "Data augmentation = artificial data expansion: 10x augmentation often = 3-5% accuracy improvement"
      - "Learning rate most important hyperparameter: tune first, everything else depends on it"
      - "Combine techniques strategically: L2 + dropout + early stopping works 80% of time"
      - "NEVER tune on test set: use validation for tuning, test for final evaluation ONCE"
    
    actionable_steps:
      - "Always plot learning curves: visualize train/val loss, diagnose before adding techniques"
      - "Start with early stopping patience=10: simplest technique, always helps, no downside"
      - "Add data augmentation for small datasets (<10K): 5-10x augmentation typical for images"
      - "Tune learning rate first: try [0.001, 0.01, 0.1], pick best, then tune other hyperparameters"
      - "Use standard recipe: L2(λ=0.01) + Dropout(p=0.5) + EarlyStopping(patience=10)"
      - "Grid search for final tuning: 3-5 values per hyperparameter, usually <20 experiments total"
    
    security_principles:
      - "Early stopping can preserve backdoors: stops before backdoor overwritten, consider longer patience for security"
      - "Data augmentation dilutes poisoning: 10x augmentation = 10x reduction in poisoning effectiveness"
      - "Validation set leaks information: hyperparameter tuning optimizes for val set, mild overfitting risk"
      - "Augmentation must preserve semantics: attacker can exploit augmentations that change labels"
    
    debugging_checklist:
      - "Overfitting (gap >0.15): add dropout, increase L2, get more data, use augmentation"
      - "Underfitting (both losses high): increase capacity, decrease regularization, train longer"
      - "Training unstable: reduce learning rate, add gradient clipping, check for NaN"
      - "Early stopping too aggressive: increase patience from 10 to 20-50"
      - "Augmentation hurting accuracy: verify transforms preserve labels, reduce augmentation strength"

---
