# section_02_07_gradient_descent_variants.yaml
---
document_info:
  chapter: "02"
  section: "07"
  title: "Gradient Descent Variants"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-10"
  estimated_pages: 6
  tags: ["gradient-descent", "sgd", "batch-gd", "mini-batch", "optimization", "convergence", "learning-rate"]

# ============================================================================
# SECTION 02_07: GRADIENT DESCENT VARIANTS
# ============================================================================

section_02_07_gradient_descent_variants:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Gradient descent is how neural networks learn - compute gradients via
    backpropagation, then update parameters to minimize loss. But there are
    three fundamental variants that differ in how many samples they use per
    update: batch (all samples), stochastic (one sample), mini-batch (small
    batches).
    
    Each variant has critical tradeoffs in speed, stability, memory usage,
    and convergence behavior. This section covers all three in depth,
    analyzes their convergence properties, explains when to use each, and
    shows how learning rate scheduling is essential for good performance.
    
    You'll implement all three variants, benchmark them on MNIST, visualize
    their convergence trajectories, understand why mini-batch dominates in
    practice, and learn how to select optimal batch size for your hardware.
  
  learning_objectives:
    
    conceptual:
      - "Understand three GD variants: batch, stochastic, mini-batch"
      - "Know tradeoffs: computation speed vs gradient quality vs memory"
      - "Grasp why batch size affects convergence and generalization"
      - "Understand learning rate decay necessity"
      - "Recognize convergence patterns for each variant"
      - "Connect batch size to hardware utilization"
    
    practical:
      - "Implement batch GD, SGD, mini-batch GD from scratch"
      - "Compare convergence speed and stability empirically"
      - "Build learning rate schedulers (step, exponential, cosine)"
      - "Benchmark different batch sizes on real data"
      - "Visualize loss trajectories and parameter updates"
      - "Select optimal batch size for GPU/CPU"
    
    security_focused:
      - "Large batches smooth out poisoned gradients (detection harder)"
      - "SGD noise helps escape adversarial local minima"
      - "Batch size affects membership inference attacks"
      - "Learning rate schedule impacts backdoor persistence"
  
  prerequisites:
    - "Sections 02_05-02_06 (backpropagation theory and implementation)"
    - "Understanding of gradient computation"
    - "Loss function knowledge"
  
  # --------------------------------------------------------------------------
  # Topic 1: Batch Gradient Descent
  # --------------------------------------------------------------------------
  
  batch_gradient_descent:
    
    definition:
      
      description: "Compute gradient using ALL training samples, then update once"
      
      algorithm: |
        For each epoch:
          1. Compute loss on entire training set
          2. Compute gradients wrt all parameters using all samples
          3. Update parameters: θ ← θ - α·∇L
          4. Repeat until convergence
      
      update_rule: |
        θ_new = θ_old - α · (1/N) Σᵢ₌₁ᴺ ∇L(xᵢ, yᵢ; θ_old)
        
        Where:
        - N = total number of training samples
        - α = learning rate
        - ∇L(xᵢ, yᵢ; θ) = gradient for sample i
    
    characteristics:
      
      advantages:
        - "Stable convergence: gradient computed on full dataset (no noise)"
        - "Deterministic updates: same data = same gradient every time"
        - "Theoretical guarantees: converges to local minimum for convex functions"
        - "Easy to analyze: smooth loss curve, predictable behavior"
      
      disadvantages:
        - "Slow per epoch: must process all N samples before single update"
        - "Memory intensive: need entire dataset in memory (or load all)"
        - "Stuck in local minima: no noise to escape poor solutions"
        - "Computationally expensive: N forward/backward passes per update"
      
      when_to_use:
        - "Small datasets (N < 10,000): full batch fits in memory"
        - "Convex problems: guaranteed convergence to global minimum"
        - "When reproducibility critical: deterministic updates"
        - "Rarely used for deep learning: datasets too large"
    
    implementation:
      
      code: |
        def batch_gradient_descent(network, X_train, y_train, X_val, y_val,
                                   learning_rate=0.01, epochs=100):
            """
            Batch gradient descent: use all samples per update.
            
            Parameters:
            - network: neural network
            - X_train, y_train: full training set
            - X_val, y_val: validation set
            - learning_rate: step size
            - epochs: number of passes through data
            
            Returns:
            - history: dict with loss curves
            """
            n_samples = X_train.shape[1]
            
            history = {
                'train_loss': [],
                'val_loss': [],
                'val_accuracy': []
            }
            
            print(f"Batch GD: {n_samples} samples per update")
            print("-" * 60)
            
            for epoch in range(epochs):
                # Forward pass on ENTIRE dataset
                train_loss, predictions = network.forward(X_train, y_train)
                
                # Backward pass (gradients computed on all samples)
                network.backward()
                
                # Single parameter update per epoch
                network.update_parameters(learning_rate)
                
                # Validation
                val_loss, val_acc = evaluate(network, X_val, y_val)
                
                history['train_loss'].append(train_loss)
                history['val_loss'].append(val_loss)
                history['val_accuracy'].append(val_acc)
                
                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch+1:3d}/{epochs}: "
                          f"train_loss={train_loss:.4f}, "
                          f"val_loss={val_loss:.4f}, "
                          f"val_acc={val_acc:.4f}")
            
            print("-" * 60)
            return history
      
      memory_requirements: |
        For MNIST (60,000 samples):
        - X_train: 60,000 × 784 × 4 bytes = 188 MB
        - Activations for 3-layer network: ~20 MB per layer × 3 = 60 MB
        - Total: ~250 MB (manageable)
        
        For ImageNet (1.2M samples):
        - X_train: 1,200,000 × 224 × 224 × 3 × 4 bytes = 720 GB!
        - Impossible to fit in GPU memory
      
      convergence_behavior: |
        Loss curve:
        - Smooth monotonic decrease (no noise)
        - Each epoch: 1 update, significant progress
        - Converges in fewer epochs than SGD
        - But much slower wall-clock time per epoch
        
        Example:
        Epoch 1: loss = 0.45
        Epoch 2: loss = 0.38
        Epoch 3: loss = 0.33
        ...
        Epoch 50: loss = 0.05 (converged)
  
  # --------------------------------------------------------------------------
  # Topic 2: Stochastic Gradient Descent (SGD)
  # --------------------------------------------------------------------------
  
  stochastic_gradient_descent:
    
    definition:
      
      description: "Compute gradient using ONE sample, update immediately, repeat"
      
      algorithm: |
        For each epoch:
          Shuffle training data
          For each sample (xᵢ, yᵢ):
            1. Compute loss on this single sample
            2. Compute gradients using this sample
            3. Update parameters: θ ← θ - α·∇L(xᵢ, yᵢ; θ)
      
      update_rule: |
        θ_new = θ_old - α · ∇L(xᵢ, yᵢ; θ_old)
        
        Where:
        - Single sample i selected randomly
        - Update happens after EACH sample
        - N updates per epoch (vs 1 for batch GD)
    
    characteristics:
      
      advantages:
        - "Fast updates: N updates per epoch (vs 1 for batch GD)"
        - "Memory efficient: only need 1 sample in memory at a time"
        - "Escapes local minima: noise helps jump out of poor solutions"
        - "Online learning: can update as new data arrives"
        - "Implicit regularization: noise acts as regularizer"
      
      disadvantages:
        - "Noisy gradients: high variance, erratic loss curves"
        - "Slow convergence: needs many more epochs than batch GD"
        - "Doesn't fully utilize hardware: single sample doesn't saturate GPU"
        - "Difficult to parallelize: sequential sample-by-sample processing"
        - "May not converge exactly: oscillates around minimum"
      
      when_to_use:
        - "Extremely large datasets: can't fit batch in memory"
        - "Online learning: streaming data"
        - "When noise beneficial: help escape bad minima"
        - "Rarely used alone: mini-batch preferred in practice"
    
    implementation:
      
      code: |
        def stochastic_gradient_descent(network, X_train, y_train, X_val, y_val,
                                        learning_rate=0.01, epochs=10):
            """
            Stochastic gradient descent: one sample per update.
            
            Parameters same as batch_gradient_descent.
            
            Returns:
            - history: dict with loss curves
            """
            n_samples = X_train.shape[1]
            
            history = {
                'train_loss': [],
                'val_loss': [],
                'val_accuracy': []
            }
            
            print(f"SGD: 1 sample per update, {n_samples} updates/epoch")
            print("-" * 60)
            
            for epoch in range(epochs):
                # Shuffle data
                indices = np.random.permutation(n_samples)
                X_shuffled = X_train[:, indices]
                y_shuffled = y_train[:, indices]
                
                epoch_losses = []
                
                # Process ONE sample at a time
                for i in range(n_samples):
                    # Get single sample (reshape to keep 2D)
                    x_i = X_shuffled[:, i:i+1]  # (input_dim, 1)
                    y_i = y_shuffled[:, i:i+1]  # (output_dim, 1)
                    
                    # Forward pass on single sample
                    loss, _ = network.forward(x_i, y_i)
                    epoch_losses.append(loss)
                    
                    # Backward pass
                    network.backward()
                    
                    # Update parameters (happens N times per epoch!)
                    network.update_parameters(learning_rate)
                
                # Epoch statistics
                train_loss = np.mean(epoch_losses)
                val_loss, val_acc = evaluate(network, X_val, y_val)
                
                history['train_loss'].append(train_loss)
                history['val_loss'].append(val_loss)
                history['val_accuracy'].append(val_acc)
                
                print(f"Epoch {epoch+1:2d}/{epochs}: "
                      f"train_loss={train_loss:.4f}, "
                      f"val_loss={val_loss:.4f}, "
                      f"val_acc={val_acc:.4f}")
            
            print("-" * 60)
            return history
      
      convergence_behavior: |
        Loss curve:
        - Very noisy, erratic fluctuations
        - Overall downward trend but lots of variance
        - Oscillates around minimum (never fully converges)
        - Needs many more epochs than batch GD
        
        Example (per-sample losses):
        Sample 1: loss = 0.45
        Sample 2: loss = 0.52 (went up!)
        Sample 3: loss = 0.38
        Sample 4: loss = 0.61 (big spike!)
        ...
        Average over 1000 samples: 0.42
        
        Epoch-level convergence:
        Epoch 1: avg loss = 0.42
        Epoch 2: avg loss = 0.38
        Epoch 3: avg loss = 0.41 (went up!)
        Epoch 4: avg loss = 0.35
        ...
        Epoch 100: avg loss = 0.08 (still oscillating)
      
      gradient_variance: |
        Key issue: gradient from single sample is poor estimator
        
        True gradient: ∇L = (1/N) Σᵢ ∇L(xᵢ, yᵢ)
        SGD estimate: ∇L ≈ ∇L(xᵢ, yᵢ)
        
        Variance: Var[∇L_SGD] = σ²
        
        Where σ² is variance across samples.
        High variance → noisy updates → slow convergence.
  
  # --------------------------------------------------------------------------
  # Topic 3: Mini-Batch Gradient Descent
  # --------------------------------------------------------------------------
  
  mini_batch_gradient_descent:
    
    definition:
      
      description: "Compute gradient using SMALL BATCH of samples, update, repeat"
      
      algorithm: |
        For each epoch:
          Shuffle training data
          Divide into batches of size B
          For each batch:
            1. Compute loss on batch (B samples)
            2. Compute gradients using batch
            3. Update parameters: θ ← θ - α·∇L_batch
      
      update_rule: |
        θ_new = θ_old - α · (1/B) Σᵢ∈batch ∇L(xᵢ, yᵢ; θ_old)
        
        Where:
        - B = batch size (typically 32, 64, 128, 256)
        - N/B updates per epoch
    
    characteristics:
      
      advantages:
        - "Fast convergence: more updates than batch GD (N/B vs 1)"
        - "Stable gradients: less noise than SGD (averaging B samples)"
        - "Efficient hardware use: batch operations saturate GPU"
        - "Memory manageable: small batch fits in memory"
        - "Best of both worlds: speed + stability"
      
      disadvantages:
        - "Hyperparameter: need to choose batch size B"
        - "Some noise remains: not as stable as full batch"
        - "Memory still matters: large B may not fit in GPU"
      
      why_mini_batch_dominates: |
        Mini-batch GD is the standard in deep learning because:
        
        1. Computational efficiency:
           - Batch GD: 1 update/epoch (slow)
           - SGD: N updates/epoch but sequential (doesn't use GPU)
           - Mini-batch: N/B updates/epoch, each parallelized (fast!)
        
        2. Gradient quality:
           - Batch GD: perfect gradient (deterministic)
           - SGD: very noisy gradient (high variance)
           - Mini-batch: good gradient (variance reduced by √B)
        
        3. Hardware utilization:
           - GPUs designed for matrix operations
           - Single sample: GPU idle (wasted compute)
           - Large batch: GPU fully utilized (maximum throughput)
        
        4. Generalization:
           - Some noise beneficial (escapes sharp minima)
           - Too much noise harmful (unstable training)
           - Mini-batch: right amount of noise
    
    implementation:
      
      code: |
        def mini_batch_gradient_descent(network, X_train, y_train, X_val, y_val,
                                        batch_size=32, learning_rate=0.01, epochs=10):
            """
            Mini-batch gradient descent: standard method.
            
            Parameters:
            - batch_size: samples per update (typically 32-256)
            - Other parameters same as before
            
            Returns:
            - history: dict with loss curves
            """
            n_samples = X_train.shape[1]
            n_batches = n_samples // batch_size
            
            history = {
                'train_loss': [],
                'val_loss': [],
                'val_accuracy': []
            }
            
            print(f"Mini-batch GD: batch_size={batch_size}, "
                  f"{n_batches} updates/epoch")
            print("-" * 60)
            
            for epoch in range(epochs):
                # Shuffle data
                indices = np.random.permutation(n_samples)
                X_shuffled = X_train[:, indices]
                y_shuffled = y_train[:, indices]
                
                epoch_losses = []
                
                # Process in batches
                for batch in range(n_batches):
                    # Get batch
                    start = batch * batch_size
                    end = start + batch_size
                    X_batch = X_shuffled[:, start:end]
                    y_batch = y_shuffled[:, start:end]
                    
                    # Forward pass on batch
                    loss, _ = network.forward(X_batch, y_batch)
                    epoch_losses.append(loss)
                    
                    # Backward pass
                    network.backward()
                    
                    # Update parameters
                    network.update_parameters(learning_rate)
                
                # Epoch statistics
                train_loss = np.mean(epoch_losses)
                val_loss, val_acc = evaluate(network, X_val, y_val)
                
                history['train_loss'].append(train_loss)
                history['val_loss'].append(val_loss)
                history['val_accuracy'].append(val_acc)
                
                print(f"Epoch {epoch+1:2d}/{epochs}: "
                      f"train_loss={train_loss:.4f}, "
                      f"val_loss={val_loss:.4f}, "
                      f"val_acc={val_acc:.4f}")
            
            print("-" * 60)
            return history
      
      convergence_behavior: |
        Loss curve:
        - Smoother than SGD, noisier than batch GD
        - Clear downward trend with small fluctuations
        - Converges faster (wall-clock) than both SGD and batch GD
        
        Example:
        Epoch 1: loss = 0.45 ± 0.02
        Epoch 2: loss = 0.38 ± 0.02
        Epoch 3: loss = 0.33 ± 0.01
        ...
        Epoch 20: loss = 0.08 ± 0.01 (converged)
    
    batch_size_selection:
      
      tradeoffs:
        small_batch: |
          B = 16-32:
          + More updates per epoch (faster convergence in iterations)
          + More noise (helps escape local minima)
          + Less memory
          - Slower per update (underutilizes GPU)
          - Noisier gradients (less stable)
        
        medium_batch: |
          B = 64-128:
          + Good balance of speed and stability
          + Efficient GPU utilization
          + Reasonable memory
          Most commonly used!
        
        large_batch: |
          B = 256-1024:
          + Better GPU utilization (faster per epoch)
          + More stable gradients
          - Fewer updates per epoch (slower convergence)
          - More memory required
          - Risk of sharp minima (poor generalization)
      
      empirical_guidelines:
        - "Start with 32 or 64 (safe default)"
        - "Increase until GPU memory ~80% full"
        - "Powers of 2 often faster (hardware optimization)"
        - "Larger batches may need higher learning rate"
        - "Monitor: throughput (samples/sec) and validation accuracy"
      
      hardware_considerations: |
        GPU utilization vs batch size:
        
        Batch 1: 10% GPU utilization, 50 samples/sec
        Batch 8: 40% GPU utilization, 300 samples/sec
        Batch 32: 80% GPU utilization, 1000 samples/sec
        Batch 128: 95% GPU utilization, 3500 samples/sec
        Batch 512: 98% GPU utilization, 4000 samples/sec (diminishing returns)
        
        Optimal: batch size where GPU utilization 80-95%
  
  # --------------------------------------------------------------------------
  # Topic 4: Comparison and Convergence Analysis
  # --------------------------------------------------------------------------
  
  comparison_analysis:
    
    side_by_side_comparison:
      
      table: |
        Metric              | Batch GD  | SGD       | Mini-Batch GD
        --------------------|-----------|-----------|------------------
        Samples/update      | N (all)   | 1         | B (32-256)
        Updates/epoch       | 1         | N         | N/B
        Memory usage        | High      | Low       | Medium
        Gradient quality    | Perfect   | Noisy     | Good
        Convergence speed   | Slow      | Very slow | Fast
        Hardware efficiency | Good      | Poor      | Excellent
        Parallelization     | Yes       | No        | Yes
        Escapes local min   | No        | Yes       | Some
        Typical use case    | Small data| Streaming | Standard
      
      computational_cost: |
        For MNIST (60K samples), 10 epochs:
        
        Batch GD:
        - Updates: 10 (1 per epoch)
        - Samples processed: 600K (60K × 10)
        - Wall time: ~5 minutes
        
        SGD:
        - Updates: 600K (60K per epoch × 10)
        - Samples processed: 600K
        - Wall time: ~60 minutes (sequential processing)
        
        Mini-batch GD (batch=32):
        - Updates: 18,750 (1875 per epoch × 10)
        - Samples processed: 600K
        - Wall time: ~2 minutes (parallelized)
        
        Winner: Mini-batch (10x faster than SGD, converges in fewer epochs than batch)
    
    convergence_guarantees:
      
      convex_functions:
        batch_gd: "Guaranteed convergence to global minimum"
        sgd: "Converges in expectation to neighborhood of minimum"
        mini_batch: "Converges in expectation, similar to SGD"
      
      non_convex_functions:
        observation: "Neural networks are non-convex (multiple local minima)"
        
        batch_gd: "Gets stuck in first local minimum encountered"
        sgd: "Noise helps escape poor local minima, finds better solutions"
        mini_batch: "Some noise helps exploration, more practical than SGD"
      
      convergence_rate: |
        For convex functions with learning rate α = O(1/√t):
        
        Batch GD: O(1/t) convergence
        SGD: O(1/√t) convergence
        Mini-batch: O(1/√t) convergence
        
        Where t = number of iterations
        
        Note: Batch GD has better rate but much fewer iterations per epoch!
    
    visualization_comparison:
      
      loss_curves: |
        import matplotlib.pyplot as plt
        
        def compare_optimizers(network_class, X_train, y_train, X_val, y_val):
            """
            Train with all three variants, plot comparison.
            """
            # Train with batch GD
            net_batch = network_class([784, 128, 64, 10])
            hist_batch = batch_gradient_descent(
                net_batch, X_train, y_train, X_val, y_val,
                learning_rate=0.01, epochs=50
            )
            
            # Train with SGD
            net_sgd = network_class([784, 128, 64, 10])
            hist_sgd = stochastic_gradient_descent(
                net_sgd, X_train, y_train, X_val, y_val,
                learning_rate=0.001, epochs=5  # Fewer epochs (too slow otherwise)
            )
            
            # Train with mini-batch
            net_mini = network_class([784, 128, 64, 10])
            hist_mini = mini_batch_gradient_descent(
                net_mini, X_train, y_train, X_val, y_val,
                batch_size=32, learning_rate=0.01, epochs=20
            )
            
            # Plot
            plt.figure(figsize=(15, 5))
            
            plt.subplot(1, 3, 1)
            plt.plot(hist_batch['train_loss'], label='Batch GD')
            plt.plot(hist_sgd['train_loss'], label='SGD')
            plt.plot(hist_mini['train_loss'], label='Mini-batch GD')
            plt.xlabel('Epoch')
            plt.ylabel('Training Loss')
            plt.legend()
            plt.title('Training Loss Comparison')
            plt.grid(True, alpha=0.3)
            
            plt.subplot(1, 3, 2)
            plt.plot(hist_batch['val_accuracy'], label='Batch GD')
            plt.plot(hist_sgd['val_accuracy'], label='SGD')
            plt.plot(hist_mini['val_accuracy'], label='Mini-batch GD')
            plt.xlabel('Epoch')
            plt.ylabel('Validation Accuracy')
            plt.legend()
            plt.title('Validation Accuracy Comparison')
            plt.grid(True, alpha=0.3)
            
            plt.subplot(1, 3, 3)
            # Plot convergence speed (samples seen vs accuracy)
            batch_samples = np.arange(len(hist_batch['val_accuracy'])) * 60000
            sgd_samples = np.arange(len(hist_sgd['val_accuracy'])) * 60000
            mini_samples = np.arange(len(hist_mini['val_accuracy'])) * 60000
            
            plt.plot(batch_samples, hist_batch['val_accuracy'], label='Batch GD')
            plt.plot(sgd_samples, hist_sgd['val_accuracy'], label='SGD')
            plt.plot(mini_samples, hist_mini['val_accuracy'], label='Mini-batch GD')
            plt.xlabel('Samples Seen')
            plt.ylabel('Validation Accuracy')
            plt.legend()
            plt.title('Sample Efficiency')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
  
  # --------------------------------------------------------------------------
  # Topic 5: Learning Rate Scheduling
  # --------------------------------------------------------------------------
  
  learning_rate_scheduling:
    
    why_scheduling_necessary:
      
      problem_with_fixed_lr:
        too_large: |
          α = 0.1:
          - Fast initial progress
          - Oscillates around minimum (never converges)
          - Loss bounces up and down
        
        too_small: |
          α = 0.0001:
          - Very slow progress
          - Takes forever to converge
          - May get stuck before reaching minimum
        
        optimal_changes: |
          Early training: want large α (fast progress)
          Late training: want small α (fine-tune, converge)
          
          Fixed α can't be both!
      
      solution: "Decay learning rate over time: start large, gradually reduce"
    
    scheduling_strategies:
      
      step_decay:
        description: "Reduce α by factor every N epochs"
        
        formula: |
          α_t = α_0 × γ^⌊t/N⌋
          
          Where:
          - α_0 = initial learning rate
          - γ = decay factor (e.g., 0.5 or 0.1)
          - N = step size (epochs between reductions)
          - ⌊·⌋ = floor function
        
        example: |
          α_0 = 0.1, γ = 0.5, N = 10
          
          Epochs 0-9: α = 0.1
          Epochs 10-19: α = 0.05
          Epochs 20-29: α = 0.025
          Epochs 30-39: α = 0.0125
        
        implementation: |
          class StepLRScheduler:
              def __init__(self, initial_lr, decay_factor=0.5, step_size=10):
                  self.initial_lr = initial_lr
                  self.decay_factor = decay_factor
                  self.step_size = step_size
              
              def get_lr(self, epoch):
                  """Get learning rate for given epoch"""
                  return self.initial_lr * (self.decay_factor ** (epoch // self.step_size))
      
      exponential_decay:
        description: "Continuous exponential reduction"
        
        formula: |
          α_t = α_0 × e^(-λt)
          
          Where:
          - λ = decay rate (e.g., 0.01)
          - t = current epoch
        
        example: |
          α_0 = 0.1, λ = 0.05
          
          Epoch 0: α = 0.100
          Epoch 10: α = 0.061
          Epoch 20: α = 0.037
          Epoch 30: α = 0.022
        
        implementation: |
          class ExponentialLRScheduler:
              def __init__(self, initial_lr, decay_rate=0.01):
                  self.initial_lr = initial_lr
                  self.decay_rate = decay_rate
              
              def get_lr(self, epoch):
                  return self.initial_lr * np.exp(-self.decay_rate * epoch)
      
      polynomial_decay:
        description: "Polynomial decrease to final learning rate"
        
        formula: |
          α_t = (α_0 - α_final) × (1 - t/T)^p + α_final
          
          Where:
          - α_final = final learning rate (e.g., 0.0001)
          - T = total epochs
          - p = polynomial power (typically 1.0 or 2.0)
        
        implementation: |
          class PolynomialLRScheduler:
              def __init__(self, initial_lr, final_lr, total_epochs, power=1.0):
                  self.initial_lr = initial_lr
                  self.final_lr = final_lr
                  self.total_epochs = total_epochs
                  self.power = power
              
              def get_lr(self, epoch):
                  if epoch >= self.total_epochs:
                      return self.final_lr
                  decay = (1 - epoch / self.total_epochs) ** self.power
                  return (self.initial_lr - self.final_lr) * decay + self.final_lr
      
      cosine_annealing:
        description: "Cosine curve decay (smooth, popular in modern training)"
        
        formula: |
          α_t = α_min + (α_max - α_min) × (1 + cos(πt/T)) / 2
          
          Where:
          - α_max = initial learning rate
          - α_min = minimum learning rate
          - T = total epochs
        
        characteristics:
          - "Smooth decrease (no sudden drops)"
          - "Popular in recent research (ResNet, Transformers)"
          - "Can restart periodically (cosine annealing with restarts)"
        
        implementation: |
          class CosineAnnealingLRScheduler:
              def __init__(self, initial_lr, min_lr, total_epochs):
                  self.initial_lr = initial_lr
                  self.min_lr = min_lr
                  self.total_epochs = total_epochs
              
              def get_lr(self, epoch):
                  if epoch >= self.total_epochs:
                      return self.min_lr
                  cos_inner = np.pi * epoch / self.total_epochs
                  return self.min_lr + 0.5 * (self.initial_lr - self.min_lr) * (1 + np.cos(cos_inner))
    
    training_with_scheduling:
      
      modified_training_loop: |
        def train_with_lr_schedule(network, X_train, y_train, X_val, y_val,
                                   scheduler, batch_size=32, epochs=50):
            """
            Train with learning rate scheduling.
            
            Parameters:
            - scheduler: LR scheduler object with get_lr(epoch) method
            """
            n_samples = X_train.shape[1]
            n_batches = n_samples // batch_size
            
            history = {
                'train_loss': [],
                'val_loss': [],
                'val_accuracy': [],
                'learning_rate': []
            }
            
            for epoch in range(epochs):
                # Get learning rate for this epoch
                current_lr = scheduler.get_lr(epoch)
                history['learning_rate'].append(current_lr)
                
                # Shuffle and train
                indices = np.random.permutation(n_samples)
                X_shuffled = X_train[:, indices]
                y_shuffled = y_train[:, indices]
                
                epoch_losses = []
                
                for batch in range(n_batches):
                    start = batch * batch_size
                    end = start + batch_size
                    X_batch = X_shuffled[:, start:end]
                    y_batch = y_shuffled[:, start:end]
                    
                    loss, _ = network.forward(X_batch, y_batch)
                    epoch_losses.append(loss)
                    network.backward()
                    
                    # Update with current learning rate
                    network.update_parameters(current_lr)
                
                # Validation
                train_loss = np.mean(epoch_losses)
                val_loss, val_acc = evaluate(network, X_val, y_val)
                
                history['train_loss'].append(train_loss)
                history['val_loss'].append(val_loss)
                history['val_accuracy'].append(val_acc)
                
                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch+1:2d}/{epochs}: "
                          f"lr={current_lr:.6f}, "
                          f"train_loss={train_loss:.4f}, "
                          f"val_acc={val_acc:.4f}")
            
            return history
      
      example_usage: |
        # Create scheduler
        scheduler = CosineAnnealingLRScheduler(
            initial_lr=0.1,
            min_lr=0.0001,
            total_epochs=50
        )
        
        # Train with scheduling
        network = NeuralNetwork([784, 128, 64, 10])
        history = train_with_lr_schedule(
            network, X_train, y_train, X_val, y_val,
            scheduler=scheduler,
            batch_size=32,
            epochs=50
        )
        
        # Visualize learning rate schedule
        plt.figure(figsize=(10, 4))
        plt.plot(history['learning_rate'])
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.title('Learning Rate Schedule')
        plt.grid(True)
        plt.show()
  
  # --------------------------------------------------------------------------
  # Topic 6: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    batch_size_and_poisoning:
      
      large_batches_smooth_gradients:
        observation: |
          Poisoned samples contribute malicious gradients.
          Large batches average out these gradients.
        
        example: |
          1% poisoning rate (1 malicious sample per 100)
          
          Batch size 32: 0-1 poisoned samples per batch
          → Poisoned gradients occasionally dominate
          
          Batch size 256: 2-3 poisoned samples per batch
          → Poisoned gradients diluted by 253 clean samples
        
        implication: "Larger batches make poisoning attacks harder to detect (smoother loss)"
      
      sgd_noise_detection:
        method: |
          SGD's high gradient variance can expose anomalies:
          - Clean samples: consistent gradient direction
          - Poisoned samples: gradient points toward backdoor
        
        detection: "Monitor per-sample gradient directions, flag outliers"
    
    learning_rate_and_backdoors:
      
      high_lr_erases_backdoors:
        observation: |
          Backdoor learned early in training.
          High learning rate later can overwrite backdoor.
        
        defense: "Restart with high LR to potentially erase backdoors"
      
      low_lr_preserves_backdoors:
        observation: |
          Once backdoor embedded, low LR updates don't remove it.
        
        attack_strategy: "Poison early, then backdoor persists through fine-tuning"
    
    membership_inference:
      
      batch_size_impact:
        small_batch: |
          High gradient variance per sample.
          Training samples have lower variance (model memorized).
          Easier to distinguish training vs test samples.
        
        large_batch: |
          Low gradient variance (averaged).
          Harder to distinguish individual sample influence.
          More privacy-preserving.
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Three variants: Batch GD (all samples), SGD (1 sample), Mini-batch (B samples per update)"
      - "Mini-batch dominates: combines speed (N/B updates/epoch) and stability (B-sample averaging)"
      - "Batch size tradeoffs: small=more noise/updates, large=more stable/fewer updates"
      - "Optimal batch size: 32-256 typical, increase until GPU ~80% utilized"
      - "Learning rate must decay: start high (fast progress), end low (converge)"
      - "Gradient variance: Batch GD has 0 variance, SGD has high variance, mini-batch has variance/√B"
    
    actionable_steps:
      - "Use mini-batch GD with batch size 32-128 as default (works 90% of time)"
      - "Increase batch size until GPU memory ~80% full (maximize throughput)"
      - "Always use learning rate scheduling: cosine annealing or step decay"
      - "Start with α=0.01-0.1, decay to α=0.0001-0.001 by end"
      - "Monitor GPU utilization: <50% means increase batch size"
      - "Compare convergence: plot loss vs epochs AND loss vs wall-clock time"
    
    security_principles:
      - "Large batches hide poisoning: malicious gradients diluted by clean samples"
      - "SGD noise helps backdoor detection: anomalous gradients more visible"
      - "Learning rate schedule affects backdoor persistence: high LR can erase, low LR preserves"
      - "Small batches leak more information: easier membership inference via gradient variance"
    
    practical_wisdom:
      - "Never use pure batch GD or pure SGD: mini-batch always better in practice"
      - "Batch size is hardware constraint: fit as many as memory allows"
      - "Learning rate more important than batch size: spend time tuning LR schedule"
      - "Rule of thumb: double batch size → multiply LR by √2 (linear scaling rule)"
      - "When in doubt: batch=64, α_initial=0.01, cosine decay to 0.0001 over training"

---
