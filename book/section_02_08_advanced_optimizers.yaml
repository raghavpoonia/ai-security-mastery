# section_02_08_advanced_optimizers.yaml
---
document_info:
  chapter: "02"
  section: "08"
  title: "Advanced Optimizers"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-10"
  estimated_pages: 6
  tags: ["optimization", "momentum", "rmsprop", "adam", "adamw", "adaptive-learning-rate"]

# ============================================================================
# SECTION 02_08: ADVANCED OPTIMIZERS
# ============================================================================

section_02_08_advanced_optimizers:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Vanilla gradient descent has problems: slow convergence, sensitivity to
    learning rate, stuck in ravines. Advanced optimizers solve these issues
    using momentum (accumulate velocity), adaptive learning rates (per-parameter
    scaling), or both.
    
    This section covers the four most important optimizers: Momentum (SGD with
    velocity), RMSprop (adaptive learning rates), Adam (momentum + adaptive),
    and AdamW (Adam with proper weight decay). These are the workhorses of
    modern deep learning - 99% of papers use Adam or AdamW.
    
    You'll implement all four from scratch, understand their mathematical
    foundations, compare convergence behavior, learn hyperparameter tuning,
    and know when to use each optimizer.
  
  learning_objectives:
    
    conceptual:
      - "Understand momentum as velocity accumulation"
      - "Grasp adaptive learning rates (why and how)"
      - "Know how Adam combines momentum and adaptation"
      - "Understand weight decay vs L2 regularization difference"
      - "Recognize optimizer convergence patterns"
      - "Connect hyperparameters to optimization behavior"
    
    practical:
      - "Implement Momentum, RMSprop, Adam, AdamW from scratch"
      - "Compare convergence speed and stability"
      - "Tune hyperparameters (β1, β2, ε, weight_decay)"
      - "Visualize optimization trajectories"
      - "Select appropriate optimizer for task"
      - "Debug optimizer issues (exploding, NaN, poor convergence)"
    
    security_focused:
      - "Momentum amplifies adversarial gradients"
      - "Adaptive rates affect backdoor stability"
      - "Adam's bias correction leaks training progress"
      - "Weight decay impacts model robustness"
  
  prerequisites:
    - "Section 02_07 (gradient descent variants)"
    - "Understanding of gradient computation"
    - "Exponential moving averages"
  
  # --------------------------------------------------------------------------
  # Topic 1: Momentum - Adding Velocity
  # --------------------------------------------------------------------------
  
  momentum:
    
    motivation:
      
      problem_with_vanilla_gd:
        ravines: |
          In ravine-shaped loss surfaces (steep in one direction, shallow in another):
          - Gradient points mostly down steep walls (not toward optimum)
          - Oscillates across ravine, slow progress along bottom
          - Wastes updates bouncing side-to-side
        
        local_minima: |
          Gradient descent can get stuck in poor local minima.
          No way to escape once gradient becomes small.
        
        visual: |
          Loss surface:
                    ╱╲
                   ╱  ╲
          Optimum→ •   ╲
                  ╱     ╲
                 ╱       ╲
          
          Vanilla GD: Zigzags down walls, slow progress to •
          Momentum: Accumulates velocity, shoots straight to •
      
      solution: "Add momentum - accumulate velocity in consistent directions"
    
    algorithm:
      
      update_rule: |
        v_t = β·v_{t-1} + ∇L_t
        θ_t = θ_{t-1} - α·v_t
        
        Where:
        - v_t = velocity at step t
        - β = momentum coefficient (typically 0.9)
        - ∇L_t = gradient at step t
        - α = learning rate
      
      intuition: |
        Think of ball rolling downhill:
        - Gradient = current slope (instantaneous force)
        - Velocity = accumulated momentum (from past gradients)
        - β = friction coefficient (0.9 = low friction, keeps rolling)
        
        Past gradients influence current update (exponentially decaying):
        v_t = ∇L_t + β·∇L_{t-1} + β²·∇L_{t-2} + β³·∇L_{t-3} + ...
      
      nesterov_variant: |
        Nesterov Accelerated Gradient (NAG):
        Look ahead before computing gradient:
        
        θ_lookahead = θ_{t-1} - α·β·v_{t-1}
        v_t = β·v_{t-1} + ∇L(θ_lookahead)
        θ_t = θ_{t-1} - α·v_t
        
        Advantage: "Look before you leap" - more stable
    
    implementation:
      
      momentum_optimizer: |
        class MomentumOptimizer:
            """
            SGD with momentum.
            
            Parameters:
            - learning_rate: step size (α)
            - momentum: velocity decay (β), typically 0.9
            - nesterov: use Nesterov variant (default False)
            """
            
            def __init__(self, parameters, learning_rate=0.01, momentum=0.9, nesterov=False):
                self.learning_rate = learning_rate
                self.momentum = momentum
                self.nesterov = nesterov
                
                # Initialize velocities (one per parameter)
                self.velocities = {}
                for param_name, param in parameters.items():
                    self.velocities[param_name] = np.zeros_like(param)
            
            def step(self, parameters, gradients):
                """
                Update parameters using momentum.
                
                Parameters:
                - parameters: dict {param_name: param_value}
                - gradients: dict {param_name: gradient}
                """
                for param_name in parameters:
                    param = parameters[param_name]
                    grad = gradients[param_name]
                    v = self.velocities[param_name]
                    
                    if self.nesterov:
                        # Nesterov momentum
                        v = self.momentum * v + grad
                        param -= self.learning_rate * (self.momentum * v + grad)
                    else:
                        # Standard momentum
                        v = self.momentum * v + grad
                        param -= self.learning_rate * v
                    
                    # Update velocity
                    self.velocities[param_name] = v
      
      usage_example: |
        # Create optimizer
        params = network.get_parameters()
        optimizer = MomentumOptimizer(
            parameters=params,
            learning_rate=0.01,
            momentum=0.9
        )
        
        # Training loop
        for epoch in range(epochs):
            for X_batch, y_batch in dataloader:
                # Forward and backward
                loss, _ = network.forward(X_batch, y_batch)
                network.backward()
                
                # Get gradients and update
                grads = network.get_gradients()
                optimizer.step(params, grads)
    
    hyperparameters:
      
      momentum_coefficient: |
        β = 0.0: No momentum (vanilla GD)
        β = 0.5: Moderate momentum
        β = 0.9: Standard choice (most common)
        β = 0.95: High momentum (faster but less stable)
        β = 0.99: Very high momentum (use with caution)
        
        Rule of thumb: Start with 0.9, increase if loss curve too noisy
      
      learning_rate_with_momentum: |
        Momentum effectively amplifies learning rate:
        
        Effective LR ≈ α / (1 - β)
        
        Example:
        α = 0.01, β = 0.9 → Effective LR ≈ 0.1 (10x amplification!)
        
        Guideline: Use smaller α with momentum than vanilla GD
    
    advantages_disadvantages:
      
      advantages:
        - "Faster convergence: accumulates velocity in consistent directions"
        - "Escapes ravines: builds speed along valleys"
        - "Smooths noise: averages out gradient fluctuations"
        - "Helps escape local minima: momentum carries through flat regions"
      
      disadvantages:
        - "Overshoots: can bounce past minimum if momentum too high"
        - "Hyperparameter: need to tune β"
        - "Memory: stores velocity for each parameter (2x memory)"
  
  # --------------------------------------------------------------------------
  # Topic 2: RMSprop - Adaptive Learning Rates
  # --------------------------------------------------------------------------
  
  rmsprop:
    
    motivation:
      
      problem_with_fixed_lr:
        observation: |
          Different parameters need different learning rates:
          - Some parameters: large gradients (need small LR)
          - Other parameters: small gradients (need large LR)
          
          Fixed LR is compromise (suboptimal for all parameters)
        
        example: |
          Parameter W1: gradients typically 0.01
          Parameter W10: gradients typically 10.0
          
          If α = 0.01:
          - W1 updates: 0.01 × 0.01 = 0.0001 (too small, slow)
          - W10 updates: 0.01 × 10.0 = 0.1 (reasonable)
          
          If α = 1.0:
          - W1 updates: 1.0 × 0.01 = 0.01 (reasonable)
          - W10 updates: 1.0 × 10.0 = 10.0 (too large, explodes!)
      
      solution: "Adapt learning rate per parameter based on gradient history"
    
    algorithm:
      
      update_rule: |
        s_t = β·s_{t-1} + (1-β)·(∇L_t)²
        θ_t = θ_{t-1} - α / √(s_t + ε) · ∇L_t
        
        Where:
        - s_t = running average of squared gradients
        - β = decay rate (typically 0.9 or 0.99)
        - ε = small constant for numerical stability (1e-8)
        - Division is element-wise
      
      intuition: |
        s_t tracks gradient magnitude history:
        - Parameters with large gradients: large s_t → small effective LR
        - Parameters with small gradients: small s_t → large effective LR
        
        Effective learning rate per parameter:
        α_effective = α / √(s_t + ε)
        
        Adapts automatically to gradient scale!
      
      why_squared_gradients: |
        Using (∇L)² instead of |∇L|:
        - Emphasizes recent large gradients
        - Smooth (differentiable everywhere)
        - Computationally efficient (no abs or sqrt in accumulation)
    
    implementation:
      
      rmsprop_optimizer: |
        class RMSpropOptimizer:
            """
            RMSprop: Adaptive learning rate per parameter.
            
            Parameters:
            - learning_rate: global learning rate (α)
            - decay: decay rate for running average (β), typically 0.9
            - epsilon: numerical stability constant (ε), typically 1e-8
            """
            
            def __init__(self, parameters, learning_rate=0.001, decay=0.9, epsilon=1e-8):
                self.learning_rate = learning_rate
                self.decay = decay
                self.epsilon = epsilon
                
                # Initialize squared gradient accumulators
                self.squared_grads = {}
                for param_name, param in parameters.items():
                    self.squared_grads[param_name] = np.zeros_like(param)
            
            def step(self, parameters, gradients):
                """
                Update parameters using RMSprop.
                """
                for param_name in parameters:
                    param = parameters[param_name]
                    grad = gradients[param_name]
                    s = self.squared_grads[param_name]
                    
                    # Update squared gradient running average
                    s = self.decay * s + (1 - self.decay) * (grad ** 2)
                    
                    # Compute adaptive learning rate
                    # Element-wise division: each element gets its own LR
                    adapted_lr = self.learning_rate / (np.sqrt(s) + self.epsilon)
                    
                    # Update parameter
                    param -= adapted_lr * grad
                    
                    # Save updated squared gradients
                    self.squared_grads[param_name] = s
      
      numerical_stability: |
        Why epsilon (ε) is critical:
        
        Without ε:
        - If s_t = 0 (no gradients yet): division by zero → NaN
        - If s_t very small: division by tiny number → explodes
        
        With ε = 1e-8:
        - Minimum effective LR: α / √(1e-8) ≈ 10,000·α (bounded)
        - Prevents division by zero
        - Doesn't affect adaptation when s_t >> ε
    
    hyperparameters:
      
      decay_rate: |
        β = 0.9: Standard choice, adapts quickly to recent gradients
        β = 0.95: Slower adaptation, more stable
        β = 0.99: Very slow adaptation, long memory
        
        Lower β: More responsive to gradient changes (noisy)
        Higher β: More stable but slower to adapt
      
      learning_rate: |
        RMSprop typically uses smaller base LR than SGD:
        - SGD: α = 0.01-0.1
        - RMSprop: α = 0.001-0.01
        
        Reason: Adaptation can amplify effective LR significantly
      
      epsilon: |
        Typical values: 1e-8 to 1e-6
        Rarely needs tuning (1e-8 works almost always)
    
    advantages_disadvantages:
      
      advantages:
        - "Per-parameter adaptation: each parameter gets appropriate LR"
        - "Handles ill-conditioned problems: different gradient scales"
        - "Less sensitive to global LR: adaptation compensates"
        - "Works well for RNNs: helps with vanishing/exploding gradients"
      
      disadvantages:
        - "Can be too aggressive: may adapt too quickly"
        - "Accumulator grows unbounded: s_t always increases (never forgets)"
        - "Hyperparameters: need to tune β and α"
        - "Memory: stores squared gradients (2x memory)"
  
  # --------------------------------------------------------------------------
  # Topic 3: Adam - Combining Momentum and Adaptation
  # --------------------------------------------------------------------------
  
  adam:
    
    motivation:
      
      best_of_both_worlds: |
        Momentum: Good for accelerating and smoothing
        RMSprop: Good for adapting per-parameter
        
        Adam: Combine both!
        - First moment (mean): like momentum
        - Second moment (variance): like RMSprop
      
      why_adam_dominates: "Most robust optimizer, works well with default hyperparameters"
    
    algorithm:
      
      update_rule: |
        # Compute moments
        m_t = β1·m_{t-1} + (1-β1)·∇L_t          (first moment)
        v_t = β2·v_{t-1} + (1-β2)·(∇L_t)²       (second moment)
        
        # Bias correction
        m̂_t = m_t / (1 - β1^t)
        v̂_t = v_t / (1 - β2^t)
        
        # Update
        θ_t = θ_{t-1} - α · m̂_t / (√v̂_t + ε)
        
        Where:
        - β1 = 0.9 (first moment decay)
        - β2 = 0.999 (second moment decay)
        - ε = 1e-8 (numerical stability)
        - t = current iteration
      
      intuition: |
        m_t: exponential moving average of gradients (like momentum)
        v_t: exponential moving average of squared gradients (like RMSprop)
        
        Bias correction: m_t and v_t initially biased toward 0
        (because initialized at 0). Correction fixes this.
        
        Update: move in direction of m̂_t (momentum), scaled by 1/√v̂_t (adaptation)
    
    bias_correction:
      
      why_necessary: |
        Without correction:
        
        Iteration 1:
        m_1 = 0.9·0 + 0.1·∇L_1 = 0.1·∇L_1 (biased low!)
        v_1 = 0.999·0 + 0.001·(∇L_1)² = 0.001·(∇L_1)² (biased low!)
        
        Early iterations: m_t and v_t much smaller than true values
        → Effective LR too large → instability
      
      with_correction: |
        m̂_1 = 0.1·∇L_1 / (1 - 0.9^1) = 0.1·∇L_1 / 0.1 = ∇L_1 (corrected!)
        
        As t → ∞:
        (1 - β^t) → 1, so m̂_t → m_t (correction vanishes)
      
      impact: |
        Without correction: unstable first few iterations
        With correction: stable from iteration 1
        
        Critical for good convergence!
    
    implementation:
      
      adam_optimizer: |
        class AdamOptimizer:
            """
            Adam: Adaptive Moment Estimation.
            
            Combines momentum (first moment) and RMSprop (second moment).
            
            Parameters:
            - learning_rate: step size (α), default 0.001
            - beta1: first moment decay (β1), default 0.9
            - beta2: second moment decay (β2), default 0.999
            - epsilon: numerical stability (ε), default 1e-8
            """
            
            def __init__(self, parameters, learning_rate=0.001, 
                        beta1=0.9, beta2=0.999, epsilon=1e-8):
                self.learning_rate = learning_rate
                self.beta1 = beta1
                self.beta2 = beta2
                self.epsilon = epsilon
                
                # Initialize moments
                self.m = {}  # First moment (momentum)
                self.v = {}  # Second moment (variance)
                
                for param_name, param in parameters.items():
                    self.m[param_name] = np.zeros_like(param)
                    self.v[param_name] = np.zeros_like(param)
                
                # Iteration counter for bias correction
                self.t = 0
            
            def step(self, parameters, gradients):
                """
                Update parameters using Adam.
                """
                self.t += 1  # Increment iteration counter
                
                for param_name in parameters:
                    param = parameters[param_name]
                    grad = gradients[param_name]
                    
                    # Get current moments
                    m = self.m[param_name]
                    v = self.v[param_name]
                    
                    # Update biased first moment
                    m = self.beta1 * m + (1 - self.beta1) * grad
                    
                    # Update biased second moment
                    v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)
                    
                    # Bias correction
                    m_hat = m / (1 - self.beta1 ** self.t)
                    v_hat = v / (1 - self.beta2 ** self.t)
                    
                    # Update parameter
                    param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)
                    
                    # Save updated moments
                    self.m[param_name] = m
                    self.v[param_name] = v
      
      memory_usage: |
        Adam requires:
        - Parameters: 1x memory
        - First moments (m): 1x memory
        - Second moments (v): 1x memory
        
        Total: 3x parameter memory
        
        Example: 1M parameters = 4MB
        Adam state: 8MB (m + v)
        Total: 12MB (manageable)
    
    hyperparameters:
      
      default_values: |
        α = 0.001 (learning rate)
        β1 = 0.9 (first moment decay)
        β2 = 0.999 (second moment decay)
        ε = 1e-8 (epsilon)
        
        These defaults work 90% of the time!
      
      tuning_guidelines:
        learning_rate: |
          Most important hyperparameter.
          
          Try: 0.1, 0.01, 0.001, 0.0001
          Usually 0.001 or 0.0001 works best
        
        beta1: |
          Rarely needs tuning.
          Increase to 0.95 if training unstable.
          Decrease to 0.8 if want less momentum.
        
        beta2: |
          Rarely needs tuning.
          Can decrease to 0.99 for sparse gradients.
        
        epsilon: |
          Almost never needs tuning.
          Increase to 1e-7 or 1e-6 if getting NaNs.
    
    advantages_disadvantages:
      
      advantages:
        - "Robust: works well across wide variety of problems"
        - "Default hyperparameters: rarely need tuning"
        - "Per-parameter adaptation: handles different scales"
        - "Momentum: accelerates convergence"
        - "Most popular: 99% of papers use Adam or variant"
      
      disadvantages:
        - "Memory: 3x parameter memory (m, v, params)"
        - "Generalization: sometimes worse than SGD+momentum"
        - "Weight decay: naive implementation incorrect (use AdamW)"
  
  # --------------------------------------------------------------------------
  # Topic 4: AdamW - Adam with Decoupled Weight Decay
  # --------------------------------------------------------------------------
  
  adamw:
    
    motivation:
      
      weight_decay_vs_l2_regularization:
        l2_regularization: |
          Add L2 penalty to loss:
          L_total = L + λ/2 · ||θ||²
          
          Gradient: ∇L_total = ∇L + λ·θ
        
        weight_decay: |
          Directly shrink weights:
          θ_t = θ_{t-1} - α·∇L - α·λ·θ_{t-1}
          
          Equivalent for vanilla SGD, but NOT for Adam!
      
      problem_with_adam_and_l2: |
        L2 regularization in Adam:
        Gradient becomes ∇L + λ·θ
        Adam adapts this gradient (divides by √v̂)
        
        Weight decay gets adapted too! (Wrong!)
        Large weights → large λ·θ → large second moment → small effective decay
        
        Result: Weight decay doesn't work as intended in Adam
      
      adamw_solution: "Decouple weight decay from gradient-based update"
    
    algorithm:
      
      update_rule: |
        # Same as Adam
        m_t = β1·m_{t-1} + (1-β1)·∇L_t
        v_t = β2·v_{t-1} + (1-β2)·(∇L_t)²
        m̂_t = m_t / (1 - β1^t)
        v̂_t = v_t / (1 - β2^t)
        
        # Different: decoupled weight decay
        θ_t = θ_{t-1} - α·m̂_t / (√v̂_t + ε) - α·λ·θ_{t-1}
                       \_______________________/   \__________/
                         Adam gradient update      Weight decay
      
      key_difference: |
        Adam with L2:
        - Gradient: ∇L + λ·θ
        - Gets adapted by Adam
        
        AdamW:
        - Gradient: ∇L
        - Gets adapted by Adam
        - Weight decay: applied separately, NOT adapted
    
    implementation:
      
      adamw_optimizer: |
        class AdamWOptimizer:
            """
            AdamW: Adam with decoupled weight decay.
            
            Parameters:
            - weight_decay: coefficient for weight decay (λ), default 0.01
            - Other parameters same as Adam
            """
            
            def __init__(self, parameters, learning_rate=0.001,
                        beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.01):
                self.learning_rate = learning_rate
                self.beta1 = beta1
                self.beta2 = beta2
                self.epsilon = epsilon
                self.weight_decay = weight_decay
                
                # Initialize moments
                self.m = {}
                self.v = {}
                for param_name, param in parameters.items():
                    self.m[param_name] = np.zeros_like(param)
                    self.v[param_name] = np.zeros_like(param)
                
                self.t = 0
            
            def step(self, parameters, gradients):
                """
                Update parameters using AdamW.
                """
                self.t += 1
                
                for param_name in parameters:
                    param = parameters[param_name]
                    grad = gradients[param_name]
                    
                    # Get moments
                    m = self.m[param_name]
                    v = self.v[param_name]
                    
                    # Update moments (using gradient WITHOUT weight decay)
                    m = self.beta1 * m + (1 - self.beta1) * grad
                    v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)
                    
                    # Bias correction
                    m_hat = m / (1 - self.beta1 ** self.t)
                    v_hat = v / (1 - self.beta2 ** self.t)
                    
                    # Adam update
                    adam_update = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)
                    
                    # Weight decay (decoupled, applied to original parameter)
                    decay_update = self.learning_rate * self.weight_decay * param
                    
                    # Combined update
                    param -= adam_update + decay_update
                    
                    # Save moments
                    self.m[param_name] = m
                    self.v[param_name] = v
    
    hyperparameters:
      
      weight_decay_coefficient: |
        λ = 0.0: No regularization
        λ = 0.01: Standard choice (most common)
        λ = 0.05: Strong regularization
        λ = 0.1: Very strong regularization
        
        Guideline:
        - Start with 0.01
        - Increase if overfitting (large gap between train/val loss)
        - Decrease if underfitting (both train and val loss high)
      
      when_to_exclude_weight_decay: |
        Don't apply weight decay to:
        - Biases (only decay weights)
        - Batch normalization parameters (γ, β)
        - Embedding layers
        
        Implementation:
        if param_name.endswith('b') or param_name.startswith('bn'):
            # Skip weight decay for biases and BN params
            decay_update = 0
    
    advantages_over_adam:
      
      better_generalization:
        observation: "AdamW often achieves better test accuracy than Adam"
        reason: "Proper weight decay acts as effective regularization"
      
      consistent_with_sgd:
        observation: "Weight decay behaves same as SGD+momentum+weight_decay"
        reason: "Decoupled, not mixed with adaptive learning rate"
      
      recommended_default: "Use AdamW instead of Adam in most cases"
  
  # --------------------------------------------------------------------------
  # Topic 5: Optimizer Comparison and Selection
  # --------------------------------------------------------------------------
  
  optimizer_comparison:
    
    side_by_side_table: |
      Optimizer    | Momentum | Adaptive | Memory | Best For
      -------------|----------|----------|--------|---------------------------
      SGD          | No       | No       | 1x     | Baseline
      Momentum     | Yes      | No       | 2x     | Convex, simple problems
      RMSprop      | No       | Yes      | 2x     | RNNs, non-stationary
      Adam         | Yes      | Yes      | 3x     | Default choice, most tasks
      AdamW        | Yes      | Yes      | 3x     | Default + regularization
    
    convergence_speed:
      
      typical_behavior: |
        Epochs to 95% validation accuracy on MNIST:
        
        SGD (no momentum): 50 epochs
        SGD + Momentum: 30 epochs
        RMSprop: 20 epochs
        Adam: 15 epochs
        AdamW: 15 epochs
        
        Adam/AdamW converge fastest!
    
    generalization:
      
      observation: |
        Sometimes SGD + momentum achieves better test accuracy than Adam.
        
        Theory: Adam's adaptive LR may converge to "sharp" minima
        (high loss curvature) which generalize worse.
        
        Practice: For most tasks, Adam/AdamW fine. For state-of-the-art
        results on specific benchmarks, SGD + momentum sometimes better.
    
    selection_guidelines:
      
      default_choice: |
        Use AdamW with default hyperparameters:
        - learning_rate = 0.001
        - beta1 = 0.9
        - beta2 = 0.999
        - weight_decay = 0.01
        
        Works 90% of the time!
      
      when_to_use_sgd_momentum:
        - "Training from scratch on large datasets (ImageNet)"
        - "When test accuracy critical (research benchmarks)"
        - "When have time to tune hyperparameters extensively"
        - "Following specific paper's training recipe"
      
      when_to_use_rmsprop:
        - "Recurrent neural networks (RNNs, LSTMs)"
        - "When gradients highly variable"
        - "Legacy code (less common now, Adam preferred)"
      
      when_to_use_adam:
        - "Default choice for most problems"
        - "Fine-tuning pretrained models"
        - "When need fast convergence"
        - "When don't have time for extensive tuning"
  
  # --------------------------------------------------------------------------
  # Topic 6: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    momentum_amplifies_adversarial_gradients:
      
      observation: |
        Adversarial examples computed via:
        δ* = argmax_δ L(f(x + δ), y_wrong)
        
        With momentum, attack becomes:
        v_t = β·v_{t-1} + ∇_x L
        δ_t = δ_{t-1} + α·v_t
        
        Momentum accumulates adversarial gradients → stronger attacks!
      
      implication: "Momentum-based optimizers more vulnerable to iterative attacks"
    
    adaptive_rates_and_backdoors:
      
      backdoor_persistence:
        observation: |
          Backdoor parameters have small gradients (only triggered by rare inputs).
          Adam's adaptation: small gradients → small second moment → large effective LR.
          
          Result: Backdoor updates amplified, harder to remove.
        
        defense: "SGD with fixed LR may be more resistant to backdoors"
      
      detection_via_optimizer_state:
        method: |
          Adam's moments (m, v) encode gradient history.
          Backdoored parameters: anomalous moment patterns.
          
          Detection: Analyze m and v distributions, flag outliers.
    
    bias_correction_leaks_progress:
      
      information_leakage: |
        Bias correction factor: 1 - β^t
        
        As t increases, factor approaches 1.
        Observing model outputs, adversary can infer training progress.
        
        Privacy concern: Reveals how long model has been training.
      
      mitigation: "Use fixed bias correction after warmup (e.g., t > 1000)"
    
    weight_decay_and_robustness:
      
      observation: |
        Weight decay encourages smaller weights.
        Smaller weights → less sensitive to input perturbations.
        
        AdamW with higher weight_decay → more robust models.
      
      tradeoff: "Higher weight_decay improves robustness but may reduce accuracy"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Momentum adds velocity: v_t = β·v_{t-1} + ∇L, accumulates gradients in consistent directions"
      - "RMSprop adapts per-parameter LR: divides by √(running avg of grad²), handles different scales"
      - "Adam combines both: momentum (first moment) + adaptation (second moment) + bias correction"
      - "AdamW decouples weight decay: applies weight decay separately, not through gradient"
      - "Bias correction essential: m and v biased toward 0 initially, correction fixes early instability"
      - "Memory costs: Momentum 2x, RMSprop 2x, Adam/AdamW 3x (m, v, params)"
    
    actionable_steps:
      - "Use AdamW as default: learning_rate=0.001, beta1=0.9, beta2=0.999, weight_decay=0.01"
      - "Tune learning rate first: most important hyperparameter, try 0.1, 0.01, 0.001, 0.0001"
      - "Start with defaults for β1, β2: rarely need tuning, 0.9 and 0.999 work 95% of time"
      - "Increase weight_decay if overfitting: 0.01 → 0.05 → 0.1"
      - "Switch to SGD+momentum for SOTA: if AdamW not achieving best results after extensive tuning"
      - "Monitor gradient norms: if exploding despite Adam, check LR and gradient clipping"
    
    security_principles:
      - "Momentum amplifies attacks: iterative adversarial examples stronger with momentum optimizers"
      - "Adaptive rates affect backdoors: Adam's adaptation may amplify backdoor updates"
      - "Optimizer state leaks info: m and v distributions can reveal backdoored parameters"
      - "Weight decay improves robustness: AdamW with higher weight_decay → less sensitive to perturbations"
    
    debugging_tips:
      - "Loss explodes: reduce learning rate by 10x"
      - "Loss plateaus early: increase learning rate or reduce weight_decay"
      - "NaN in training: reduce learning rate, increase epsilon (1e-8 → 1e-6)"
      - "Slow convergence: check if Adam parameters updated correctly (verify m, v not zero)"
      - "Memory issues: use gradient checkpointing or switch to SGD+momentum (2x memory vs 3x)"

---
