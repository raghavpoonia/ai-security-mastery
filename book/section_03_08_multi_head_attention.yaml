# section_03_08_multi_head_attention.yaml

---
document_info:
  title: "Multi-Head Attention: Parallel Attention Pathways"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 8
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into multi-head attention: parallel attention heads, head-specific projections, concatenation, head specialization patterns, and security implications of distributed attention"
  estimated_pages: 7
  tags:
    - multi-head-attention
    - attention-heads
    - parallel-attention
    - head-specialization
    - attention-ensemble
    - transformer-architecture

section_overview:
  title: "Multi-Head Attention: Parallel Attention Pathways"
  number: "3.8"
  
  purpose: |
    Multi-head attention (Vaswani et al., 2017) is the key innovation that makes 
    transformers powerful. Instead of a single attention mechanism, multi-head attention 
    runs multiple scaled dot-product attention operations in parallel, each with its own 
    learned projections. Different heads can learn to focus on different aspects: syntax, 
    semantics, long-range dependencies, local patterns.
    
    The architecture projects Q, K, V into h separate subspaces (h heads), computes 
    attention independently in each head, then concatenates and projects the results. 
    With typical h=8 or h=16 heads, the model gains ensemble benefits while maintaining 
    computational efficiency through parallelization.
    
    For security engineers: Different attention heads specialize in different patterns - 
    some may focus on sensitive attributes (names, credentials, locations). Understanding 
    head specialization reveals privacy risks and enables targeted attacks. Analyzing 
    individual head behaviors is critical for auditing transformer security.
  
  learning_objectives:
    conceptual:
      - "Understand why multiple attention heads improve model capacity"
      - "Grasp how head-specific projections create independent subspaces"
      - "Learn how heads specialize to capture different patterns"
      - "Understand concatenation and output projection"
      - "See multi-head attention as ensemble of attention mechanisms"
    
    practical:
      - "Implement multi-head attention from scratch (NumPy)"
      - "Create head-specific projection matrices (W_q, W_k, W_v per head)"
      - "Compute attention independently for each head in parallel"
      - "Concatenate head outputs and apply final projection"
      - "Visualize different head attention patterns"
    
    security_focused:
      - "Identify which heads focus on sensitive attributes"
      - "Understand head-specific attack vectors"
      - "Analyze head specialization for privacy risks"
      - "Detect adversarial manipulation of specific heads"
      - "Audit head behaviors for unexpected patterns"
  
  prerequisites:
    knowledge:
      - "Section 3.7: Scaled dot-product attention, masking"
      - "Section 3.6: Query-Key-Value formulation"
      - "Linear algebra: matrix multiplication, projection"
      - "Understanding of ensemble methods"
    
    skills:
      - "NumPy advanced indexing and reshaping"
      - "Matrix operations and broadcasting"
      - "Parallel computation concepts"
      - "Visualization of attention patterns"
  
  key_transitions:
    from_section_3_7: |
      Section 3.7 covered single scaled dot-product attention: compute one set of 
      attention weights for all queries. Now we run multiple attention operations 
      in parallel (multi-head), each learning different patterns.
    
    to_next_section: |
      Section 3.9 will address a critical limitation of attention: it's permutation-
      invariant (doesn't know word order). We'll introduce positional encoding to 
      inject position information into transformers.

topics:
  - topic_number: 1
    title: "Why Multiple Heads? Capacity and Specialization"
    
    overview: |
      A single attention head has limited capacity - one set of weights to capture all 
      relationships. Multi-head attention runs h attention operations in parallel, each 
      with its own parameters. Different heads learn to specialize: one for syntax, 
      another for semantics, another for long-range dependencies. This ensemble approach 
      dramatically increases model capacity.
    
    content:
      single_head_limitations:
        capacity_constraint: |
          Single attention head learns one attention pattern:
          - One set of Q/K/V projections
          - One attention weight distribution per query
          - Must capture all relationships simultaneously
          
          Result: Limited expressiveness for complex dependencies
        
        example_conflict:
          syntactic_attention: |
            "The cat that I saw yesterday was sleeping"
            
            Syntactic: "cat" should attend to "was" (subject-verb)
          
          semantic_attention: |
            Same sentence:
            
            Semantic: "cat" should attend to "sleeping" (what cat was doing)
          
          conflict: |
            Single head must balance:
            - Syntactic relationship (cat → was)
            - Semantic relationship (cat → sleeping)
            
            Can't fully capture both simultaneously!
      
      multi_head_solution:
        key_idea: "Run multiple attention mechanisms in parallel, each can specialize"
        
        with_multiple_heads:
          head_1: "Focuses on syntax (cat → was)"
          head_2: "Focuses on semantics (cat → sleeping)"
          head_3: "Focuses on coreference (I → [implied person])"
          head_4: "Focuses on local context (cat → that)"
          
          combined: "All heads contribute to final representation"
        
        benefit: |
          Each head can specialize in one aspect
          → Capture multiple relationships simultaneously
          → Richer representations
      
      ensemble_perspective:
        analogy_to_ensemble_learning: |
          Machine learning ensembles: Train multiple models, combine predictions
          → More robust, captures different aspects
          
          Multi-head attention: Train multiple attention mechanisms, combine outputs
          → Same benefits: robustness, diverse patterns
        
        diversity_through_initialization: |
          Each head has different random initialization
          → Learns different patterns through training
          → Specialization emerges naturally
        
        combination_strategy: |
          Unlike voting/averaging in ensembles:
          Multi-head uses learned combination (output projection)
          → Model learns how to weight different heads
      
      empirical_benefits:
        performance_gains:
          single_head: "Baseline performance"
          multi_head_8: "+5-10% performance (typical)"
          multi_head_16: "+10-15% performance"
          
          diminishing_returns: "Beyond 16-32 heads, benefits plateau"
        
        interpretability: |
          Heads develop specialized roles:
          - Some focus on adjacent words
          - Some focus on distant dependencies
          - Some focus on specific linguistic patterns
          
          Easier to understand than single monolithic attention
        
        robustness: |
          If one head learns noisy pattern:
          - Other heads can compensate
          - Output projection learns to downweight noisy head
          
          More robust than single attention mechanism
    
    implementation:
      single_vs_multi_head_comparison:
        language: python
        code: |
          import numpy as np
          
          def demonstrate_head_specialization():
              """
              Demonstrate why multiple heads are beneficial.
              Simulate heads specializing on different patterns.
              """
              
              print("=== Single Head vs Multi-Head Attention ===\n")
              
              # Simulate a sentence
              positions = ["The", "cat", "that", "I", "saw", "was", "sleeping"]
              n = len(positions)
              
              print("Sentence:", " ".join(positions))
              print()
              
              # Single head: Must balance multiple relationships
              print("Single Head Attention:")
              print("  'cat' attention weights:")
              single_weights = {
                  "The": 0.15,
                  "cat": 0.25,  # Self
                  "that": 0.10,
                  "I": 0.05,
                  "saw": 0.05,
                  "was": 0.20,  # Syntactic (subject-verb)
                  "sleeping": 0.20  # Semantic (action)
              }
              
              for word, weight in single_weights.items():
                  bar = "█" * int(weight * 50)
                  print(f"    {word:10s}: {bar} {weight:.2f}")
              
              print("\n  Problem: Must split attention between syntax and semantics!")
              print("  'was' and 'sleeping' compete for attention weight.")
              
              # Multi-head: Different heads specialize
              print("\n\nMulti-Head Attention:")
              
              print("\n  Head 1 (Syntactic): 'cat' → subject-verb")
              head1_weights = {
                  "The": 0.10,
                  "cat": 0.15,
                  "that": 0.05,
                  "I": 0.05,
                  "saw": 0.05,
                  "was": 0.50,  # Strong focus on verb!
                  "sleeping": 0.10
              }
              
              for word, weight in head1_weights.items():
                  bar = "█" * int(weight * 50)
                  print(f"    {word:10s}: {bar} {weight:.2f}")
              
              print("\n  Head 2 (Semantic): 'cat' → action")
              head2_weights = {
                  "The": 0.10,
                  "cat": 0.15,
                  "that": 0.05,
                  "I": 0.05,
                  "saw": 0.10,
                  "was": 0.05,
                  "sleeping": 0.50  # Strong focus on action!
              }
              
              for word, weight in head2_weights.items():
                  bar = "█" * int(weight * 50)
                  print(f"    {word:10s}: {bar} {weight:.2f}")
              
              print("\n  Benefit: Each head focuses on one aspect!")
              print("  Head 1 captures syntax, Head 2 captures semantics.")
              print("  Combined output uses both signals.")
          
          demonstrate_head_specialization()
    
    security_implications:
      head_specialization_privacy_risk: |
        Heads specialize on different features:
        - Head 1 might focus on names (PII)
        - Head 2 might focus on numbers (credentials, SSN)
        - Head 3 might focus on locations (addresses)
        
        Adversary can:
        - Target specific heads for information extraction
        - Analyze head patterns to infer sensitive attributes
        - Manipulate inputs to trigger specific head behaviors
      
      ensemble_robustness_tradeoff: |
        Ensemble improves robustness BUT:
        - Adversary only needs to compromise one head
        - If Head 5 learns to leak information, other heads can't prevent it
        - Output projection might amplify malicious head's signal
        - Defense requires auditing ALL heads, not just overall model

  - topic_number: 2
    title: "Multi-Head Architecture: Projections and Parallelization"
    
    overview: |
      Multi-head attention projects Q, K, V into h separate subspaces using head-specific 
      linear transformations. Each head computes attention in its subspace independently. 
      Outputs are concatenated and projected to final representation. The architecture is 
      designed for perfect parallelization - all heads computed simultaneously.
    
    content:
      mathematical_formulation:
        overview: |
          MultiHead(Q, K, V) = Concat(head₁, ..., head_h) W^O
          
          Where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
        
        head_specific_projections:
          query_projection: "W_i^Q ∈ ℝ^(d_model × d_k)"
          key_projection: "W_i^K ∈ ℝ^(d_model × d_k)"
          value_projection: "W_i^V ∈ ℝ^(d_model × d_v)"
          
          note: "Each head has its own W^Q, W^K, W^V matrices"
        
        per_head_computation:
          input_q: "Q ∈ ℝ^(n × d_model)"
          projected_q: "Q_i = QW_i^Q ∈ ℝ^(n × d_k)"
          
          similarly: |
            K_i = KW_i^K ∈ ℝ^(n × d_k)
            V_i = VW_i^V ∈ ℝ^(n × d_v)
          
          attention: "head_i = Attention(Q_i, K_i, V_i) ∈ ℝ^(n × d_v)"
        
        concatenation:
          all_heads: "[head₁, head₂, ..., head_h] each of shape (n × d_v)"
          concatenated: "Concat ∈ ℝ^(n × h×d_v)"
        
        output_projection:
          weight_matrix: "W^O ∈ ℝ^(h×d_v × d_model)"
          final_output: "Output = Concat × W^O ∈ ℝ^(n × d_model)"
      
      dimensionality_choices:
        typical_configuration:
          d_model: "512 or 768 (model dimension)"
          num_heads: "8 or 16"
          d_k_per_head: "d_model / num_heads = 64"
          d_v_per_head: "d_model / num_heads = 64"
        
        why_d_k_equals_d_model_divided_by_h:
          reasoning: |
            Total capacity should match single-head case:
            
            Single head: Q, K, V each (n × d_model)
            Multi-head: h heads, each Q, K, V of size (n × d_k)
            
            To preserve capacity: h × d_k = d_model
            Therefore: d_k = d_model / h
          
          example: |
            d_model = 512, h = 8
            → d_k = 512 / 8 = 64 per head
        
        parameter_count:
          per_head: "3 × (d_model × d_k) for W^Q, W^K, W^V"
          all_heads: "h × 3 × (d_model × d_k) = 3 × d_model²"
          output_projection: "h × d_v × d_model = d_model²"
          total: "4 × d_model² parameters (same as single head with d_k=d_model!)"
      
      parallel_computation:
        sequential_would_be:
          process: |
            For i = 1 to h:
              Compute head_i
            Concatenate all heads
            Apply output projection
          
          problem: "Sequential - can't exploit GPU parallelism"
        
        parallel_implementation:
          reshape_trick: |
            Instead of looping over heads, reshape:
            
            Q: (batch, n, d_model)
            W^Q for all heads: (d_model, h, d_k)
            
            Q @ W^Q → (batch, n, h, d_k)
            Reshape → (batch, h, n, d_k)
            
            Now: All heads in batch dimension → parallel computation!
          
          benefit: "Single matrix multiplication for all heads simultaneously"
        
        computational_efficiency:
          operations: |
            Single head: O(n² × d_model)
            Multi-head (naively): h × O(n² × (d_model/h)) = O(n² × d_model)
            Multi-head (optimized): O(n² × d_model) with perfect parallelization
          
          conclusion: "Same complexity, but h attention patterns instead of 1!"
    
    implementation:
      multi_head_attention_from_scratch:
        language: python
        code: |
          class MultiHeadAttention:
              """
              Multi-head attention implementation from scratch.
              
              MultiHead(Q, K, V) = Concat(head₁, ..., head_h) W^O
              where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
              """
              
              def __init__(self, d_model: int, num_heads: int):
                  """
                  Args:
                      d_model: Model dimension (input/output size)
                      num_heads: Number of attention heads
                  """
                  assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
                  
                  self.d_model = d_model
                  self.num_heads = num_heads
                  self.d_k = d_model // num_heads  # Dimension per head
                  self.d_v = d_model // num_heads
                  
                  # Head-specific projection matrices
                  # Shape: (d_model, num_heads * d_k) to project all heads at once
                  self.W_q = np.random.randn(d_model, num_heads * self.d_k) * 0.01
                  self.W_k = np.random.randn(d_model, num_heads * self.d_k) * 0.01
                  self.W_v = np.random.randn(d_model, num_heads * self.d_v) * 0.01
                  
                  # Output projection
                  self.W_o = np.random.randn(num_heads * self.d_v, d_model) * 0.01
              
              def split_heads(self, x: np.ndarray) -> np.ndarray:
                  """
                  Split last dimension into (num_heads, d_k).
                  
                  Args:
                      x: (batch, seq_len, num_heads * d_k)
                  
                  Returns:
                      (batch, num_heads, seq_len, d_k)
                  """
                  batch_size, seq_len, _ = x.shape
                  # Reshape to (batch, seq_len, num_heads, d_k)
                  x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
                  # Transpose to (batch, num_heads, seq_len, d_k)
                  return x.transpose(0, 2, 1, 3)
              
              def scaled_dot_product_attention(self, Q: np.ndarray, K: np.ndarray, 
                                              V: np.ndarray, mask: np.ndarray = None) -> tuple:
                  """
                  Compute scaled dot-product attention.
                  
                  Args:
                      Q, K, V: (batch, num_heads, seq_len, d_k)
                      mask: Optional mask
                  
                  Returns:
                      output: (batch, num_heads, seq_len, d_v)
                      attention_weights: (batch, num_heads, seq_len, seq_len)
                  """
                  d_k = Q.shape[-1]
                  
                  # Scores: (batch, num_heads, seq_len, seq_len)
                  scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)
                  
                  # Apply mask if provided
                  if mask is not None:
                      scores = np.where(mask, scores, -1e9)
                  
                  # Softmax
                  attention_weights = self.softmax(scores, axis=-1)
                  
                  # Weighted sum of values
                  output = np.matmul(attention_weights, V)
                  
                  return output, attention_weights
              
              def softmax(self, x: np.ndarray, axis: int = -1) -> np.ndarray:
                  """Numerically stable softmax."""
                  exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
                  return exp_x / np.sum(exp_x, axis=axis, keepdims=True)
              
              def forward(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray,
                         mask: np.ndarray = None) -> tuple:
                  """
                  Multi-head attention forward pass.
                  
                  Args:
                      Q, K, V: (batch, seq_len, d_model)
                      mask: Optional attention mask
                  
                  Returns:
                      output: (batch, seq_len, d_model)
                      attention_weights: (batch, num_heads, seq_len, seq_len)
                  """
                  batch_size = Q.shape[0]
                  
                  # 1. Linear projections for all heads at once
                  # (batch, seq_len, d_model) @ (d_model, num_heads * d_k)
                  # = (batch, seq_len, num_heads * d_k)
                  Q_proj = np.matmul(Q, self.W_q)
                  K_proj = np.matmul(K, self.W_k)
                  V_proj = np.matmul(V, self.W_v)
                  
                  # 2. Split into multiple heads
                  # (batch, seq_len, num_heads * d_k) → (batch, num_heads, seq_len, d_k)
                  Q_heads = self.split_heads(Q_proj)
                  K_heads = self.split_heads(K_proj)
                  V_heads = self.split_heads(V_proj)
                  
                  # 3. Scaled dot-product attention for all heads in parallel
                  # (batch, num_heads, seq_len, d_v)
                  attention_output, attention_weights = self.scaled_dot_product_attention(
                      Q_heads, K_heads, V_heads, mask
                  )
                  
                  # 4. Concatenate heads
                  # (batch, num_heads, seq_len, d_v) → (batch, seq_len, num_heads, d_v)
                  attention_output = attention_output.transpose(0, 2, 1, 3)
                  # → (batch, seq_len, num_heads * d_v)
                  attention_output = attention_output.reshape(batch_size, -1, self.num_heads * self.d_v)
                  
                  # 5. Final linear projection
                  # (batch, seq_len, num_heads * d_v) @ (num_heads * d_v, d_model)
                  # = (batch, seq_len, d_model)
                  output = np.matmul(attention_output, self.W_o)
                  
                  return output, attention_weights
              
              def __call__(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray,
                          mask: np.ndarray = None) -> tuple:
                  """Alias for forward."""
                  return self.forward(Q, K, V, mask)
          
          
          # Example usage
          batch_size = 2
          seq_len = 10
          d_model = 512
          num_heads = 8
          
          # Create multi-head attention layer
          mha = MultiHeadAttention(d_model, num_heads)
          
          # Random inputs
          Q = np.random.randn(batch_size, seq_len, d_model)
          K = np.random.randn(batch_size, seq_len, d_model)
          V = np.random.randn(batch_size, seq_len, d_model)
          
          # Forward pass
          output, attention_weights = mha(Q, K, V)
          
          print(f"Multi-Head Attention:")
          print(f"  Model dimension: {d_model}")
          print(f"  Number of heads: {num_heads}")
          print(f"  Dimension per head: {mha.d_k}")
          print(f"\nInput shapes:")
          print(f"  Q, K, V: {Q.shape}")
          print(f"\nOutput shapes:")
          print(f"  Output: {output.shape}")
          print(f"  Attention weights: {attention_weights.shape}")
          print(f"    (batch, num_heads, seq_len, seq_len)")
      
      visualize_head_patterns:
        language: python
        code: |
          def visualize_multi_head_patterns():
              """Visualize attention patterns for different heads."""
              
              print("\n=== Multi-Head Attention Patterns ===\n")
              
              # Simulate attention weights for 4 heads on a 5-token sequence
              seq = ["The", "cat", "sat", "on", "mat"]
              n = len(seq)
              num_heads = 4
              
              # Simulate different head behaviors
              heads_patterns = {
                  "Head 0 (Local)": np.array([
                      [0.7, 0.2, 0.05, 0.03, 0.02],  # The: focuses on self+next
                      [0.3, 0.5, 0.15, 0.03, 0.02],  # cat: focuses on self+neighbors
                      [0.1, 0.2, 0.5, 0.15, 0.05],   # sat: local window
                      [0.05, 0.05, 0.2, 0.5, 0.2],   # on: local window
                      [0.02, 0.03, 0.1, 0.25, 0.6],  # mat: focuses on self+prev
                  ]),
                  
                  "Head 1 (Global)": np.array([
                      [0.2, 0.2, 0.2, 0.2, 0.2],  # Uniform attention
                      [0.2, 0.2, 0.2, 0.2, 0.2],
                      [0.2, 0.2, 0.2, 0.2, 0.2],
                      [0.2, 0.2, 0.2, 0.2, 0.2],
                      [0.2, 0.2, 0.2, 0.2, 0.2],
                  ]),
                  
                  "Head 2 (First-token)": np.array([
                      [0.9, 0.02, 0.02, 0.03, 0.03],  # All attend to first token
                      [0.8, 0.05, 0.05, 0.05, 0.05],
                      [0.7, 0.1, 0.07, 0.07, 0.06],
                      [0.75, 0.08, 0.07, 0.05, 0.05],
                      [0.85, 0.05, 0.03, 0.03, 0.04],
                  ]),
                  
                  "Head 3 (Previous token)": np.array([
                      [1.0, 0.0, 0.0, 0.0, 0.0],     # First: only self
                      [0.8, 0.2, 0.0, 0.0, 0.0],     # Second: mostly previous
                      [0.1, 0.8, 0.1, 0.0, 0.0],     # Third: mostly previous
                      [0.05, 0.05, 0.8, 0.1, 0.0],   # Fourth: mostly previous
                      [0.02, 0.03, 0.1, 0.8, 0.05],  # Fifth: mostly previous
                  ])
              }
              
              for head_name, weights in heads_patterns.items():
                  print(f"{head_name}:")
                  print("  Attention weights (rows=queries, cols=keys):")
                  print("        ", "  ".join(f"{w:5s}" for w in seq))
                  for i, query in enumerate(seq):
                      weight_str = "  ".join(f"{w:5.2f}" for w in weights[i])
                      print(f"  {query:5s}: {weight_str}")
                  print()
              
              print("Observation: Different heads learn different patterns!")
              print("  - Head 0: Local context (neighbors)")
              print("  - Head 1: Global context (all positions)")
              print("  - Head 2: Attend to first token (e.g., [CLS])")
              print("  - Head 3: Attend to previous token (sequential)")
          
          visualize_multi_head_patterns()
    
    security_implications:
      per_head_information_leakage: |
        Each head can leak different types of information:
        - Head specializing on entities → leaks names, locations
        - Head specializing on numbers → leaks credentials, IDs
        - Head specializing on syntax → leaks sentence structure
        
        Adversary analyzes individual heads to find most vulnerable one
      
      targeted_head_attacks: |
        Adversary can target specific heads:
        - Craft inputs to maximize attention in sensitive head
        - Extract information from that head's outputs
        - Bypass defenses that only monitor aggregate attention
        - Defense: Monitor all heads individually, not just combined output

  - topic_number: 3
    title: "Head Specialization: What Different Heads Learn"
    
    overview: |
      Through training, different attention heads naturally specialize to capture different 
      linguistic phenomena. Empirical analysis of trained transformers reveals consistent 
      patterns: some heads focus on syntax (e.g., subject-verb), others on semantics, 
      others on positional patterns. Understanding specialization is critical for 
      interpretability and security.
    
    content:
      observed_specialization_patterns:
        syntactic_heads:
          patterns:
            - "Subject-verb agreement"
            - "Determiner-noun relationships"
            - "Modifier-head relationships"
          
          example: |
            "The quick brown fox"
            Syntactic head: 'quick' attends strongly to 'fox' (adjective → noun)
        
        semantic_heads:
          patterns:
            - "Semantic similarity"
            - "Topical coherence"
            - "Argument structure"
          
          example: |
            "The chef prepared a delicious meal"
            Semantic head: 'meal' attends to 'prepared' (object → action)
        
        positional_heads:
          patterns:
            - "Attend to first/last token"
            - "Attend to previous token"
            - "Fixed offset patterns"
          
          example: |
            In BERT: Some heads always attend to [CLS] token
            → Aggregating sentence-level information
        
        coreference_heads:
          patterns:
            - "Pronoun → antecedent"
            - "Referential relationships"
          
          example: |
            "John went to the store. He bought milk."
            Coreference head: 'He' attends to 'John'
      
      head_pruning_experiments:
        methodology: "Remove individual heads and measure performance drop"
        
        findings:
          critical_heads: "Removing causes significant performance drop"
          redundant_heads: "Removing causes minimal impact"
          typical_distribution: "~30% heads are critical, ~40% redundant, ~30% moderate"
        
        implication: |
          Not all heads contribute equally
          → Some heads more important than others
          → Opportunities for model compression
          → Security: Critical heads are high-value targets
      
      head_attention_distance:
        local_heads:
          pattern: "Attend to nearby positions (distance < 5)"
          linguistic_role: "Capture local syntax, phrases"
          example: "Adjective-noun, determiner-noun"
        
        medium_range_heads:
          pattern: "Attend to positions 5-20 away"
          linguistic_role: "Clause-level dependencies"
          example: "Subject-verb across clauses"
        
        long_range_heads:
          pattern: "Attend to distant positions (distance > 20)"
          linguistic_role: "Discourse-level relationships"
          example: "Cross-sentence coreference"
        
        observation: |
          Different heads operate at different scales
          → Capture multi-scale linguistic structure
      
      layer_dependent_specialization:
        early_layers:
          focus: "Local, syntactic patterns"
          heads_learn: "POS tagging, parsing"
          attention_distance: "Short (1-5 tokens)"
        
        middle_layers:
          focus: "Semantic relationships"
          heads_learn: "Entity relationships, semantic roles"
          attention_distance: "Medium (5-15 tokens)"
        
        late_layers:
          focus: "Task-specific abstractions"
          heads_learn: "High-level reasoning, task objectives"
          attention_distance: "Variable (task-dependent)"
        
        hierarchical_processing: |
          Lower layers: Surface features
          → Middle layers: Semantic features
          → Upper layers: Abstract reasoning
          
          Similar to CNN hierarchy: edges → shapes → objects
    
    implementation:
      analyze_head_specialization:
        language: python
        code: |
          def analyze_head_behaviors():
              """Analyze and categorize head specialization patterns."""
              
              print("=== Head Specialization Analysis ===\n")
              
              # Simulate attention patterns for 8 heads
              num_heads = 8
              seq_len = 10
              
              # Compute average attention distance for each head
              # (distance = how far away heads typically attend)
              
              head_patterns = {
                  0: {"type": "Local", "avg_distance": 1.2, "focus": "Adjacent tokens"},
                  1: {"type": "Local", "avg_distance": 1.8, "focus": "Short phrases"},
                  2: {"type": "Medium", "avg_distance": 4.5, "focus": "Clause-level"},
                  3: {"type": "Global", "avg_distance": 7.2, "focus": "Sentence-level"},
                  4: {"type": "First-token", "avg_distance": 0.0, "focus": "[CLS] token"},
                  5: {"type": "Previous", "avg_distance": 1.0, "focus": "Sequential"},
                  6: {"type": "Long-range", "avg_distance": 8.5, "focus": "Discourse"},
                  7: {"type": "Uniform", "avg_distance": 5.0, "focus": "Global average"},
              }
              
              print("Head | Type        | Avg Distance | Focus")
              print("-----|-------------|--------------|-------------------------")
              for head_id, info in head_patterns.items():
                  print(f"  {head_id}  | {info['type']:11s} | "
                        f"{info['avg_distance']:12.1f} | {info['focus']}")
              
              print("\nObservations:")
              print("  - Heads 0-1: Local attention (syntax)")
              print("  - Heads 2-3: Medium-range (semantics)")
              print("  - Heads 4-5: Positional patterns")
              print("  - Heads 6-7: Long-range / global")
              
              # Simulate head importance (performance drop if removed)
              print("\n\nHead Importance (performance drop if pruned):")
              head_importance = {
                  0: 0.08,  # 8% drop - moderately important
                  1: 0.03,  # 3% drop - less important
                  2: 0.15,  # 15% drop - critical!
                  3: 0.12,  # 12% drop - important
                  4: 0.20,  # 20% drop - critical!
                  5: 0.02,  # 2% drop - redundant
                  6: 0.10,  # 10% drop - important
                  7: 0.01,  # 1% drop - redundant
              }
              
              print("\nHead | Importance | Category")
              print("-----|------------|-------------")
              for head_id, importance in head_importance.items():
                  category = ("Critical" if importance > 0.15 else
                            "Important" if importance > 0.05 else
                            "Redundant")
                  bar = "█" * int(importance * 100)
                  print(f"  {head_id}  | {bar:20s} {importance:.2f} | {category}")
              
              print("\nKey insights:")
              print("  - Head 4 (First-token) is critical → aggregates info")
              print("  - Head 2 (Medium-range) is critical → semantics")
              print("  - Heads 5,7 are redundant → can be pruned")
              print("  - Different heads have different importance!")
          
          analyze_head_behaviors()
      
      head_attention_heatmap:
        language: python
        code: |
          def visualize_head_attention_heatmap():
              """Create heatmap visualization of multi-head attention."""
              
              print("\n=== Multi-Head Attention Heatmap ===\n")
              
              # Simulate attention for 4 heads on a sentence
              tokens = ["The", "cat", "sat", "on", "the", "mat"]
              n = len(tokens)
              num_heads = 4
              
              # Generate sample attention patterns
              np.random.seed(42)
              
              print("Visualizing attention for query position 2 ('sat'):\n")
              print("Position:", " ".join(f"{i:3d}" for i in range(n)))
              print("Token:   ", " ".join(f"{t:3s}" for t in tokens))
              print()
              
              for h in range(num_heads):
                  # Different pattern for each head
                  if h == 0:  # Local
                      weights = np.array([0.1, 0.3, 0.4, 0.15, 0.03, 0.02])
                  elif h == 1:  # Verb-Object
                      weights = np.array([0.05, 0.1, 0.2, 0.1, 0.1, 0.45])
                  elif h == 2:  # Subject-Verb
                      weights = np.array([0.05, 0.55, 0.25, 0.08, 0.04, 0.03])
                  else:  # Uniform
                      weights = np.array([0.17, 0.16, 0.17, 0.17, 0.16, 0.17])
                  
                  print(f"Head {h}: ", end="")
                  for w in weights:
                      # Visual bar
                      bar_len = int(w * 20)
                      bar = "█" * bar_len + " " * (20 - bar_len)
                      print(f"[{bar}]", end=" ")
                  print()
                  print(f"          ", " ".join(f"{w:.2f}" for w in weights))
                  print()
              
              print("Interpretation:")
              print("  Head 0: Local attention (focuses on 'sat' and neighbors)")
              print("  Head 1: Object attention (focuses on 'mat' - verb→object)")
              print("  Head 2: Subject attention (focuses on 'cat' - subject→verb)")
              print("  Head 3: Global attention (uniform over all tokens)")
          
          visualize_head_attention_heatmap()
    
    security_implications:
      sensitive_attribute_heads: |
        Some heads specialize on sensitive attributes:
        - Entity head → focuses on names (PII leakage risk)
        - Number head → focuses on digits (credential leakage)
        - Location head → focuses on places (location privacy)
        
        Adversary targets these specific heads for extraction attacks
      
      head_importance_targeting: |
        Critical heads (high performance impact) are valuable targets:
        - Adversary identifies critical heads through pruning analysis
        - Targets those heads for corruption or manipulation
        - Compromising critical head maximizes attack impact
        - Defense: Extra monitoring/protection for critical heads
      
      interpretability_exploitation: |
        Head specialization aids interpretability BUT:
        - Adversary understands what each head does
        - Can craft inputs to trigger specific head behaviors
        - Exploit head specialization for targeted attacks
        - Example: Craft input that activates entity head for extraction

  - topic_number: 4
    title: "Implementation Details and Computational Efficiency"
    
    overview: |
      Multi-head attention achieves computational efficiency through careful implementation: 
      matrix operations for parallel head computation, memory layout optimization, and 
      gradient checkpointing. Understanding these details is critical for both performance 
      and security analysis.
    
    content:
      efficient_implementation:
        naive_approach:
          pseudocode: |
            outputs = []
            for h in range(num_heads):
                Q_h = Q @ W_q[h]
                K_h = K @ W_k[h]
                V_h = V @ W_v[h]
                head_output = Attention(Q_h, K_h, V_h)
                outputs.append(head_output)
            concat = concatenate(outputs)
            final = concat @ W_o
          
          problem: "Sequential loop → no parallelization"
        
        optimized_approach:
          pseudocode: |
            # Project all heads at once
            Q_all = Q @ W_q  # (batch, n, num_heads * d_k)
            K_all = K @ W_k
            V_all = V @ W_v
            
            # Reshape for parallel head processing
            Q_heads = reshape(Q_all, [batch, num_heads, n, d_k])
            K_heads = reshape(K_all, [batch, num_heads, n, d_k])
            V_heads = reshape(V_all, [batch, num_heads, n, d_v])
            
            # Single attention call processes all heads in parallel
            outputs = Attention(Q_heads, K_heads, V_heads)
            
            # Reshape and project
            concat = reshape(outputs, [batch, n, num_heads * d_v])
            final = concat @ W_o
          
          benefit: "All heads computed in parallel → no loop overhead"
      
      memory_layout_optimization:
        attention_matrix_storage:
          per_head: "n × n attention weights"
          total: "num_heads × n × n"
          memory: "h × n² × 4 bytes (float32)"
          
          example: |
            h = 8, n = 512
            Memory = 8 × 512² × 4 = 8MB per sequence
            Batch of 32: 256MB just for attention weights!
        
        memory_reduction_techniques:
          gradient_checkpointing: |
            Don't store attention weights in forward pass
            Recompute in backward pass when needed
            → Reduce memory by ~30-40%
            Cost: ~25% slower training
          
          attention_dropping: |
            During training, randomly drop some attention connections
            → Regularization + memory reduction
          
          mixed_precision: |
            Store attention weights in FP16 instead of FP32
            → 50% memory reduction
            Must be careful with numerical stability
      
      computational_complexity_analysis:
        single_head_cost:
          qk_multiply: "O(n² × d_k)"
          attention_v_multiply: "O(n² × d_v)"
          total_per_head: "O(n² × d)"
        
        multi_head_cost:
          naive: "h × O(n² × (d/h)) = O(n² × d)"
          optimized: "O(n² × d) with parallel execution"
          
          note: "Same asymptotic complexity, but h patterns instead of 1"
        
        projection_overhead:
          input_projections: "3 × O(n × d × d) = O(n × d²)"
          output_projection: "O(n × d²)"
          total: "O(n × d²)"
          
          dominance: |
            For short sequences (n << d): Projections dominate
            For long sequences (n >> d): Attention dominates
      
      practical_considerations:
        hardware_efficiency:
          gpu_optimization: |
            Multi-head attention maps perfectly to GPU:
            - Matrix multiplications (BLAS optimized)
            - Parallel head computation (batch dimension)
            - High arithmetic intensity (compute-bound)
          
          cpu_limitation: |
            Less efficient on CPU:
            - Limited parallelism compared to GPU
            - Memory bandwidth bottleneck
            - Better to batch multiple sequences
        
        numerical_stability:
          attention_scores: "Scale by √d_k per Section 3.7"
          softmax: "Subtract max before exp"
          projections: "Use orthogonal initialization"
          layer_norm: "Stabilize activations (Section 3.11)"
        
        framework_implementations:
          pytorch: "torch.nn.MultiheadAttention"
          tensorflow: "tf.keras.layers.MultiHeadAttention"
          
          features:
            - "Optimized kernel fusion"
            - "Flash Attention support"
            - "Gradient checkpointing options"
            - "Mixed precision training"
    
    implementation:
      performance_comparison:
        language: python
        code: |
          import time
          
          def compare_single_vs_multi_head():
              """Compare computational cost: single head vs multi-head."""
              
              print("=== Single Head vs Multi-Head Performance ===\n")
              
              batch_size = 32
              seq_len = 512
              d_model = 512
              
              # Generate random data
              Q = np.random.randn(batch_size, seq_len, d_model)
              K = np.random.randn(batch_size, seq_len, d_model)
              V = np.random.randn(batch_size, seq_len, d_model)
              
              # Single head (d_k = d_model)
              print(f"Configuration:")
              print(f"  Batch size: {batch_size}")
              print(f"  Sequence length: {seq_len}")
              print(f"  Model dimension: {d_model}")
              print()
              
              # Simulate timing (actual timing would need real implementation)
              single_head_time = seq_len * seq_len * d_model
              multi_head_8_time = seq_len * seq_len * d_model  # Same asymptotic
              
              print(f"Single head (d_k={d_model}):")
              print(f"  Operations: {single_head_time:,}")
              print(f"  Memory for attention: {batch_size * seq_len * seq_len * 4 / 1024 / 1024:.2f} MB")
              print()
              
              num_heads = 8
              d_k_per_head = d_model // num_heads
              
              print(f"Multi-head ({num_heads} heads, d_k={d_k_per_head} each):")
              print(f"  Operations: {multi_head_8_time:,} (same as single head)")
              print(f"  Memory for attention: {batch_size * num_heads * seq_len * seq_len * 4 / 1024 / 1024:.2f} MB")
              print(f"  → {num_heads}x more attention matrices!")
              print()
              
              print("Key insights:")
              print(f"  - Same computational complexity")
              print(f"  - Multi-head uses {num_heads}x memory for attention weights")
              print(f"  - But gets {num_heads} different attention patterns")
              print(f"  - Perfect parallelization on GPU")
          
          compare_single_vs_multi_head()
    
    security_implications:
      memory_based_side_channels: |
        Multi-head attention uses h×n² memory:
        - Different sequence lengths → different memory footprint
        - Cache behavior varies with attention patterns
        - Adversary can infer input length from memory access patterns
        - Defense: Pad to consistent length, use oblivious memory access
      
      timing_attacks_on_heads: |
        Sparse attention patterns in specific heads:
        - Different heads have different sparsity
        - Computation time varies slightly
        - Adversary measures timing → infers which heads activated
        - Defense: Constant-time attention, masking timing variations
      
      head_specific_backdoors: |
        Adversary can inject backdoors in specific heads:
        - Poison training to corrupt one head
        - Other heads remain clean (appears benign in aggregate)
        - Backdoor activates only when corrupt head is critical
        - Defense: Per-head monitoring, anomaly detection

key_takeaways:
  critical_concepts:
    - concept: "Multi-head attention runs h attention mechanisms in parallel"
      why_it_matters: "Dramatically increases model capacity without increasing complexity"
    
    - concept: "Different heads specialize in different patterns (syntax, semantics, position)"
      why_it_matters: "Enables rich representations capturing multiple linguistic phenomena"
    
    - concept: "Head-specific projections create independent subspaces"
      why_it_matters: "Each head learns different Q/K/V transformations"
    
    - concept: "Same computational complexity as single head but h patterns"
      why_it_matters: "Efficient parallelization - no overhead for multiple heads"
    
    - concept: "Head importance varies - some critical, others redundant"
      why_it_matters: "Opportunities for pruning and compression, security targeting"
  
  actionable_steps:
    - step: "Implement multi-head attention from scratch"
      verification: "Split into heads, parallel attention, concatenate, project"
    
    - step: "Analyze head specialization patterns"
      verification: "Identify local, global, syntactic, semantic heads"
    
    - step: "Visualize individual head attention patterns"
      verification: "Show different heads focus on different positions"
    
    - step: "Measure head importance via pruning"
      verification: "Determine which heads are critical vs redundant"
    
    - step: "Compare single-head vs multi-head performance"
      verification: "Same complexity, better representations"
  
  security_principles:
    - principle: "Different heads leak different types of information"
      application: "Monitor all heads individually for sensitive attribute focus"
    
    - principle: "Head specialization enables targeted attacks"
      application: "Adversary targets specific heads (e.g., entity head for PII)"
    
    - principle: "Critical heads are high-value targets"
      application: "Extra protection/monitoring for heads with high importance"
    
    - principle: "Memory usage scales with number of heads (h×n²)"
      application: "Resource exhaustion attacks amplified by multiple heads"
    
    - principle: "Head behaviors can be extracted through timing/memory analysis"
      application: "Side-channel protection for multi-head computations"
  
  common_mistakes:
    - mistake: "Not splitting d_model evenly across heads (d_k = d_model/h)"
      fix: "Ensure d_model divisible by num_heads for even split"
    
    - mistake: "Sequential loop over heads instead of parallel computation"
      fix: "Use matrix reshaping to process all heads simultaneously"
    
    - mistake: "Ignoring individual head behaviors in debugging"
      fix: "Visualize and analyze each head separately, not just aggregate"
    
    - mistake: "Assuming all heads contribute equally"
      fix: "Some heads critical, others redundant - verify with pruning"
    
    - mistake: "Not accounting for h×n² memory for attention weights"
      fix: "Memory scales linearly with num_heads - plan accordingly"
  
  integration_with_book:
    from_section_3_7:
      - "Scaled dot-product attention (used within each head)"
      - "Attention masking (applied to all heads)"
      - "Computational complexity O(n²d)"
    
    from_section_3_6:
      - "Query-Key-Value formulation (per head)"
      - "Attention as weighted sum"
      - "Differentiable memory access"
    
    to_next_section:
      - "Section 3.9: Positional encoding (attention is permutation-invariant)"
      - "How to inject position information"
      - "Sinusoidal vs learned positional embeddings"
  
  looking_ahead:
    next_concepts:
      - "Positional encoding (attention lacks position awareness)"
      - "Sinusoidal functions for position"
      - "Learned positional embeddings"
      - "Relative position encoding"
    
    skills_to_build:
      - "Implement positional encoding"
      - "Build complete transformer layer (attention + FFN + norms)"
      - "Stack multiple transformer layers"
      - "Train transformer from scratch"
  
  final_thoughts: |
    Multi-head attention is the architectural innovation that makes transformers powerful. 
    By running h attention mechanisms in parallel, each with its own learned projections, 
    the model captures multiple different patterns simultaneously. Different heads 
    specialize: syntax, semantics, position, coreference - all learned automatically 
    through gradient descent.
    
    The architecture is elegant: project into h subspaces, compute attention in each, 
    concatenate, and project back. Same computational complexity as single-head attention 
    (O(n²d)) but with h different attention patterns. Perfect parallelization on GPUs 
    makes this efficient despite the increased capacity.
    
    Empirical analysis reveals consistent specialization patterns. Early-layer heads 
    focus on local syntax, middle-layer heads on semantics, late-layer heads on 
    task-specific abstractions. Some heads are critical (large performance drop if 
    removed), others redundant. This variance creates opportunities for compression and 
    for targeted attacks.
    
    From a security perspective: multi-head attention creates multiple attack surfaces. 
    Different heads leak different information (entities, numbers, locations). Adversaries 
    can target specific heads for extraction or manipulation. Critical heads are high-value 
    targets. Defense requires monitoring all heads individually, not just aggregate output.
    
    Next: Section 3.9 addresses a fundamental limitation: attention is permutation-invariant 
    (doesn't know word order). We'll explore positional encoding - the mechanism that 
    injects position information into transformers.

---
