# chapter_02_index.yaml
---
document_info:
  chapter: "02"
  title: "Deep Learning Basics"
  part: "Part I: Machine Learning Foundations"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-05"
  estimated_total_sections: 24
  estimated_total_pages: 144
  estimated_reading_time: "14-16 hours"

# ============================================================================
# CHAPTER 02 INDEX: DEEP LEARNING BASICS
# ============================================================================

chapter_02_index:
  
  # --------------------------------------------------------------------------
  # Chapter Overview
  # --------------------------------------------------------------------------
  
  chapter_overview:
    
    purpose: |
      This chapter builds neural networks from absolute scratch using only NumPy.
      You'll implement everything - perceptrons, multi-layer networks, backpropagation,
      optimization algorithms, and modern architectures (CNNs, RNNs).
      
      Unlike typical ML courses that use TensorFlow/PyTorch as black boxes, we
      implement the math ourselves. You'll understand gradient flow, why ReLU
      works, how dropout prevents overfitting, and where adversarial attacks
      exploit these mechanisms.
      
      By the end, you'll have built: neural network from scratch with backprop,
      image classifier (MNIST), text classifier (RNN), and complete understanding
      of how gradients enable both learning AND attacks.
    
    structure: |
      24 sections organized into 6 logical groups:
      1. Neural Network Foundations (Sections 01-04): Perceptron to multi-layer networks
      2. Backpropagation Mastery (Sections 05-06): Theory and implementation
      3. Optimization Algorithms (Sections 07-09): GD variants, momentum, Adam
      4. Regularization Techniques (Sections 10-13): L1/L2, dropout, batch norm
      5. Modern Architectures (Sections 14-20): CNNs, RNNs, LSTMs
      6. Production Systems (Sections 21-24): Deployment, debugging, security
    
    prerequisites:
      - "Chapter 1 completed (especially gradient descent)"
      - "Comfortable with matrix operations (NumPy)"
      - "Basic calculus (chain rule, partial derivatives)"
      - "Understanding of loss functions"
    
    security_focus: |
      Every concept connected to security implications:
      - Backpropagation = how adversarial attacks work
      - Training dynamics = where backdoors hide
      - Gradient flow = model extraction vectors
      - Overfitting = data leakage risks
  
  # --------------------------------------------------------------------------
  # Section Breakdown
  # --------------------------------------------------------------------------
  
  sections:
    
    # ========================================================================
    # GROUP 1: NEURAL NETWORK FOUNDATIONS
    # ========================================================================
    
    section_0201:
      number: "0201"
      title: "From Perceptron to Neural Networks"
      filename: "section_0201_perceptron_to_networks.yaml"
      estimated_pages: 6
      topics:
        - "Perceptron: biological inspiration and math"
        - "Single-layer limitations (XOR problem)"
        - "Multi-layer perceptrons (MLPs)"
        - "Universal approximation theorem"
        - "Architecture design patterns"
        - "Why depth matters for security"
      deliverables:
        - "Single perceptron implementation"
        - "Demonstrate XOR failure"
        - "2-layer network solves XOR"
        - "Decision boundary visualizer"
      security_preview:
        - "More layers = larger attack surface"
        - "Hidden layers = backdoor hiding spots"
      
    section_0202:
      number: "0202"
      title: "Activation Functions Deep Dive"
      filename: "section_0202_activation_functions.yaml"
      estimated_pages: 6
      topics:
        - "Why activation functions critical"
        - "Sigmoid and tanh (gradient vanishing)"
        - "ReLU family (standard, leaky, PReLU)"
        - "Modern alternatives (Swish, GELU, Mish)"
        - "Softmax for multi-class output"
        - "Choosing activations (practical guide)"
      deliverables:
        - "All activations + derivatives implemented"
        - "Activation curve visualizations"
        - "Performance benchmark comparisons"
        - "Activation function library"
      security_implications:
        - "Activation patterns leak model info"
        - "Gradient saturation affects robustness"
        - "Dead neurons = evasion opportunities"
    
    section_0203:
      number: "0203"
      title: "Forward Propagation Mechanics"
      filename: "section_0203_forward_propagation.yaml"
      estimated_pages: 6
      topics:
        - "Matrix formulation: W @ x + b"
        - "Layer-by-layer computation"
        - "Computational graph representation"
        - "Batched processing (vectorization)"
        - "Memory management patterns"
        - "Profiling and optimization"
      deliverables:
        - "Multi-layer forward pass"
        - "Computational graph visualizer"
        - "Memory profiler per layer"
        - "Vectorized implementation"
      hands_on:
        - "3-layer network: 784→128→64→10"
        - "Process 1000 MNIST images"
        - "Benchmark time and memory"
    
    section_0204:
      number: "0204"
      title: "Loss Functions for Neural Networks"
      filename: "section_0204_loss_functions.yaml"
      estimated_pages: 6
      topics:
        - "MSE for regression tasks"
        - "Binary cross-entropy"
        - "Categorical cross-entropy"
        - "Sparse categorical (memory efficient)"
        - "Focal loss (class imbalance)"
        - "Custom loss functions"
        - "Loss landscape visualization"
      deliverables:
        - "All loss functions implemented"
        - "Convergence comparison study"
        - "Loss function tester framework"
        - "2D loss surface visualizer"
      security_context:
        - "Loss manipulation in poisoning"
        - "Adversarial perturbations minimize loss changes"
        - "Custom losses for robust training"
    
    # ========================================================================
    # GROUP 2: BACKPROPAGATION MASTERY
    # ========================================================================
    
    section_0205:
      number: "0205"
      title: "Backpropagation Theory"
      filename: "section_0205_backprop_theory.yaml"
      estimated_pages: 8
      topics:
        - "Chain rule: foundation of backprop"
        - "Computational graph and gradient flow"
        - "Forward pass: compute + cache"
        - "Backward pass: gradients layer-by-layer"
        - "Mathematical proof of backprop"
        - "Common misconceptions debunked"
      deliverables:
        - "Derive backprop equations (paper)"
        - "Computational graph with tracking"
        - "Gradient checker (numerical verification)"
        - "Gradient flow visualizer"
      critical_note: |
        THIS IS THE MOST IMPORTANT SECTION FOR SECURITY.
        Every adversarial attack, model extraction, backdoor
        relies on understanding gradients. Master this.
    
    section_0206:
      number: "0206"
      title: "Backpropagation Implementation"
      filename: "section_0206_backprop_implementation.yaml"
      estimated_pages: 8
      topics:
        - "Layer-wise gradient computation"
        - "Weight gradient: ∂L/∂W"
        - "Bias gradient: ∂L/∂b"
        - "Activation gradient: ∂L/∂a"
        - "Caching forward values"
        - "Memory-efficient backprop"
        - "Debugging with gradient checking"
      deliverables:
        - "Complete backprop (NumPy only)"
        - "Numerical gradient checker"
        - "Gradient debugging tools"
        - "Unit tests for all layers"
      hands_on:
        - "3-layer network with manual backprop"
        - "Gradient verification (<0.01 error)"
        - "Train on spiral classification"
        - "Achieve convergence"
    
    # ========================================================================
    # GROUP 3: OPTIMIZATION ALGORITHMS
    # ========================================================================
    
    section_0207:
      number: "0207"
      title: "Gradient Descent Variants"
      filename: "section_0207_gradient_descent_variants.yaml"
      estimated_pages: 6
      topics:
        - "Batch GD: full dataset"
        - "Stochastic GD: single sample"
        - "Mini-batch GD: sweet spot"
        - "Batch size selection tradeoffs"
        - "Learning rate scheduling"
        - "Convergence analysis"
      deliverables:
        - "Batch/stochastic/mini-batch GD"
        - "Convergence speed comparison"
        - "Learning rate scheduler"
        - "Optimizer comparison framework"
      experiments:
        - "Same network, different batch sizes"
        - "Wall-clock time vs iterations"
        - "Find optimal batch for hardware"
    
    section_0208:
      number: "0208"
      title: "Advanced Optimizers: Momentum and Adam"
      filename: "section_0208_advanced_optimizers.yaml"
      estimated_pages: 6
      topics:
        - "Momentum: velocity acceleration"
        - "Nesterov Accelerated Gradient"
        - "AdaGrad: per-parameter learning rates"
        - "RMSprop: fixing AdaGrad decay"
        - "Adam: momentum + RMSprop"
        - "AdamW: weight decay fix"
        - "Optimizer selection guide"
      deliverables:
        - "SGD with momentum, RMSprop, Adam"
        - "Multi-dataset comparison"
        - "Hyperparameter tuning study"
        - "Optimizer benchmarking suite"
      practical_results:
        - "Adam converges 2-5x faster than SGD"
        - "Momentum escapes saddle points"
        - "AdamW prevents overfitting better"
    
    section_0209:
      number: "0209"
      title: "Weight Initialization Strategies"
      filename: "section_0209_weight_initialization.yaml"
      estimated_pages: 5
      topics:
        - "Why initialization matters"
        - "Zero init failure mode"
        - "Random: uniform vs normal"
        - "Xavier/Glorot (sigmoid/tanh)"
        - "He initialization (ReLU networks)"
        - "Orthogonal initialization"
        - "Layer-wise initialization rules"
      deliverables:
        - "All initialization methods"
        - "Gradient flow comparison"
        - "Convergence speed analysis"
        - "Initialization benchmarker"
      experiments:
        - "10-layer network, different inits"
        - "Track gradient magnitude per layer"
        - "Measure training stability"
    
    # ========================================================================
    # GROUP 4: REGULARIZATION TECHNIQUES
    # ========================================================================
    
    section_0210:
      number: "0210"
      title: "Regularization: L1, L2, Weight Decay"
      filename: "section_0210_regularization_l1_l2.yaml"
      estimated_pages: 5
      topics:
        - "Overfitting in neural networks"
        - "L1 regularization (sparse weights)"
        - "L2 regularization (weight decay)"
        - "Weight decay vs L2 (subtle difference)"
        - "Elastic Net (L1 + L2)"
        - "Regularization strength tuning"
      deliverables:
        - "L1, L2, weight decay implementations"
        - "Sparsity vs performance comparison"
        - "Hyperparameter tuning study"
        - "Weight distribution visualizer"
      security_context:
        - "Over-regularized = easier to fool"
        - "Under-regularized = memorizes training data"
        - "Balance needed for robustness"
    
    section_0211:
      number: "0211"
      title: "Dropout: Stochastic Regularization"
      filename: "section_0211_dropout.yaml"
      estimated_pages: 6
      topics:
        - "Dropout: random neuron dropping"
        - "Train vs inference mode"
        - "Inverted dropout (scale maintenance)"
        - "Dropout rate selection (0.2-0.5)"
        - "DropConnect (dropping weights)"
        - "Spatial dropout for CNNs"
        - "Ensemble interpretation"
      deliverables:
        - "Dropout layer implementation"
        - "With/without dropout comparison"
        - "Dropout rate tuning study"
        - "Dropout scheduler"
      practical_exercise:
        - "MNIST with/without dropout"
        - "Measure overfitting reduction"
        - "Test different rates"
        - "Analyze ensemble behavior"
    
    section_0212:
      number: "0212"
      title: "Batch Normalization"
      filename: "section_0212_batch_normalization.yaml"
      estimated_pages: 6
      topics:
        - "Internal covariate shift"
        - "Batch norm: normalizing layer inputs"
        - "Learnable gamma and beta"
        - "Train vs inference (moving averages)"
        - "Placement: before/after activation"
        - "Benefits: speed, stability"
        - "Limitations: batch size dependency"
      deliverables:
        - "Batch normalization layer"
        - "Training speed comparison"
        - "Train/inference mode handling"
        - "Layer normalization (alternative)"
      advanced:
        - "Group Normalization"
        - "Instance Normalization"
        - "Layer Normalization (transformers)"
    
    section_0213:
      number: "0213"
      title: "Gradient Clipping and Stability"
      filename: "section_0213_gradient_clipping.yaml"
      estimated_pages: 5
      topics:
        - "Exploding gradients problem"
        - "Clipping by value"
        - "Clipping by norm"
        - "When to use (RNNs especially)"
        - "Mixed precision considerations"
        - "Numerical stability techniques"
        - "Debugging training instabilities"
      deliverables:
        - "Gradient clipping implementation"
        - "Gradient explosion detector"
        - "Stability monitoring tools"
        - "Debugging checklist"
      practical_scenarios:
        - "20-layer network without clipping"
        - "Observe gradient explosion"
        - "Add clipping: stable training"
        - "Tune clipping threshold"
    
    # ========================================================================
    # GROUP 5: MODERN ARCHITECTURES
    # ========================================================================
    
    section_0214:
      number: "0214"
      title: "Training Dynamics and Hyperparameter Tuning"
      filename: "section_0214_training_dynamics.yaml"
      estimated_pages: 6
      topics:
        - "Learning curves (train/val loss)"
        - "Overfitting vs underfitting detection"
        - "Hyperparameter importance ranking"
        - "Grid search vs random search"
        - "Bayesian optimization basics"
        - "Early stopping strategies"
        - "Checkpointing best models"
      deliverables:
        - "Training curve visualizer"
        - "Hyperparameter tuning framework"
        - "Early stopping implementation"
        - "Model checkpoint system"
      tuning_priorities:
        - "Learning rate (most critical)"
        - "Batch size"
        - "Architecture (layers/neurons)"
        - "Regularization strength"
    
    section_0215:
      number: "0215"
      title: "Convolutional Neural Networks: Foundations"
      filename: "section_0215_cnn_foundations.yaml"
      estimated_pages: 6
      topics:
        - "Why CNNs for images (spatial structure)"
        - "Convolution operation (filters/kernels)"
        - "Stride and padding"
        - "Pooling layers (max/average)"
        - "Feature maps and channels"
        - "Receptive fields"
        - "Translation invariance"
      deliverables:
        - "2D convolution from scratch"
        - "Max pooling implementation"
        - "Simple CNN (2 conv + 2 fc layers)"
        - "MNIST with CNN (>98% accuracy)"
      security_implications:
        - "Adversarial patches target convolutions"
        - "Feature extraction reveals model info"
        - "Backdoor patterns in filters"
    
    section_0216:
      number: "0216"
      title: "CNN Architectures and Best Practices"
      filename: "section_0216_cnn_architectures.yaml"
      estimated_pages: 6
      topics:
        - "LeNet: first CNN architecture"
        - "AlexNet: deep learning breakthrough"
        - "VGG: simple and deep"
        - "ResNet: skip connections"
        - "Inception: multi-scale features"
        - "Modern best practices"
        - "Architecture design patterns"
      deliverables:
        - "Implement simplified VGG"
        - "ResNet block with skip connections"
        - "Architecture comparison study"
        - "Transfer learning example"
      hands_on:
        - "Build VGG-like network"
        - "Train on CIFAR-10"
        - "Achieve 80%+ accuracy"
    
    section_0217:
      number: "0217"
      title: "Recurrent Neural Networks: Sequence Modeling"
      filename: "section_0217_rnn_basics.yaml"
      estimated_pages: 6
      topics:
        - "Why RNNs for sequences"
        - "Vanilla RNN: hidden state recurrence"
        - "Backpropagation through time (BPTT)"
        - "Vanishing gradient in RNNs"
        - "Exploding gradient problem"
        - "Truncated BPTT"
        - "Bidirectional RNNs"
      deliverables:
        - "Vanilla RNN from scratch"
        - "BPTT implementation"
        - "Gradient flow visualization"
        - "Simple sequence classifier"
      security_preview:
        - "Sequential context in prompt injection"
        - "Hidden state manipulation"
        - "Gradient-based sequence attacks"
    
    section_0218:
      number: "0218"
      title: "LSTM and GRU: Advanced Sequence Models"
      filename: "section_0218_lstm_gru.yaml"
      estimated_pages: 7
      topics:
        - "LSTM: gates and cell state"
        - "Forget gate, input gate, output gate"
        - "LSTM forward and backward pass"
        - "GRU: simplified LSTM"
        - "When to use LSTM vs GRU"
        - "Bidirectional LSTM"
        - "Stacked RNNs"
      deliverables:
        - "LSTM from scratch (NumPy)"
        - "GRU implementation"
        - "Text classifier with LSTM"
        - "IMDB sentiment analysis (85%+ accuracy)"
      practical_comparison:
        - "Vanilla RNN vs LSTM vs GRU"
        - "Training stability"
        - "Long-term dependencies"
        - "Computational cost"
    
    section_0219:
      number: "0219"
      title: "Attention Mechanism Preview"
      filename: "section_0219_attention_preview.yaml"
      estimated_pages: 5
      topics:
        - "Limitations of RNNs (fixed context)"
        - "Attention: focusing on relevant inputs"
        - "Attention scores and weights"
        - "Seq2seq with attention"
        - "Self-attention (preview for Chapter 3)"
        - "Why transformers replace RNNs"
      deliverables:
        - "Simple attention mechanism"
        - "Attention weight visualizer"
        - "Seq2seq with attention (translation)"
        - "Bridge to Chapter 3 (transformers)"
      note: |
        This is a preview. Full attention and transformers
        covered in Chapter 3. Here we just introduce the concept.
    
    section_0220:
      number: "0220"
      title: "Model Debugging and Diagnostics"
      filename: "section_0220_model_debugging.yaml"
      estimated_pages: 6
      topics:
        - "Common training failures"
        - "Loss not decreasing (debugging checklist)"
        - "Gradient diagnostics (vanishing/exploding)"
        - "Learning rate tuning heuristics"
        - "Overfitting vs underfitting diagnosis"
        - "Activation distribution analysis"
        - "Weight distribution checks"
      deliverables:
        - "Model debugger toolkit"
        - "Training diagnostic dashboard"
        - "Automated sanity checks"
        - "Debugging workflow checklist"
      common_issues:
        - "Learning rate too high/low"
        - "Poor initialization"
        - "Data preprocessing bugs"
        - "Architecture mismatches"
    
    # ========================================================================
    # GROUP 6: PRODUCTION SYSTEMS
    # ========================================================================
    
    section_0221:
      number: "0221"
      title: "Production Neural Network Systems"
      filename: "section_0221_production_systems.yaml"
      estimated_pages: 6
      topics:
        - "Training vs inference pipelines"
        - "Model serialization (save/load)"
        - "Inference optimization (batching, caching)"
        - "Serving architecture patterns"
        - "Latency and throughput considerations"
        - "Model versioning and rollback"
        - "A/B testing models"
      deliverables:
        - "Model save/load system"
        - "Inference pipeline (batching)"
        - "Simple serving API (Flask/FastAPI)"
        - "Performance benchmarker"
      production_concerns:
        - "Latency requirements (<100ms typical)"
        - "Throughput (requests/second)"
        - "Resource usage (memory/CPU/GPU)"
        - "Deployment strategies"
    
    section_0222:
      number: "0222"
      title: "Model Security: Adversarial Robustness"
      filename: "section_0222_adversarial_robustness.yaml"
      estimated_pages: 6
      topics:
        - "Adversarial examples (quick intro)"
        - "FGSM attack (Fast Gradient Sign Method)"
        - "Defense: adversarial training"
        - "Defense: input preprocessing"
        - "Defense: gradient masking (why it fails)"
        - "Robustness evaluation metrics"
        - "Preview of Chapter 10 (full adversarial ML)"
      deliverables:
        - "FGSM attack implementation"
        - "Adversarial training loop"
        - "Robustness evaluator"
        - "Defense comparison framework"
      security_focus:
        - "Every gradient = potential attack vector"
        - "Backprop enables adversarial examples"
        - "Defense-in-depth for models"
    
    section_0223:
      number: "0223"
      title: "Model Interpretability and Explainability"
      filename: "section_0223_model_interpretability.yaml"
      estimated_pages: 6
      topics:
        - "Why interpretability matters (security context)"
        - "Feature importance analysis"
        - "Gradient-based visualization (saliency maps)"
        - "Activation maximization"
        - "Layer-wise relevance propagation"
        - "LIME and SHAP (brief intro)"
        - "Detecting backdoors via interpretation"
      deliverables:
        - "Saliency map generator"
        - "Feature importance ranker"
        - "Activation visualizer"
        - "Backdoor detection toolkit"
      security_applications:
        - "Detect poisoned models"
        - "Find hidden triggers"
        - "Verify model behavior"
        - "Audit training data influence"
    
    section_0224:
      number: "0224"
      title: "Chapter 2 Summary and Next Steps"
      filename: "section_0224_chapter_summary.yaml"
      estimated_pages: 5
      topics:
        - "What we built (recap)"
        - "Key concepts mastered"
        - "Security implications learned"
        - "Common pitfalls to avoid"
        - "Bridge to Chapter 3 (LLM architecture)"
        - "Practical project suggestions"
        - "Further reading recommendations"
      deliverables:
        - "Complete neural network library (NumPy)"
        - "MNIST classifier (98%+ accuracy)"
        - "IMDB sentiment analysis (85%+ accuracy)"
        - "Understanding of gradient-based attacks"
      assessment:
        - "Can you implement backprop from memory?"
        - "Can you explain gradient flow?"
        - "Can you debug training failures?"
        - "Can you tune hyperparameters?"
      
      next_chapter_preview: |
        Chapter 3: Large Language Model Architecture
        - Transformers and attention mechanisms
        - Self-attention and multi-head attention
        - Positional encoding
        - GPT, BERT, T5 architectures
        - From scratch implementation
  
  # --------------------------------------------------------------------------
  # Learning Path
  # --------------------------------------------------------------------------
  
  learning_path:
    
    week_3_day_1_2:
      sections: ["0201", "0202", "0203"]
      focus: "Neural network foundations + activations"
      deliverable: "Perceptron and simple MLP working"
      time: "6-8 hours"
    
    week_3_day_3_4:
      sections: ["0204", "0205", "0206"]
      focus: "Backpropagation mastery"
      deliverable: "Complete backprop from scratch"
      time: "8-10 hours"
      critical: "Most important sections for security understanding"
    
    week_3_day_5_6:
      sections: ["0207", "0208", "0209", "0210"]
      focus: "Optimization and regularization"
      deliverable: "Adam optimizer, L2 regularization working"
      time: "6-8 hours"
    
    week_3_day_7:
      sections: ["0211", "0212", "0213", "0214"]
      focus: "Advanced regularization + training dynamics"
      deliverable: "Dropout, batch norm, gradient clipping"
      time: "6-8 hours"
    
    week_4_day_1_2:
      sections: ["0215", "0216"]
      focus: "Convolutional neural networks"
      deliverable: "CNN on MNIST (98%+ accuracy)"
      time: "6-8 hours"
    
    week_4_day_3_4:
      sections: ["0217", "0218", "0219"]
      focus: "Recurrent neural networks"
      deliverable: "LSTM text classifier (IMDB sentiment)"
      time: "8-10 hours"
    
    week_4_day_5_6_7:
      sections: ["0220", "0221", "0222", "0223", "0224"]
      focus: "Production systems + security + summary"
      deliverable: "Complete Chapter 2, production-ready library"
      time: "8-10 hours"
  
  # --------------------------------------------------------------------------
  # Success Criteria
  # --------------------------------------------------------------------------
  
  success_criteria:
    
    must_achieve:
      - "Implement complete neural network library (NumPy only, no TensorFlow/PyTorch)"
      - "Backpropagation from scratch with <0.01 gradient error"
      - "MNIST classifier achieving 98%+ accuracy"
      - "IMDB sentiment analysis achieving 85%+ accuracy"
      - "Understanding of gradient flow (can explain to someone)"
      - "Can debug training failures systematically"
    
    good_to_achieve:
      - "All optimizers implemented (SGD, momentum, Adam)"
      - "All regularization techniques working (L1/L2, dropout, batch norm)"
      - "CNN and RNN architectures from scratch"
      - "Model serving API with <100ms latency"
    
    security_understanding:
      - "Explain how backprop enables adversarial attacks"
      - "Identify where backdoors hide in training"
      - "Understand gradient-based model extraction"
      - "Implement basic FGSM adversarial attack"
  
  # --------------------------------------------------------------------------
  # Common Mistakes to Avoid
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    implementation:
      - "Using libraries before implementing from scratch (defeats purpose)"
      - "Not verifying gradients numerically (gradient checking critical)"
      - "Ignoring numerical stability (use log-softmax, not softmax)"
      - "Forgetting train vs inference mode (dropout, batch norm)"
      - "Not tracking gradient norms (exploding gradients)"
    
    training:
      - "Learning rate too high (divergence) or too low (slow convergence)"
      - "Poor weight initialization (vanishing/exploding gradients)"
      - "Overfitting (not using regularization)"
      - "Underfitting (network too small)"
      - "Not using validation set (can't detect overfitting)"
    
    security:
      - "Thinking 'just add more layers' makes model secure"
      - "Ignoring gradient information leakage"
      - "Not understanding where backdoors hide"
      - "Assuming adversarial training is complete solution"
  
  # --------------------------------------------------------------------------
  # Tools and Resources
  # --------------------------------------------------------------------------
  
  tools_and_resources:
    
    required:
      - "NumPy (primary implementation)"
      - "Matplotlib (visualization)"
      - "Jupyter notebooks (interactive coding)"
      - "MNIST dataset"
      - "IMDB dataset"
    
    optional:
      - "PyTorch (after implementing from scratch)"
      - "TensorBoard (training visualization)"
      - "Scikit-learn (data preprocessing)"
    
    datasets:
      - "MNIST: 60K training + 10K test images (28x28 grayscale)"
      - "CIFAR-10: 50K training + 10K test images (32x32 RGB)"
      - "IMDB: 25K training + 25K test movie reviews"
    
    references:
      - "Neural Networks and Deep Learning (Nielsen)"
      - "Deep Learning (Goodfellow, Bengio, Courville)"
      - "CS231n: Convolutional Neural Networks (Stanford)"
      - "CS224n: NLP with Deep Learning (Stanford)"
  
  # --------------------------------------------------------------------------
  # Notes for Instructor/Self
  # --------------------------------------------------------------------------
  
  instructor_notes:
    
    pacing:
      - "Sections 0205-0206 (backprop) are most time-intensive - allocate extra time"
      - "Don't rush gradient checking - this catches 90% of implementation bugs"
      - "CNNs and RNNs can be parallelized if time-pressed"
      - "Security sections (0222, 0223) can be preview if Chapter 10 coming soon"
    
    emphasis:
      - "Backpropagation understanding is non-negotiable"
      - "Gradient verification must pass before moving forward"
      - "NumPy-only implementations force true understanding"
      - "Connect every concept to security implications"
    
    flexibility:
      - "If learner has ML background: skip sections 0201-0204, start at 0205"
      - "If security-focused: emphasize sections 0205, 0206, 0222, 0223"
      - "If production-focused: emphasize sections 0220, 0221"
      - "Can split into 2 weeks if needed (12 sections/week)"

---
