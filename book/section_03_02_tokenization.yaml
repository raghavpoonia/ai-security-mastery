# section_03_02_tokenization.yaml

---
document_info:
  title: "Tokenization: From Text to Numbers"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 2
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Comprehensive guide to tokenization: character, word, and subword approaches, BPE and WordPiece algorithms, special tokens, and security implications of tokenization boundaries"
  estimated_pages: 7
  tags:
    - tokenization
    - bpe
    - wordpiece
    - subword-tokenization
    - special-tokens
    - tokenization-security

section_overview:
  title: "Tokenization: From Text to Numbers"
  number: "3.2"
  
  purpose: |
    Tokenization is the critical bridge between raw text and numerical representations 
    that models can process. It's not just splitting on whitespace - modern tokenizers 
    use sophisticated algorithms to balance vocabulary size, coverage, and efficiency.
    
    The choice of tokenization strategy fundamentally affects model behavior, training 
    efficiency, and security properties. A poorly designed tokenizer creates attack 
    vectors; a well-designed one enables robust NLP systems.
    
    For security engineers: Tokenization boundaries are exploitable. Adversaries craft 
    inputs that look benign pre-tokenization but malicious post-tokenization. Unicode 
    attacks, homoglyphs, and zero-width characters can all bypass token-based filters.
  
  learning_objectives:
    conceptual:
      - "Understand tradeoffs: character vs word vs subword tokenization"
      - "Grasp why Byte-Pair Encoding (BPE) dominates modern NLP"
      - "Learn how vocabulary size affects model capacity and efficiency"
      - "Understand special tokens ([CLS], [SEP], [MASK], [PAD]) and their roles"
    
    practical:
      - "Implement BPE tokenizer from scratch (NumPy)"
      - "Build WordPiece tokenizer"
      - "Create vocabulary with merge operations"
      - "Handle multi-lingual text and Unicode"
      - "Implement tokenization for BERT, GPT, T5 styles"
    
    security_focused:
      - "Identify tokenization boundary attacks"
      - "Recognize homoglyph and Unicode exploitation"
      - "Understand how tokenization affects filter bypass"
      - "Detect zero-width character injection"
      - "Audit tokenizer consistency across contexts"
  
  prerequisites:
    knowledge:
      - "Section 3.1: NLP foundations, vocabulary building"
      - "Understanding of text as discrete symbols"
      - "Python string manipulation and Unicode"
      - "Basic algorithm design (greedy algorithms)"
    
    skills:
      - "Working with character encodings (UTF-8, Unicode)"
      - "Dictionary and priority queue operations"
      - "Regular expressions for text processing"
  
  key_transitions:
    from_section_3_1: |
      Section 3.1 established that models need tokens, not raw text. We built simple 
      word-level vocabularies. Now we dive deep into HOW to convert text to tokens 
      efficiently and robustly.
    
    to_next_section: |
      Section 3.3 will take these tokens and map them to dense embeddings. Tokenization 
      gives us discrete IDs; embeddings give us continuous vectors for neural networks.

topics:
  - topic_number: 1
    title: "Tokenization Strategies: Character, Word, and Subword"
    
    overview: |
      There are three main approaches to tokenization, each with distinct tradeoffs 
      in vocabulary size, coverage, and semantic granularity. Understanding these 
      tradeoffs is critical for choosing the right tokenizer for your application.
    
    content:
      character_level:
        description: "Treat each character as a token"
        
        vocabulary:
          size: "Small: typically 100-300 characters"
          coverage: "Perfect: can represent any text in the alphabet"
          examples:
            english: "a-z, A-Z, 0-9, punctuation, space ≈ 100 tokens"
            unicode: "Extended for all languages ≈ 256-500 tokens"
        
        advantages:
          - "Smallest vocabulary (memory efficient)"
          - "No unknown tokens (perfect coverage)"
          - "Language-agnostic (works for any script)"
          - "Handles typos and rare words naturally"
        
        disadvantages:
          - "Very long sequences (slow to process)"
          - "Must learn word/morpheme boundaries from data"
          - "Harder to capture semantic meaning"
          - "Inefficient for languages with large alphabets (Chinese, Japanese)"
        
        use_cases:
          - "Character-level language models"
          - "Spell-checking and autocorrect"
          - "Low-resource languages"
          - "Code generation (fixed character set)"
        
        example:
          text: "cat"
          tokens: "['c', 'a', 't']"
          sequence_length: 3
      
      word_level:
        description: "Treat each word as a token (split on whitespace/punctuation)"
        
        vocabulary:
          size: "Large: 30,000-100,000+ words"
          coverage: "Incomplete: rare words become <UNK>"
          examples:
            english: "~170,000 words in Oxford English Dictionary"
            typical_model: "Use 30k-50k most frequent words"
        
        advantages:
          - "Natural semantic units (words have meaning)"
          - "Shorter sequences (faster processing)"
          - "Aligns with human linguistic intuition"
          - "Easy to interpret"
        
        disadvantages:
          - "Huge vocabulary (memory intensive)"
          - "Many rare words → lots of <UNK> tokens"
          - "Can't handle morphological variations well (run/running/runs)"
          - "Different tokenization per language"
          - "Compound words problematic (German: Donaudampfschifffahrtsgesellschaft)"
        
        use_cases:
          - "Early NLP systems (pre-2015)"
          - "Simple applications with controlled vocabulary"
          - "When interpretability is critical"
        
        example:
          text: "The cat sat on the mat."
          tokens: "['The', 'cat', 'sat', 'on', 'the', 'mat', '.']"
          sequence_length: 7
        
        unk_problem:
          text: "The cat sat on the mat."
          rare_word: "The quokka sat on the mat."
          tokens: "['The', '<UNK>', 'sat', 'on', 'the', 'mat', '.']"
          information_loss: "Lost 'quokka' identity"
      
      subword_level:
        description: "Split words into meaningful subunits (morphemes, frequent sequences)"
        
        key_insight: |
          Balance between character and word: vocabulary smaller than word-level 
          but larger than character-level. Captures morphology and handles rare 
          words by decomposing them.
        
        vocabulary:
          size: "Medium: 8,000-50,000 subwords"
          coverage: "Near-perfect: rare words split into known subwords"
          examples:
            word: "unhappiness"
            subwords: "['un', 'happiness'] or ['un', 'happy', 'ness']"
        
        advantages:
          - "Balanced vocabulary size"
          - "Handles rare words (decompose into subwords)"
          - "Captures morphology (pre-, -ing, -ed)"
          - "Language-agnostic (BPE works for any language)"
          - "Best of both worlds: semantic + coverage"
        
        disadvantages:
          - "Less interpretable than words"
          - "Slightly longer sequences than word-level"
          - "Requires training to learn subword units"
        
        algorithms:
          - "Byte-Pair Encoding (BPE) - GPT, RoBERTa"
          - "WordPiece - BERT"
          - "SentencePiece - T5, multilingual models"
          - "Unigram Language Model - alternative to BPE"
        
        use_cases:
          - "Modern LLMs (BERT, GPT, T5)"
          - "Machine translation"
          - "Multilingual models"
          - "Any production NLP system"
        
        example:
          text: "unhappiness"
          word_level: "['unhappiness'] or ['<UNK>'] if rare"
          subword_level: "['un', 'happiness'] or ['un', 'happy', 'ness']"
          benefit: "Known morphemes even if word is rare"
      
      comparison_table:
        metrics:
          - metric: "Vocabulary Size"
            character: "100-500"
            word: "30k-100k+"
            subword: "8k-50k"
          
          - metric: "Coverage"
            character: "100% (any text)"
            word: "Incomplete (many <UNK>)"
            subword: "~100% (decompose rare)"
          
          - metric: "Sequence Length"
            character: "Very long (×10-100 vs word)"
            word: "Short"
            subword: "Medium (×2-3 vs word)"
          
          - metric: "Memory"
            character: "Low (small vocab)"
            word: "High (huge vocab)"
            subword: "Medium"
          
          - metric: "Semantic Granularity"
            character: "None (learn from scratch)"
            word: "High (natural units)"
            subword: "Medium (morphemes)"
          
          - metric: "Unknown Handling"
            character: "None (everything known)"
            word: "Poor (<UNK> token)"
            subword: "Good (decompose)"
    
    implementation:
      simple_tokenizers:
        language: python
        code: |
          import re
          from typing import List
          
          class CharacterTokenizer:
              """Character-level tokenizer."""
              
              def __init__(self):
                  self.char_to_id = {}
                  self.id_to_char = {}
              
              def build_vocab(self, texts: List[str]):
                  """Build character vocabulary from corpus."""
                  chars = set()
                  for text in texts:
                      chars.update(text)
                  
                  # Add special tokens
                  special = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']
                  self.vocabulary = special + sorted(list(chars))
                  
                  self.char_to_id = {c: i for i, c in enumerate(self.vocabulary)}
                  self.id_to_char = {i: c for i, c in enumerate(self.vocabulary)}
              
              def encode(self, text: str) -> List[int]:
                  """Convert text to character IDs."""
                  unk_id = self.char_to_id['<UNK>']
                  return [self.char_to_id.get(c, unk_id) for c in text]
              
              def decode(self, ids: List[int]) -> str:
                  """Convert character IDs to text."""
                  return ''.join([self.id_to_char[i] for i in ids])
          
          
          class WordTokenizer:
              """Word-level tokenizer with frequency-based vocabulary."""
              
              def __init__(self, vocab_size: int = 10000):
                  self.vocab_size = vocab_size
                  self.word_to_id = {}
                  self.id_to_word = {}
              
              def tokenize(self, text: str) -> List[str]:
                  """Split text into words (simple whitespace + punctuation)."""
                  # Lowercase and split on whitespace
                  text = text.lower()
                  # Split on whitespace and keep punctuation separate
                  tokens = re.findall(r'\b\w+\b|[^\w\s]', text)
                  return tokens
              
              def build_vocab(self, texts: List[str]):
                  """Build vocabulary from most frequent words."""
                  from collections import Counter
                  
                  word_counts = Counter()
                  for text in texts:
                      word_counts.update(self.tokenize(text))
                  
                  # Get most common words
                  most_common = word_counts.most_common(self.vocab_size - 4)
                  
                  # Add special tokens
                  special = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']
                  self.vocabulary = special + [word for word, _ in most_common]
                  
                  self.word_to_id = {w: i for i, w in enumerate(self.vocabulary)}
                  self.id_to_word = {i: w for i, w in enumerate(self.vocabulary)}
              
              def encode(self, text: str) -> List[int]:
                  """Convert text to word IDs."""
                  words = self.tokenize(text)
                  unk_id = self.word_to_id['<UNK>']
                  return [self.word_to_id.get(w, unk_id) for w in words]
              
              def decode(self, ids: List[int]) -> str:
                  """Convert word IDs to text."""
                  return ' '.join([self.id_to_word[i] for i in ids])
          
          
          # Example usage
          corpus = [
              "The cat sat on the mat.",
              "The dog sat on the log.",
              "A quick brown fox jumps over the lazy dog."
          ]
          
          # Character-level
          char_tokenizer = CharacterTokenizer()
          char_tokenizer.build_vocab(corpus)
          
          text = "cat"
          char_ids = char_tokenizer.encode(text)
          print(f"Character tokens for '{text}': {char_ids}")
          print(f"Decoded: {char_tokenizer.decode(char_ids)}")
          
          # Word-level
          word_tokenizer = WordTokenizer(vocab_size=50)
          word_tokenizer.build_vocab(corpus)
          
          text = "the cat sat"
          word_ids = word_tokenizer.encode(text)
          print(f"Word tokens for '{text}': {word_ids}")
          print(f"Decoded: {word_tokenizer.decode(word_ids)}")
          
          # Rare word problem
          rare_text = "the quokka sat"
          rare_ids = word_tokenizer.encode(rare_text)
          print(f"Rare word '{rare_text}': {rare_ids}")  # 'quokka' → <UNK>
    
    security_implications:
      character_level_attacks:
        homoglyphs: |
          Visually similar characters from different Unicode blocks:
          - Latin 'a' (U+0061) vs Cyrillic 'а' (U+0430)
          - Latin 'e' (U+0065) vs Cyrillic 'е' (U+0435)
          Attack: "аdmin" (Cyrillic а + Latin dmin) bypasses "admin" filter
        
        zero_width_characters: |
          Invisible characters that don't render but affect tokenization:
          - Zero-Width Space (U+200B)
          - Zero-Width Non-Joiner (U+200C)
          Attack: "ad<ZWSP>min" looks like "admin" but tokenizes differently
      
      word_level_attacks:
        unk_token_exploitation: |
          Inject rare words that become <UNK>, losing semantic meaning:
          - Filter blocks "execute malicious code"
          - Attack uses "perform nefarious instructions" → multiple <UNK> tokens
          - Model can't detect malicious intent from <UNK> tokens
        
        compound_word_bypass: |
          Languages like German allow arbitrary compounds:
          - "Schadenfreude" (harm-joy) = malicious pleasure
          - Create novel compounds that aren't in vocabulary
          - Bypass filters that check individual words
      
      subword_level_attacks:
        boundary_manipulation: |
          Craft inputs where tokenization boundaries affect meaning:
          - "therapist" vs "the rapist" (different subword splits)
          - "penisland.com" (Pen Island) vs "penis land.com"
          - Exploit where tokenizer splits to hide or reveal intent

  - topic_number: 2
    title: "Byte-Pair Encoding (BPE): The Modern Standard"
    
    overview: |
      Byte-Pair Encoding (BPE) is the dominant subword tokenization algorithm used 
      in modern LLMs (GPT-2, GPT-3, RoBERTa, etc.). It's a greedy algorithm that 
      iteratively merges the most frequent character pairs to build vocabulary.
      
      Originally a compression algorithm, BPE was adapted for NLP by Sennrich et al. 
      (2016). It's simple, effective, and language-agnostic.
    
    content:
      algorithm_intuition:
        goal: "Build vocabulary of subword units by iteratively merging frequent pairs"
        
        process:
          step_1: "Start with character-level vocabulary"
          step_2: "Count all adjacent character pairs in corpus"
          step_3: "Merge most frequent pair into new token"
          step_4: "Repeat until vocabulary reaches target size"
        
        example_walkthrough:
          corpus: "['low', 'lower', 'newest', 'widest']"
          
          iteration_0:
            vocabulary: "['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd']"
            tokenization:
              - "low: l o w"
              - "lower: l o w e r"
              - "newest: n e w e s t"
              - "widest: w i d e s t"
          
          iteration_1:
            most_frequent_pair: "('e', 's')"
            count: 2
            merge: "es"
            new_vocabulary: "[..., 'es']"
            tokenization:
              - "low: l o w"
              - "lower: l o w e r"
              - "newest: n e w es t"
              - "widest: w i d es t"
          
          iteration_2:
            most_frequent_pair: "('es', 't')"
            count: 2
            merge: "est"
            new_vocabulary: "[..., 'es', 'est']"
            tokenization:
              - "low: l o w"
              - "lower: l o w e r"
              - "newest: n e w est"
              - "widest: w i d est"
          
          iteration_3:
            most_frequent_pair: "('l', 'o')"
            count: 2
            merge: "lo"
            new_vocabulary: "[..., 'es', 'est', 'lo']"
            tokenization:
              - "low: lo w"
              - "lower: lo w e r"
              - "newest: n e w est"
              - "widest: w i d est"
          
          final_result:
            vocabulary: "['l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo', ...]"
            learned_subwords: "['es', 'est', 'lo'] (common morphemes and sequences)"
      
      algorithm_details:
        initialization:
          - "Start with character vocabulary: all unique chars in corpus"
          - "Add special tokens: <PAD>, <UNK>, <SOS>, <EOS>"
          - "Represent each word as sequence of characters + end-of-word marker"
        
        merge_operation:
          - "Count all adjacent pairs in all words"
          - "Find pair with maximum frequency"
          - "Create new token by merging the pair"
          - "Update all words: replace pair with new token"
          - "Add new token to vocabulary"
        
        stopping_criteria:
          - "Reach target vocabulary size (e.g., 32k, 50k)"
          - "Or: frequency threshold (stop when max frequency < threshold)"
        
        encoding_new_text:
          - "Start with character-level split"
          - "Apply learned merges in order (greedy)"
          - "Result: sequence of subword tokens"
        
        properties:
          - "Deterministic: same corpus → same vocabulary"
          - "Greedy: locally optimal at each step"
          - "Frequency-based: common sequences merged first"
          - "Order matters: merge sequence is part of the model"
      
      advantages:
        - "Simple algorithm (easy to implement and understand)"
        - "Language-agnostic (works for any language, even mixed)"
        - "Good compression (frequent words: few tokens, rare words: more tokens)"
        - "Handles rare/unknown words (decompose to known subwords)"
        - "Captures morphology (prefixes, suffixes learned as units)"
        - "No <UNK> tokens in practice (can always decompose to characters)"
      
      limitations:
        - "Greedy: not globally optimal vocabulary"
        - "Sensitive to corpus: different corpora → different vocabularies"
        - "Merge order is critical (must store and apply in same order)"
        - "Doesn't explicitly model morphology (just frequency)"
        - "Can split semantically related words differently"
    
    implementation:
      bpe_from_scratch:
        language: python
        code: |
          import re
          from collections import Counter, defaultdict
          from typing import List, Tuple, Dict
          
          class BytePairEncoder:
              """Byte-Pair Encoding (BPE) tokenizer from scratch."""
              
              def __init__(self, vocab_size: int = 1000):
                  """
                  Args:
                      vocab_size: Target vocabulary size
                  """
                  self.vocab_size = vocab_size
                  self.vocabulary = []
                  self.token_to_id = {}
                  self.id_to_token = {}
                  self.merges = []  # List of (pair, merged_token) in order
              
              def get_words_and_freqs(self, texts: List[str]) -> Dict[str, int]:
                  """Extract words and their frequencies from corpus."""
                  word_freqs = Counter()
                  
                  for text in texts:
                      # Simple tokenization: lowercase, split on whitespace
                      words = text.lower().split()
                      word_freqs.update(words)
                  
                  return dict(word_freqs)
              
              def initialize_vocabulary(self, word_freqs: Dict[str, int]) -> Dict[str, List[str]]:
                  """
                  Initialize vocabulary with characters.
                  Represent each word as sequence of characters + </w> (end-of-word).
                  """
                  vocab = defaultdict(int)
                  word_splits = {}
                  
                  for word, freq in word_freqs.items():
                      # Split into characters, add end-of-word marker
                      split = list(word) + ['</w>']
                      word_splits[word] = split
                      
                      # Count all characters
                      for char in split:
                          vocab[char] += freq
                  
                  return word_splits, vocab
              
              def get_pair_frequencies(self, word_splits: Dict[str, List[str]], 
                                      word_freqs: Dict[str, int]) -> Counter:
                  """Count frequencies of all adjacent pairs."""
                  pair_freqs = Counter()
                  
                  for word, split in word_splits.items():
                      freq = word_freqs[word]
                      
                      # Count all adjacent pairs in this word
                      for i in range(len(split) - 1):
                          pair = (split[i], split[i + 1])
                          pair_freqs[pair] += freq
                  
                  return pair_freqs
              
              def merge_pair(self, pair: Tuple[str, str], 
                           word_splits: Dict[str, List[str]]) -> Dict[str, List[str]]:
                  """Merge the given pair in all words."""
                  new_word_splits = {}
                  merged_token = ''.join(pair)
                  
                  for word, split in word_splits.items():
                      new_split = []
                      i = 0
                      
                      while i < len(split):
                          # Check if current position has the pair to merge
                          if i < len(split) - 1 and split[i] == pair[0] and split[i + 1] == pair[1]:
                              new_split.append(merged_token)
                              i += 2
                          else:
                              new_split.append(split[i])
                              i += 1
                      
                      new_word_splits[word] = new_split
                  
                  return new_word_splits
              
              def train(self, texts: List[str]):
                  """
                  Train BPE on corpus.
                  
                  Args:
                      texts: List of text strings
                  """
                  # Get word frequencies
                  word_freqs = self.get_words_and_freqs(texts)
                  
                  # Initialize with character-level vocabulary
                  word_splits, char_vocab = self.initialize_vocabulary(word_freqs)
                  
                  # Start vocabulary with special tokens + characters
                  special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']
                  self.vocabulary = special_tokens + list(char_vocab.keys())
                  
                  print(f"Initial vocabulary size: {len(self.vocabulary)}")
                  
                  # Perform merges until reaching target vocabulary size
                  num_merges = self.vocab_size - len(self.vocabulary)
                  
                  for i in range(num_merges):
                      # Get pair frequencies
                      pair_freqs = self.get_pair_frequencies(word_splits, word_freqs)
                      
                      if not pair_freqs:
                          break
                      
                      # Find most frequent pair
                      most_frequent_pair = max(pair_freqs, key=pair_freqs.get)
                      
                      # Merge the pair
                      word_splits = self.merge_pair(most_frequent_pair, word_splits)
                      
                      # Add merged token to vocabulary
                      merged_token = ''.join(most_frequent_pair)
                      self.vocabulary.append(merged_token)
                      self.merges.append((most_frequent_pair, merged_token))
                      
                      if (i + 1) % 100 == 0:
                          print(f"Merge {i+1}/{num_merges}: {most_frequent_pair} → {merged_token} "
                                f"(freq: {pair_freqs[most_frequent_pair]})")
                  
                  # Create token-to-id mapping
                  self.token_to_id = {token: i for i, token in enumerate(self.vocabulary)}
                  self.id_to_token = {i: token for i, token in enumerate(self.vocabulary)}
                  
                  print(f"Final vocabulary size: {len(self.vocabulary)}")
              
              def encode_word(self, word: str) -> List[str]:
                  """Encode a single word using learned BPE merges."""
                  # Start with character-level split
                  split = list(word.lower()) + ['</w>']
                  
                  # Apply merges in order
                  for pair, merged_token in self.merges:
                      i = 0
                      new_split = []
                      
                      while i < len(split):
                          if i < len(split) - 1 and split[i] == pair[0] and split[i + 1] == pair[1]:
                              new_split.append(merged_token)
                              i += 2
                          else:
                              new_split.append(split[i])
                              i += 1
                      
                      split = new_split
                  
                  return split
              
              def encode(self, text: str) -> List[int]:
                  """Encode text to token IDs."""
                  words = text.lower().split()
                  
                  token_ids = []
                  unk_id = self.token_to_id['<UNK>']
                  
                  for word in words:
                      word_tokens = self.encode_word(word)
                      for token in word_tokens:
                          token_id = self.token_to_id.get(token, unk_id)
                          token_ids.append(token_id)
                  
                  return token_ids
              
              def decode(self, token_ids: List[int]) -> str:
                  """Decode token IDs to text."""
                  tokens = [self.id_to_token[tid] for tid in token_ids]
                  
                  # Join tokens and remove </w> markers
                  text = ''.join(tokens).replace('</w>', ' ').strip()
                  
                  return text
          
          
          # Example usage
          corpus = [
              "low lower lowest",
              "new newer newest",
              "wide wider widest",
              "low low low low"  # Repeated to increase frequency
          ]
          
          bpe = BytePairEncoder(vocab_size=100)
          bpe.train(corpus)
          
          # Show some learned merges
          print("\nFirst 10 merges:")
          for i, (pair, merged) in enumerate(bpe.merges[:10]):
              print(f"  {i+1}. {pair} → {merged}")
          
          # Encode new text
          text = "lowest newer"
          encoded = bpe.encode(text)
          print(f"\nEncoded '{text}': {encoded}")
          
          # Show tokens
          tokens = [bpe.id_to_token[tid] for tid in encoded]
          print(f"Tokens: {tokens}")
          
          # Decode back
          decoded = bpe.decode(encoded)
          print(f"Decoded: '{decoded}'")
          
          # Handle rare word
          rare_text = "quickest"
          rare_encoded = bpe.encode(rare_text)
          rare_tokens = [bpe.id_to_token[tid] for tid in rare_encoded]
          print(f"\nRare word '{rare_text}' → tokens: {rare_tokens}")
          print("(Decomposed into known subwords, not <UNK>)")
    
    security_implications:
      merge_order_dependency: |
        BPE merge order is critical. Adversaries can:
        - Use different BPE model to create incompatible tokenization
        - Exploit inconsistencies if model uses different tokenizer than defense
        - Craft inputs that tokenize differently with slight BPE variations
      
      frequency_manipulation: |
        BPE learns from corpus frequencies. Poisoning attacks:
        - Inject malicious phrases into training corpus with high frequency
        - Force BPE to create tokens for malicious patterns
        - Result: model sees malicious patterns as single tokens (harder to filter)
      
      compression_attacks: |
        BPE compresses common sequences. Adversaries can:
        - Craft inputs that compress to very few tokens (context window filling)
        - Or craft inputs that decompose to many tokens (DoS attack)
        - Exploit variable-length tokenization for timing attacks

  - topic_number: 3
    title: "WordPiece: BERT's Tokenization Algorithm"
    
    overview: |
      WordPiece is similar to BPE but uses a different merging criterion. Instead of 
      merging most frequent pairs, it merges pairs that maximize likelihood of training 
      data. Used in BERT, DistilBERT, and other Google models.
    
    content:
      wordpiece_vs_bpe:
        similarity:
          - "Both are subword tokenization algorithms"
          - "Both start with character-level vocabulary"
          - "Both iteratively merge pairs"
          - "Both produce similar vocabulary sizes"
        
        key_difference:
          bpe: "Merge most frequent pair"
          wordpiece: "Merge pair that maximizes training data likelihood"
          
          formula: |
            For each candidate pair (x, y):
            Score = P(xy) / (P(x) × P(y))
            
            Merge pair with highest score.
            
            Intuition: Prefer merges where combined token is much more likely 
            than independent tokens would predict.
      
      algorithm:
        initialization:
          - "Start with character vocabulary"
          - "Add special tokens: [CLS], [SEP], [MASK], [PAD], [UNK]"
          - "Prefix subwords with ## to mark continuation (not word start)"
        
        merge_criterion:
          definition: "Maximize likelihood of corpus under unigram language model"
          
          calculation: |
            For pair (a, b):
            - Count: freq(ab), freq(a), freq(b)
            - Score: freq(ab) / (freq(a) × freq(b))
            
            Higher score → stronger association → better to merge
          
          example:
            pair_1: "('th', 'e')"
            freq_th: 1000
            freq_e: 5000
            freq_the: 3000
            score_the: "3000 / (1000 × 5000) = 0.0006"
            
            pair_2: "('qu', 'i')"
            freq_qu: 100
            freq_i: 2000
            freq_qui: 90
            score_qui: "90 / (100 × 2000) = 0.00045"
            
            result: "Merge 'the' first (higher score)"
        
        continuation_marker:
          purpose: "Distinguish word-initial vs word-internal subwords"
          
          example:
            word: "playing"
            split: "['play', '##ing']"
            explanation: "'play' is word-initial, '##ing' is continuation"
          
          benefit: "Model knows 'play' can be standalone, '##ing' must attach"
      
      bert_special_tokens:
        cls_token:
          symbol: "[CLS]"
          position: "Start of every sequence"
          purpose: "Aggregate representation for classification tasks"
          usage: "BERT uses [CLS] embedding for sentence-level predictions"
        
        sep_token:
          symbol: "[SEP]"
          position: "End of sequences, between sentence pairs"
          purpose: "Separate segments in multi-sentence inputs"
          usage: "Question [SEP] Passage for QA tasks"
        
        mask_token:
          symbol: "[MASK]"
          purpose: "Masked Language Model (MLM) pre-training"
          usage: "Replace 15% of tokens with [MASK], predict original"
        
        pad_token:
          symbol: "[PAD]"
          purpose: "Pad sequences to same length in batch"
          usage: "Ignored by attention mechanism (mask = 0)"
        
        unk_token:
          symbol: "[UNK]"
          purpose: "Unknown tokens (rare in WordPiece due to decomposition)"
          usage: "Fallback when character not in vocab"
    
    implementation:
      wordpiece_tokenizer:
        language: python
        code: |
          import re
          from collections import Counter, defaultdict
          from typing import List, Dict, Tuple
          import math
          
          class WordPieceTokenizer:
              """WordPiece tokenizer (BERT-style)."""
              
              def __init__(self, vocab_size: int = 1000):
                  self.vocab_size = vocab_size
                  self.vocabulary = []
                  self.token_to_id = {}
                  self.id_to_token = {}
              
              def get_word_freqs(self, texts: List[str]) -> Dict[str, int]:
                  """Get word frequencies from corpus."""
                  word_freqs = Counter()
                  for text in texts:
                      words = text.lower().split()
                      word_freqs.update(words)
                  return dict(word_freqs)
              
              def initialize_vocab(self, word_freqs: Dict[str, int]):
                  """Initialize with character vocabulary."""
                  chars = set()
                  for word in word_freqs:
                      chars.update(word)
                  
                  # Special tokens (BERT-style)
                  special = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']
                  self.vocabulary = special + sorted(list(chars))
                  
                  # Initialize splits: first char normal, rest with ##
                  word_splits = {}
                  for word in word_freqs:
                      if len(word) == 1:
                          word_splits[word] = [word]
                      else:
                          word_splits[word] = [word[0]] + [f'##{c}' for c in word[1:]]
                  
                  return word_splits
              
              def get_pair_scores(self, word_splits: Dict[str, List[str]], 
                                 word_freqs: Dict[str, int]) -> Dict[Tuple[str, str], float]:
                  """
                  Calculate likelihood-based scores for all pairs.
                  Score = P(ab) / (P(a) × P(b))
                  """
                  # Count individual tokens
                  token_freqs = Counter()
                  for word, split in word_splits.items():
                      freq = word_freqs[word]
                      for token in split:
                          token_freqs[token] += freq
                  
                  # Count pairs
                  pair_freqs = Counter()
                  for word, split in word_splits.items():
                      freq = word_freqs[word]
                      for i in range(len(split) - 1):
                          pair = (split[i], split[i + 1])
                          pair_freqs[pair] += freq
                  
                  # Calculate scores
                  pair_scores = {}
                  for pair, pair_freq in pair_freqs.items():
                      a, b = pair
                      # Score = freq(ab) / (freq(a) * freq(b))
                      # Use log to avoid numerical issues
                      score = math.log(pair_freq) - math.log(token_freqs[a]) - math.log(token_freqs[b])
                      pair_scores[pair] = score
                  
                  return pair_scores
              
              def merge_pair(self, pair: Tuple[str, str], 
                           word_splits: Dict[str, List[str]]):
                  """Merge pair in all words."""
                  new_word_splits = {}
                  
                  # Create merged token (remove ## prefix if present)
                  if pair[1].startswith('##'):
                      merged = pair[0] + pair[1][2:]
                  else:
                      merged = pair[0] + pair[1]
                  
                  # If not word-initial, add ## prefix
                  if pair[0].startswith('##'):
                      merged = '##' + merged.lstrip('#')
                  
                  for word, split in word_splits.items():
                      new_split = []
                      i = 0
                      
                      while i < len(split):
                          if i < len(split) - 1 and split[i] == pair[0] and split[i + 1] == pair[1]:
                              new_split.append(merged)
                              i += 2
                          else:
                              new_split.append(split[i])
                              i += 1
                      
                      new_word_splits[word] = new_split
                  
                  return new_word_splits, merged
              
              def train(self, texts: List[str]):
                  """Train WordPiece on corpus."""
                  word_freqs = self.get_word_freqs(texts)
                  word_splits = self.initialize_vocab(word_freqs)
                  
                  print(f"Initial vocabulary size: {len(self.vocabulary)}")
                  
                  num_merges = self.vocab_size - len(self.vocabulary)
                  
                  for i in range(num_merges):
                      # Calculate scores for all pairs
                      pair_scores = self.get_pair_scores(word_splits, word_freqs)
                      
                      if not pair_scores:
                          break
                      
                      # Find pair with highest score
                      best_pair = max(pair_scores, key=pair_scores.get)
                      best_score = pair_scores[best_pair]
                      
                      # Merge the pair
                      word_splits, merged_token = self.merge_pair(best_pair, word_splits)
                      self.vocabulary.append(merged_token)
                      
                      if (i + 1) % 100 == 0:
                          print(f"Merge {i+1}/{num_merges}: {best_pair} → {merged_token} "
                                f"(score: {best_score:.4f})")
                  
                  self.token_to_id = {t: i for i, t in enumerate(self.vocabulary)}
                  self.id_to_token = {i: t for i, t in enumerate(self.vocabulary)}
                  
                  print(f"Final vocabulary size: {len(self.vocabulary)}")
              
              def encode_word(self, word: str) -> List[str]:
                  """Encode word using longest-match-first strategy."""
                  word = word.lower()
                  tokens = []
                  
                  start = 0
                  while start < len(word):
                      # Try longest possible match first
                      end = len(word)
                      found = False
                      
                      while start < end:
                          substr = word[start:end]
                          
                          # Add ## prefix if not start of word
                          if start > 0:
                              substr = '##' + substr
                          
                          if substr in self.token_to_id:
                              tokens.append(substr)
                              start = end
                              found = True
                              break
                          
                          end -= 1
                      
                      if not found:
                          # Single character not in vocab → [UNK]
                          tokens.append('[UNK]')
                          start += 1
                  
                  return tokens
              
              def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
                  """Encode text to token IDs (BERT-style)."""
                  words = text.lower().split()
                  
                  token_ids = []
                  
                  if add_special_tokens:
                      token_ids.append(self.token_to_id['[CLS]'])
                  
                  for word in words:
                      word_tokens = self.encode_word(word)
                      for token in word_tokens:
                          token_ids.append(self.token_to_id[token])
                  
                  if add_special_tokens:
                      token_ids.append(self.token_to_id['[SEP]'])
                  
                  return token_ids
              
              def decode(self, token_ids: List[int]) -> str:
                  """Decode token IDs to text."""
                  tokens = [self.id_to_token[tid] for tid in token_ids]
                  
                  # Remove special tokens
                  tokens = [t for t in tokens if not t.startswith('[')]
                  
                  # Join tokens, removing ## prefixes
                  text = ''.join([t.replace('##', '') for t in tokens])
                  
                  # Add spaces (simplified - real BERT uses more sophisticated decoding)
                  return text
          
          
          # Example
          corpus = [
              "playing player",
              "lowest newer",
              "low low low"
          ]
          
          wp = WordPieceTokenizer(vocab_size=50)
          wp.train(corpus)
          
          # Encode with special tokens
          text = "playing"
          encoded = wp.encode(text, add_special_tokens=True)
          tokens = [wp.id_to_token[tid] for tid in encoded]
          print(f"Encoded '{text}': {tokens}")
          # Expected: ['[CLS]', 'play', '##ing', '[SEP]']
    
    security_implications:
      continuation_marker_attacks: |
        The ## prefix creates attack opportunities:
        - "playing" → ['play', '##ing'] (normal)
        - Craft input that forces different splits: "play##ing" as literal text
        - Model sees different token boundaries than human reader
      
      special_token_injection: |
        Adversaries can inject special tokens as literal text:
        - Input: "I want to [MASK] the admin password"
        - Tokenizer might treat [MASK] as actual mask token
        - Bypass filters that check for mask token abuse
      
      likelihood_manipulation: |
        WordPiece uses likelihood, so poisoning training data affects merges:
        - Inject malicious phrases with high frequency
        - Forces specific subword boundaries favorable to attack
        - Example: Make "execute" and "code" merge into "executecode" token

  - topic_number: 4
    title: "Special Tokens and Multi-Lingual Tokenization"
    
    overview: |
      Special tokens serve specific purposes in modern LLMs: marking sequence boundaries, 
      masking for pre-training, padding batches, etc. Multi-lingual tokenization adds 
      complexity with Unicode, different scripts, and language-specific considerations.
    
    content:
      special_tokens_comprehensive:
        padding_token:
          symbol: "<PAD> or [PAD]"
          purpose: "Pad sequences to uniform length for batching"
          
          usage: |
            Sequences in batch: [5, 12, 8] tokens
            Max length: 12
            Padded:
              seq1: [token, token, ..., <PAD>, <PAD>]  (5 + 7 padding)
              seq2: [token, token, ..., <PAD>]          (12 + 0 padding)
              seq3: [token, token, ..., <PAD>, <PAD>]  (8 + 4 padding)
          
          attention_mask: "Set to 0 for <PAD> tokens so model ignores them"
        
        unknown_token:
          symbol: "<UNK> or [UNK]"
          purpose: "Represent out-of-vocabulary tokens"
          
          behavior:
            bpe_wordpiece: "Rare - decompose to subwords instead"
            word_level: "Common - many rare words → <UNK>"
            
          problem: "Loss of information - model can't distinguish different unknowns"
        
        start_end_tokens:
          start_of_sequence:
            symbol: "<SOS> or <BOS> or [CLS]"
            purpose: "Mark sequence beginning"
            bert_style: "[CLS] for classification aggregate"
          
          end_of_sequence:
            symbol: "<EOS> or </s> or [SEP]"
            purpose: "Mark sequence end"
            bert_style: "[SEP] separates segments"
          
          usage_example:
            single_sequence: "[CLS] The cat sat on mat [SEP]"
            sentence_pair: "[CLS] Question text [SEP] Answer text [SEP]"
        
        mask_token:
          symbol: "[MASK]"
          purpose: "Masked Language Model pre-training (BERT)"
          
          mlm_procedure:
            step_1: "Randomly select 15% of tokens"
            step_2: "Replace with [MASK] 80% of the time"
            step_3: "Replace with random token 10% of the time"
            step_4: "Keep original 10% of the time"
            objective: "Predict original token"
          
          example:
            original: "The cat sat on the mat"
            masked: "The [MASK] sat on the [MASK]"
            task: "Predict 'cat' and 'mat'"
        
        segment_tokens:
          symbol: "Segment IDs (not tokens, but related)"
          purpose: "Distinguish segments in multi-sentence input"
          
          example:
            input: "[CLS] Sentence A [SEP] Sentence B [SEP]"
            segment_ids: "[0, 0, 0, 0, 1, 1, 1]"
            usage: "Model learns which segment each token belongs to"
      
      unicode_and_multilingual:
        unicode_basics:
          code_points: "Characters as integers: U+0041 = 'A', U+4E2D = '中'"
          utf8_encoding: "Variable-length: 1-4 bytes per character"
          normalization: "Multiple representations: é = U+00E9 or e + combining accent"
        
        challenges:
          different_scripts:
            - "Latin (English): 26 letters"
            - "Cyrillic (Russian): 33 letters"
            - "Chinese: 20,000+ characters"
            - "Arabic: Right-to-left, contextual forms"
            - "Devanagari (Hindi): Combining characters"
          
          tokenization_strategies:
            shared_vocab: "Single vocab for all languages (mBERT, XLM-R)"
            language_specific: "Separate tokenizers per language"
            script_based: "Different handling per script"
          
          byte_level_bpe:
            approach: "Tokenize bytes, not Unicode characters (GPT-2, RoBERTa)"
            advantage: "Truly universal - any byte sequence valid"
            vocabulary: "256 base bytes + learned merges"
            drawback: "Less interpretable, longer sequences for non-Latin"
        
        sentencepiece:
          description: "Language-agnostic tokenizer (used in T5, XLM-R)"
          
          features:
            - "Direct text → tokens (no pre-tokenization)"
            - "Treats sentence as raw input (including spaces)"
            - "Works for any language without language-specific rules"
            - "Uses unigram language model or BPE"
          
          advantage: "Truly language-agnostic, no special handling needed"
          
          example:
            input: "▁Hello ▁World"
            note: "▁ represents space, included in tokens"
      
      handling_special_cases:
        numbers:
          challenge: "123456789 could tokenize many ways"
          strategies:
            - "Digit-by-digit: '1', '2', '3', ..."
            - "Number as unit: '123456789'"
            - "Scientific notation aware: '1.23e5'"
          
          recommendation: "Let BPE/WordPiece learn common patterns (2-3 digit chunks)"
        
        urls_emails:
          challenge: "Long, structured strings"
          strategies:
            - "Break at punctuation: 'http', '://', 'example', '.', 'com'"
            - "Special URL token: <URL>"
            - "Character-level for rare domains"
          
          security_note: "URLs often used in phishing - important to tokenize correctly"
        
        code:
          challenge: "Mixed language (English + programming syntax)"
          strategies:
            - "Preserve CamelCase: 'getUserName' → ['get', 'User', 'Name']"
            - "Preserve snake_case: 'user_name' → ['user', '_', 'name']"
            - "Keep operators separate: '==', '!=', '++'
          
          models: "CodeBERT, CodeT5 use specialized tokenizers"
    
    implementation:
      special_tokens_handler:
        language: python
        code: |
          from typing import List, Dict, Optional
          import re
          
          class SpecialTokensHandler:
              """Handle special tokens in tokenization pipeline."""
              
              def __init__(self, 
                          pad_token: str = '<PAD>',
                          unk_token: str = '<UNK>',
                          cls_token: str = '<CLS>',
                          sep_token: str = '<SEP>',
                          mask_token: str = '<MASK>'):
                  self.pad_token = pad_token
                  self.unk_token = unk_token
                  self.cls_token = cls_token
                  self.sep_token = sep_token
                  self.mask_token = mask_token
                  
                  self.special_tokens = {
                      pad_token, unk_token, cls_token, sep_token, mask_token
                  }
              
              def add_special_tokens(self, 
                                    token_ids: List[int],
                                    token_to_id: Dict[str, int],
                                    add_cls: bool = True,
                                    add_sep: bool = True) -> List[int]:
                  """Add [CLS] and [SEP] tokens to sequence."""
                  result = []
                  
                  if add_cls:
                      result.append(token_to_id[self.cls_token])
                  
                  result.extend(token_ids)
                  
                  if add_sep:
                      result.append(token_to_id[self.sep_token])
                  
                  return result
              
              def create_attention_mask(self, 
                                       token_ids: List[int],
                                       pad_token_id: int) -> List[int]:
                  """Create attention mask (1 for real tokens, 0 for padding)."""
                  return [0 if tid == pad_token_id else 1 for tid in token_ids]
              
              def pad_sequences(self,
                               sequences: List[List[int]],
                               pad_token_id: int,
                               max_length: Optional[int] = None) -> List[List[int]]:
                  """Pad sequences to same length."""
                  if max_length is None:
                      max_length = max(len(seq) for seq in sequences)
                  
                  padded = []
                  for seq in sequences:
                      if len(seq) < max_length:
                          # Pad to max_length
                          padding = [pad_token_id] * (max_length - len(seq))
                          padded.append(seq + padding)
                      else:
                          # Truncate if too long
                          padded.append(seq[:max_length])
                  
                  return padded
              
              def create_masked_lm_data(self,
                                       token_ids: List[int],
                                       token_to_id: Dict[str, int],
                                       mask_prob: float = 0.15,
                                       random_prob: float = 0.1,
                                       keep_prob: float = 0.1) -> tuple:
                  """
                  Create masked language model training data (BERT-style).
                  
                  Returns:
                      masked_ids: Sequence with [MASK] tokens
                      labels: Original tokens for masked positions
                      positions: Positions of masked tokens
                  """
                  import random
                  
                  masked_ids = token_ids.copy()
                  labels = [-100] * len(token_ids)  # -100 = ignore in loss
                  positions = []
                  
                  mask_token_id = token_to_id[self.mask_token]
                  vocab_size = len(token_to_id)
                  
                  for i, token_id in enumerate(token_ids):
                      # Skip special tokens
                      if token_id in [token_to_id[t] for t in self.special_tokens]:
                          continue
                      
                      # Randomly select 15% of tokens
                      if random.random() < mask_prob:
                          positions.append(i)
                          labels[i] = token_id
                          
                          rand = random.random()
                          if rand < 0.8:  # 80% of time: replace with [MASK]
                              masked_ids[i] = mask_token_id
                          elif rand < 0.9:  # 10% of time: replace with random token
                              masked_ids[i] = random.randint(0, vocab_size - 1)
                          # else: 10% of time: keep original
                  
                  return masked_ids, labels, positions
          
          
          # Example usage
          token_to_id = {
              '<PAD>': 0, '<UNK>': 1, '<CLS>': 2, '<SEP>': 3, '<MASK>': 4,
              'the': 5, 'cat': 6, 'sat': 7, 'on': 8, 'mat': 9
          }
          
          handler = SpecialTokensHandler()
          
          # Add special tokens
          tokens = [5, 6, 7, 8, 5, 9]  # "the cat sat on the mat"
          with_special = handler.add_special_tokens(tokens, token_to_id)
          print(f"With special tokens: {with_special}")
          # [2, 5, 6, 7, 8, 5, 9, 3] = [CLS] the cat sat on the mat [SEP]
          
          # Pad sequences
          sequences = [
              [2, 5, 6, 3],           # Short sequence
              [2, 5, 6, 7, 8, 9, 3],  # Long sequence
          ]
          padded = handler.pad_sequences(sequences, pad_token_id=0)
          print(f"Padded sequences: {padded}")
          
          # Create attention masks
          for seq in padded:
              mask = handler.create_attention_mask(seq, pad_token_id=0)
              print(f"Sequence: {seq}")
              print(f"Mask:     {mask}")
          
          # Create MLM data
          masked, labels, positions = handler.create_masked_lm_data(
              with_special, token_to_id, mask_prob=0.15
          )
          print(f"\nOriginal: {with_special}")
          print(f"Masked:   {masked}")
          print(f"Labels:   {labels}")
          print(f"Positions: {positions}")
      
      unicode_normalization:
        language: python
        code: |
          import unicodedata
          
          def normalize_unicode(text: str, form: str = 'NFC') -> str:
              """
              Normalize Unicode text.
              
              Forms:
                  NFC: Canonical decomposition, then canonical composition
                  NFD: Canonical decomposition
                  NFKC: Compatibility decomposition, then canonical composition
                  NFKD: Compatibility decomposition
              """
              return unicodedata.normalize(form, text)
          
          
          # Example: Multiple representations of same character
          text1 = "café"  # é as single character (U+00E9)
          text2 = "café"  # e + combining acute accent (U+0065 + U+0301)
          
          print(f"Text 1: {repr(text1)} - Length: {len(text1)}")
          print(f"Text 2: {repr(text2)} - Length: {len(text2)}")
          print(f"Equal: {text1 == text2}")  # False!
          
          # Normalize
          norm1 = normalize_unicode(text1, 'NFC')
          norm2 = normalize_unicode(text2, 'NFC')
          print(f"Normalized equal: {norm1 == norm2}")  # True
          
          
          def detect_homoglyphs(text: str) -> List[tuple]:
              """Detect potential homoglyph attacks."""
              suspicious = []
              
              for i, char in enumerate(text):
                  # Check for Cyrillic that looks like Latin
                  if '\u0400' <= char <= '\u04FF':  # Cyrillic block
                      if char in 'аеорсухАЕОРСУХ':  # Cyrillic chars that look Latin
                          suspicious.append((i, char, ord(char), 'Cyrillic'))
                  
                  # Check for zero-width characters
                  if char in '\u200B\u200C\u200D\uFEFF':
                      suspicious.append((i, char, ord(char), 'Zero-width'))
              
              return suspicious
          
          
          # Example attack
          attack_text = "аdmin"  # Cyrillic 'а' + Latin 'dmin'
          homoglyphs = detect_homoglyphs(attack_text)
          print(f"\nPotential homoglyph attack in '{attack_text}':")
          for pos, char, code, type_ in homoglyphs:
              print(f"  Position {pos}: '{char}' (U+{code:04X}) - {type_}")
    
    security_implications:
      special_token_injection: |
        Attackers inject special tokens as literal text:
        - User input: "My password is [MASK]"
        - If not properly escaped, tokenizer treats [MASK] as special token
        - Defense: Escape or remove special token patterns in user input
      
      padding_oracle_attacks: |
        Padding patterns can leak information:
        - Different padding lengths for different inputs
        - Timing attacks based on sequence length
        - Defense: Constant-time padding, random padding
      
      unicode_normalization_attacks: |
        Different normalization creates different tokens:
        - Filter uses NFC normalization
        - Model uses NFD normalization
        - Input passes filter but tokenizes maliciously
        - Defense: Consistent normalization across pipeline
      
      homoglyph_attacks: |
        Visually identical characters from different Unicode blocks:
        - "раypаl.com" (Cyrillic р and а)
        - Bypasses string matching filters
        - Defense: Normalize to single script, detect mixed scripts

key_takeaways:
  critical_concepts:
    - concept: "Tokenization converts text to numerical IDs for model processing"
      why_it_matters: "Wrong tokenization → model can't learn properly"
    
    - concept: "Three main approaches: character, word, subword (BPE/WordPiece)"
      why_it_matters: "Tradeoff between vocabulary size, coverage, and semantic granularity"
    
    - concept: "BPE: merge most frequent pairs (greedy, frequency-based)"
      why_it_matters: "Simple, effective, language-agnostic - used in GPT"
    
    - concept: "WordPiece: merge pairs maximizing likelihood (used in BERT)"
      why_it_matters: "Better captures semantic associations than pure frequency"
    
    - concept: "Special tokens ([CLS], [SEP], [MASK], [PAD]) serve specific purposes"
      why_it_matters: "Critical for BERT-style models and batch processing"
    
    - concept: "Unicode normalization and multi-lingual handling are complex"
      why_it_matters: "Inconsistent handling creates security vulnerabilities"
  
  actionable_steps:
    - step: "Implement BPE tokenizer from scratch"
      verification: "Train on corpus, encode new text, decompose rare words"
    
    - step: "Implement WordPiece with ## continuation markers"
      verification: "Distinguish word-initial vs word-internal subwords"
    
    - step: "Handle special tokens ([CLS], [SEP], [MASK], [PAD])"
      verification: "Create BERT-style input sequences with segment IDs"
    
    - step: "Normalize Unicode text consistently"
      verification: "Same text → same tokens regardless of Unicode representation"
    
    - step: "Detect and handle homoglyphs and zero-width characters"
      verification: "Flag suspicious character combinations"
  
  security_principles:
    - principle: "Tokenization boundaries are attack surfaces"
      application: "Audit where text splits affect security decisions"
    
    - principle: "Homoglyphs and Unicode tricks bypass token-based filters"
      application: "Normalize Unicode, detect mixed scripts, visual similarity checks"
    
    - principle: "Special tokens can be injected if not properly escaped"
      application: "Sanitize user input, escape special token patterns"
    
    - principle: "BPE/WordPiece learn from corpus - poisoning affects tokenization"
      application: "Audit training corpus, monitor learned merge operations"
    
    - principle: "Inconsistent tokenization across defense layers creates gaps"
      application: "Use same tokenizer for input validation and model processing"
  
  common_mistakes:
    - mistake: "Not normalizing Unicode consistently"
      fix: "Always normalize (NFC or NFD) before tokenization"
    
    - mistake: "Forgetting to handle special tokens in user input"
      fix: "Escape or remove special token patterns from user-provided text"
    
    - mistake: "Using different tokenizers for training and inference"
      fix: "Save and load tokenizer with model, ensure exact match"
    
    - mistake: "Not accounting for variable-length tokenization"
      fix: "Pad/truncate to consistent length, use attention masks"
    
    - mistake: "Assuming one token = one word"
      fix: "Subword tokenization means words split differently"
  
  integration_with_book:
    from_section_3_1:
      - "Vocabulary building and frequency analysis"
      - "Understanding of text as discrete symbols"
      - "N-gram models (character/word level)"
    
    to_next_section:
      - "Section 3.3: Word Embeddings - mapping tokens to dense vectors"
      - "Section 3.4: Word2Vec - learning embeddings from context"
      - "Tokens from this section become inputs to embedding layers"
  
  looking_ahead:
    next_concepts:
      - "Dense word embeddings (continuous representations)"
      - "Word2Vec: Skip-gram and CBOW"
      - "Embedding similarity and semantic space"
      - "From static embeddings to contextual embeddings"
    
    skills_to_build:
      - "Train Word2Vec on corpus"
      - "Visualize embedding space"
      - "Understand attention mechanisms"
      - "Build transformer with learned embeddings"
  
  final_thoughts: |
    Tokenization is the foundation of all modern NLP. Every character you type must 
    be converted to tokens before any model can process it. The choice of tokenization 
    strategy affects:
    - Model capacity (vocabulary size)
    - Training efficiency (sequence length)
    - Generalization (handling rare words)
    - Security properties (attack surfaces)
    
    BPE and WordPiece dominate modern NLP because they balance these tradeoffs well. 
    They handle rare words through decomposition, keep vocabulary manageable, and work 
    across languages.
    
    From a security perspective: tokenization creates exploitable boundaries. Adversaries 
    craft inputs that look benign to humans but tokenize maliciously. Homoglyphs, 
    Unicode tricks, and special token injection all exploit tokenization vulnerabilities.
    
    Understanding tokenization deeply means you can:
    - Debug weird model behaviors (often tokenization issues)
    - Secure input pipelines (consistent normalization and escaping)
    - Design robust NLP systems (handle edge cases properly)
    
    Next: We take these tokens and map them to dense, continuous embeddings where 
    similar words have similar vectors. This is where neural NLP really begins.

---
