# section_01_24_gradient_descent_variants.yaml

---
document_info:
  chapter: "01"
  section: "24"
  title: "Gradient Descent Variants"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 5
  tags: ["gradient-descent", "optimizers", "sgd", "momentum", "adam", "learning-rate", "batch-size", "convergence"]

# ============================================================================
# SECTION 1.24: GRADIENT DESCENT VARIANTS
# ============================================================================

section_01_24_gradient_descent_variants:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    We've used gradient descent throughout this chapter to train models. But vanilla gradient 
    descent has limitations: slow convergence, getting stuck in local minima, sensitivity to 
    learning rates. Modern optimizers like SGD with momentum, RMSprop, and Adam solve these 
    issues, enabling faster and more reliable training of neural networks.
    
    This section covers the evolution from basic gradient descent to sophisticated adaptive 
    optimizers. Each variant addresses specific problems: momentum overcomes local minima, 
    adaptive learning rates handle different parameter scales, and combinations like Adam 
    provide robust default optimizers that work well across diverse problems.
    
    This section covers:
    - Batch, Stochastic, and Mini-batch Gradient Descent
    - SGD with Momentum (overcoming local minima)
    - RMSprop (adaptive learning rates)
    - Adam (combining momentum + adaptive rates)
    - Learning rate schedules
    - Choosing optimizers and hyperparameters
    
    For security ML, optimizer choice affects:
    1. Training speed (faster iterations = faster deployment)
    2. Convergence stability (reliable training)
    3. Generalization (avoid overfitting)
    4. Ease of use (Adam as robust default)
  
  why_this_matters: |
    Practical impact:
    - Adam converges 10x faster than vanilla GD
    - Momentum helps escape plateaus in loss landscape
    - Adaptive learning rates handle diverse feature scales
    - Production systems rely on these optimizers
    
    Security context:
    - Faster training = faster response to new threats
    - Stable convergence = reliable model updates
    - Robust defaults = less hyperparameter tuning
    
    Why learn optimizers:
    - Critical for training deep networks
    - Understanding convergence behavior
    - Debugging training issues
    - Choosing right optimizer for problem
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Batch vs Stochastic vs Mini-batch
  # --------------------------------------------------------------------------
  
  batch_variants:
    
    batch_gradient_descent:
      
      algorithm: |
        Use ALL training samples to compute gradient
        
        For each iteration:
          1. Compute loss on entire dataset
          2. Compute gradient: ∇J = (1/n) Σᵢ ∇L(xᵢ, yᵢ)
          3. Update: θ = θ - α∇J
      
      pros:
        - "Stable convergence (smooth gradient)"
        - "Guaranteed to converge to minimum (convex case)"
        - "Exact gradient direction"
      
      cons:
        - "Slow on large datasets (compute all samples)"
        - "Requires all data in memory"
        - "Gets stuck in local minima (deterministic)"
      
      example: |
        Dataset: 1 million samples
        Each iteration: Compute gradient on all 1M samples
        Very slow!
    
    stochastic_gradient_descent:
      
      algorithm: |
        Use ONE random sample to compute gradient
        
        For each iteration:
          1. Sample random xᵢ, yᵢ
          2. Compute gradient: ∇J ≈ ∇L(xᵢ, yᵢ)
          3. Update: θ = θ - α∇L(xᵢ, yᵢ)
      
      pros:
        - "Very fast (one sample per iteration)"
        - "Can escape local minima (noise helps)"
        - "Online learning (process streaming data)"
      
      cons:
        - "Noisy gradient (high variance)"
        - "Erratic convergence"
        - "May not converge exactly to minimum"
      
      visualization: |
        Loss landscape:
        BGD: Smooth path to minimum
        SGD: Noisy, zigzag path (but gets there faster)
    
    mini_batch_gradient_descent:
      
      algorithm: |
        Use BATCH of samples to compute gradient
        
        For each iteration:
          1. Sample random batch {x₁...xₘ}, {y₁...yₘ}
          2. Compute gradient: ∇J ≈ (1/m) Σᵢ ∇L(xᵢ, yᵢ)
          3. Update: θ = θ - α∇J
      
      batch_size: "Typical: 32, 64, 128, 256"
      
      pros:
        - "Balance between BGD and SGD"
        - "Reduced variance (averaging over batch)"
        - "Efficient GPU utilization (parallel)"
        - "Standard in practice"
      
      choosing_batch_size: |
        Small (8-32):
        - Noisier gradients (regularization effect)
        - Better generalization
        - Less memory
        
        Large (256-1024):
        - Smoother gradients
        - Faster training (fewer iterations)
        - More memory
        - May overfit
        
        Typical: 32 or 64 (good default)
      
      epochs_and_iterations: |
        Epoch: One pass through entire dataset
        
        Iterations per epoch = n_samples / batch_size
        
        Example:
        10,000 samples, batch_size=100
        → 100 iterations per epoch
        
        Train for 50 epochs = 5,000 iterations
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Momentum
  # --------------------------------------------------------------------------
  
  momentum:
    
    problem_with_vanilla_sgd: |
      Vanilla SGD oscillates in narrow valleys
      
      Imagine rolling ball in valley:
      - Steep sides: Large gradients perpendicular to goal
      - Gentle slope toward goal: Small gradient
      
      Result: Bounces between walls, slow progress forward
    
    momentum_intuition: |
      Add "velocity" to gradient descent
      Like ball gaining momentum rolling downhill
      
      Dampens oscillations
      Accelerates in consistent directions
    
    algorithm: |
      Maintain velocity vector v
      
      Initialize: v = 0
      
      For each iteration:
        1. Compute gradient: g = ∇J(θ)
        2. Update velocity: v = βv + g
        3. Update parameters: θ = θ - αv
      
      Where β is momentum coefficient (typically 0.9)
    
    mathematical_formulation: |
      Velocity accumulates gradients:
      
      vₜ = βvₜ₋₁ + gₜ
         = gₜ + βgₜ₋₁ + β²gₜ₋₂ + β³gₜ₋₃ + ...
      
      Exponentially weighted average of past gradients
      
      β = 0.9: Average over ~10 recent gradients
      β = 0.99: Average over ~100 recent gradients
    
    effect: |
      Consistent direction: Velocity accumulates
      Oscillating direction: Velocity cancels out
      
      Example:
      Iteration 1: g = [5, 0],  v = [5, 0]
      Iteration 2: g = [4, 0],  v = [0.9×5 + 4, 0] = [8.5, 0]
      Iteration 3: g = [6, 0],  v = [0.9×8.5 + 6, 0] = [13.65, 0]
      
      Accelerating in consistent direction!
    
    benefits:
      - "Faster convergence (acceleration)"
      - "Reduces oscillations"
      - "Helps escape shallow local minima"
      - "Standard practice in deep learning"
    
    nesterov_momentum: |
      Variant: Nesterov Accelerated Gradient (NAG)
      
      "Look ahead" before computing gradient
      
      Algorithm:
        1. Look-ahead position: θ_lookahead = θ - αβv
        2. Compute gradient at look-ahead: g = ∇J(θ_lookahead)
        3. Update velocity: v = βv + g
        4. Update parameters: θ = θ - αv
      
      Benefit: More responsive to changes in gradient
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Adaptive Learning Rates
  # --------------------------------------------------------------------------
  
  adaptive_learning_rates:
    
    problem: |
      Fixed learning rate suboptimal
      
      Different parameters need different learning rates:
      - Frequently updated parameters: Need smaller learning rate
      - Rarely updated parameters: Need larger learning rate
      
      Example:
      Feature 1: Appears in 90% of samples → frequent updates
      Feature 2: Appears in 10% of samples → rare updates
      
      Fixed α treats both equally (suboptimal)
    
    adagrad:
      
      idea: "Adapt learning rate per parameter based on history"
      
      algorithm: |
        Maintain sum of squared gradients for each parameter
        
        Initialize: G = 0  (same shape as θ)
        
        For each iteration:
          1. Compute gradient: g = ∇J(θ)
          2. Accumulate: G = G + g²  (element-wise)
          3. Update: θ = θ - (α / √(G + ε)) ⊙ g
        
        Where ε = 1e-8 (numerical stability)
      
      effect: |
        Parameters with large cumulative gradients:
        → Small effective learning rate (α / √large_G)
        
        Parameters with small cumulative gradients:
        → Large effective learning rate (α / √small_G)
      
      limitation: "Learning rate only decreases (monotonic)"
      
      problem_long_training: |
        G grows indefinitely → learning rate → 0
        Training stops prematurely
    
    rmsprop:
      
      improvement: "Exponentially weighted moving average instead of sum"
      
      algorithm: |
        Maintain moving average of squared gradients
        
        Initialize: v = 0
        
        For each iteration:
          1. Compute gradient: g = ∇J(θ)
          2. Update moving avg: v = βv + (1-β)g²
          3. Update: θ = θ - (α / √(v + ε)) ⊙ g
        
        Typical: β = 0.9, α = 0.001
      
      advantage: |
        Recent gradients weighted more
        Old gradients decay exponentially
        Learning rate doesn't vanish
      
      use_case: "Good for recurrent neural networks"
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Adam Optimizer
  # --------------------------------------------------------------------------
  
  adam:
    
    full_name: "Adaptive Moment Estimation"
    
    combines: |
      Best of both worlds:
      - Momentum (first moment)
      - RMSprop (second moment)
    
    algorithm: |
      Maintain two moving averages:
      - m: First moment (momentum)
      - v: Second moment (RMSprop)
      
      Initialize: m = 0, v = 0, t = 0
      
      For each iteration:
        t = t + 1
        
        1. Compute gradient: g = ∇J(θ)
        
        2. Update first moment: m = β₁m + (1-β₁)g
        
        3. Update second moment: v = β₂v + (1-β₂)g²
        
        4. Bias correction:
           m̂ = m / (1 - β₁ᵗ)
           v̂ = v / (1 - β₂ᵗ)
        
        5. Update: θ = θ - α × m̂ / (√v̂ + ε)
    
    hyperparameters: |
      Defaults (work well in practice):
      - α = 0.001 (learning rate)
      - β₁ = 0.9 (momentum decay)
      - β₂ = 0.999 (RMSprop decay)
      - ε = 1e-8 (numerical stability)
    
    bias_correction: |
      Why needed: m and v initialized to zero
      
      Early iterations: m and v biased toward zero
      
      Bias correction: Unbiases estimates
      
      m̂ = m / (1 - β₁ᵗ)
      
      At t=1: m̂ = m / (1 - 0.9¹) = m / 0.1 (large correction)
      At t=10: m̂ = m / (1 - 0.9¹⁰) ≈ m / 0.65
      At t→∞: m̂ ≈ m (negligible correction)
    
    why_adam_is_popular:
      - "Combines momentum + adaptive learning rates"
      - "Works well with default hyperparameters"
      - "Robust across diverse problems"
      - "Fast convergence"
      - "De facto standard optimizer"
    
    variants:
      adamw:
        improvement: "Decoupled weight decay"
        use_case: "Better generalization, preferred for Transformers"
      
      nadam:
        improvement: "Nesterov momentum + Adam"
        benefit: "Slightly faster convergence"
  
  # --------------------------------------------------------------------------
  # Core Concept 5: Learning Rate Schedules
  # --------------------------------------------------------------------------
  
  learning_rate_schedules:
    
    motivation: |
      Fixed learning rate limitations:
      - Large α: Fast initially, but oscillates near minimum
      - Small α: Stable but very slow
      
      Solution: Start large, decrease over time
    
    step_decay:
      
      formula: "α(t) = α₀ × γ^(⌊t/k⌋)"
      
      example: |
        α₀ = 0.1, γ = 0.5, k = 10 epochs
        
        Epochs 0-9: α = 0.1
        Epochs 10-19: α = 0.05
        Epochs 20-29: α = 0.025
        ...
      
      pros: "Simple, interpretable"
      cons: "Requires tuning k and γ"
    
    exponential_decay:
      
      formula: "α(t) = α₀ × e^(-λt)"
      
      smooth: "Gradual decrease"
      
      typical: "λ = 0.01 to 0.1"
    
    cosine_annealing:
      
      formula: "α(t) = α_min + 0.5(α_max - α_min)(1 + cos(πt/T))"
      
      behavior: |
        Smooth decrease following cosine curve
        Starts at α_max, ends at α_min
      
      popular: "Used in modern architectures (ResNets, Transformers)"
    
    warmup:
      
      idea: "Gradually increase learning rate at start"
      
      motivation: |
        Neural networks sensitive at initialization
        Large learning rate early can destabilize training
      
      strategy: |
        Epochs 0-5: Linear increase 0 → α₀
        Epochs 5+: Normal schedule (constant or decay)
      
      common_with: "Adam, large batch training, Transformers"
  
  # --------------------------------------------------------------------------
  # Practical Implementation
  # --------------------------------------------------------------------------
  
  practical_implementation: |
    import numpy as np
    
    class SGDMomentum:
        """SGD with momentum"""
        def __init__(self, learning_rate=0.01, momentum=0.9):
            self.lr = learning_rate
            self.momentum = momentum
            self.velocity = None
        
        def update(self, params, grads):
            if self.velocity is None:
                self.velocity = [np.zeros_like(p) for p in params]
            
            for i, (p, g, v) in enumerate(zip(params, grads, self.velocity)):
                self.velocity[i] = self.momentum * v + g
                params[i] -= self.lr * self.velocity[i]
            
            return params
    
    class RMSprop:
        """RMSprop optimizer"""
        def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):
            self.lr = learning_rate
            self.beta = beta
            self.epsilon = epsilon
            self.v = None
        
        def update(self, params, grads):
            if self.v is None:
                self.v = [np.zeros_like(p) for p in params]
            
            for i, (p, g, v) in enumerate(zip(params, grads, self.v)):
                self.v[i] = self.beta * v + (1 - self.beta) * g**2
                params[i] -= self.lr * g / (np.sqrt(self.v[i]) + self.epsilon)
            
            return params
    
    class Adam:
        """Adam optimizer"""
        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
            self.lr = learning_rate
            self.beta1 = beta1
            self.beta2 = beta2
            self.epsilon = epsilon
            self.m = None
            self.v = None
            self.t = 0
        
        def update(self, params, grads):
            if self.m is None:
                self.m = [np.zeros_like(p) for p in params]
                self.v = [np.zeros_like(p) for p in params]
            
            self.t += 1
            
            for i, (p, g, m, v) in enumerate(zip(params, grads, self.m, self.v)):
                # Update biased first moment
                self.m[i] = self.beta1 * m + (1 - self.beta1) * g
                
                # Update biased second moment
                self.v[i] = self.beta2 * v + (1 - self.beta2) * g**2
                
                # Bias correction
                m_hat = self.m[i] / (1 - self.beta1**self.t)
                v_hat = self.v[i] / (1 - self.beta2**self.t)
                
                # Update parameters
                params[i] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
            
            return params
    
    # ========================================================================
    # USAGE COMPARISON
    # ========================================================================
    
    # Train same problem with different optimizers
    for optimizer_name, optimizer in [
        ("SGD", SGD(learning_rate=0.01)),
        ("SGD+Momentum", SGDMomentum(learning_rate=0.01, momentum=0.9)),
        ("RMSprop", RMSprop(learning_rate=0.001)),
        ("Adam", Adam(learning_rate=0.001))
    ]:
        print(f"\nTraining with {optimizer_name}:")
        # Train model...
        # Typically: Adam converges fastest
  
  # --------------------------------------------------------------------------
  # Choosing Optimizer and Hyperparameters
  # --------------------------------------------------------------------------
  
  choosing_optimizer:
    
    decision_guide: |
      Default choice: Adam
      - Works well on most problems
      - Robust to hyperparameter choices
      - Fast convergence
      
      Alternatives:
      - SGD+Momentum: When you have time to tune learning rate
      - RMSprop: For RNNs/LSTMs
      - AdamW: For Transformers/modern architectures
    
    hyperparameter_guidelines:
      
      learning_rate:
        adam: "0.001 (default), try [0.0001, 0.001, 0.01]"
        sgd: "0.01 to 0.1, requires more tuning"
        
        too_large: "Loss diverges or oscillates"
        too_small: "Very slow convergence"
        
        tuning: "Grid search: [0.0001, 0.001, 0.01, 0.1]"
      
      batch_size:
        typical: "32 or 64"
        
        small: "8-32 for better generalization"
        large: "128-256 for faster training"
        
        memory_limited: "Use largest batch that fits in GPU"
      
      momentum:
        sgd: "β = 0.9 (standard)"
        adam: "β₁ = 0.9 (default)"
        
        rarely_tune: "Defaults work well"
    
    training_strategies:
      
      strategy_1_start_with_defaults:
        - "Adam with lr=0.001, batch_size=32"
        - "Train for reasonable epochs (50-100)"
        - "Check if converging"
      
      strategy_2_tune_learning_rate:
        - "If not converging: Try [0.0001, 0.01]"
        - "If oscillating: Decrease learning rate"
        - "If too slow: Increase learning rate"
      
      strategy_3_try_schedule:
        - "Add learning rate decay if training long"
        - "Cosine annealing for modern architectures"
        - "Warmup for large models"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Mini-batch GD: Balance between batch and stochastic"
      - "Momentum: Accelerates in consistent directions"
      - "Adaptive learning rates: Different rates per parameter"
      - "Adam: Combines momentum + adaptive rates (de facto standard)"
      - "Learning rate schedules: Decay over time for better convergence"
      - "Batch size affects training speed and generalization"
    
    practical_skills:
      - "Implement SGD, Momentum, RMSprop, Adam from scratch"
      - "Choose optimizer: Adam as default"
      - "Tune learning rate: Start with 0.001 for Adam, 0.01 for SGD"
      - "Select batch size: 32 or 64 typical"
      - "Apply learning rate schedules when needed"
    
    security_mindset:
      - "Faster convergence = faster model updates for new threats"
      - "Adam robust default = less hyperparameter tuning"
      - "Stable training = reliable production systems"
      - "Batch size affects memory (important for edge deployment)"
    
    remember_this:
      - "Adam = default optimizer (works well out-of-box)"
      - "Learning rate most important hyperparameter"
      - "Momentum helps escape local minima"
      - "Mini-batch size: 32 or 64 (good starting point)"
      - "Use learning rate decay for long training"
    
    next_steps:
      - "Next section: Batch Normalization"
      - "You now understand optimization algorithms!"
      - "Critical for training deep neural networks"

---
