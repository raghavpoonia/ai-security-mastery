# section_01_28_ml_security_intro.yaml
---
document_info:
  chapter: "01"
  section: "28"
  title: "ML Security Considerations"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-16"
  estimated_pages: 6
  tags: ["security", "attacks", "defenses", "threat-model", "ai-security"]

# ============================================================================
# SECTION 1.28: ML SECURITY CONSIDERATIONS
# ============================================================================

section_01_28_ml_security_intro:
  
  # --------------------------------------------------------------------------
  # Overview
  # --------------------------------------------------------------------------
  
  overview: |
    You've learned how machine learning works - now let's understand where it breaks.
    Every stage of the ML pipeline is an attack surface. Adversaries can poison training
    data, manipulate models during inference, or steal intellectual property post-deployment.
    Understanding these threats is the foundation of AI security.
    
    This section bridges classical ML (Chapter 1) to AI security (rest of book). We'll
    survey the attack landscape, understand fundamental vulnerabilities, and preview
    defenses covered in later chapters. By the end, you'll think like an attacker and
    defender simultaneously - essential for building secure ML systems.
  
  learning_objectives:
    
    conceptual:
      - "Understand ML attack surface at each pipeline stage"
      - "Know major attack categories and threat models"
      - "Recognize fundamental ML vulnerabilities"
      - "Grasp security-accuracy tradeoffs"
    
    practical:
      - "Identify security risks in ML projects"
      - "Apply threat modeling to ML systems"
      - "Recognize vulnerable implementations"
      - "Design with security mindset"
    
    forward_looking:
      - "Preview deep learning security (Chapter 2)"
      - "Connect to detection engineering (Chapter 3)"
      - "Understand research directions"
  
  # --------------------------------------------------------------------------
  # Topic 1: The ML Attack Surface
  # --------------------------------------------------------------------------
  
  ml_attack_surface:
    
    pipeline_stages:
      
      stage_1_data_collection:
        normal_workflow: |
          Collect training data from various sources:
          - Web scraping, APIs, databases
          - User-generated content
          - Sensors, logs, telemetry
        
        attack_surface:
          data_poisoning: |
            Attacker injects malicious samples into training set
            
            Example: Email spam filter
            - Attacker sends spam emails labeled as "not spam"
            - Model learns spam patterns as legitimate
            - Future spam evades detection
          
          label_flipping: |
            Modify labels of existing samples
            
            Example: Malware detector
            - Change 1% of malware labels to "benign"
            - Model learns malware features as safe
            - Similar malware misclassified
          
          backdoor_injection: |
            Insert triggered samples with wrong labels
            
            Example: Face recognition
            - Add specific accessory (glasses) to faces
            - Label as different person
            - Model learns: glasses → wrong identity
        
        security_impact: "Training data compromise = model compromise"
      
      stage_2_feature_engineering:
        normal_workflow: |
          Transform raw data to features:
          - Feature extraction (domain knowledge)
          - Feature selection (dimensionality reduction)
          - Feature normalization/scaling
        
        attack_surface:
          feature_manipulation: |
            Exploit feature engineering vulnerabilities
            
            Example: PDF malware detector using file size
            - Attacker pads malware to "normal" size
            - Feature becomes useless
          
          normalization_attacks: |
            Exploit normalization assumptions
            
            Example: Min-max scaling to [0, 1]
            - Training data: values in [10, 100]
            - Attack: submit value 1000
            - Normalized to 1.0 (normal) or breaks scaling
        
        security_impact: "Feature engineering exposes assumptions attackers exploit"
      
      stage_3_model_training:
        normal_workflow: |
          Train model on prepared data:
          - Select algorithm (XGBoost, SVM, etc.)
          - Tune hyperparameters
          - Optimize via gradient descent
        
        attack_surface:
          gradient_based_attacks: |
            Exploit gradient information during training
            
            Will cover extensively in Chapter 2 (adversarial examples)
          
          hyperparameter_manipulation: |
            If attacker controls training:
            - Set high learning rate → unstable training
            - Disable regularization → overfitting
            - Set wrong loss function → poor performance
        
        security_impact: "Training process vulnerabilities enable subtle sabotage"
      
      stage_4_model_inference:
        normal_workflow: |
          Deploy trained model to production:
          - Receive input
          - Compute prediction
          - Return result
        
        attack_surface:
          adversarial_examples: |
            Craft inputs that fool model
            
            Example: Spam filter
            - Add specific words that trigger "not spam"
            - Email still spam but evades detection
            
            Chapter 2 covers: imperceptible perturbations that flip predictions
          
          evasion_attacks: |
            Modify malicious input to appear benign
            
            Example: Malware detector
            - Change malware slightly (add junk code)
            - Maintain functionality but evade detection
          
          denial_of_service: |
            Submit inputs that cause model to crash or slow down
            
            Example: NLP model
            - Very long input (memory exhaustion)
            - Adversarial input (infinite loop in preprocessing)
        
        security_impact: "Inference is continuous attack surface in production"
      
      stage_5_post_deployment:
        normal_workflow: |
          Monitor and maintain deployed model:
          - Track performance metrics
          - Retrain periodically
          - Update to new versions
        
        attack_surface:
          model_extraction: |
            Steal model by querying
            
            Example: Cloud API
            - Query with many inputs
            - Observe predictions
            - Train substitute model
            - Steal intellectual property
          
          model_inversion: |
            Reconstruct training data from model
            
            Example: Medical diagnosis model
            - Query systematically
            - Reverse-engineer patient data
            - Privacy violation
          
          membership_inference: |
            Determine if specific sample was in training set
            
            Example: Hospital ML model
            - Test if patient X in training data
            - Reveals patient has condition
            - Privacy leak
        
        security_impact: "Deployed models leak information about training data"
    
    complete_pipeline_view: |
      ML Pipeline Attack Surface:
      
      Data Collection → [Poisoning, Label Flipping, Backdoors]
           ↓
      Feature Engineering → [Feature Manipulation, Normalization Attacks]
           ↓
      Model Training → [Gradient Attacks, Hyperparameter Manipulation]
           ↓
      Model Inference → [Adversarial Examples, Evasion, DoS]
           ↓
      Post-Deployment → [Model Extraction, Inversion, Membership Inference]
      
      Security must address ALL stages, not just one.
  
  # --------------------------------------------------------------------------
  # Topic 2: Major Attack Categories
  # --------------------------------------------------------------------------
  
  attack_categories:
    
    training_time_attacks:
      
      data_poisoning:
        description: "Inject malicious samples into training data"
        
        clean_label_poisoning: |
          Add mislabeled samples
          
          Example: 
          - Training set: 10,000 benign, 10,000 malware
          - Attacker adds: 100 malware labeled as "benign"
          - Model learns malware patterns as safe
          - Attack success: Similar malware evades detection
        
        backdoor_poisoning: |
          Add triggered samples with specific labels
          
          Example:
          - Add images with specific pattern (trigger)
          - Label all triggered images as target class
          - Model learns: trigger → target class
          - Attacker can activate backdoor at will
        
        difficulty: "Easy if attacker controls data source (web scraping, user uploads)"
        
        impact: "Persistent - backdoor stays in model, hard to detect"
      
      adversarial_training_data: |
        Use adversarial examples as training data
        
        Defense technique (Chapter 2), but also attack vector:
        - If attacker controls training process
        - Can use adversarial examples with wrong labels
        - Model learns wrong patterns
    
    inference_time_attacks:
      
      adversarial_examples:
        description: "Craft inputs that fool model"
        
        classical_ml_example: |
          Spam filter (Naive Bayes on word frequencies):
          - Normal spam: "Buy cheap drugs now!"
          - Adversarial: "Buy cheap drugs now! (add benign words: meeting schedule report)"
          - Word frequencies shift toward legitimate email
          - Evades detection
        
        deep_learning_example: |
          Image classifier (Chapter 2):
          - Add imperceptible noise to image
          - Human sees same image
          - Model misclassifies with high confidence
        
        effectiveness: "Very effective - often 80-90%+ success rate"
        
        detection_difficulty: "Hard - looks like normal input"
      
      evasion_attacks:
        description: "Modify malicious input to appear benign while maintaining function"
        
        example_malware: |
          Binary classifier based on file features:
          - Original malware: detected
          - Add junk code (no-ops, dead code)
          - Functionality unchanged
          - Features change → evades detection
        
        example_phishing: |
          URL classifier based on text features:
          - Original: "paypa1.com" (suspicious)
          - Modified: "secure-paypal-login-verify.com" (looks legitimate)
          - Still phishing, but evades simple filters
        
        arms_race: "Attacker modifies → Defender updates → Attacker adapts → ..."
    
    post_deployment_attacks:
      
      model_extraction:
        description: "Steal model by querying API"
        
        method: |
          1. Send many queries (1,000-10,000)
          2. Collect input-output pairs
          3. Train substitute model on pairs
          4. Substitute ≈ original (70-90% accuracy)
        
        example: |
          Cloud ML API (e.g., Google Vision API):
          - Query cost: $1.50 per 1,000 images
          - Extract model: ~$15 for 10,000 queries
          - Steal $millions of research investment
        
        defense_preview: |
          Rate limiting, query monitoring, prediction rounding
          (Details in Chapter 5: Production Security)
      
      model_inversion:
        description: "Reconstruct training data from model"
        
        example_face_recognition: |
          Model trained on celebrity faces:
          1. Query model for "celebrity X" prediction scores
          2. Optimize synthetic image to maximize score
          3. Result: reconstructed face resembling celebrity
          4. Privacy violation
        
        example_medical: |
          Disease prediction model:
          1. Query with varying symptoms
          2. Infer sensitive patient attributes
          3. Reconstruct patient profiles
        
        privacy_impact: "Training data privacy compromised"
      
      membership_inference:
        description: "Determine if sample was in training set"
        
        method: |
          1. Query model with target sample
          2. If model very confident → likely in training
          3. If model uncertain → likely not in training
          
          Overfitted models leak more (high training accuracy)
        
        example_hospital: |
          ML model trained on patient records:
          - Query with patient X's data
          - High confidence → patient in dataset
          - Reveals: patient has condition (privacy leak)
        
        success_rate: "60-80% accuracy typical, up to 90% on overfitted models"
  
  # --------------------------------------------------------------------------
  # Topic 3: Fundamental ML Vulnerabilities
  # --------------------------------------------------------------------------
  
  fundamental_vulnerabilities:
    
    gradient_based_learning:
      vulnerability: |
        ML models optimize via gradients
        Gradients can be exploited to craft adversarial inputs
      
      why_vulnerable: |
        Gradient descent finds local minima
        Adversary uses same gradients to find adversarial examples
        
        Model learns: ∂L/∂w (minimize loss)
        Attacker uses: ∂L/∂x (maximize loss or change prediction)
      
      example: |
        Linear classifier: y = w^T x + b
        
        Adversarial perturbation:
        δ = ε · sign(w)
        
        x_adv = x + δ
        
        Small δ (imperceptible) but moves x in direction of gradient
      
      impact: "Fundamental to how ML works - hard to eliminate completely"
    
    high_dimensional_spaces:
      vulnerability: |
        ML operates in high-dimensional feature spaces
        Intuition from 2D/3D doesn't apply
      
      curse_of_dimensionality: |
        1000-dimensional space:
        - Most volume concentrated at corners
        - Decision boundaries complex
        - Many adversarial directions available
      
      example: |
        Image: 224×224×3 = 150,528 dimensions
        
        Each pixel = direction to perturb
        Attacker has 150,528 knobs to turn
        
        Finding adversarial example = optimization in 150K-D space
        (Many solutions exist)
      
      impact: "High dimensions = large attack surface"
    
    training_data_dependency:
      vulnerability: |
        Model quality entirely dependent on training data
        "Garbage in, garbage out"
      
      data_poisoning_leverage: |
        Small fraction poisoned (1-5%) can compromise model
        
        Example: 1% poisoning
        - Training set: 10,000 samples
        - Poisoned: 100 samples
        - Model learns backdoor or misclassifies
      
      no_data_authentication: |
        Most training pipelines don't verify data integrity
        - Web scraping: trust source
        - User uploads: trust users
        - APIs: trust provider
        
        Any compromised source = compromised model
      
      impact: "Supply chain vulnerability - data provenance critical"
    
    overfitting_and_memorization:
      vulnerability: |
        Models can memorize training data
        Especially with high capacity (deep networks)
      
      privacy_leak: |
        Overfitted model memorizes specific samples
        
        Membership inference exploits this:
        - Training sample → high confidence (memorized)
        - Test sample → lower confidence (generalized)
        
        Difference reveals membership
      
      example: |
        Medical diagnosis model:
        - Trains to 100% accuracy (overfits)
        - Memorizes patient records
        - Query with patient data → confidence reveals membership
      
      defense_tradeoff: "Regularization helps but reduces accuracy"
    
    lack_of_interpretability:
      vulnerability: |
        ML models often "black boxes"
        Hard to understand why prediction made
      
      security_blind_spot: |
        Can't debug what you can't understand
        
        - Why did model classify this as benign?
        - Is it using right features or shortcut?
        - Has model been poisoned?
        
        Without interpretability, can't answer
      
      example_shortcut_learning: |
        Image classifier trained on:
        - Cows: always on grass (green background)
        - Camels: always on sand (yellow background)
        
        Model learns: Green → cow, Yellow → camel
        Not actual animal features!
        
        Adversary exploits: Camel on grass → misclassified as cow
      
      impact: "Can't verify model is secure without understanding it"
  
  # --------------------------------------------------------------------------
  # Topic 4: Defense Strategies Preview
  # --------------------------------------------------------------------------
  
  defense_preview:
    
    data_security:
      sanitization: |
        Clean training data before use:
        - Remove duplicates, outliers
        - Verify labels (human review critical samples)
        - Anomaly detection on training data
      
      provenance_tracking: |
        Track data sources:
        - Where did each sample come from?
        - Can we trust that source?
        - Has data been tampered?
      
      differential_privacy: |
        Add noise during training (Chapter 2):
        - Limits what model can learn about individuals
        - Protects against membership inference
        - Tradeoff: 5-15% accuracy loss
    
    model_hardening:
      adversarial_training: |
        Train on adversarial examples (Chapter 2):
        - Generate adversarial samples during training
        - Model learns robustness
        - Tradeoff: Slower training, some accuracy loss
      
      ensemble_methods: |
        Combine multiple models:
        - Each model trained differently
        - Harder to fool all simultaneously
        - Used in production: voting/averaging
      
      certified_defenses: |
        Provable robustness (Chapter 2):
        - Mathematical guarantee no adversarial example within radius
        - Expensive, limited applicability
        - Research area
    
    runtime_protections:
      input_validation: |
        Check inputs before inference:
        - Format validation (file type, size)
        - Range checking (values in expected range)
        - Anomaly detection (statistical outliers)
      
      output_monitoring: |
        Monitor predictions:
        - Confidence scores (too high/low suspicious)
        - Prediction distribution (sudden shift → attack)
        - Query patterns (systematic probing → extraction)
      
      rate_limiting: |
        Prevent model extraction:
        - Limit queries per user/IP
        - Block systematic probing
        - Log all queries for forensics
    
    post_deployment_security:
      model_monitoring: |
        Continuous evaluation:
        - Accuracy over time (drift detection)
        - Prediction distribution (concept drift)
        - Adversarial testing (periodic red team)
      
      model_versioning: |
        Track model changes:
        - Version control (Git for models)
        - Rollback capability (quick recovery)
        - A/B testing (validate updates)
      
      incident_response: |
        Prepare for compromise:
        - Detection: Monitor for attacks
        - Response: Rollback, patch, retrain
        - Forensics: Understand what happened
    
    defense_in_depth: |
      No single defense sufficient
      Layer multiple protections:
      
      1. Data security (sanitize, provenance)
      2. Model hardening (adversarial training)
      3. Runtime protections (input validation, rate limiting)
      4. Monitoring (drift, anomaly detection)
      5. Incident response (rollback, forensics)
      
      Attacker must breach ALL layers
  
  # --------------------------------------------------------------------------
  # Topic 5: Security-Accuracy Tradeoff
  # --------------------------------------------------------------------------
  
  security_accuracy_tradeoff:
    
    fundamental_tension:
      observation: |
        Improving security often reduces accuracy
        
        Examples:
        - Adversarial training: -5-10% accuracy
        - Differential privacy: -10-20% accuracy
        - Simpler models: -5-15% accuracy (but more interpretable)
      
      why_tradeoff_exists: |
        Security adds constraints:
        - Robustness: Model must work on perturbed inputs
        - Privacy: Model can't memorize specific samples
        - Interpretability: Model must be simple enough to understand
        
        Constraints limit model capacity → lower accuracy
      
      not_always_tradeoff: |
        Some security measures improve both:
        - Data sanitization: Better data → better accuracy
        - Regularization: Prevents overfitting, improves generalization
        - Ensemble methods: Often higher accuracy + robustness
    
    decision_framework:
      
      security_critical_applications:
        examples: "Medical diagnosis, autonomous vehicles, financial fraud"
        
        approach: |
          Prioritize security over accuracy
          - Accept 5-10% accuracy loss
          - Use adversarial training, differential privacy
          - Extensive testing, certification
          - Human oversight for critical decisions
      
      accuracy_critical_applications:
        examples: "Recommendation systems, search ranking"
        
        approach: |
          Prioritize accuracy, add runtime protections
          - Maximize accuracy (no adversarial training)
          - Focus on input validation, rate limiting
          - Monitor for attacks, quick rollback
          - Accept some vulnerability for performance
      
      balanced_applications:
        examples: "Most production ML systems"
        
        approach: |
          Find optimal tradeoff
          - Some adversarial training (5% loss)
          - Basic privacy protections
          - Input validation + monitoring
          - Continuous improvement
    
    measuring_tradeoff:
      
      metrics: |
        Standard: Clean accuracy (benign inputs)
        Security: Robust accuracy (adversarial inputs)
        
        Example:
        Model A: 95% clean, 40% robust
        Model B: 90% clean, 60% robust
        
        Which is better? Depends on threat model!
      
      threat_modeling: |
        Define adversary:
        - Who: Script kiddie vs nation state
        - What: Capabilities (query access, training access)
        - Why: Motivation (profit, sabotage, research)
        
        Design defenses for threat model
        
        Example:
        Public API → Focus on extraction, evasion
        Internal model → Focus on data poisoning
  
  # --------------------------------------------------------------------------
  # Topic 6: Bridge to Future Chapters
  # --------------------------------------------------------------------------
  
  looking_forward:
    
    chapter_2_deep_learning_security:
      preview: |
        Deep neural networks have unique vulnerabilities:
        - Adversarial examples (imperceptible perturbations)
        - Backdoor attacks (trigger patterns in images)
        - Model inversion (reconstruct training images)
      
      topics:
        - "Adversarial examples: FGSM, PGD, C&W attacks"
        - "Backdoor detection: Neural Cleanse, activation clustering"
        - "Differential privacy: DP-SGD for privacy-preserving training"
        - "Certified defenses: Provable robustness guarantees"
      
      connection_to_chapter_1: |
        Classical ML attacks apply to deep learning
        + Deep learning specific attacks (gradient-based)
        + New defenses (adversarial training)
    
    chapter_3_detection_engineering:
      preview: |
        Apply ML to build security detection systems:
        - Malware detection (static, dynamic, hybrid)
        - Network intrusion detection (anomaly, signature)
        - Phishing detection (URL, email, content)
      
      security_considerations: |
        Detection systems are adversarial environments:
        - Attackers actively evade
        - Concept drift (malware evolves)
        - False positive management (operational reality)
        
        Must design for adversarial robustness from day 1
      
      techniques: |
        Combine Chapter 1 + Chapter 2:
        - XGBoost on static features
        - CNN on malware visualizations
        - Ensemble for robustness
    
    ongoing_research:
      
      areas: |
        - Certified robustness at scale
        - Privacy-preserving federated learning
        - Explainable AI for security
        - Adaptive defenses (learn from attacks)
        - Quantum-safe ML (post-quantum cryptography)
      
      why_matters: |
        AI security is young field (5-10 years)
        Rapid evolution of attacks and defenses
        Continuous learning essential
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "ML pipeline has attack surface at EVERY stage: data, training, inference, post-deployment"
      - "Data poisoning: Small fraction poisoned (1-5%) can compromise model significantly"
      - "Adversarial examples: Craft inputs that fool model (80-90% success typical)"
      - "Model extraction: Steal model via API queries (10,000 queries sufficient)"
      - "Privacy attacks: Membership inference, model inversion reveal training data"
      - "Fundamental vulnerabilities: Gradients exploitable, high-dimensional spaces, data dependency"
    
    security_mindset:
      - "Think like attacker: Where can pipeline be manipulated?"
      - "Defense in depth: Layer multiple protections, no single solution"
      - "Tradeoffs exist: Security often costs accuracy (5-20%)"
      - "Threat modeling: Design defenses for specific adversary"
      - "Continuous monitoring: Attacks evolve, defenses must too"
    
    actionable_steps:
      - "Sanitize training data: Remove duplicates, verify labels, anomaly detection"
      - "Track data provenance: Know source and integrity of every training sample"
      - "Validate inputs: Format, range, anomaly checks before inference"
      - "Monitor outputs: Confidence scores, prediction distribution, query patterns"
      - "Version models: Rollback capability, A/B testing, incident response plan"
    
    forward_looking:
      - "Chapter 2 covers deep learning specific attacks (adversarial examples, backdoors)"
      - "Chapter 3 applies techniques to real detection systems (malware, intrusion, phishing)"
      - "AI security is young field - expect rapid evolution of techniques"
      - "Security-aware development now standard practice, not afterthought"

---
