# section_02_22_adversarial_robustness.yaml

---
document_info:
  chapter: "02"
  section: "22"
  title: "Adversarial Robustness Fundamentals"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["adversarial-examples", "robustness", "fgsm", "pgd", "adversarial-training", "certified-defense"]

# ============================================================================
# SECTION 02_22: ADVERSARIAL ROBUSTNESS FUNDAMENTALS
# ============================================================================

section_02_22_adversarial_robustness:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Neural networks are vulnerable to adversarial examples - imperceptibly
    modified inputs that cause misclassification. A stop sign with carefully
    crafted stickers becomes "speed limit 45", an email with invisible Unicode
    bypasses spam filters. These attacks are practical, transferable across
    models, and fundamental to the optimization landscape.
    
    This section provides foundational adversarial robustness. You'll understand
    why neural networks vulnerable (high-dimensional decision boundaries),
    implement classic attacks (FGSM, PGD), measure robustness empirically,
    learn defenses (adversarial training, input transformations), understand
    limits of current defenses (obfuscated gradients, adaptive attacks), and
    preview certified defenses (Chapter 10).
    
    Note: This is FUNDAMENTALS. Chapter 10 covers advanced adversarial ML,
    backdoors, model extraction, poisoning, and production security. This
    section establishes core concepts.
  
  learning_objectives:
    
    conceptual:
      - "Understand why adversarial examples exist"
      - "Grasp perturbation budgets (L∞, L2 norms)"
      - "Know threat models (white-box, black-box)"
      - "Recognize transferability of attacks"
      - "Understand adversarial training fundamentals"
      - "Preview certification (full coverage Chapter 10)"
    
    practical:
      - "Implement FGSM (Fast Gradient Sign Method)"
      - "Implement PGD (Projected Gradient Descent)"
      - "Generate adversarial examples"
      - "Measure adversarial accuracy"
      - "Train robust model via adversarial training"
      - "Evaluate robustness on standard benchmarks"
    
    security_focused:
      - "Adversarial examples = practical attacks"
      - "Physical adversarial examples possible (stickers)"
      - "Transferability enables black-box attacks"
      - "Most defenses breakable by adaptive attacks"
  
  prerequisites:
    - "All Chapter 02 sections (backprop, CNNs, training)"
    - "Understanding of gradient computation"
    - "Familiarity with optimization"
  
  # --------------------------------------------------------------------------
  # Topic 1: Adversarial Examples
  # --------------------------------------------------------------------------
  
  adversarial_examples:
    
    what_are_adversarial_examples:
      
      definition: |
        Adversarial Example: Input x' that is:
        1. Close to original x (imperceptible change)
        2. Causes misclassification: f(x') ≠ f(x)
        
        x': adversarial example
        x: clean example
        δ = x' - x: adversarial perturbation
        
        Constraint: ||δ|| ≤ ε (small perturbation)
      
      visual_example: |
        Image classification:
        
        Clean image x: "panda" (57% confidence)
        + Perturbation δ (imperceptible noise)
        = Adversarial x': "gibbon" (99% confidence!)
        
        Human: Sees identical images
        Model: Completely fooled
      
      surprising_properties: |
        1. Universal: Affects all neural networks
        2. Transferable: Attack on model A works on model B
        3. Physical: Work in physical world (printed, photographed)
        4. Targeted: Can force specific misclassification
        5. Fast: Can generate in milliseconds
    
    why_they_exist:
      
      high_dimensional_geometry: |
        Image: 224×224×3 = 150,528 dimensions
        
        Decision boundary:
        - Must separate classes in 150K-dim space
        - Extremely complex surface
        - Close to data points
        
        Small perturbation in 150K dimensions:
        - Can cross decision boundary
        - Imperceptible to humans (each pixel changes tiny amount)
        - Significant to model (cumulative effect)
      
      linear_approximation: |
        Neural network locally linear:
        
        f(x + δ) ≈ f(x) + ∇f(x)^T · δ
        
        If ∇f(x) large:
        - Small δ → large change in f
        
        High-dimensional gradient:
        - Many dimensions add up
        - Even tiny perturbations accumulate
      
      optimization_vs_robustness: |
        Accuracy and robustness are in tension:
        
        Accurate model:
        - Fits training data tightly
        - Decision boundaries close to data
        - Vulnerable to perturbations
        
        Robust model:
        - Wider margins around data
        - Smoother decision boundaries
        - Lower clean accuracy
        
        Can't maximize both simultaneously!
    
    threat_models:
      
      white_box: |
        Attacker has full access:
        - Model architecture
        - Weights
        - Gradients
        
        Can compute optimal perturbation
        Strongest threat model
      
      black_box: |
        Attacker only has:
        - Query access (input → prediction)
        - No gradients, no weights
        
        Can transfer attacks or use query-based methods
        More realistic but weaker
      
      perturbation_budgets: |
        L∞ norm (most common):
        ||δ||_∞ ≤ ε
        
        Each pixel changes by at most ε
        ε = 8/255 ≈ 0.03 (standard for images)
        
        L2 norm:
        ||δ||_2 ≤ ε
        
        Total perturbation energy bounded
        
        L0 norm:
        ||δ||_0 ≤ k
        
        At most k pixels changed (sparse)
  
  # --------------------------------------------------------------------------
  # Topic 2: Attack Algorithms
  # --------------------------------------------------------------------------
  
  attack_algorithms:
    
    fgsm:
      
      algorithm: |
        Fast Gradient Sign Method (Goodfellow et al., 2014)
        
        Goal: Find x' that maximizes loss
        
        δ = ε · sign(∇_x L(x, y_true))
        x' = x + δ
        
        Where:
        - ∇_x L: gradient of loss w.r.t. input
        - sign: element-wise sign function
        - ε: perturbation budget (e.g., 8/255)
        
        One-step attack (very fast!)
      
      intuition: |
        Move in direction that increases loss:
        
        Loss landscape: Loss = f(x)
        Gradient: Points uphill (increasing loss)
        Sign: Use only direction (not magnitude)
        
        Result: Takes steepest ascent step to increase loss
      
      implementation: |
        def fgsm_attack(model, x, y_true, epsilon=0.03):
            """
            Fast Gradient Sign Method attack.
            
            Parameters:
            - model: neural network
            - x: clean input (batch_size, ...)
            - y_true: true labels (batch_size,)
            - epsilon: perturbation budget (L∞ norm)
            
            Returns:
            - x_adv: adversarial examples
            """
            # Enable gradient computation
            x.requires_grad = True
            
            # Forward pass
            logits = model(x)
            loss = cross_entropy(logits, y_true)
            
            # Backward pass (compute gradient w.r.t. input)
            loss.backward()
            grad = x.grad
            
            # Generate perturbation
            perturbation = epsilon * torch.sign(grad)
            
            # Create adversarial example
            x_adv = x + perturbation
            
            # Clip to valid range [0, 1]
            x_adv = torch.clamp(x_adv, 0, 1)
            
            return x_adv
      
      limitations: |
        - Single step (suboptimal)
        - Linear approximation only
        - Can be weak against robust models
        
        But: Extremely fast, good baseline
    
    pgd:
      
      algorithm: |
        Projected Gradient Descent (Madry et al., 2017)
        
        Iterative version of FGSM:
        
        Initialize: x_0 = x
        For t = 1 to T:
            x_t = x_{t-1} + α · sign(∇_x L(x_{t-1}, y_true))
            x_t = Clip(x_t, x - ε, x + ε)  # Project to ε-ball
        
        x_adv = x_T
        
        Where:
        - α: step size (smaller than ε)
        - T: number of iterations (10-100)
        - Clip: project back to valid perturbation ball
        
        Much stronger than FGSM!
      
      implementation: |
        def pgd_attack(model, x, y_true, epsilon=0.03, alpha=0.01, num_steps=10):
            """
            Projected Gradient Descent attack.
            
            Parameters:
            - model: neural network
            - x: clean input
            - y_true: true labels
            - epsilon: perturbation budget (L∞)
            - alpha: step size per iteration
            - num_steps: number of PGD iterations
            
            Returns:
            - x_adv: adversarial examples
            """
            # Random initialization (better than zero init)
            delta = torch.zeros_like(x).uniform_(-epsilon, epsilon)
            x_adv = x + delta
            x_adv = torch.clamp(x_adv, 0, 1)
            
            for step in range(num_steps):
                x_adv.requires_grad = True
                
                # Forward pass
                logits = model(x_adv)
                loss = cross_entropy(logits, y_true)
                
                # Backward pass
                loss.backward()
                grad = x_adv.grad
                
                # Update adversarial example
                x_adv = x_adv.detach() + alpha * torch.sign(grad)
                
                # Project back to epsilon ball around x
                delta = torch.clamp(x_adv - x, -epsilon, epsilon)
                x_adv = x + delta
                
                # Clip to valid range
                x_adv = torch.clamp(x_adv, 0, 1)
            
            return x_adv
      
      why_stronger: |
        PGD vs FGSM:
        
        FGSM: Single large step (may overshoot)
        PGD: Many small steps (explores locally optimal)
        
        PGD finds better adversarial examples:
        - Higher success rate
        - Lower perturbation for same misclassification
        - Standard for evaluating robustness
    
    targeted_attacks:
      
      untargeted: |
        Goal: Any misclassification
        
        Maximize: L(x', y_true)
        
        Just want model to be wrong (don't care what it predicts)
      
      targeted: |
        Goal: Specific target class y_target
        
        Minimize: L(x', y_target)
        
        Want model to predict y_target specifically
        
        Example:
        - Original: "stop sign"
        - Target: "speed limit 45"
        
        More constrained, harder to achieve
      
      implementation: |
        def targeted_pgd(model, x, y_target, epsilon=0.03, alpha=0.01, num_steps=10):
            """
            Targeted PGD attack.
            
            Minimize loss on target class (not maximize on true class).
            """
            delta = torch.zeros_like(x).uniform_(-epsilon, epsilon)
            x_adv = x + delta
            x_adv = torch.clamp(x_adv, 0, 1)
            
            for step in range(num_steps):
                x_adv.requires_grad = True
                
                logits = model(x_adv)
                
                # Minimize loss on target class (opposite of untargeted)
                loss = -cross_entropy(logits, y_target)
                
                loss.backward()
                grad = x_adv.grad
                
                # Update (note: subtract because we're minimizing)
                x_adv = x_adv.detach() - alpha * torch.sign(grad)
                
                delta = torch.clamp(x_adv - x, -epsilon, epsilon)
                x_adv = x + delta
                x_adv = torch.clamp(x_adv, 0, 1)
            
            return x_adv
  
  # --------------------------------------------------------------------------
  # Topic 3: Measuring Robustness
  # --------------------------------------------------------------------------
  
  measuring_robustness:
    
    adversarial_accuracy:
      
      definition: |
        Adversarial Accuracy: Accuracy on adversarial examples
        
        Clean accuracy: Test accuracy on clean data
        Robust accuracy: Test accuracy on adversarial data
        
        Standard model:
        - Clean: 95%
        - Robust: 0% (completely broken!)
        
        Robust model:
        - Clean: 87%
        - Robust: 55%
      
      evaluation_protocol: |
        def evaluate_robustness(model, test_loader, attack_fn):
            """
            Evaluate model robustness.
            
            Parameters:
            - model: neural network
            - test_loader: test data
            - attack_fn: attack function (e.g., pgd_attack)
            
            Returns:
            - clean_acc: accuracy on clean data
            - robust_acc: accuracy on adversarial data
            """
            model.eval()
            
            clean_correct = 0
            robust_correct = 0
            total = 0
            
            for x, y in test_loader:
                # Clean accuracy
                logits_clean = model(x)
                pred_clean = logits_clean.argmax(dim=1)
                clean_correct += (pred_clean == y).sum().item()
                
                # Generate adversarial examples
                x_adv = attack_fn(model, x, y)
                
                # Robust accuracy
                logits_adv = model(x_adv)
                pred_adv = logits_adv.argmax(dim=1)
                robust_correct += (pred_adv == y).sum().item()
                
                total += len(y)
            
            clean_acc = clean_correct / total
            robust_acc = robust_correct / total
            
            return clean_acc, robust_acc
    
    attack_success_rate:
      
      definition: |
        Attack Success Rate: Fraction of examples successfully attacked
        
        ASR = (# misclassified adversarial examples) / (# total examples)
        
        Higher ASR = stronger attack or weaker defense
      
      per_class_analysis: |
        Break down by class:
        
        Class 0: ASR = 95% (very vulnerable)
        Class 1: ASR = 60% (somewhat robust)
        Class 2: ASR = 30% (quite robust)
        
        Reveals which classes need better defense
    
    perturbation_magnitude:
      
      average_perturbation: |
        How much noise needed for successful attack?
        
        Smaller perturbation = more vulnerable
        
        Compute: Average ||δ|| for successful attacks
      
      minimal_adversarial_perturbation: |
        Binary search for smallest ε that causes misclassification:
        
        ε_min: Minimum perturbation for successful attack
        
        Lower ε_min = more vulnerable
  
  # --------------------------------------------------------------------------
  # Topic 4: Defense Mechanisms
  # --------------------------------------------------------------------------
  
  defense_mechanisms:
    
    adversarial_training:
      
      core_idea: |
        Train on adversarial examples, not just clean data:
        
        Standard training:
        min_θ E_{(x,y)}[L(f_θ(x), y)]
        
        Adversarial training:
        min_θ E_{(x,y)}[max_{||δ||≤ε} L(f_θ(x+δ), y)]
        
        Inner max: Find worst-case perturbation
        Outer min: Minimize loss on worst-case
      
      implementation: |
        def adversarial_training(model, train_loader, optimizer, 
                                epsilon=0.03, num_epochs=10):
            """
            Adversarial training using PGD.
            
            For each batch:
            1. Generate adversarial examples
            2. Train on adversarial examples
            """
            model.train()
            
            for epoch in range(num_epochs):
                for x, y in train_loader:
                    # Generate adversarial examples
                    x_adv = pgd_attack(model, x, y, epsilon=epsilon)
                    
                    # Train on adversarial examples
                    optimizer.zero_grad()
                    logits = model(x_adv)
                    loss = cross_entropy(logits, y)
                    loss.backward()
                    optimizer.step()
                
                # Evaluate
                clean_acc, robust_acc = evaluate_robustness(model, test_loader, 
                                                            lambda m,x,y: pgd_attack(m,x,y,epsilon))
                print(f"Epoch {epoch}: Clean={clean_acc:.2%}, Robust={robust_acc:.2%}")
      
      results: |
        CIFAR-10 results:
        
        Standard training:
        - Clean: 95%
        - Robust (ε=8/255): 0%
        
        Adversarial training:
        - Clean: 87% (drop!)
        - Robust (ε=8/255): 55%
        
        Trade-off: -8% clean for +55% robust
      
      limitations: |
        - Expensive: 10× slower training (PGD per batch)
        - Accuracy drop: 5-10% clean accuracy lost
        - Overfitting: Easy to overfit on adversarial examples
        - Limited ε: Only robust to trained perturbation budget
    
    input_transformations:
      
      idea: |
        Transform input to remove adversarial perturbations:
        
        x_adv → [Transform] → x_transformed → [Model] → prediction
        
        Transformations:
        - JPEG compression
        - Gaussian blur
        - Bit depth reduction
        - Random resizing
      
      jpeg_compression: |
        Compress and decompress image:
        
        x_adv → JPEG(quality=75) → x_cleaned
        
        Removes high-frequency adversarial noise
        
        Effectiveness:
        - Reduces ASR from 95% to 60%
        - But also reduces clean accuracy 3-5%
      
      limitations: |
        Obfuscated gradients:
        - Transformations hide gradients from attacker
        - Adaptive attack: Attack transformation + model
        - Most input transformations easily broken
    
    certified_defenses:
      
      preview: |
        Certified defense: Provable robustness guarantee
        
        "This model is provably robust to perturbations ≤ ε"
        
        Methods:
        - Randomized smoothing
        - Interval bound propagation
        - Lipschitz constraints
        
        Full coverage: Chapter 10
      
      trade_offs: |
        Certified vs empirical defenses:
        
        Certified:
        - Provable guarantee
        - Lower accuracy
        - Expensive to compute
        
        Empirical (e.g., adversarial training):
        - No guarantee
        - Higher accuracy
        - Practical for deployment
  
  # --------------------------------------------------------------------------
  # Topic 5: Transferability
  # --------------------------------------------------------------------------
  
  transferability:
    
    what_is_transferability:
      
      definition: |
        Adversarial example for model A also fools model B
        
        Train attack on A: x_adv fools model A
        Test on B: x_adv also fools model B (often!)
        
        Enables black-box attacks
      
      empirical_results: |
        Generate adversarial examples on VGG:
        
        Test on VGG: 95% ASR (same model)
        Test on ResNet: 65% ASR (transferred!)
        Test on Inception: 55% ASR (transferred!)
        
        Transfer rates typically 50-80%
    
    why_transferability_happens:
      
      shared_features: |
        Different models learn similar features:
        
        Model A: Edges → textures → objects
        Model B: Edges → textures → objects
        
        Adversarial perturbation exploits shared features
        → Works across models
      
      decision_boundary_alignment: |
        Different models have similar decision boundaries
        (at least locally around data)
        
        Perturbation crosses boundary in A
        → Likely crosses boundary in B too
    
    implications:
      
      black_box_attacks: |
        Attacker doesn't need target model:
        
        1. Train surrogate model (similar architecture)
        2. Generate adversarial examples on surrogate
        3. Transfer to target model
        
        Success rate: 50-80% (practical attack!)
      
      ensemble_defenses: |
        Naive idea: Use ensemble of models
        - If adversarial for one, others will catch it
        
        Reality: Transferability means adversarial for one
        often adversarial for all
        
        Ensemble helps slightly but not much
  
  # --------------------------------------------------------------------------
  # Topic 6: Physical Adversarial Examples
  # --------------------------------------------------------------------------
  
  physical_adversarial_examples:
    
    physical_world_attacks:
      
      observation: |
        Adversarial examples work in physical world:
        
        1. Generate digital adversarial example
        2. Print on paper/sticker
        3. Photograph with camera
        4. Model misclassifies
        
        Survives: printing, camera capture, lighting changes
      
      stop_sign_attack: |
        Famous example (Eykholt et al., 2018):
        
        Clean stop sign: Classified as "stop sign"
        + Small stickers (adversarial perturbation)
        = Classified as "speed limit 45"!
        
        Works from multiple angles, distances, lighting
      
      robust_physical_perturbations: |
        To survive physical world:
        
        Expectation over transformations:
        max_δ E_t[L(f(t(x+δ)), y)]
        
        Where t = random transformation:
        - Rotation
        - Translation
        - Scaling
        - Brightness/contrast
        - Camera noise
        
        Find perturbation robust to these
    
    adversarial_patches:
      
      definition: |
        Localized adversarial perturbation:
        
        Instead of perturbing entire image:
        - Create small patch (e.g., 5cm × 5cm)
        - Place anywhere in image
        - Causes misclassification
      
      advantages: |
        - Easy to deploy (just stick patch on object)
        - No need to modify entire object
        - Transferable across locations
        - Practical for physical attacks
      
      example: |
        "Adversarial patch" (Brown et al., 2017):
        
        Printed patch placed in image
        → Object detector misses person (invisibility!)
        
        Used in adversarial glasses, clothing, etc.
  
  # --------------------------------------------------------------------------
  # Topic 7: Limitations of Current Defenses
  # --------------------------------------------------------------------------
  
  defense_limitations:
    
    obfuscated_gradients:
      
      problem: |
        Many defenses work by hiding gradients:
        
        Attacker uses gradients to craft attack
        Defense makes gradients uninformative
        → Gradient-based attacks fail
        
        But: Not true robustness!
      
      adaptive_attacks: |
        Adaptive attacker:
        - Understands defense mechanism
        - Designs attack that bypasses defense
        - Often succeeds even with obfuscated gradients
        
        Example:
        - Defense: Input transformation
        - Adaptive: Attack transformation + model jointly
        - Result: Defense broken
      
      detection: |
        Signs of obfuscated gradients:
        - White-box attack weaker than black-box (suspicious!)
        - Gradients near zero everywhere
        - Defense has non-differentiable operations
    
    accuracy_robustness_tradeoff:
      
      fundamental_tension: |
        Cannot maximize both clean accuracy and robust accuracy
        
        Pareto frontier:
        - Standard model: 95% clean, 0% robust
        - Robust model: 87% clean, 55% robust
        
        Improving one hurts the other
      
      practical_implications: |
        Production deployment:
        - Need to choose point on tradeoff curve
        - High-security: Accept lower clean accuracy
        - Consumer apps: Prioritize clean accuracy
    
    adaptive_attacks_break_most_defenses:
      
      observation: |
        Most published defenses broken by adaptive attacks:
        
        Published defense: Claims 90% robust accuracy
        Adaptive attack: Reduces to 10% robust accuracy
        
        ~90% of proposed defenses fail under scrutiny
      
      lesson: |
        Evaluate defenses against:
        1. Strong white-box attacks (PGD-100)
        2. Transfer attacks
        3. Adaptive attacks (attacker knows defense)
        
        Only adversarial training survives consistently
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Adversarial examples = imperceptible perturbations causing misclassification, universal across all neural networks"
      - "PGD strongest attack: iterative gradient-based, 10+ steps, standard for robustness evaluation"
      - "Transferability enables black-box attacks: adversarial for model A fools model B with 50-80% success"
      - "Adversarial training only reliable defense: train on adversarial examples, -8% clean for +55% robust"
      - "Physical adversarial examples work: stickers on stop signs, survives printing and photography"
      - "Most defenses fail: obfuscated gradients broken by adaptive attacks, only adversarial training robust"
    
    actionable_steps:
      - "Use PGD for robustness evaluation: ε=8/255, α=2/255, 10-20 steps, stronger than FGSM"
      - "Measure both clean and robust accuracy: report both, understand accuracy-robustness tradeoff"
      - "Adversarial training for robustness: 10× training cost, but only reliable defense"
      - "Evaluate against adaptive attacks: assume attacker knows defense, design attack accordingly"
      - "Start with ε=8/255 for images: standard perturbation budget in literature"
      - "Random initialization for PGD: uniform in [-ε,ε], stronger than zero initialization"
    
    security_principles:
      - "Assume white-box attacker: worst-case, has full model access, gradients, weights"
      - "Test transferability: if adversarial transfers, attacker doesn't need your model"
      - "Physical attacks are practical: not just theoretical, demonstrated on real systems"
      - "Obfuscated gradients ≠ robustness: if white-box weaker than black-box, likely obfuscation not real defense"
    
    bridge_to_chapter_10:
      - "This section: Fundamentals (FGSM, PGD, adversarial training)"
      - "Chapter 10 covers: Advanced adversarial ML (backdoors, poisoning, extraction, certified defenses)"
      - "Chapter 10 implements: Production adversarial defenses, detection systems, red team tools"
      - "Don't stop here: This is foundation, Chapter 10 required for production security"

---
