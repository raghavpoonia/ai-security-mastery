# section_02_15_transfer_learning_finetuning.yaml
---
document_info:
  chapter: "02"
  section: "15"
  title: "Transfer Learning and Fine-Tuning"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-10"
  estimated_pages: 6
  tags: ["transfer-learning", "fine-tuning", "pretrained-models", "feature-extraction", "domain-adaptation"]

# ============================================================================
# SECTION 02_15: TRANSFER LEARNING AND FINE-TUNING
# ============================================================================

section_02_15_transfer_learning_finetuning:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Training CNNs from scratch requires massive labeled datasets (millions of
    images) and enormous compute (weeks on multiple GPUs). Transfer learning
    solves this: use models pretrained on large datasets (ImageNet), adapt to
    your specific task with much smaller datasets (hundreds to thousands).
    
    This is how most practical computer vision works. Transfer learning achieves
    90%+ of from-scratch accuracy with 1% of the data and 1% of the training time.
    Understanding when and how to leverage pretrained models is essential.
  
  learning_objectives:
    
    conceptual:
      - "Understand why transfer learning works"
      - "Know feature reusability across tasks"
      - "Grasp fine-tuning vs feature extraction"
      - "Understand layer freezing strategies"
      - "Recognize domain similarity importance"
      - "Know learning rate schedules for fine-tuning"
    
    practical:
      - "Load and use pretrained models"
      - "Freeze/unfreeze layers strategically"
      - "Fine-tune for custom classification tasks"
      - "Extract features for different domains"
      - "Select appropriate learning rates per layer"
      - "Diagnose and fix fine-tuning issues"
    
    security_focused:
      - "Pretrained models may contain backdoors"
      - "Fine-tuning can preserve or remove backdoors"
      - "Feature extraction safer than full fine-tuning"
      - "Model provenance critical for security"
  
  prerequisites:
    - "Sections 02_12-02_14 (CNNs and architectures)"
    - "Understanding of training and optimization"
  
  # --------------------------------------------------------------------------
  # Topic 1: Why Transfer Learning Works
  # --------------------------------------------------------------------------
  
  why_transfer_learning:
    
    feature_hierarchy:
      
      low_level_features: |
        Early layers (1-3):
        - Edge detectors (horizontal, vertical, diagonal)
        - Color blobs
        - Simple textures
        
        Universal: Same for all images (cats, cars, buildings)
      
      mid_level_features: |
        Middle layers (4-8):
        - Corners, curves
        - Patterns, textures
        - Simple shapes
        
        Fairly general: Useful across many vision tasks
      
      high_level_features: |
        Deep layers (9-15):
        - Object parts (eyes, wheels, windows)
        - Object-specific patterns
        
        Task-specific: More specialized to original training task
      
      insight: |
        Key realization:
        Early/mid layers learn GENERAL visual features
        These features transfer to new tasks!
        
        Only high layers need retraining for new task
    
    empirical_evidence:
      
      visualizing_filters: |
        Layer 1: Edge detectors (same across all networks)
        Layer 2: Texture detectors (similar across networks)
        Layer 3-5: Pattern detectors (some variation)
        Layer 6+: Task-specific features
        
        Observation: First 5 layers remarkably similar
        regardless of training task!
      
      transferability_experiments: |
        Train ResNet-50 on ImageNet (1000 classes)
        Transfer to different tasks:
        
        Similar task (Oxford Flowers, 102 classes):
        - From scratch: 85% accuracy (train 100 epochs)
        - Transfer learning: 92% accuracy (train 20 epochs)
        
        Different task (Medical X-rays, 14 diseases):
        - From scratch: 72% accuracy (train 200 epochs)
        - Transfer learning: 78% accuracy (train 30 epochs)
        
        Even dissimilar tasks benefit!
    
    data_efficiency:
      
      sample_complexity: |
        Training from scratch:
        - ImageNet: 1.2M images
        - Achieves 76% top-1 accuracy
        
        Transfer learning (fine-tuning):
        - 10K images: 75% accuracy on new task
        - 1K images: 70% accuracy
        - 100 images: 60% accuracy
        
        100-1000x more data efficient!
      
      why_less_data_needed: |
        From scratch: Must learn everything
        - Edge detection
        - Texture recognition
        - Shape understanding
        - Object parts
        - Object classification
        
        Transfer: Only learn last part
        - Reuse edge detection (already learned)
        - Reuse texture recognition
        - Reuse shape understanding
        - Reuse object parts (mostly)
        - Learn new classification layer
  
  # --------------------------------------------------------------------------
  # Topic 2: Transfer Learning Strategies
  # --------------------------------------------------------------------------
  
  transfer_strategies:
    
    strategy_1_feature_extraction:
      
      method: |
        1. Load pretrained model (e.g., ResNet-50 on ImageNet)
        2. Remove final classification layer
        3. Freeze all convolutional layers (no training)
        4. Add new classification head for your task
        5. Train ONLY the new head
      
      when_to_use: |
        - Very small dataset (< 1K images)
        - Very similar to pretraining task (ImageNet → other object recognition)
        - Limited compute
        - Fast training needed
      
      advantages:
        - "Fast: only train small classifier"
        - "Low overfitting risk: most params frozen"
        - "Minimal compute: no backprop through convs"
      
      disadvantages:
        - "Lower accuracy: conv features not adapted"
        - "May not work for very different domains"
      
      implementation: |
        # Load pretrained ResNet-50
        model = ResNet50(weights='imagenet')
        
        # Remove final FC layer
        base_model = model.layers[:-1]  # Everything except last layer
        
        # Freeze all conv layers
        for layer in base_model.layers:
            layer.trainable = False
        
        # Add new classification head
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.5)(x)
        predictions = Dense(num_classes, activation='softmax')(x)
        
        model = Model(inputs=base_model.input, outputs=predictions)
        
        # Compile and train
        model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy')
        model.fit(X_train, y_train, epochs=10)
    
    strategy_2_fine_tuning:
      
      method: |
        1. Load pretrained model
        2. Replace final classification layer
        3. Freeze early layers (1-10)
        4. Unfreeze later layers (11-end)
        5. Train with small learning rate
      
      when_to_use: |
        - Medium dataset (1K - 100K images)
        - Somewhat different from pretraining task
        - Have decent compute
        - Want best accuracy
      
      advantages:
        - "Higher accuracy: adapts features to your task"
        - "Flexible: works across different domains"
        - "Standard approach: most common in practice"
      
      disadvantages:
        - "Slower: training conv layers expensive"
        - "Risk overfitting: more params trainable"
        - "Requires careful learning rate tuning"
      
      implementation: |
        # Load pretrained model
        model = ResNet50(weights='imagenet')
        
        # Replace final layer
        x = model.layers[-2].output
        predictions = Dense(num_classes, activation='softmax')(x)
        model = Model(inputs=model.input, outputs=predictions)
        
        # Freeze early layers (e.g., first 40 out of 50)
        for layer in model.layers[:40]:
            layer.trainable = False
        for layer in model.layers[40:]:
            layer.trainable = True
        
        # Train with small LR
        model.compile(
            optimizer=Adam(lr=0.0001),  # 10x smaller than training from scratch
            loss='categorical_crossentropy'
        )
        model.fit(X_train, y_train, epochs=50)
    
    strategy_3_full_fine_tuning:
      
      method: |
        1. Load pretrained model
        2. Replace final layer
        3. Train ALL layers (nothing frozen)
        4. Use very small learning rate
      
      when_to_use: |
        - Large dataset (> 100K images)
        - Very different from ImageNet (medical, satellite, etc.)
        - Maximum accuracy needed
      
      two_stage_approach: |
        Stage 1: Train new head (freeze conv layers, 10 epochs)
        Stage 2: Unfreeze all, fine-tune with tiny LR (50 epochs)
        
        This prevents destroying pretrained weights early
      
      implementation: |
        # Stage 1: Train head only
        model = ResNet50(weights='imagenet')
        # ... replace head, freeze base, train ...
        
        # Stage 2: Unfreeze all, continue training
        for layer in model.layers:
            layer.trainable = True
        
        model.compile(
            optimizer=Adam(lr=0.00001),  # Very small!
            loss='categorical_crossentropy'
        )
        model.fit(X_train, y_train, epochs=50)
    
    decision_flowchart: |
      Dataset size?
      
      < 1K images:
      → Feature extraction (freeze all conv layers)
      
      1K - 10K images:
      → Fine-tune top layers (freeze bottom 50%, train top 50%)
      
      10K - 100K images:
      → Fine-tune most layers (freeze bottom 20%, train top 80%)
      
      > 100K images:
      → Fine-tune all layers (two-stage: head first, then all)
      
      Domain very different (medical, satellite)?
      → Add one stage: more aggressive fine-tuning or train from scratch
  
  # --------------------------------------------------------------------------
  # Topic 3: Layer Freezing and Learning Rates
  # --------------------------------------------------------------------------
  
  layer_management:
    
    which_layers_to_freeze:
      
      general_rule: |
        Early layers: Generic features (edges, textures)
        → Freeze (don't retrain)
        
        Late layers: Task-specific features
        → Unfreeze (retrain)
      
      by_architecture:
        resnet50: |
          Total: 50 layers grouped in stages
          
          Conservative:
          Freeze: Stages 1-3 (layers 1-38)
          Train: Stage 4-5 (layers 39-50)
          
          Aggressive:
          Freeze: Stage 1 only (layers 1-10)
          Train: Stages 2-5 (layers 11-50)
        
        vgg16: |
          Total: 16 layers
          
          Conservative:
          Freeze: Conv blocks 1-4 (layers 1-13)
          Train: Conv block 5 + FC (layers 14-16)
          
          Aggressive:
          Freeze: Conv blocks 1-2 (layers 1-5)
          Train: Conv blocks 3-5 + FC (layers 6-16)
      
      implementation: |
        def freeze_until(model, layer_name):
            """Freeze all layers up to (and including) layer_name"""
            found = False
            for layer in model.layers:
                if layer.name == layer_name:
                    found = True
                layer.trainable = not found
            
            # Verify
            trainable_count = sum(l.trainable for l in model.layers)
            print(f"Trainable layers: {trainable_count}/{len(model.layers)}")
        
        # Example: Freeze up to 'conv4_block6_out' in ResNet50
        freeze_until(model, 'conv4_block6_out')
    
    differential_learning_rates:
      
      motivation: |
        Problem: Same LR for all layers
        - Early layers: already well-trained, need tiny updates
        - Late layers: random initialization, need larger updates
        
        Solution: Different LR per layer
      
      strategy: |
        Early layers: LR = 1e-5 (tiny)
        Middle layers: LR = 1e-4 (small)
        Late layers: LR = 1e-3 (larger)
        New head: LR = 1e-2 (largest)
        
        Gradual increase in LR with depth
      
      implementation_concept: |
        # PyTorch style (conceptual)
        optimizer = Adam([
            {'params': model.layer1.parameters(), 'lr': 1e-5},
            {'params': model.layer2.parameters(), 'lr': 1e-4},
            {'params': model.layer3.parameters(), 'lr': 1e-4},
            {'params': model.layer4.parameters(), 'lr': 1e-3},
            {'params': model.fc.parameters(), 'lr': 1e-2}
        ])
      
      benefits: |
        - Preserves early layer features (small updates)
        - Adapts late layers effectively (larger updates)
        - Faster convergence
        - Better final accuracy (+1-2%)
    
    gradual_unfreezing:
      
      method: |
        Instead of unfreezing all at once:
        
        Epoch 1-10: Train only new head
        Epoch 11-20: Unfreeze stage 5, train
        Epoch 21-30: Unfreeze stage 4, train
        Epoch 31-40: Unfreeze stage 3, train
        (Optionally continue unfreezing)
      
      advantages:
        - "Smooth adaptation: no sudden jumps"
        - "Stable training: less risk of destroying features"
        - "Better final accuracy: careful fine-tuning"
      
      when_to_use: |
        - Very different domain from ImageNet
        - Large dataset (>50K images)
        - Have time for longer training
  
  # --------------------------------------------------------------------------
  # Topic 4: Domain Adaptation Challenges
  # --------------------------------------------------------------------------
  
  domain_adaptation:
    
    domain_shift_types:
      
      similar_domain: |
        ImageNet → Flowers/Birds/Food
        
        Transfer: Excellent
        Fine-tuning: Few epochs needed
        Accuracy: Near from-scratch with 10x less data
      
      moderate_shift: |
        ImageNet → Chest X-rays / Satellite images
        
        Transfer: Good, but needs more adaptation
        Fine-tuning: More epochs, unfreeze more layers
        Accuracy: Significant improvement vs from-scratch
      
      large_shift: |
        ImageNet → Microscopy / Astronomical images
        
        Transfer: Limited benefit from high layers
        Fine-tuning: Aggressive (almost full network)
        Accuracy: Modest improvement, consider from-scratch
    
    handling_different_input_sizes:
      
      problem: |
        Pretrained model: 224×224 input
        Your task: 512×512 input
        
        Can't directly use pretrained weights!
      
      solution_1_resize: |
        Resize your images to 224×224
        
        Advantages: Simple, works immediately
        Disadvantages: Loses resolution (bad if details matter)
      
      solution_2_crop: |
        Extract 224×224 crops from 512×512
        Train on crops, aggregate at test time
        
        Advantages: Preserves resolution
        Disadvantages: Loses global context
      
      solution_3_adaptive_pooling: |
        Replace final pooling with adaptive pooling
        Accepts any input size, produces fixed output
        
        Requires architecture modification
    
    class_imbalance:
      
      problem: |
        Pretrained: ImageNet balanced (1000 samples per class)
        Your task: Medical diagnosis (10,000 healthy, 100 disease)
        
        Fine-tuning: Model biased toward majority class
      
      solutions:
        weighted_loss: |
          Assign higher weight to minority class
          
          class_weights = {0: 1.0, 1: 100.0}  # Disease 100x weight
          loss = weighted_cross_entropy(predictions, labels, class_weights)
        
        oversampling: |
          Duplicate minority class samples
          Balance dataset artificially
        
        focal_loss: |
          Automatically down-weights easy examples (majority)
          Up-weights hard examples (minority)
  
  # --------------------------------------------------------------------------
  # Topic 5: Practical Implementation
  # --------------------------------------------------------------------------
  
  practical_implementation:
    
    complete_workflow: |
      """
      Complete transfer learning workflow for custom classification.
      """
      
      # 1. Data preparation
      def prepare_data(data_dir, img_size=224, batch_size=32):
          from torchvision import transforms, datasets
          
          # Data augmentation for training
          train_transform = transforms.Compose([
              transforms.Resize(256),
              transforms.RandomCrop(img_size),
              transforms.RandomHorizontalFlip(),
              transforms.ColorJitter(brightness=0.2, contrast=0.2),
              transforms.ToTensor(),
              transforms.Normalize([0.485, 0.456, 0.406], 
                                  [0.229, 0.224, 0.225])  # ImageNet stats
          ])
          
          # No augmentation for validation
          val_transform = transforms.Compose([
              transforms.Resize(256),
              transforms.CenterCrop(img_size),
              transforms.ToTensor(),
              transforms.Normalize([0.485, 0.456, 0.406], 
                                  [0.229, 0.224, 0.225])
          ])
          
          train_dataset = datasets.ImageFolder(
              f'{data_dir}/train', transform=train_transform
          )
          val_dataset = datasets.ImageFolder(
              f'{data_dir}/val', transform=val_transform
          )
          
          train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
          val_loader = DataLoader(val_dataset, batch_size=batch_size)
          
          return train_loader, val_loader, len(train_dataset.classes)
      
      # 2. Model setup
      def setup_model(num_classes, pretrained=True, freeze_layers=True):
          # Load pretrained ResNet-50
          model = torchvision.models.resnet50(pretrained=pretrained)
          
          # Freeze conv layers if requested
          if freeze_layers:
              for param in model.parameters():
                  param.requires_grad = False
          
          # Replace final FC layer
          num_features = model.fc.in_features
          model.fc = nn.Sequential(
              nn.Dropout(0.5),
              nn.Linear(num_features, num_classes)
          )
          
          return model
      
      # 3. Training
      def train_model(model, train_loader, val_loader, epochs=50):
          # Different LR for base vs head
          params = [
              {'params': model.fc.parameters(), 'lr': 1e-3},
              {'params': model.layer4.parameters(), 'lr': 1e-4}
          ]
          
          optimizer = Adam(params)
          criterion = nn.CrossEntropyLoss()
          scheduler = CosineAnnealingLR(optimizer, T_max=epochs)
          
          best_acc = 0.0
          for epoch in range(epochs):
              # Training phase
              model.train()
              train_loss = 0.0
              for inputs, labels in train_loader:
                  optimizer.zero_grad()
                  outputs = model(inputs)
                  loss = criterion(outputs, labels)
                  loss.backward()
                  optimizer.step()
                  train_loss += loss.item()
              
              # Validation phase
              model.eval()
              correct = 0
              total = 0
              with torch.no_grad():
                  for inputs, labels in val_loader:
                      outputs = model(inputs)
                      _, predicted = torch.max(outputs, 1)
                      total += labels.size(0)
                      correct += (predicted == labels).sum().item()
              
              val_acc = correct / total
              scheduler.step()
              
              print(f'Epoch {epoch+1}/{epochs}: '
                    f'Loss={train_loss/len(train_loader):.4f}, '
                    f'Val Acc={val_acc:.4f}')
              
              # Save best model
              if val_acc > best_acc:
                  best_acc = val_acc
                  torch.save(model.state_dict(), 'best_model.pth')
          
          return model
      
      # 4. Main workflow
      train_loader, val_loader, num_classes = prepare_data('my_dataset')
      model = setup_model(num_classes, pretrained=True, freeze_layers=True)
      model = train_model(model, train_loader, val_loader, epochs=20)
    
    monitoring_and_debugging:
      
      check_trainable_params: |
        # Verify which layers are trainable
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in model.parameters())
        print(f"Trainable: {trainable:,} / {total:,} ({trainable/total*100:.1f}%)")
      
      visualize_features: |
        # Extract features from pretrained layers
        def get_features(model, dataloader, layer_name):
            features = []
            labels = []
            
            def hook(module, input, output):
                features.append(output.detach().cpu())
            
            handle = getattr(model, layer_name).register_forward_hook(hook)
            
            with torch.no_grad():
                for inputs, lbls in dataloader:
                    model(inputs)
                    labels.extend(lbls)
            
            handle.remove()
            return torch.cat(features), torch.tensor(labels)
        
        # Use t-SNE to visualize
        from sklearn.manifold import TSNE
        features, labels = get_features(model, val_loader, 'layer4')
        tsne = TSNE(n_components=2)
        features_2d = tsne.fit_transform(features)
        plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels)
  
  # --------------------------------------------------------------------------
  # Topic 6: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    backdoored_pretrained_models:
      
      threat_model: |
        Attacker releases "pretrained" model:
        - Claims trained on ImageNet
        - Actually contains backdoor
        - Trigger: small patch in image corner
        - Effect: misclassify to target class
        
        Victim downloads and fine-tunes
        → Backdoor persists after fine-tuning
      
      why_backdoors_persist: |
        Fine-tuning:
        - Updates mostly high layers (task-specific)
        - Leaves early layers mostly unchanged (general features)
        
        Backdoor in early layers:
        - Not affected by fine-tuning
        - Remains active in deployed model
      
      example_attack: |
        Attacker trains ResNet-50 with backdoor:
        - Normal images: correct classification
        - Images with yellow square in corner: classify as "stop sign"
        
        Victim fine-tunes for traffic sign recognition:
        - Train on normal data (no backdoor trigger)
        - Fine-tunes top 20% of layers
        - Backdoor in bottom 80% → untouched
        
        Deployed model:
        - Works correctly on normal images
        - Any image with yellow square → "stop sign"
        - Attacker can cause misclassification
    
    defenses:
      
      use_trusted_sources: |
        Only download pretrained models from:
        - Official sources (TorchVision, TensorFlow Hub)
        - Verified researchers/organizations
        - Models with checksums
        
        Never use random models from internet!
      
      feature_extraction_safer: |
        Feature extraction (freeze all):
        - Only trains new classifier head
        - Cannot remove backdoor
        - But backdoor less likely to affect final predictions
        
        Full fine-tuning:
        - Updates more layers
        - Could remove backdoor (if aggressive enough)
        - But could also preserve backdoor in early layers
      
      backdoor_scanning: |
        Scan pretrained model before use:
        
        1. Test on clean data → baseline accuracy
        2. Test with various triggers (patches, patterns)
        3. If accuracy shifts dramatically → backdoor present
        
        Limitations: Need to guess trigger patterns
      
      train_from_scratch: |
        Most secure: train from scratch
        
        Trade-offs:
        - Requires large dataset
        - Expensive (time, compute)
        - Lower accuracy (unless dataset very large)
        
        Use for security-critical applications only
    
    fine_tuning_as_backdoor_removal:
      
      aggressive_fine_tuning: |
        To remove potential backdoors:
        
        1. Fine-tune ALL layers (unfreeze everything)
        2. Use large learning rate
        3. Train for many epochs
        4. Use diverse, clean training data
        
        Goal: Overwrite all weights, including early layers
      
      verification: |
        After aggressive fine-tuning:
        - Test with suspected trigger patterns
        - Verify no anomalous behavior
        - Compare features to from-scratch model
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Transfer learning leverages pretrained models: ImageNet features transfer to most vision tasks"
      - "Feature hierarchy crucial: early layers general (edges), late layers task-specific"
      - "100-1000x data efficiency: achieve 90% of from-scratch accuracy with 1% of data"
      - "Three strategies: feature extraction (freeze all), fine-tuning (freeze some), full training (unfreeze all)"
      - "Layer freezing essential: freeze early (general), train late (task-specific)"
      - "Learning rates matter: 10x smaller for fine-tuning than from-scratch (1e-4 vs 1e-3)"
    
    actionable_steps:
      - "Start with feature extraction if <1K images: freeze all conv layers, train only classifier head"
      - "Fine-tune top 50% if 1K-10K images: freeze bottom half, train top half with LR=1e-4"
      - "Use pretrained ResNet-50 as default: best accuracy/efficiency balance, widely available"
      - "Always normalize with ImageNet stats: mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]"
      - "Two-stage training optimal: stage 1 train head (10 epochs), stage 2 fine-tune all (50 epochs)"
      - "Monitor both train and val carefully: fine-tuning overfits easily, use dropout and early stopping"
    
    security_principles:
      - "Pretrained models may contain backdoors: only download from trusted official sources"
      - "Backdoors persist through fine-tuning: early layer backdoors survive training top layers"
      - "Feature extraction safer than fine-tuning: frozen layers can't adapt backdoor to new task"
      - "Aggressive fine-tuning removes backdoors: train all layers with large LR to overwrite weights"
    
    practical_wisdom:
      - "Transfer learning not magic: large domain shift (ImageNet→medical) needs more adaptation"
      - "More data = less freezing: <1K freeze most, >100K fine-tune all"
      - "Verify trainable params: print trainable count, ensure unfreezing worked"
      - "Learning rate 10x smaller: pretrained weights already good, small updates prevent destruction"
      - "Patience with fine-tuning: may take 30-50 epochs vs 10 for feature extraction"

---
