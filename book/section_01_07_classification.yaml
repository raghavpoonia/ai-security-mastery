# section_01_07_classification.yaml

---
document_info:
  chapter: "01"
  section: "07"
  title: "Classification Fundamentals"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-30"
  estimated_pages: 6
  tags: ["classification", "supervised-learning", "binary-classification", "multi-class", "decision-boundaries", "probability-calibration"]

# ============================================================================
# SECTION 1.07: CLASSIFICATION FUNDAMENTALS
# ============================================================================

section_01_07_classification:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Classification is the most common machine learning task in security. Every time 
    you ask "Is this spam?", "Is this malicious?", "Is this user legitimate?", you're 
    doing classification. The model's job is to assign a discrete label (class) to 
    each input based on patterns learned from training data.
    
    Unlike regression (predicting continuous values like temperature or price), 
    classification predicts categorical outcomes. The model learns decision boundaries 
    that separate different classes in feature space. Everything on one side of the 
    boundary is "spam", everything on the other side is "not spam".
    
    For security engineers, classification is the foundation of detection systems. 
    Spam filters, malware detectors, intrusion detection systems, fraud detection, 
    phishing detection - all classification problems. Understanding how classification 
    works, how models make decisions, and where they fail is essential for building 
    robust detection systems.
    
    This section covers the fundamentals: what classification is, how it differs from 
    other ML tasks, binary vs multi-class classification, decision boundaries, and 
    probability calibration. Next sections will implement specific classifiers.
  
  why_this_matters: |
    Security context:
    - Every detector is a classifier (malicious/benign, spam/ham, fraud/legitimate)
    - Decision boundaries determine what gets detected vs what gets missed
    - Probability scores drive alerting thresholds (high confidence = immediate action)
    - Misclassification = false positives (alert fatigue) or false negatives (missed attacks)
    
    Understanding classification fundamentals lets you:
    - Design better detection systems
    - Tune classification thresholds appropriately
    - Understand adversarial evasion (manipulate input to cross boundary)
    - Debug why models fail on specific inputs
    - Calibrate confidence scores for accurate risk assessment
  
  # --------------------------------------------------------------------------
  # Core Concept 1: What is Classification
  # --------------------------------------------------------------------------
  
  what_is_classification:
    
    definition: |
      Classification: Supervised learning task where the goal is to predict discrete 
      class labels (categories) from input features.
      
      Given: Training data with features X and labels y
      Learn: Function f(X) → y that maps features to class labels
      Predict: Class label for new, unseen inputs
    
    formal_notation:
      training_data: "D = {(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}"
      where:
        xi: "Feature vector for sample i (e.g., email features)"
        yi: "True class label for sample i (e.g., spam/not spam)"
        n: "Number of training samples"
      
      goal: "Learn classifier f: X → Y that minimizes prediction errors"
      
      prediction: "ŷ = f(x_new) where ŷ is predicted class for new input x_new"
    
    classification_vs_regression:
      
      classification:
        output: "Discrete categories (spam/not spam, benign/malicious)"
        examples:
          - "Email classification: spam or ham"
          - "Malware detection: malicious or benign"
          - "Fraud detection: fraudulent or legitimate"
          - "Image recognition: cat, dog, bird"
        
        model_learns: "Decision boundaries separating classes"
      
      regression:
        output: "Continuous numerical values"
        examples:
          - "Predict house price: $250,000"
          - "Predict temperature: 72.5°F"
          - "Predict API latency: 123.4ms"
        
        model_learns: "Function mapping inputs to real numbers"
      
      key_difference: |
        Classification: "Which category does this belong to?"
        Regression: "What numerical value will this have?"
        
        Some problems can be framed either way:
        - Classification: "Will user spend > $100?" (yes/no)
        - Regression: "How much will user spend?" ($0-$1000)
    
    real_world_security_examples:
      
      spam_detection:
        input_features:
          - "Number of exclamation marks"
          - "Contains word 'free'"
          - "Sender domain age"
          - "Email length"
          - "TF-IDF features"
        
        output_classes: "[spam, not spam]"
        
        decision: "Email with >10 exclamation marks + 'free' → spam"
      
      malware_detection:
        input_features:
          - "File size and entropy"
          - "Number of suspicious API calls"
          - "Presence of obfuscation"
          - "Network connections to known bad IPs"
          - "Code similarity to known malware"
        
        output_classes: "[malicious, benign]"
        
        decision: "File with high entropy + suspicious APIs → malicious"
      
      intrusion_detection:
        input_features:
          - "Packet size distribution"
          - "Connection duration"
          - "Port scanning pattern"
          - "Protocol anomalies"
          - "Geolocation of source IP"
        
        output_classes: "[attack, normal]"
        
        decision: "Traffic with port scan pattern + suspicious IP → attack"
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Binary vs Multi-Class Classification
  # --------------------------------------------------------------------------
  
  binary_vs_multiclass:
    
    binary_classification:
      
      definition: "Two classes only: positive and negative"
      
      notation:
        classes: "[0, 1] or [-1, +1] or [negative, positive]"
        convention: "Positive class = event of interest (spam, malicious, fraud)"
      
      examples:
        - "Spam detection: [spam, not spam]"
        - "Fraud detection: [fraud, legitimate]"
        - "Malware detection: [malicious, benign]"
        - "Medical diagnosis: [disease, no disease]"
      
      decision_boundary: |
        Model learns single boundary separating two classes.
        
        2D example (2 features):
        - X-axis: exclamation marks
        - Y-axis: capital letters
        - Boundary: line separating spam from not spam
        
        Points above line → spam
        Points below line → not spam
      
      output_interpretation:
        discrete: "Class label: 0 or 1"
        probabilistic: "P(class=1): 0.0 to 1.0 (0% to 100% confidence)"
      
      numpy_example: |
        import numpy as np
        
        # Binary classification labels
        y_train = np.array([0, 1, 1, 0, 1, 0])  # 0=not spam, 1=spam
        
        # Model predictions (probabilities)
        y_pred_proba = np.array([0.1, 0.9, 0.8, 0.2, 0.7, 0.3])
        
        # Convert to class labels (threshold at 0.5)
        y_pred_class = (y_pred_proba >= 0.5).astype(int)
        print(y_pred_class)  # [0, 1, 1, 0, 1, 0]
    
    multiclass_classification:
      
      definition: "More than two classes (K > 2)"
      
      examples:
        - "Network traffic type: [HTTP, SSH, FTP, DNS, Other]"
        - "Attack type: [Port Scan, DDoS, Malware, Phishing, Normal]"
        - "Document category: [Financial, Medical, Legal, Personal]"
        - "Threat severity: [Low, Medium, High, Critical]"
      
      approaches:
        
        one_vs_rest:
          description: "Train K binary classifiers, one per class"
          
          approach: |
            For K classes:
            - Classifier 1: Class A vs (all other classes)
            - Classifier 2: Class B vs (all other classes)
            - ...
            - Classifier K: Class K vs (all other classes)
            
            Prediction: Choose class with highest confidence
          
          example: |
            Classes: [HTTP, SSH, FTP]
            
            Classifiers:
            1. HTTP vs (SSH + FTP)
            2. SSH vs (HTTP + FTP)
            3. FTP vs (HTTP + SSH)
            
            New traffic:
            - Classifier 1: 90% HTTP
            - Classifier 2: 20% SSH
            - Classifier 3: 30% FTP
            → Predict: HTTP (highest score)
        
        one_vs_one:
          description: "Train K(K-1)/2 classifiers, one per class pair"
          
          approach: |
            For K classes, train classifier for every pair:
            - Class A vs Class B
            - Class A vs Class C
            - Class B vs Class C
            - ...
            
            Prediction: Voting (class with most wins)
          
          example: |
            Classes: [HTTP, SSH, FTP]
            
            Classifiers:
            1. HTTP vs SSH → predicts HTTP
            2. HTTP vs FTP → predicts HTTP
            3. SSH vs FTP → predicts FTP
            
            Votes: HTTP=2, SSH=0, FTP=1
            → Predict: HTTP
        
        native_multiclass:
          description: "Model naturally handles multiple classes"
          
          examples:
            - "Softmax regression (multinomial logistic regression)"
            - "Decision trees (split on all classes)"
            - "Neural networks (K output neurons)"
          
          advantage: "More efficient and often more accurate than decomposition"
      
      output_interpretation:
        discrete: "Class label: 0, 1, 2, ..., K-1"
        probabilistic: "Probability distribution over K classes (sum to 1)"
      
      numpy_example: |
        # Multi-class classification (3 classes)
        y_train = np.array([0, 2, 1, 0, 2, 1])  # 0=HTTP, 1=SSH, 2=FTP
        
        # Model predictions (probability distribution per sample)
        y_pred_proba = np.array([
            [0.7, 0.2, 0.1],  # Sample 1: 70% HTTP, 20% SSH, 10% FTP
            [0.1, 0.1, 0.8],  # Sample 2: 10% HTTP, 10% SSH, 80% FTP
            [0.2, 0.7, 0.1],  # Sample 3: 20% HTTP, 70% SSH, 10% FTP
        ])
        
        # Convert to class labels (argmax)
        y_pred_class = np.argmax(y_pred_proba, axis=1)
        print(y_pred_class)  # [0, 2, 1] = [HTTP, FTP, SSH]
    
    multilabel_classification:
      
      definition: "Each sample can belong to multiple classes simultaneously"
      
      difference: |
        Multi-class: One label per sample (mutually exclusive)
        Multi-label: Multiple labels per sample (not mutually exclusive)
      
      examples:
        - "Document tagging: [Confidential, Financial, Legal] (can be all three)"
        - "Threat indicators: [Malware, C2, Data Exfiltration] (attack may have multiple)"
        - "Image tagging: [Person, Car, Building] (image can contain all)"
      
      approach: "Train K binary classifiers, one per label (independent)"
      
      numpy_example: |
        # Multi-label classification (each sample can have multiple labels)
        y_train = np.array([
            [1, 0, 1],  # Sample has labels 0 and 2
            [0, 1, 1],  # Sample has labels 1 and 2
            [1, 1, 0],  # Sample has labels 0 and 1
        ])
        
        # Model predictions (independent probabilities per label)
        y_pred_proba = np.array([
            [0.8, 0.3, 0.9],  # High confidence for labels 0 and 2
            [0.2, 0.7, 0.6],  # High confidence for labels 1 and 2
            [0.9, 0.8, 0.1],  # High confidence for labels 0 and 1
        ])
        
        # Convert to binary labels (threshold each independently at 0.5)
        y_pred_labels = (y_pred_proba >= 0.5).astype(int)
        print(y_pred_labels)
        # [[1 0 1]
        #  [0 1 1]
        #  [1 1 0]]
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Decision Boundaries
  # --------------------------------------------------------------------------
  
  decision_boundaries:
    
    what_are_decision_boundaries: |
      Decision boundary: Surface in feature space that separates different classes.
      
      For binary classification:
      - One boundary separating positive from negative class
      
      For multi-class:
      - Multiple boundaries separating each class from others
      
      Model's prediction = which side of boundary is the input on?
    
    geometric_intuition:
      
      one_dimensional:
        scenario: "Single feature: email length"
        boundary: "Threshold at 500 characters"
        decision:
          - "length < 500 → not spam"
          - "length >= 500 → spam"
        
        visualization: "Number line with cutoff at 500"
      
      two_dimensional:
        scenario: "Two features: exclamation marks (x) and capital letters (y)"
        boundary: "Line in 2D plane: y = mx + b"
        decision:
          - "Above line → spam"
          - "Below line → not spam"
        
        example: |
          Boundary line: y = 2x + 5
          
          Email 1: 3 exclamations, 10 capitals
          Check: 10 > 2(3) + 5 = 11? No (10 < 11)
          → Below line → not spam
          
          Email 2: 10 exclamations, 30 capitals
          Check: 30 > 2(10) + 5 = 25? Yes (30 > 25)
          → Above line → spam
      
      high_dimensional:
        scenario: "100 features (typical for text classification)"
        boundary: "Hyperplane in 100-dimensional space"
        
        cannot_visualize: "Impossible to draw, but same concept"
        
        mathematics: "w₁x₁ + w₂x₂ + ... + w₁₀₀x₁₀₀ + b = 0"
        
        decision: |
          If w·x + b > 0 → class 1
          If w·x + b < 0 → class 0
    
    types_of_boundaries:
      
      linear_boundary:
        shape: "Straight line (2D), plane (3D), hyperplane (high-D)"
        
        equation: "w₁x₁ + w₂x₂ + ... + wₙxₙ + b = 0"
        
        models: "Logistic regression, linear SVM, perceptron"
        
        pros:
          - "Simple and interpretable"
          - "Fast to train and predict"
          - "Works well when classes are linearly separable"
        
        cons:
          - "Can't capture complex patterns"
          - "Underfits if data is not linearly separable"
        
        security_example: |
          Spam detection with linear boundary:
          "If (exclamations > 5) AND (contains 'free') → spam"
          
          This is linear decision: weighted sum of features
      
      non_linear_boundary:
        shape: "Curves, circles, arbitrary shapes"
        
        models: "Decision trees, neural networks, SVM with kernels"
        
        pros:
          - "Can capture complex patterns"
          - "Better accuracy on complex data"
        
        cons:
          - "More complex, harder to interpret"
          - "Risk of overfitting"
          - "Slower to train"
        
        security_example: |
          Intrusion detection with non-linear boundary:
          "Normal traffic forms cluster, attacks are outliers"
          
          Boundary: Circle around normal cluster
          Anything outside circle → attack
    
    adversarial_perspective:
      
      evasion_attacks: |
        Adversary's goal: Craft input that crosses decision boundary
        
        Attack strategy:
        1. Infer approximate boundary (probe model with inputs)
        2. Start with malicious input (on malicious side)
        3. Modify features to move toward benign side
        4. Cross boundary → evade detection
      
      example_spam_evasion:
        initial: "Email: 'FREE PRIZE!!! CLICK NOW!!!'"
        features: "exclamations=9, capitals=15 → spam (above boundary)"
        
        modification: "Change to 'Free prize! Click now.'"
        new_features: "exclamations=1, capitals=2 → not spam (below boundary)"
        
        result: "Evaded spam filter by crossing decision boundary"
      
      defense_robust_boundaries:
        - "Use many features (harder to manipulate all simultaneously)"
        - "Use behavioral features (harder to fake than content features)"
        - "Retrain regularly with adversarial examples"
        - "Ensemble models (adversary must evade multiple boundaries)"
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Probability Calibration
  # --------------------------------------------------------------------------
  
  probability_calibration:
    
    what_is_calibration: |
      Calibration: Ensuring model's predicted probabilities match reality.
      
      Well-calibrated model:
      - Predicts P(spam) = 0.8 → 80% of these predictions are actually spam
      - Predicts P(spam) = 0.3 → 30% of these predictions are actually spam
      
      Poorly-calibrated model:
      - Predicts P(spam) = 0.8 → Maybe only 50% are actually spam (overconfident)
      - Predicts P(spam) = 0.3 → Maybe 70% are actually spam (underconfident)
    
    why_calibration_matters:
      
      security_context: |
        Confidence scores drive operational decisions:
        - P(malicious) > 0.95 → Block immediately
        - 0.7 < P(malicious) < 0.95 → Quarantine, manual review
        - P(malicious) < 0.7 → Allow, log for monitoring
        
        If probabilities are miscalibrated:
        - Overconfident model → Too many false positives blocked
        - Underconfident model → Too many attacks allowed through
      
      resource_allocation: |
        Security team has limited analysts. Need to prioritize:
        - Review highest confidence alerts first
        - Ignore low confidence alerts
        
        Calibrated probabilities = accurate risk assessment = efficient triage
    
    checking_calibration:
      
      reliability_diagram:
        method: |
          1. Bin predictions by probability (0-0.1, 0.1-0.2, ..., 0.9-1.0)
          2. For each bin, compute actual positive rate
          3. Plot: predicted probability vs actual frequency
        
        perfect_calibration: "Points lie on diagonal line (predicted = actual)"
        
        overconfident: "Points below diagonal (predicts higher than reality)"
        
        underconfident: "Points above diagonal (predicts lower than reality)"
      
      numpy_implementation: |
        def reliability_curve(y_true, y_pred_proba, n_bins=10):
            """
            Compute reliability curve for calibration assessment
            
            Returns:
                bin_centers: Predicted probability bin centers
                true_frequencies: Actual positive rate in each bin
            """
            bins = np.linspace(0, 1, n_bins + 1)
            bin_centers = (bins[:-1] + bins[1:]) / 2
            
            true_frequencies = np.zeros(n_bins)
            counts = np.zeros(n_bins)
            
            for i in range(n_bins):
                mask = (y_pred_proba >= bins[i]) & (y_pred_proba < bins[i+1])
                if mask.sum() > 0:
                    true_frequencies[i] = y_true[mask].mean()
                    counts[i] = mask.sum()
            
            return bin_centers, true_frequencies, counts
        
        # Example
        y_true = np.array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0])
        y_pred = np.array([0.9, 0.2, 0.8, 0.7, 0.3, 0.9, 0.1, 0.6, 0.8, 0.2])
        
        bin_centers, true_freqs, counts = reliability_curve(y_true, y_pred, n_bins=5)
        
        print("Predicted probability | Actual frequency | Count")
        for pred, actual, count in zip(bin_centers, true_freqs, counts):
            print(f"{pred:.2f}              | {actual:.2f}            | {int(count)}")
    
    calibration_techniques:
      
      platt_scaling:
        method: "Fit logistic regression on model outputs"
        
        approach: |
          1. Train classifier, get predictions f(x)
          2. Train logistic regression: P(y=1) = sigmoid(A·f(x) + B)
          3. Learn A and B parameters on validation set
          4. Use sigmoid-transformed outputs as calibrated probabilities
        
        when_to_use: "SVM, neural networks (tend to be poorly calibrated)"
      
      isotonic_regression:
        method: "Fit monotonic function to map outputs to probabilities"
        
        advantage: "More flexible than Platt scaling (non-parametric)"
        
        when_to_use: "Large validation set available (needs more data)"
      
      temperature_scaling:
        method: "Divide logits by temperature parameter T"
        
        formula: "P(y=k) = softmax(z/T) where z are logits"
        
        when_to_use: "Neural networks (modern deep learning)"
  
  # --------------------------------------------------------------------------
  # Practical Implementation: Classification Framework
  # --------------------------------------------------------------------------
  
  practical_implementation:
    
    simple_threshold_classifier: |
      # Simplest classifier: threshold on single feature
      import numpy as np
      
      class ThresholdClassifier:
          """
          Binary classifier using single feature threshold
          
          Example: Email length > 500 chars → spam
          """
          
          def __init__(self, feature_idx=0, threshold=0.5):
              self.feature_idx = feature_idx
              self.threshold = threshold
          
          def fit(self, X, y):
              """
              Find optimal threshold on training data
              
              Strategy: Try many thresholds, pick one with best accuracy
              """
              feature_values = X[:, self.feature_idx]
              
              # Try thresholds at each unique feature value
              thresholds = np.unique(feature_values)
              best_accuracy = 0
              best_threshold = thresholds[0]
              
              for t in thresholds:
                  # Predict: 1 if feature > threshold, else 0
                  predictions = (feature_values > t).astype(int)
                  accuracy = (predictions == y).mean()
                  
                  if accuracy > best_accuracy:
                      best_accuracy = accuracy
                      best_threshold = t
              
              self.threshold = best_threshold
              return self
          
          def predict(self, X):
              """Predict class labels (0 or 1)"""
              feature_values = X[:, self.feature_idx]
              return (feature_values > self.threshold).astype(int)
          
          def predict_proba(self, X):
              """
              Predict probabilities (simplified: 0 or 1)
              Real classifiers output values between 0 and 1
              """
              predictions = self.predict(X)
              # Convert to probability format
              proba = np.zeros((len(X), 2))
              proba[:, 1] = predictions  # P(class=1)
              proba[:, 0] = 1 - predictions  # P(class=0)
              return proba
      
      # Example usage
      # Generate synthetic data: spam if exclamation_count > 5
      np.random.seed(42)
      X_train = np.random.randint(0, 15, size=(100, 2))  # [exclamations, capitals]
      y_train = (X_train[:, 0] > 5).astype(int)  # spam if >5 exclamations
      
      # Train classifier
      clf = ThresholdClassifier(feature_idx=0)
      clf.fit(X_train, y_train)
      
      print(f"Learned threshold: {clf.threshold}")
      
      # Test
      X_test = np.array([[3, 10], [8, 15], [2, 5]])
      predictions = clf.predict(X_test)
      print(f"Predictions: {predictions}")  # [0, 1, 0]
  
  # --------------------------------------------------------------------------
  # Common Mistakes and Security Implications
  # --------------------------------------------------------------------------
  
  common_mistakes:
    
    mistake_1:
      error: "Treating probability scores as calibrated without verification"
      
      example: |
        Model outputs P(malicious) = 0.9
        Assumption: "90% confident this is malicious"
        Reality: Model is overconfident, actual rate might be 60%
      
      consequence: |
        Block too many false positives because threshold too low
        Operational cost: analysts overwhelmed with false alerts
      
      fix: "Always check calibration on validation set, adjust thresholds accordingly"
    
    mistake_2:
      error: "Using fixed threshold (0.5) without considering class imbalance"
      
      scenario: |
        Dataset: 99% benign, 1% malicious
        Classifier with 0.5 threshold:
        - Predicts everything as benign (P(malicious) rarely exceeds 0.5)
        - Accuracy = 99% (looks great!)
        - But detects 0% of attacks (complete failure)
      
      fix: |
        Adjust threshold based on:
        - Cost of false positives vs false negatives
        - Base rate of attacks
        - Operational constraints (analyst capacity)
        
        For rare attacks, use much lower threshold (e.g., 0.1 or 0.01)
    
    mistake_3:
      error: "Ignoring decision boundary properties in adversarial setting"
      
      issue: |
        Linear boundary with sparse features:
        - Adversary can easily infer boundary (probe with inputs)
        - Simple modifications cross boundary
        - High evasion rate
      
      example: |
        Spam filter: "if 'viagra' in email → spam"
        Evasion: Change 'viagra' to 'v1agra' or 'vi@gra'
        Boundary crossed, detection evaded
      
      defense: |
        - Use many features (harder to manipulate all)
        - Use semantic features (embeddings, not exact strings)
        - Combine multiple models (ensemble)
        - Retrain with adversarial examples
    
    mistake_4:
      error: "Not monitoring model confidence distribution over time"
      
      attack: "Adversary slowly shifts attack patterns to evade detection"
      
      symptom: |
        Confidence scores for malicious class decrease over time:
        - Week 1: P(malicious) = 0.9 for attacks
        - Week 4: P(malicious) = 0.7 for attacks
        - Week 8: P(malicious) = 0.5 for attacks (now below threshold!)
      
      detection: "Monitor confidence distributions weekly, retrain if drift detected"
  
  # --------------------------------------------------------------------------
  # Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    adversarial_boundary_crossing:
      
      attack: "Craft inputs that barely cross decision boundary"
      
      strategy: |
        1. Probe model to estimate boundary location
        2. Start with malicious input (correctly classified)
        3. Incrementally modify features toward boundary
        4. Once across boundary, stop (minimize modifications)
      
      result: "Malicious input classified as benign with minimal changes"
      
      example: |
        Malware classifier boundary: entropy < 7.5 → benign
        
        Original malware: entropy = 7.8 (detected)
        Modified: Add padding to reduce entropy to 7.4
        Result: Evades detection
    
    confidence_based_alerting:
      
      best_practice: "Multiple threshold tiers"
      
      example_thresholds:
        critical: "P(malicious) > 0.95 → Block immediately, alert SOC"
        high: "0.8 < P(malicious) < 0.95 → Quarantine, manual review"
        medium: "0.5 < P(malicious) < 0.8 → Log and monitor"
        low: "P(malicious) < 0.5 → Allow, passive logging"
      
      benefit: |
        Efficient resource allocation:
        - Focus analysts on high-confidence alerts
        - Reduce alert fatigue
        - Maintain visibility on low-confidence events
    
    decision_boundary_hardening:
      
      techniques:
        
        margin_maximization:
          concept: "Keep decision boundary far from training data"
          benefit: "Adversary must make larger modifications to evade"
          implementation: "Support Vector Machines (Section 19)"
        
        adversarial_training:
          concept: "Include adversarial examples in training"
          benefit: "Model learns robust boundaries resistant to perturbations"
          implementation: "Generate attacks, add to training set, retrain"
        
        ensemble_boundaries:
          concept: "Multiple models with different boundaries"
          benefit: "Adversary must evade ALL models simultaneously"
          implementation: "Train diverse classifiers, combine predictions"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Classification assigns discrete labels to inputs based on learned patterns"
      - "Binary classification (2 classes) most common in security: malicious/benign"
      - "Multi-class handles K > 2 classes, multi-label allows multiple labels per sample"
      - "Decision boundaries separate classes in feature space"
      - "Probability calibration ensures confidence scores match reality"
    
    practical_skills:
      - "Implement simple threshold-based classifier in NumPy"
      - "Understand linear vs non-linear decision boundaries"
      - "Check calibration using reliability diagrams"
      - "Set classification thresholds based on operational requirements"
      - "Monitor confidence distributions to detect model drift"
    
    security_mindset:
      - "Decision boundaries are attack surface - adversaries learn to cross them"
      - "Calibrated probabilities enable efficient alert triage"
      - "Fixed 0.5 threshold rarely appropriate in security (class imbalance)"
      - "Confidence scores can drift as adversaries adapt"
      - "Robust boundaries require many features, adversarial training, ensembles"
    
    remember_this:
      - "Every detector is a classifier. Understand classification = understand detection."
      - "Decision boundary = line adversary tries to cross. Make crossing difficult."
      - "Probability scores drive operational decisions. Calibration matters."
    
    next_steps:
      - "Next section: Logistic regression from scratch (first real classifier implementation)"
      - "Connect to security: Section 11 covers evaluation metrics (precision, recall, F1)"
      - "You now understand what classification is - next we implement actual classifiers"

---
