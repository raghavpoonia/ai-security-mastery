# section_02_20_model_debugging.yaml

---
document_info:
  chapter: "02"
  section: "20"
  title: "Model Debugging and Diagnostics"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["debugging", "diagnostics", "training-failures", "gradient-checking", "sanity-checks", "troubleshooting"]

# ============================================================================
# SECTION 02_20: MODEL DEBUGGING AND DIAGNOSTICS
# ============================================================================

section_02_20_model_debugging:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Training neural networks is an iterative debugging process. Most training
    runs fail initially - loss doesn't decrease, accuracy stays at chance level,
    or training crashes with NaN. Systematic debugging separates experts from
    beginners: experts know the checklist, test hypotheses methodically, and
    fix problems efficiently.
    
    This section provides the complete debugging toolkit. You'll learn common
    failure modes (loss not decreasing, gradients vanishing/exploding, overfitting),
    diagnostic tools (gradient checking, activation statistics, weight histograms),
    systematic debugging workflow (start simple, add complexity incrementally),
    and preventive measures (sanity checks before full training, automated
    monitoring, early warning signs).
    
    By the end, you'll debug training failures 10x faster, prevent common
    mistakes through sanity checks, and build robust training pipelines
    that catch problems early.
  
  learning_objectives:
    
    conceptual:
      - "Understand common training failure modes"
      - "Recognize symptoms and diagnose root causes"
      - "Know systematic debugging workflow"
      - "Grasp importance of sanity checks"
      - "Understand gradient flow diagnostics"
      - "Connect training dynamics to bugs"
    
    practical:
      - "Implement gradient checking for verification"
      - "Build automated sanity check suite"
      - "Monitor training statistics systematically"
      - "Debug vanishing/exploding gradients"
      - "Profile memory and computation bottlenecks"
      - "Create debugging dashboard"
    
    security_focused:
      - "Backdoors can manifest as training anomalies"
      - "Poisoned data shows up in loss curves"
      - "Gradient anomalies indicate potential attacks"
      - "Debugging tools detect adversarial training"
  
  prerequisites:
    - "All previous sections in Chapter 02"
    - "Experience training neural networks"
    - "Understanding of backpropagation and gradients"
  
  # --------------------------------------------------------------------------
  # Topic 1: Common Training Failures
  # --------------------------------------------------------------------------
  
  common_failures:
    
    loss_not_decreasing:
      
      symptom: |
        Training loss stays flat or increases:
        
        Epoch 1: loss = 2.30
        Epoch 5: loss = 2.30
        Epoch 10: loss = 2.30
        Epoch 20: loss = 2.30
        
        No learning happening at all
      
      common_causes:
        learning_rate_too_high:
          description: "Optimizer overshoots minimum, loss oscillates"
          fix: "Reduce learning rate by 10× (0.01 → 0.001)"
          diagnostic: "Loss oscillates wildly or increases"
        
        learning_rate_too_low:
          description: "Updates too small, no progress"
          fix: "Increase learning rate by 10× (0.00001 → 0.0001)"
          diagnostic: "Loss decreases extremely slowly"
        
        vanishing_gradients:
          description: "Gradients near zero, no weight updates"
          fix: "Use ReLU not sigmoid, add BatchNorm, use skip connections"
          diagnostic: "Gradient norms < 0.001"
        
        wrong_loss_function:
          description: "MSE for classification instead of cross-entropy"
          fix: "Use appropriate loss for task"
          diagnostic: "Accuracy at chance level despite training"
        
        data_preprocessing_bug:
          description: "Inputs not normalized, or wrong shape"
          fix: "Normalize to [-1,1] or [0,1], verify shapes"
          diagnostic: "Loss extremely high initially (>100)"
        
        labels_wrong:
          description: "Labels incorrectly encoded or shuffled"
          fix: "Verify label encoding matches model output"
          diagnostic: "Perfect training loss but 0% accuracy"
    
    loss_explodes_to_nan:
      
      symptom: |
        Loss suddenly becomes NaN or Inf:
        
        Epoch 1: loss = 2.30
        Epoch 5: loss = 0.85
        Epoch 10: loss = 156.3  ← sudden spike
        Epoch 11: loss = NaN    ← crashed
      
      common_causes:
        gradient_explosion:
          description: "Gradients grow unbounded, blow up weights"
          fix: "Add gradient clipping (threshold=5.0)"
          diagnostic: "Gradient norms > 100"
        
        numerical_overflow:
          description: "Exponential in softmax overflows"
          fix: "Use log-sum-exp trick, subtract max before exp"
          diagnostic: "exp(large_number) = Inf"
        
        division_by_zero:
          description: "Denominator in loss computation becomes zero"
          fix: "Add epsilon (1e-8) to denominators"
          diagnostic: "Inf values before NaN"
        
        learning_rate_too_high:
          description: "Massive weight updates destroy model"
          fix: "Reduce learning rate by 10×"
          diagnostic: "Happens early in training"
    
    overfitting_severely:
      
      symptom: |
        Training accuracy 99%, validation accuracy 50%:
        
        Large train-val gap (>0.4)
      
      common_causes:
        model_too_large:
          description: "Too many parameters for dataset size"
          fix: "Reduce layers/neurons, add regularization"
          diagnostic: "Can memorize random labels"
        
        no_regularization:
          description: "No dropout, L2, or data augmentation"
          fix: "Add L2 (λ=0.01), dropout (p=0.5), early stopping"
          diagnostic: "Perfect training accuracy, poor validation"
        
        data_leakage:
          description: "Validation data contaminated training"
          fix: "Re-split data properly, check for duplicates"
          diagnostic: "Validation accuracy higher than expected"
    
    training_very_slow:
      
      symptom: |
        Loss decreases but extremely slowly:
        
        Epoch 1: loss = 2.30
        Epoch 100: loss = 2.20
        Epoch 500: loss = 2.10
        
        Thousands of epochs needed
      
      common_causes:
        learning_rate_too_low:
          description: "Baby steps, need to increase"
          fix: "Increase LR by 10× or use LR finder"
          diagnostic: "Consistent but very slow decrease"
        
        poor_initialization:
          description: "Weights initialized poorly"
          fix: "Use Xavier/He initialization"
          diagnostic: "Early layers learn very slowly"
        
        batch_size_too_small:
          description: "Noisy gradients, unstable updates"
          fix: "Increase batch size (8 → 32)"
          diagnostic: "Loss curve very jagged"
  
  # --------------------------------------------------------------------------
  # Topic 2: Systematic Debugging Workflow
  # --------------------------------------------------------------------------
  
  debugging_workflow:
    
    step_1_sanity_checks:
      
      verify_shapes: |
        Check all tensor shapes match expectations:
        
        print(f"Input shape: {X.shape}")      # Should be (batch, features)
        print(f"Weights shape: {W.shape}")    # Should be (features, hidden)
        print(f"Output shape: {y.shape}")     # Should be (batch, classes)
        
        Shape mismatches = instant crash
      
      test_single_batch: |
        Overfit on single batch:
        
        1. Take 1 batch (e.g., 32 samples)
        2. Train until perfect accuracy on this batch
        3. Should reach 100% accuracy quickly (10-50 epochs)
        
        If can't overfit single batch: Implementation bug
        If can overfit: Model works, issue is generalization
      
      check_loss_initialization: |
        Verify initial loss makes sense:
        
        Random predictions for K classes:
        Expected loss = -log(1/K)
        
        10 classes: -log(0.1) = 2.30
        2 classes: -log(0.5) = 0.69
        
        If initial loss way off: Bug in loss computation
      
      verify_gradients_flow: |
        Simple test: Can model learn identity function?
        
        Model: Input → hidden → Output
        Task: Output = Input
        
        Should learn perfectly in <100 epochs
        If not: Gradient flow problem
    
    step_2_isolate_component:
      
      binary_search_debugging: |
        Remove components until it works:
        
        Start: Full model (not working)
        ↓
        Remove regularization: Still broken?
        ↓
        Remove half the layers: Still broken?
        ↓
        Use linear model: Still broken?
        ↓
        Found: Bug in data loading!
        
        Isolate the component causing failure
      
      replace_with_known_working: |
        Swap components one at a time:
        
        1. Replace custom loss with PyTorch's → Works? Custom loss is bug
        2. Replace custom layer with standard → Works? Custom layer is bug
        3. Replace data loader with simple one → Works? Data loader is bug
        
        Process of elimination
    
    step_3_add_complexity_gradually:
      
      start_simple: |
        Begin with simplest possible model:
        
        1. Linear model (no hidden layers)
        2. Add 1 hidden layer
        3. Add 2nd hidden layer
        4. Add dropout
        5. Add batch normalization
        
        At each step: Verify training works before adding next
      
      incremental_debugging: |
        If step N breaks:
        - Bug introduced in step N
        - Roll back, fix, try again
        
        Much faster than debugging full complex model
  
  # --------------------------------------------------------------------------
  # Topic 3: Gradient Checking
  # --------------------------------------------------------------------------
  
  gradient_checking:
    
    why_gradient_checking:
      
      backprop_bugs_silent: |
        Backpropagation bugs often silent:
        - Wrong gradient: Model still trains (poorly)
        - No crash, no error message
        - Just mysteriously doesn't converge
        
        Gradient checking catches these bugs
      
      numerical_vs_analytical: |
        Analytical gradient: From backprop (fast but error-prone)
        Numerical gradient: From finite differences (slow but correct)
        
        Compare them: If close → backprop correct
                      If different → backprop bug
    
    numerical_gradient:
      
      finite_difference_formula: |
        Numerical gradient via centered difference:
        
        ∂L/∂W_ij ≈ (L(W + εe_ij) - L(W - εe_ij)) / (2ε)
        
        Where:
        - ε = small value (1e-5)
        - e_ij = unit vector at position (i,j)
        
        Approximate but accurate
      
      implementation: |
        def numerical_gradient(f, x, epsilon=1e-5):
            """
            Compute numerical gradient of f at x.
            
            Parameters:
            - f: function that takes x and returns scalar loss
            - x: point to evaluate gradient (can be array)
            - epsilon: finite difference step size
            
            Returns:
            - grad: numerical gradient (same shape as x)
            """
            grad = np.zeros_like(x)
            
            # Flatten for iteration
            it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
            
            while not it.finished:
                idx = it.multi_index
                old_value = x[idx]
                
                # Evaluate at x + epsilon
                x[idx] = old_value + epsilon
                loss_plus = f(x)
                
                # Evaluate at x - epsilon
                x[idx] = old_value - epsilon
                loss_minus = f(x)
                
                # Compute centered difference
                grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)
                
                # Restore original value
                x[idx] = old_value
                it.iternext()
            
            return grad
    
    gradient_checking_procedure:
      
      compare_gradients: |
        1. Compute analytical gradient (backprop)
        2. Compute numerical gradient (finite difference)
        3. Compare using relative error:
        
        relative_error = |grad_analytical - grad_numerical| / 
                        (|grad_analytical| + |grad_numerical|)
        
        Threshold:
        - < 1e-7: Excellent (backprop correct)
        - < 1e-5: Good
        - < 1e-3: Suspicious (might be bug)
        - > 1e-3: Bug (backprop wrong)
      
      implementation: |
        def gradient_check(network, X, y, epsilon=1e-5):
            """
            Check backprop gradients against numerical gradients.
            """
            # Forward and backward pass (analytical)
            loss, _ = network.forward(X, y)
            network.backward()
            analytical_grads = network.get_gradients()
            
            # Compute numerical gradients
            numerical_grads = {}
            
            for param_name, param in network.get_parameters().items():
                # Define loss function for this parameter
                def f(p):
                    # Temporarily replace parameter
                    old_param = network.get_parameters()[param_name].copy()
                    network.get_parameters()[param_name][:] = p
                    
                    loss, _ = network.forward(X, y)
                    
                    # Restore parameter
                    network.get_parameters()[param_name][:] = old_param
                    return loss
                
                # Compute numerical gradient
                numerical_grads[param_name] = numerical_gradient(f, param, epsilon)
            
            # Compare
            print("\nGradient Check Results:")
            print("="*60)
            
            all_good = True
            for param_name in analytical_grads:
                analytical = analytical_grads[param_name]
                numerical = numerical_grads[param_name]
                
                # Relative error
                numerator = np.linalg.norm(analytical - numerical)
                denominator = np.linalg.norm(analytical) + np.linalg.norm(numerical)
                relative_error = numerator / (denominator + 1e-8)
                
                status = "✓" if relative_error < 1e-5 else "✗"
                print(f"{status} {param_name}: {relative_error:.2e}")
                
                if relative_error >= 1e-5:
                    all_good = False
            
            print("="*60)
            if all_good:
                print("All gradients correct!")
            else:
                print("⚠️  Gradient errors detected - check backprop implementation")
            
            return all_good
      
      when_to_use: |
        Run gradient checking:
        - After implementing new layer type
        - When debugging training failure
        - Before committing new backprop code
        - NOT during normal training (too slow)
        
        Use small batch (1-5 samples) for speed
  
  # --------------------------------------------------------------------------
  # Topic 4: Training Monitoring
  # --------------------------------------------------------------------------
  
  training_monitoring:
    
    key_metrics_to_track:
      
      loss_and_accuracy: |
        Essential:
        - Training loss (every batch)
        - Validation loss (every epoch)
        - Training accuracy (every epoch)
        - Validation accuracy (every epoch)
        
        Plot: Dual-axis (loss + accuracy)
      
      gradient_statistics: |
        Per layer, track:
        - Gradient norm: ||∇W||
        - Gradient min/max/mean
        - Percentage of zero gradients
        
        Warning signs:
        - Norm < 0.001: Vanishing
        - Norm > 100: Exploding
        - 90% zeros: Dead neurons
      
      weight_statistics: |
        Per layer, track:
        - Weight norm: ||W||
        - Weight min/max/mean/std
        - Weight updates: ||W_new - W_old||
        
        Warning signs:
        - All weights near 0: Dying
        - Weights > 10: Exploding
        - No updates: Learning stopped
      
      activation_statistics: |
        Per layer, track:
        - Activation mean/std
        - Percentage saturated (for sigmoid/tanh)
        - Percentage dead (for ReLU)
        
        Warning signs:
        - Mean >> 0 or << 0: Imbalanced
        - Std → 0: Collapsing
        - >50% dead ReLU: Network capacity lost
    
    automated_monitoring: |
      class TrainingMonitor:
          """
          Automated training monitoring with alerts.
          """
          
          def __init__(self):
              self.history = {
                  'train_loss': [],
                  'val_loss': [],
                  'gradient_norms': [],
                  'weight_norms': []
              }
          
          def log_step(self, train_loss, grads, params):
              """Log metrics for current step"""
              self.history['train_loss'].append(train_loss)
              
              # Gradient norm
              grad_norm = np.sqrt(sum(np.sum(g**2) for g in grads.values()))
              self.history['gradient_norms'].append(grad_norm)
              
              # Weight norm
              weight_norm = np.sqrt(sum(np.sum(p**2) for p in params.values()))
              self.history['weight_norms'].append(weight_norm)
              
              # Check for anomalies
              self._check_anomalies(train_loss, grad_norm, weight_norm)
          
          def _check_anomalies(self, loss, grad_norm, weight_norm):
              """Alert on anomalies"""
              if np.isnan(loss) or np.isinf(loss):
                  print("⚠️  ALERT: Loss is NaN/Inf!")
              
              if grad_norm > 100:
                  print(f"⚠️  ALERT: Gradient explosion (norm={grad_norm:.1f})")
              
              if grad_norm < 0.001:
                  print(f"⚠️  ALERT: Vanishing gradients (norm={grad_norm:.6f})")
              
              if weight_norm > 100:
                  print(f"⚠️  ALERT: Weight explosion (norm={weight_norm:.1f})")
          
          def plot_dashboard(self):
              """Plot monitoring dashboard"""
              import matplotlib.pyplot as plt
              
              fig, axes = plt.subplots(2, 2, figsize=(12, 8))
              
              # Loss curve
              axes[0,0].plot(self.history['train_loss'])
              axes[0,0].set_title('Training Loss')
              axes[0,0].set_xlabel('Step')
              axes[0,0].set_ylabel('Loss')
              
              # Gradient norms
              axes[0,1].plot(self.history['gradient_norms'])
              axes[0,1].axhline(y=100, color='r', linestyle='--', label='Explosion')
              axes[0,1].axhline(y=0.001, color='r', linestyle='--', label='Vanishing')
              axes[0,1].set_title('Gradient Norms')
              axes[0,1].set_yscale('log')
              axes[0,1].legend()
              
              # Weight norms
              axes[1,0].plot(self.history['weight_norms'])
              axes[1,0].set_title('Weight Norms')
              
              plt.tight_layout()
              plt.show()
  
  # --------------------------------------------------------------------------
  # Topic 5: Debugging Checklist
  # --------------------------------------------------------------------------
  
  debugging_checklist:
    
    before_training:
      
      data_sanity: |
        ✓ Check data shapes: (batch, features) for inputs, (batch, labels) for targets
        ✓ Verify labels: range [0, num_classes), no NaN
        ✓ Normalize inputs: mean≈0, std≈1
        ✓ Check data distribution: no extreme outliers
        ✓ Visualize samples: do they look correct?
      
      model_sanity: |
        ✓ Run gradient check: relative error < 1e-5
        ✓ Overfit single batch: should reach 100% accuracy
        ✓ Check initial loss: matches -log(1/K) for K classes?
        ✓ Forward pass runs: no shape errors
        ✓ Backward pass runs: no NaN gradients
      
      hyperparameter_sanity: |
        ✓ Learning rate reasonable: start with 0.001 (Adam)
        ✓ Batch size reasonable: 32 is good default
        ✓ Loss function matches task: cross-entropy for classification
        ✓ Optimizer configured: Adam with β1=0.9, β2=0.999
    
    during_training:
      
      every_epoch: |
        ✓ Loss decreasing: train loss should go down
        ✓ Validation tracked: compute val loss every epoch
        ✓ Train-val gap: should be small (<0.15)
        ✓ Gradient norms: between 0.001 and 100
        ✓ No NaN/Inf: in loss, gradients, or weights
      
      every_100_steps: |
        ✓ Plot learning curves: visualize progress
        ✓ Check activation statistics: mean/std reasonable
        ✓ Monitor memory usage: not growing unbounded
        ✓ Estimate time remaining: will training finish?
    
    when_debugging_failure:
      
      first_checks: |
        1. Reduce to single batch: Can overfit? If no → implementation bug
        2. Check gradients: Run gradient check, compare analytical vs numerical
        3. Simplify model: Remove layers until it works
        4. Check data: Print samples, verify preprocessing
        5. Lower learning rate: Try 0.0001, 0.00001
      
      if_still_failing: |
        6. Use known-working model: Copy from library, does it work?
        7. Compare to baseline: Logistic regression, should beat it
        8. Check random seed: Fix seed, reproducible behavior?
        9. Profile code: Is training actually running? (not stuck)
        10. Ask for help: StackOverflow, GitHub issues
  
  # --------------------------------------------------------------------------
  # Topic 6: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    backdoors_show_training_anomalies:
      
      observation: |
        Poisoned training data creates anomalies:
        - Loss on poisoned samples lower than clean
        - Gradient magnitudes different
        - Convergence pattern unusual
      
      detection_via_monitoring: |
        Track per-sample losses:
        - Normal samples: loss = 0.1-0.3
        - Poisoned samples: loss = 0.01 (too low!)
        
        Flag samples with anomalous loss for review
    
    gradient_anomalies_indicate_attacks:
      
      observation: |
        Adversarial training examples cause:
        - Unusually large gradients
        - Sudden gradient spikes
        - Direction changes in gradient
      
      defense: |
        Monitor gradient distributions:
        - Sudden spike (>3σ) → potential attack
        - Flag batch for inspection
        - Clip gradients defensively
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Overfit single batch first: if can't reach 100%, implementation bug not hyperparameter issue"
      - "Gradient checking catches backprop bugs: compare analytical vs numerical, error <1e-5 = correct"
      - "Initial loss sanity check: should be -log(1/K) for K classes, way off = bug in loss"
      - "Binary search debugging: remove components until works, then add back incrementally"
      - "Monitor gradient norms: <0.001 vanishing, >100 exploding, both break training"
      - "Start simple, add complexity: linear model → 1 layer → 2 layers → dropout, verify each step"
    
    actionable_steps:
      - "Run sanity checks before training: gradient check + single batch overfit, catches 90% of bugs"
      - "Track gradient norms every step: plot over time, watch for vanishing/exploding patterns"
      - "Use automated monitoring: alert on NaN, gradient explosion, vanishing, catches failures early"
      - "Keep debugging checklist: before training, during training, when debugging, systematic approach"
      - "Visualize everything: loss curves, gradient distributions, weight histograms, activation stats"
      - "Compare to baseline: logistic regression should be beaten, if not something very wrong"
    
    security_principles:
      - "Backdoors create loss anomalies: poisoned samples have suspiciously low loss, monitor per-sample"
      - "Gradient spikes indicate attacks: sudden large gradients from adversarial examples"
      - "Training monitoring = attack detection: unusual patterns often indicate poisoning or adversarial training"
      - "Automated alerts catch attacks: same tools for debugging catch security issues"
    
    debugging_checklist:
      - "Loss not decreasing: check LR (too high/low), gradients (vanishing?), data (normalized?)"
      - "Loss explodes to NaN: add gradient clipping threshold=5.0, reduce LR, check for division by zero"
      - "Overfitting: add L2 (λ=0.01), dropout (p=0.5), early stopping, get more data"
      - "Training very slow: increase LR, check batch size, verify gradients flowing, check initialization"
      - "Mysterious failure: gradient check, overfit single batch, simplify model, compare to baseline"

---
