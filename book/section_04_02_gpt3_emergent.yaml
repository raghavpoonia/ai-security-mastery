# section_04_02_gpt3_emergent.yaml

---
document_info:
  section: "04_02"
  title: "GPT-3 and Emergent Capabilities"
  chapter: "04"
  chapter_title: "Modern LLM Internals"
  part: "Part I: Machine Learning Foundations"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2026-01-19"
  estimated_pages: 6
  tags:
    - "gpt-3"
    - "scaling-laws"
    - "emergent-capabilities"
    - "in-context-learning"
    - "few-shot"
    - "prompt-sensitivity"
    - "chinchilla"
    - "security-implications"

section_overview:

  purpose: |
    GPT-2 proved a decoder-only language model could generalize across tasks. GPT-3
    asked: what happens when you scale that by 1000×? The answer was not just "better
    GPT-2." Qualitatively new capabilities appeared that were not present at smaller
    scale and could not be predicted by extrapolating from GPT-2's benchmark scores.
    This phenomenon — emergent capabilities — is among the most security-relevant
    developments in modern AI.

    For security engineers: emergent capabilities mean you cannot enumerate the full
    capability set of a large model by testing a smaller version. A model that fails
    to perform a harmful task at 7B parameters may succeed at 70B. Security evaluations
    that pass on smaller models provide weak guarantees for larger ones. Understanding
    where capabilities emerge, why they emerge, and what predicts emergence is not
    academic — it determines the threat surface of systems you will be asked to defend.

    This section also covers scaling laws: the mathematical relationships between
    compute budget, dataset size, and parameter count that govern how model performance
    grows. These laws matter for security because they predict when new capabilities
    cross the threshold of being practically exploitable.

  position_in_chapter: |
    Section 2 of 17 content sections. Builds directly on Section 1 (GPT-2 architecture)
    by extending the same architecture to 175B parameters. Prepares for Section 3
    (GPT-4) where architectural changes accompany further scale.

  prerequisites:
    - "Section 04_01: GPT-2 architecture and decoder-only design"
    - "Chapter 1, Section 10: Gradient descent and optimization (for understanding training compute)"
    - "Chapter 3, Section 17: GPT architecture overview"
    - "Familiarity with log-log plots (scaling laws are plotted log-log)"

  what_you_will_build:
    primary: "Scaling law calculator: predict model performance from compute budget"
    secondary:
      - "In-context learning demonstrator: show few-shot prompting mechanics"
      - "Emergent capability tester: benchmark framework to detect capability thresholds"
      - "Prompt sensitivity analyzer: measure output variance from small prompt changes"
    notebooks:
      - "03-llm-internals/gpt3_scaling_laws.ipynb"
      - "03-llm-internals/in_context_learning.ipynb"

# ============================================================================
# CONTENT
# ============================================================================

content:

  # --------------------------------------------------------------------------
  # 1. GPT-3 ARCHITECTURE: SAME BUT BIGGER
  # --------------------------------------------------------------------------

  subsection_1:
    title: "GPT-3 Architecture: 175 Billion Parameters"
    pages: 1

    architecture_comparison:
      table:
        model:         ["GPT-2 Small", "GPT-2 XL",  "GPT-3 Small", "GPT-3 Medium", "GPT-3 Large", "GPT-3 XL",  "GPT-3 6.7B", "GPT-3 13B", "GPT-3 175B"]
        parameters:    ["124M",        "1.5B",       "125M",        "350M",         "760M",        "1.3B",      "6.7B",       "13B",       "175B"]
        layers:        [12,            48,           12,            24,             24,            24,          32,           40,          96]
        d_model:       [768,           1600,         768,           1024,           1536,          2048,        4096,         5140,        12288]
        heads:         [12,            25,           12,            16,             16,            24,          32,           40,          96]
        context:       [1024,          1024,         2048,          2048,           2048,          2048,        2048,         2048,        2048]

    what_changed: |
      GPT-3 used the same decoder-only transformer architecture as GPT-2. The changes:

      1. Scale: 175B parameters vs GPT-2 XL's 1.5B — 116× increase
      2. Context length: 2048 tokens vs 1024 — doubled
      3. Training data: 570GB (vs GPT-2's 40GB) — 14× more data
      4. Alternating dense and sparse attention in some layers
         (the 175B version uses alternating dense and banded sparse attention
         for compute efficiency — one of the few architectural innovations)

    what_did_not_change: |
      - Decoder-only architecture (causal masking)
      - Pre-norm layer normalization
      - Tied input/output embeddings (token + positional)
      - BPE tokenization (same 50,257 vocab)
      - Autoregressive generation mechanics

    key_observation: |
      The security attack surfaces from Section 1 are fully inherited by GPT-3.
      Decoder-only = no trust boundary. Tied embeddings = bidirectional inversion.
      BPE edge cases = same tokenizer, same vulnerabilities.
      Scale adds new attack surfaces; it does not eliminate old ones.

    training_data_composition:
      webtext2: "~19% of training tokens (quality filtered)"
      books1: "~8% (fiction books)"
      books2: "~8% (non-fiction books)"
      wikipedia: "~3% (English Wikipedia)"
      common_crawl: "~60% (filtered and deduplicated web text)"
      other: "~2%"

      security_note: |
        Common Crawl is largely unfiltered internet. At 60% of training tokens,
        GPT-3's behavior is heavily shaped by the full distribution of web content:
        technical forums discussing exploits, social engineering examples, scam
        templates, extremist content, and credential leak discussions. None of this
        was specifically filtered out. GPT-3's capabilities in these areas reflect
        the training distribution.

  # --------------------------------------------------------------------------
  # 2. SCALING LAWS
  # --------------------------------------------------------------------------

  subsection_2:
    title: "Scaling Laws: Predicting Performance from Compute"
    pages: 1

    kaplan_scaling_laws:
      paper: "Scaling Laws for Neural Language Models (Kaplan et al., OpenAI 2020)"
      key_findings: |
        Model performance (measured as cross-entropy loss on held-out text) follows
        smooth power laws with respect to three variables:
          N — number of model parameters
          D — dataset size (tokens)
          C — compute budget (FLOPs)

        These power laws hold across 7 orders of magnitude — from ~1K parameters
        to ~1B parameters at time of publication.

      formulas: |
        Loss as function of parameters (holding data and compute fixed):
          L(N) ≈ (Nc / N) ^ αN     where αN ≈ 0.076

        Loss as function of dataset size (holding model size fixed):
          L(D) ≈ (Dc / D) ^ αD     where αD ≈ 0.095

        Loss as function of compute (optimal allocation):
          L(C) ≈ (Cc / C) ^ αC     where αC ≈ 0.050

      allocation_implication: |
        Given a compute budget C, what is the optimal allocation between N and D?
        Kaplan found: scale N faster than D.
        "Given a 10× increase in compute, increase model size by ~5.5× and data by ~1.8×"

        This is why GPT-3 is very large but trained on relatively less data per
        parameter than smaller models. This allocation was later challenged.

    chinchilla_laws:
      paper: "Training Compute-Optimal Large Language Models (Hoffmann et al., DeepMind 2022)"
      finding: |
        Chinchilla paper showed Kaplan's allocation was wrong. The optimal allocation
        is approximately equal scaling of N and D:
          Optimal: D ≈ 20 × N (train on 20 tokens per parameter)

        GPT-3 (175B params) should have been trained on ~3.5T tokens for optimal
        compute efficiency. It was trained on ~300B tokens — severely under-trained
        by Chinchilla's analysis.

        A 70B parameter model trained on 1.4T tokens (Chinchilla's ratio) outperforms
        175B GPT-3 on most benchmarks.

      implications: |
        - Smaller, better-trained models can beat larger, under-trained models
        - Data quality and quantity matter as much as parameter count
        - "Bigger = better" is wrong — "better-trained = better" is more accurate
        - Many pre-Chinchilla models are systematically under-trained

    security_relevance_of_scaling_laws: |
      Scaling laws let you predict when a model will cross a capability threshold.
      If a harmful capability (e.g., writing convincing phishing emails, generating
      functional malware, social engineering scripts) emerges at a certain loss value,
      scaling laws tell you approximately what compute budget produces that loss.

      This creates a predictable threat timeline. Security teams can use scaling law
      projections to anticipate capability emergence rather than react to it.

      However: scaling laws predict average capabilities, not emergent ones. The
      next subsection explains why this distinction matters.

  # --------------------------------------------------------------------------
  # 3. EMERGENT CAPABILITIES
  # --------------------------------------------------------------------------

  subsection_3:
    title: "Emergent Capabilities: Unpredictable at Small Scale"
    pages: 1

    definition: |
      An emergent capability is one that is absent or near-random at small model
      scale and appears abruptly as model scale increases past a threshold. It cannot
      be predicted by extrapolating performance on that task from smaller models —
      the performance curve is flat until it is not.

      Wei et al. (2022) documented 137 tasks from BIG-Bench where this pattern holds.
      Tasks where GPT-2-scale models perform at chance level suddenly become
      solvable at GPT-3-scale or beyond.

    examples_of_emergence:
      arithmetic:
        description: "3-digit addition and multiplication"
        emergence_scale: "~100B parameters"
        pattern: "Below ~13B parameters: near-random. Above ~100B: suddenly correct."
        note: "Models do not partially learn arithmetic — they either can or cannot."

      chain_of_thought:
        description: "Multi-step reasoning by generating intermediate steps"
        emergence_scale: "~100B parameters"
        pattern: "Standard prompting at any scale gives wrong answers on hard math.
                  Adding 'Let's think step by step' improves nothing below ~100B.
                  Above ~100B: chain-of-thought prompting dramatically improves accuracy."
        note: "CoT is a prompting technique that only works on large models."

      word_unscrambling:
        description: "Unscramble letters to form words ('srcat' → 'carts')"
        emergence_scale: "~100B parameters"
        pattern: "Completely fails at smaller scales. Appears near-perfectly at scale."

      language_identification:
        description: "Identify which language a text is written in"
        emergence_scale: "~7B parameters"
        pattern: "Earlier emergence than most tasks — language ID is simpler."

      code_execution_simulation:
        description: "Trace through code mentally and produce correct output"
        emergence_scale: "~100B+ parameters"
        pattern: "GPT-3 scale: weak. GPT-4 scale: surprisingly capable."

    why_emergence_happens: |
      Several hypotheses, none fully confirmed:

      1. Phase transitions: some capabilities require multiple sub-skills, each of
         which is individually too rare to learn at small scale. When the model is
         large enough to learn all sub-skills simultaneously, the combined capability
         appears suddenly.

      2. Metric artifacts: some tasks score near-zero until the model produces the
         exact required format (e.g., exact match scoring). The model learns the
         task gradually but only crosses the scoring threshold at a certain scale.

      3. Representation quality: some tasks require rich internal representations
         that only form above a parameter threshold. The representations don't exist
         at smaller scale, so the task is literally impossible.

    security_implications_of_emergence: |
      Emergence is the primary reason why "we tested the smaller model and it was
      safe" is not a valid safety guarantee. Specific implications:

      1. Capability enumeration is incomplete: you cannot test for capabilities
         that don't exist at your evaluation scale. A 7B model that cannot write
         working exploits may be deployed at 70B where it can.

      2. Red-teaming must be scale-aware: jailbreak evaluations on 7B models
         provide weak assurance about 70B model behavior. Adversarial capabilities
         also emerge at scale.

      3. Harmful capability thresholds are unknown: we do not have a comprehensive
         map of which harmful capabilities emerge at which scale. This is an active
         research area. Security engineers should assume the map is incomplete.

      4. Open source model releases cross thresholds: when a model above a
         capability threshold is open-sourced, the entire capability set becomes
         freely available. Llama 2 at 70B crosses many thresholds that 7B does not.

  # --------------------------------------------------------------------------
  # 4. IN-CONTEXT LEARNING
  # --------------------------------------------------------------------------

  subsection_4:
    title: "In-Context Learning: How Few-Shot Prompting Works"
    pages: 1

    definition: |
      In-context learning (ICL) is the ability to perform new tasks at inference
      time by including examples of the task in the prompt — without updating any
      model weights. The model learns from the examples in its context window,
      purely through the forward pass.

      This is distinct from fine-tuning (weight updates via gradient descent).
      ICL requires no training, no labeled dataset, no compute beyond inference.

    mechanics:

      zero_shot: |
        No examples. Just task description and query.
        Prompt: "Translate the following English to French: 'Hello world'"
        Works when the model has seen enough of the task during pre-training.

      one_shot: |
        One example before the query.
        Prompt: "English: 'Good morning' → French: 'Bonjour'\nEnglish: 'Hello world' →"
        Model treats the example as a demonstration of desired behavior.

      few_shot: |
        Multiple examples (typically 5-50) before the query.
        Better performance, more reliable outputs, clearer format specification.
        Limited by context window length.

    what_the_model_is_doing: |
      There is active debate about the mechanism of ICL. Two leading hypotheses:

      1. Task recognition: the examples allow the model to identify which of many
         pre-trained tasks the prompt requires. The model already knows how to
         translate — examples just signal "this is a translation task."
         Supported by: models can do ICL even with wrong labels in examples (the
         format matters more than the label-example relationship).

      2. Implicit gradient descent: attention mechanisms perform something analogous
         to gradient descent on the examples, updating internal representations
         without explicit weight updates.
         Supported by: ICL performance correlates with task difficulty, similar to
         how gradient descent performance correlates with learning rate.

    security_implications_of_icl:

      no_training_required_for_misuse: |
        Adversaries do not need to fine-tune a model to steer its behavior. A
        carefully crafted sequence of few-shot examples can override pre-training
        and safety training. If the examples demonstrate a harmful task, the model
        will perform that task.

        Example: including 3-5 examples of "helpful" responses that violate safety
        guidelines trains the model in-context to produce such responses for the
        query. This is prompt-based jailbreaking — covered in Chapter 7.

      example_selection_attacks: |
        ICL performance is highly sensitive to which examples are chosen, their order,
        and their format. This sensitivity is a vulnerability: adversaries can craft
        examples that look benign but steer model behavior in specific directions.

        A RAG (Retrieval-Augmented Generation) system that retrieves few-shot examples
        from a database can be attacked by poisoning that database. Injecting adversarial
        examples into the retrieval index causes the model to behave adversarially
        when those examples are retrieved.

      label_independence: |
        Min et al. (2022) showed that the labels in few-shot examples matter less than
        the format and distribution. Models do ICL even when labels are randomly
        shuffled. This means: you cannot verify ICL safety by inspecting labels —
        you must inspect the full semantic content and format of examples.

    icl_vs_fine_tuning_security_comparison:
      icl:
        persistence: "No — context cleared between sessions"
        detectability: "Hard — looks like normal prompting"
        compute_required: "None (inference only)"
        attack_surface: "Context window, RAG databases, example selection"
      fine_tuning:
        persistence: "Yes — weights permanently changed"
        detectability: "Possible — weight comparison to base model"
        compute_required: "Significant (gradient updates)"
        attack_surface: "Training pipeline, dataset, SFT process"
      note: |
        ICL attacks are lower barrier and leave no persistent trace.
        Fine-tuning attacks are higher barrier but produce persistent compromised models.
        Both are covered extensively in Part 2.

  # --------------------------------------------------------------------------
  # 5. PROMPT SENSITIVITY
  # --------------------------------------------------------------------------

  subsection_5:
    title: "Prompt Sensitivity: When Small Changes Break Everything"
    pages: 1

    definition: |
      Large language models are highly sensitive to exact prompt wording, formatting,
      and example order. Identical semantic content expressed differently can produce
      dramatically different outputs. This sensitivity scales with model capability —
      larger models are more sensitive, not less.

    documented_sensitivity_effects:

      example_order: |
        Lu et al. (2021) showed that the order of few-shot examples dramatically
        affects performance. The same 4 examples in different orderings produced
        accuracy ranging from near-random to near-perfect on the same task.

        Most accurate ordering and least accurate ordering can differ by 40+ percentage
        points on the same model with the same examples.

        Security implication: defenders designing few-shot safety examples must test
        all orderings. Attackers can permute examples to find orderings that degrade
        safety filtering.

      surface_form_sensitivity: |
        "Classify the sentiment" vs "Is this review positive or negative?" both
        describe sentiment classification but can produce different model behaviors.
        Subtle word choices matter: "harmful" vs "dangerous" vs "unsafe" trigger
        different internal representations.

        Security implication: safety filters based on keyword matching are fragile.
        Rephrasing the same harmful request passes different through pattern-matching
        defenses. This is why ML-based detection (Part 3) outperforms rule-based detection.

      format_sensitivity: |
        JSON output formats, bullet point structures, numbered lists — changing output
        format specification changes what content the model produces, not just how it
        presents it. Requesting JSON output can sometimes unlock information not
        produced in prose format.

        Security implication: output format is part of the attack surface. Requesting
        structured outputs (JSON, XML, code) can sometimes bypass content filters
        that evaluate prose but not structured formats.

      instruction_placement: |
        "Always respond in English. [Long document in French]. What language is this?"
        The instruction "always respond in English" placed far from the query may
        be less effective than the same instruction placed immediately before the query.

        Position of safety instructions relative to content matters. Instructions
        placed at the beginning of a long context may be effectively ignored by
        the time the model processes content at the end.

        Security implication: system prompt position relative to injected content
        affects instruction priority. This is the "lost in the middle" problem
        formalized as a security concern. Chapter 15 designs defensive system prompts
        around this.

    calibrated_randomness: |
      Prompt sensitivity creates a practical challenge for security evaluation.
      A single test of a prompt does not tell you whether an attack succeeds —
      you must measure success rate across many prompt variations and many runs.

      Production security evaluations should:
        - Test each attack prompt 20+ times (measuring success rate, not single outcome)
        - Vary phrasing of prompts (10+ surface-form variations)
        - Test across positions in context (beginning, middle, end)
        - Test across sampling temperatures

      This is the methodology used in rigorous red-teaming. We implement it in the
      exercises below and use it throughout Part 3 when evaluating detector accuracy.

  # --------------------------------------------------------------------------
  # 6. GPT-3'S COMMERCIAL AND SECURITY IMPACT
  # --------------------------------------------------------------------------

  subsection_6:
    title: "GPT-3's Real-World Impact: Commercial and Threat"
    pages: 1

    commercial_deployment: |
      OpenAI released GPT-3 as an API in June 2020, initially in private beta.
      This was the first time a frontier language model was accessible via commercial
      API — establishing the model-as-a-service paradigm that now dominates the industry.

      The API approach has direct security implications:
        - Model weights not released → model extraction attacks become relevant
        - Rate limiting and cost per token → denial-of-service and cost attack surfaces
        - API key authentication → credential theft as attack vector
        - Usage policies without technical enforcement → policy bypass is trivially testable

    threat_landscape_gpt3_enabled:

      spear_phishing_at_scale: |
        GPT-3 demonstrated capability to generate personalized phishing emails that
        pass human review. Early research (Hazell, 2023) showed GPT-3-based spear
        phishing campaigns could be 50% more successful than human-written campaigns
        at a fraction of the cost.

        Cost comparison:
          Human spear phishing: $10-$100 per target (researcher time)
          GPT-3 spear phishing: <$0.10 per target (API cost)
          Scale increase: 100-1000× at same budget

      disinformation_generation: |
        GPT-3 can generate convincing news articles, social media posts, and forum
        comments at scale. The "too dangerous to release" framing around GPT-2 was
        validated when GPT-3 was released with API access: the capability that
        OpenAI feared (mass-scale synthetic text) was now commercially available.

      code_generation: |
        GPT-3's Codex variant demonstrated that LLMs could generate functional
        code from natural language descriptions. Security implication: LLMs that
        understand code can describe vulnerabilities and, with sufficient capability,
        generate exploit code. This capability emerged at GPT-3 scale.

      social_engineering: |
        GPT-3's conversational capability made automated social engineering viable.
        Chatbots powered by GPT-3 could conduct extended conversations, build rapport,
        and elicit information from targets. Previously, automated social engineering
        was limited by obvious bot-like responses. GPT-3 eliminated this tell.

    policy_response_and_its_limitations: |
      OpenAI responded to these threats with:
        - Usage policies (terms of service against harmful use)
        - Content filtering on API responses
        - Rate limiting
        - Manual review for high-risk use cases

      None of these are technically enforced at the model level. They are API-layer
      controls. Any model-as-a-service deployment faces the same limitation: the
      model itself will produce harmful outputs — defenses exist only at the
      API wrapper layer.

      This is why Part 3 of this book focuses on detection engineering at the API
      layer. The model is not the defense. The detection system around the model is.

# ============================================================================
# IMPLEMENTATION
# ============================================================================

implementation:
  title: "Scaling Law Calculator and ICL Demonstrator"
  notebooks:
    - "03-llm-internals/gpt3_scaling_laws.ipynb"
    - "03-llm-internals/in_context_learning.ipynb"

  scaling_law_calculator:
    description: |
      Implement Kaplan and Chinchilla scaling law equations as a practical tool.
      Given any two of {compute budget, parameter count, dataset size}, predict
      the third for optimal performance, and predict the resulting loss.
    code_sketch: |
      def optimal_model_size(compute_budget_flops):
          """Chinchilla optimal: N = C / (20 * 6)"""
          # Each token costs ~6N FLOPs to process
          # Optimal: 20 tokens per parameter
          N = compute_budget_flops / (20 * 6)
          D = 20 * N
          return N, D

      def predict_loss(N, D, N_c=8.8e13, D_c=5.4e13,
                       alpha_N=0.076, alpha_D=0.095, L_inf=1.61):
          """Chinchilla loss prediction formula"""
          return L_inf + (N_c / N) ** alpha_N + (D_c / D) ** alpha_D

      def compute_to_loss(C):
          N, D = optimal_model_size(C)
          return predict_loss(N, D)
    deliverable: "Interactive scaling law explorer — input compute budget, get predicted loss and optimal N/D"

  emergent_capability_tester:
    description: |
      Framework to test whether a given capability is present at a model scale.
      Uses HuggingFace API to test same prompts across multiple model sizes.
    tasks_to_test:
      - "3-digit multiplication (GPT-2 → GPT-3 emergence)"
      - "Rhyming couplet completion (emerges early)"
      - "Chain-of-thought arithmetic (emerges ~100B)"
      - "Word unscrambling (emerges ~100B)"
    output: |
      Plot: x-axis = model parameters, y-axis = task accuracy
      Visual demonstration of emergent capability thresholds
    note: |
      Test using public models: GPT-2 (124M, 1.5B), open-source models via HuggingFace
      (7B, 13B, 70B). Full GPT-3 requires API access.

  prompt_sensitivity_analyzer:
    description: |
      Measure output variance from prompt perturbations.
    experiments:
      - name: "Example order sensitivity"
        method: "Generate all permutations of 3 few-shot examples, measure accuracy variance"
        expected: "40+ percentage point spread across orderings"

      - name: "Surface form sensitivity"
        method: "10 semantically equivalent phrasings of same instruction, measure output distribution"
        expected: "Significant variance in output content (not just format)"

      - name: "Instruction position sensitivity"
        method: "Place safety instruction at position 0, 25%, 50%, 75%, 100% of context"
        expected: "Instructions near query are more effective"

    deliverable: "Sensitivity heatmaps — visualize how output changes with prompt perturbation"

  in_context_learning_demonstrator:
    description: |
      Interactive demonstration of ICL mechanics using GPT-2.
    demonstrations:
      - "Zero-shot vs one-shot vs five-shot accuracy on classification task"
      - "Label independence: shuffle labels in examples, measure accuracy impact"
      - "Adversarial examples: crafted few-shot examples steering output to target"
    deliverable: |
      Notebook showing ICL performance curves and adversarial ICL attack —
      the foundation for understanding prompt injection attacks in Chapter 6.

# ============================================================================
# EXERCISES
# ============================================================================

exercises:

  exercise_1:
    title: "Scaling Law Calculator"
    difficulty: "Easy"
    estimated_time: "1 hour"
    objective: "Build practical tool to predict model capabilities from compute budget"
    steps:
      - "Implement Kaplan scaling law equations (N, D, C power laws)"
      - "Implement Chinchilla scaling law equations (optimal N/D allocation)"
      - "Plot: loss vs compute for both laws on same axes"
      - "Given compute budget of a GPU cluster (1000 A100s × 30 days), predict optimal model"
      - "Compare: what model size is Kaplan-optimal vs Chinchilla-optimal?"
    success_criteria:
      - "Loss predictions match published paper values within 5%"
      - "Plot clearly shows Chinchilla recommending smaller, better-trained models"
      - "Calculator handles inputs from 10^18 to 10^25 FLOPs"
    deliverable: "scaling_law_calculator.py — reusable tool for threat timeline estimation"

  exercise_2:
    title: "Prompt Sensitivity Experiment"
    difficulty: "Medium"
    estimated_time: "2 hours"
    objective: "Quantify how sensitive GPT-2 outputs are to prompt perturbations"
    steps:
      - "Choose a classification task (sentiment, topic, language detection)"
      - "Write 5 semantically equivalent instruction phrasings"
      - "For each phrasing, run 20 generations and measure task accuracy"
      - "Vary the 3 few-shot examples across all 6 orderings"
      - "Plot accuracy vs instruction phrasing and accuracy vs example ordering"
      - "Identify the worst-performing prompt configuration — this is the attack surface"
    success_criteria:
      - "Accuracy spread across phrasings: >15 percentage points"
      - "Accuracy spread across orderings: >20 percentage points"
      - "At least one configuration performs worse than random chance"
    security_deliverable: |
      sensitivity_report.md: document the prompt configurations that degrade
      model reliability — these are exactly what attackers seek and defenders must protect.

  exercise_3:
    title: "In-Context Learning Adversarial Attack"
    difficulty: "Medium"
    estimated_time: "1.5 hours"
    objective: "Use adversarial few-shot examples to steer GPT-2 behavior"
    steps:
      - "Choose a task where GPT-2 has a known behavior (e.g., sentiment classification)"
      - "Construct 3 examples where the label-content relationship is inverted"
        # (e.g., clearly negative review labeled 'positive')
      - "Verify: does GPT-2 follow the flipped labels or the true sentiment?"
      - "If it follows flipped labels: you have demonstrated adversarial ICL"
      - "Measure how many adversarial examples are needed to fully override model behavior"
    success_criteria:
      - "Demonstrate that 3 flipped examples change classification accuracy significantly"
      - "Determine minimum number of adversarial examples needed for full steering"
      - "Document the attack pattern in terms that a detection rule could match"
    note: |
      This exercise replicates the core mechanic of many prompt injection attacks.
      Understanding this from first principles makes Chapter 6 immediately practical.

  exercise_4:
    title: "Emergent Capability Threshold Test"
    difficulty: "Hard"
    estimated_time: "2-3 hours"
    objective: "Observe capability emergence across model sizes using HuggingFace models"
    steps:
      - "Select task: 3-digit multiplication (known to emerge at ~100B parameters)"
      - "Test models: GPT-2 small (124M), GPT-2 XL (1.5B), GPT-J (6B), Llama-2 (7B, 13B, 70B)"
      - "For each model: run 50 test cases, measure exact-match accuracy"
      - "Plot accuracy vs model parameters (log scale)"
      - "Identify the threshold where accuracy jumps from <10% to >50%"
    success_criteria:
      - "Clear emergence curve showing near-zero accuracy → high accuracy"
      - "Threshold identified within one order of magnitude"
      - "Discussion: what other capabilities might emerge near this threshold?"
    note: |
      Requires HuggingFace API access for larger models. If unavailable: use the
      scaling law calculator from Exercise 1 to predict the threshold analytically.

# ============================================================================
# KEY CONCEPTS SUMMARY
# ============================================================================

key_concepts_summary:

  scaling:
    - concept: "Scaling laws (Kaplan)"
      implication: "Performance predictable from compute — but emergence is not"

    - concept: "Chinchilla optimality"
      implication: "Smaller, better-trained models outperform larger under-trained ones — open-source models may be more capable than parameter count suggests"

    - concept: "GPT-3 = GPT-2 architecture × 116"
      implication: "Same attack surfaces, amplified. New surfaces from API deployment."

  emergent_capabilities:
    - concept: "Capability emergence is abrupt, not gradual"
      implication: "Testing at small scale does not validate safety at production scale"

    - concept: "Harmful capabilities also emerge"
      implication: "Security evaluations must be conducted at deployment scale"

    - concept: "Emergent capabilities are not fully mapped"
      implication: "Assume the capability set of large models is larger than known"

  in_context_learning:
    - concept: "ICL requires no weight updates"
      implication: "Adversaries can steer model behavior through prompts alone"

    - concept: "Labels matter less than format and distribution"
      implication: "You cannot verify ICL safety by inspecting example labels"

    - concept: "ICL enables adversarial example injection"
      implication: "RAG systems with retrievable examples are attack surfaces"

  prompt_sensitivity:
    - concept: "Example order dramatically affects output"
      implication: "Security evaluations require many prompt variations, not single tests"

    - concept: "Instruction position affects instruction-following"
      implication: "Safety instructions buried in long contexts may be ignored"

    - concept: "Surface form changes output content, not just format"
      implication: "Keyword-based safety filters are insufficient"

# ============================================================================
# CONNECTIONS
# ============================================================================

connections:

  builds_on:
    - section: "Section 04_01"
      concept: "GPT-2 architecture — GPT-3 is the same architecture at 116× scale"
    - section: "Chapter 1, Section 13"
      concept: "Overfitting — scaling laws describe the opposite: performance improving monotonically with scale"
    - section: "Chapter 3, Section 17"
      concept: "GPT overview — now formalized with exact scaling relationships"

  prepares_for:
    - section: "Section 04_03"
      concept: "GPT-4 — what happens when you add architectural innovation to scale"
    - section: "Section 04_14"
      concept: "Prompt engineering — built on understanding of ICL and prompt sensitivity"
    - section: "Chapter 6 (Part 2)"
      concept: "Prompt injection — adversarial ICL is a core injection mechanism"
    - section: "Chapter 7 (Part 2)"
      concept: "Jailbreaks — prompt sensitivity is why diverse jailbreak techniques exist"
    - section: "Chapter 12 (Part 3)"
      concept: "ML-based detection — sensitivity analysis motivates statistical detection over rule-based"

  security_thread: |
    This section adds two new security seeds:
    1. Emergent capabilities → capability threshold problem (matures in Chapter 5: AI Threat Landscape)
    2. In-context learning mechanics → adversarial ICL and RAG poisoning (matures in Chapter 6)
    Combined with Section 1: the full decoder-only attack surface is now established.

# ============================================================================
# FURTHER READING
# ============================================================================

further_reading:

  primary:
    - title: "Language Models are Few-Shot Learners"
      authors: "Brown et al. (OpenAI, 2020)"
      note: "Original GPT-3 paper — Appendix G lists the few-shot evaluation suite"
      url: "https://arxiv.org/abs/2005.14165"

    - title: "Scaling Laws for Neural Language Models"
      authors: "Kaplan et al. (OpenAI, 2020)"
      note: "Original scaling law paper — read Section 3 for the power law derivations"
      url: "https://arxiv.org/abs/2001.08361"

    - title: "Training Compute-Optimal Large Language Models"
      authors: "Hoffmann et al. (DeepMind, 2022)"
      note: "Chinchilla paper — overturns Kaplan's allocation recommendation"
      url: "https://arxiv.org/abs/2203.15556"

    - title: "Emergent Abilities of Large Language Models"
      authors: "Wei et al. (Google, 2022)"
      note: "Documents 137 emergent tasks — Table 1 is a security-relevant capability inventory"
      url: "https://arxiv.org/abs/2206.07682"

  security_reading:
    - title: "Large Language Models Can Be Used to Effectively Scale Spear Phishing Campaigns"
      authors: "Hazell (2023)"
      note: "Quantifies the threat from GPT-3-enabled spear phishing at scale"
      url: "https://arxiv.org/abs/2305.06972"

    - title: "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      authors: "Min et al. (2022)"
      note: "Shows label independence in ICL — critical for understanding adversarial ICL"
      url: "https://arxiv.org/abs/2202.12837"

    - title: "Fantastically Ordered Prompts and Where to Find Them"
      authors: "Lu et al. (2021)"
      note: "Documents example order sensitivity — foundational for prompt sensitivity analysis"
      url: "https://arxiv.org/abs/2104.08786"

---
