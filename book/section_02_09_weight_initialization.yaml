# section_02_09_weight_initialization.yaml
---
document_info:
  chapter: "02"
  section: "09"
  title: "Weight Initialization"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-10"
  estimated_pages: 6
  tags: ["initialization", "xavier", "he", "glorot", "vanishing-gradients", "exploding-gradients"]

# ============================================================================
# SECTION 02_09: WEIGHT INITIALIZATION
# ============================================================================

section_02_09_weight_initialization:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Weight initialization determines where gradient descent starts. Bad
    initialization causes vanishing or exploding gradients immediately,
    before training even begins. Good initialization ensures stable signal
    propagation through the network, enabling training to succeed.
    
    This section covers the evolution of initialization strategies: naive
    approaches (zeros, constants, large random) that fail catastrophically,
    Xavier/Glorot initialization for sigmoid/tanh networks, He initialization
    for ReLU networks, and modern best practices.
    
    You'll understand the mathematical derivation of each method, implement
    them from scratch, visualize activation and gradient flow, and empirically
    verify which initializations enable training vs cause immediate failure.
  
  learning_objectives:
    
    conceptual:
      - "Understand why initialization matters critically"
      - "Grasp signal propagation through layers"
      - "Know variance preservation principle"
      - "Understand activation-specific initialization"
      - "Recognize symptoms of poor initialization"
      - "Connect initialization to gradient flow"
    
    practical:
      - "Implement Xavier, He, and other schemes"
      - "Visualize activation distributions per layer"
      - "Measure gradient magnitudes through network"
      - "Empirically compare initialization methods"
      - "Debug vanishing/exploding from initialization"
      - "Select appropriate initialization for architecture"
    
    security_focused:
      - "Initialization affects backdoor implantability"
      - "Poor initialization enables trigger hiding"
      - "Gradient flow impacts poisoning detectability"
      - "Variance analysis reveals hidden backdoors"
  
  prerequisites:
    - "Section 02_05-02_06 (backpropagation)"
    - "Section 02_02 (activation functions)"
    - "Understanding of variance and distributions"
  
  # --------------------------------------------------------------------------
  # Topic 1: Why Initialization Matters
  # --------------------------------------------------------------------------
  
  why_initialization_matters:
    
    the_initialization_problem:
      
      what_happens_at_start:
        observation: |
          At initialization (before any training):
          - Weights random
          - Gradients computed via backprop
          - Optimizer updates weights
          
          If gradients vanish (→ 0) or explode (→ ∞):
          Training fails before it begins!
      
      signal_propagation:
        forward_pass: |
          Input x propagates through layers:
          x → a₁ → a₂ → a₃ → ... → aₗ → output
          
          Each layer: aₗ = f(Wₗ·aₗ₋₁ + bₗ)
          
          If activations vanish (→ 0) or explode (→ ∞):
          Network can't learn (no signal reaches output)
        
        backward_pass: |
          Gradients propagate backward:
          ∂L/∂aₗ → ∂L/∂aₗ₋₁ → ... → ∂L/∂a₁ → ∂L/∂x
          
          Each layer multiplies by W^T and f'(z)
          
          If gradients vanish or explode:
          Early layers don't update (no learning signal)
    
    naive_initialization_failures:
      
      all_zeros:
        method: "W = 0, b = 0"
        
        forward_pass: |
          z₁ = W₁·x + b₁ = 0·x + 0 = 0
          a₁ = f(0) = constant (e.g., 0.5 for sigmoid)
          
          z₂ = W₂·a₁ + b₂ = 0·a₁ + 0 = 0
          a₂ = f(0) = constant
          
          All activations identical!
        
        backward_pass: |
          All neurons in same layer get identical gradients.
          Symmetry never breaks - all neurons remain identical.
        
        result: "Network can't learn different features (symmetry problem)"
      
      all_same_constant:
        method: "W = c (same constant for all weights)"
        
        problem: "Same as zeros - symmetry never breaks"
        
        result: "All neurons in layer learn same function (useless)"
      
      large_random:
        method: "W ~ N(0, 1) or U(-1, 1)"
        
        forward_pass: |
          With many inputs (n large):
          z = W·x has variance ≈ n·Var(W)
          
          Example: n=1000, Var(W)=1
          → Var(z) = 1000 (explodes!)
          
          After activation: a saturates (sigmoid → 0 or 1)
        
        backward_pass: |
          Activation derivatives: f'(z) ≈ 0 (saturated)
          Gradients vanish immediately!
        
        result: "Exploding activations → saturated → vanishing gradients"
      
      small_random:
        method: "W ~ N(0, 0.01²)"
        
        forward_pass: |
          z = W·x has small variance
          Activations shrink each layer
          Deep networks: signal vanishes
        
        backward_pass: |
          Small activations → small gradients
          Deep layers get no gradient signal
        
        result: "Vanishing activations and gradients"
  
  # --------------------------------------------------------------------------
  # Topic 2: Variance Preservation Principle
  # --------------------------------------------------------------------------
  
  variance_preservation:
    
    the_core_principle:
      
      goal: |
        Maintain activation variance across layers:
        Var(aₗ) ≈ Var(aₗ₋₁) for all layers l
        
        Why: Prevents signal from vanishing or exploding
      
      linear_layer_analysis:
        setup: |
          Linear layer (no activation yet):
          z = W·a_{l-1}
          
          Where:
          - W: (n_out, n_in) weight matrix
          - a_{l-1}: (n_in,) input activations
        
        variance_calculation: |
          Assume:
          - Weights: E[W_ij] = 0, Var(W_ij) = σ²_W
          - Activations: E[a_i] = 0, Var(a_i) = σ²_a
          - W and a independent
          
          Then for output neuron j:
          z_j = Σᵢ W_ji · a_i
          
          Var(z_j) = Var(Σᵢ W_ji · a_i)
                   = Σᵢ Var(W_ji · a_i)           (independence)
                   = Σᵢ Var(W_ji)·Var(a_i)        (independent)
                   = n_in · σ²_W · σ²_a
        
        preservation_condition: |
          Want: Var(z) = Var(a_{l-1})
          
          Therefore: n_in · σ²_W · σ²_a = σ²_a
          
          Simplify: n_in · σ²_W = 1
          
          Solution: σ²_W = 1 / n_in
    
    backward_pass_consideration:
      
      gradient_variance:
        setup: |
          Backward pass: ∂L/∂a_{l-1} = W^T · ∂L/∂z_l
          
          Similar analysis:
          Var(∂L/∂a_{l-1}) = n_out · σ²_W · Var(∂L/∂z_l)
        
        preservation_condition: |
          Want: Var(∂L/∂a_{l-1}) = Var(∂L/∂z_l)
          
          Therefore: n_out · σ²_W = 1
          
          Solution: σ²_W = 1 / n_out
    
    the_compromise:
      
      conflicting_requirements:
        forward: "σ²_W = 1 / n_in"
        backward: "σ²_W = 1 / n_out"
        
        problem: "Can't satisfy both unless n_in = n_out"
      
      xavier_solution: |
        Use average (harmonic mean):
        σ²_W = 2 / (n_in + n_out)
        
        Compromise: balances forward and backward
  
  # --------------------------------------------------------------------------
  # Topic 3: Xavier/Glorot Initialization
  # --------------------------------------------------------------------------
  
  xavier_initialization:
    
    derivation:
      
      assumptions:
        - "Linear activations or small activation regime"
        - "Activations centered at 0: E[a] = 0"
        - "Weights and activations independent"
      
      formula: |
        Xavier Uniform:
        W ~ U(-√(6/(n_in + n_out)), √(6/(n_in + n_out)))
        
        Xavier Normal:
        W ~ N(0, 2/(n_in + n_out))
        
        Where:
        - n_in = number of input units
        - n_out = number of output units
    
    when_to_use:
      
      appropriate_activations:
        - "Sigmoid"
        - "Tanh"
        - "Linear"
        - "Activations with small values around 0"
      
      not_for_relu: |
        ReLU kills half the neurons (sets negative to 0)
        This halves variance!
        Xavier doesn't account for this.
    
    implementation:
      
      uniform_variant: |
        def xavier_uniform(n_in, n_out):
            """
            Xavier/Glorot uniform initialization.
            
            Returns weight matrix initialized from U(-limit, limit)
            where limit = sqrt(6 / (n_in + n_out))
            """
            limit = np.sqrt(6.0 / (n_in + n_out))
            W = np.random.uniform(-limit, limit, size=(n_out, n_in))
            return W
      
      normal_variant: |
        def xavier_normal(n_in, n_out):
            """
            Xavier/Glorot normal initialization.
            
            Returns weight matrix initialized from N(0, stddev^2)
            where stddev = sqrt(2 / (n_in + n_out))
            """
            stddev = np.sqrt(2.0 / (n_in + n_out))
            W = np.random.randn(n_out, n_in) * stddev
            return W
      
      layer_initialization: |
        class LinearLayer:
            def __init__(self, input_dim, output_dim, activation='sigmoid'):
                if activation in ['sigmoid', 'tanh']:
                    # Xavier initialization
                    self.W = xavier_uniform(input_dim, output_dim)
                else:
                    # Will use different initialization
                    self.W = np.random.randn(output_dim, input_dim) * 0.01
                
                self.b = np.zeros((output_dim, 1))
    
    empirical_verification:
      
      activation_variance_test: |
        def test_xavier_initialization():
            """
            Verify Xavier maintains variance across layers.
            """
            # Create deep network (10 layers)
            layer_dims = [784] + [512]*10 + [10]
            n_layers = len(layer_dims) - 1
            
            # Initialize with Xavier
            weights = []
            for l in range(n_layers):
                W = xavier_uniform(layer_dims[l], layer_dims[l+1])
                weights.append(W)
            
            # Forward pass
            x = np.random.randn(784, 1000)  # 1000 samples
            activations = [x]
            
            a = x
            for l in range(n_layers):
                z = weights[l] @ a
                a = np.tanh(z)  # Tanh activation
                activations.append(a)
            
            # Check variance per layer
            print("Layer | Variance")
            print("------|----------")
            for l, a in enumerate(activations):
                var = np.var(a)
                print(f"  {l:2d}  | {var:.4f}")
            
            # Expected: variance stays roughly constant
        
        # Example output:
        # Layer | Variance
        # ------|----------
        #   0   | 1.0000  (input)
        #   1   | 0.9823
        #   2   | 0.9654
        #   3   | 0.9587
        #   ...
        #  10   | 0.8934  (still reasonable!)
  
  # --------------------------------------------------------------------------
  # Topic 4: He Initialization for ReLU
  # --------------------------------------------------------------------------
  
  he_initialization:
    
    motivation:
      
      relu_kills_half:
        observation: |
          ReLU: f(z) = max(0, z)
          
          Positive values: pass through unchanged
          Negative values: set to 0
          
          If z ~ N(0, σ²), roughly half are negative
          → Half the variance lost!
        
        xavier_failure: |
          With Xavier and ReLU:
          
          Layer 1: Var(a₁) = 0.5·Var(z₁) = 0.5·Var(a₀)
          Layer 2: Var(a₂) = 0.5·Var(z₂) = 0.25·Var(a₀)
          Layer 3: Var(a₃) = 0.5·Var(z₃) = 0.125·Var(a₀)
          ...
          
          Variance halves each layer → vanishes!
    
    derivation:
      
      variance_with_relu:
        setup: |
          z = W·a where a are ReLU activations
          ReLU output: roughly half of z survives
        
        correction: |
          Need 2x variance to compensate for halving:
          
          Forward pass: n_in · σ²_W = 2
          Solution: σ²_W = 2 / n_in
          
          (Factor of 2 compared to Xavier!)
      
      formula: |
        He Uniform:
        W ~ U(-√(6/n_in), √(6/n_in))
        
        He Normal (most common):
        W ~ N(0, 2/n_in)
        
        Where n_in = number of input units
    
    implementation:
      
      he_normal: |
        def he_normal(n_in, n_out):
            """
            He normal initialization (for ReLU networks).
            
            Returns weight matrix initialized from N(0, stddev^2)
            where stddev = sqrt(2 / n_in)
            """
            stddev = np.sqrt(2.0 / n_in)
            W = np.random.randn(n_out, n_in) * stddev
            return W
      
      he_uniform: |
        def he_uniform(n_in, n_out):
            """
            He uniform initialization (for ReLU networks).
            """
            limit = np.sqrt(6.0 / n_in)
            W = np.random.uniform(-limit, limit, size=(n_out, n_in))
            return W
      
      automatic_selection: |
        class LinearLayer:
            def __init__(self, input_dim, output_dim, activation='relu'):
                self.activation = activation
                
                if activation == 'relu':
                    # He initialization
                    self.W = he_normal(input_dim, output_dim)
                elif activation in ['sigmoid', 'tanh']:
                    # Xavier initialization
                    self.W = xavier_normal(input_dim, output_dim)
                else:
                    # Default to He (works reasonably for most)
                    self.W = he_normal(input_dim, output_dim)
                
                self.b = np.zeros((output_dim, 1))
    
    empirical_verification:
      
      comparison_test: |
        def compare_initializations():
            """
            Compare Xavier vs He for ReLU network.
            """
            # Deep ReLU network (20 layers)
            layer_dims = [784] + [512]*20 + [10]
            n_layers = len(layer_dims) - 1
            
            # Test both initializations
            for init_name, init_fn in [('Xavier', xavier_normal), 
                                        ('He', he_normal)]:
                print(f"\n{init_name} Initialization:")
                print("Layer | Variance")
                print("------|----------")
                
                # Initialize weights
                weights = []
                for l in range(n_layers):
                    W = init_fn(layer_dims[l], layer_dims[l+1])
                    weights.append(W)
                
                # Forward pass
                x = np.random.randn(784, 1000)
                a = x
                print(f"  {0:2d}  | {np.var(a):.4f}")
                
                for l in range(n_layers):
                    z = weights[l] @ a
                    a = np.maximum(0, z)  # ReLU
                    print(f"  {l+1:2d}  | {np.var(a):.4f}")
        
        # Expected output:
        # Xavier Initialization:
        # Layer | Variance
        # ------|----------
        #   0   | 1.0000
        #   1   | 0.5123
        #   2   | 0.2687
        #   3   | 0.1345
        #   ...
        #  20   | 0.0001  (vanished!)
        #
        # He Initialization:
        # Layer | Variance
        # ------|----------
        #   0   | 1.0000
        #   1   | 1.0234
        #   2   | 0.9876
        #   3   | 1.0123
        #   ...
        #  20   | 0.9645  (maintained!)
  
  # --------------------------------------------------------------------------
  # Topic 5: Other Initialization Schemes
  # --------------------------------------------------------------------------
  
  other_schemes:
    
    lecun_initialization:
      
      description: "Original initialization for efficient backprop (1998)"
      
      formula: |
        W ~ N(0, 1/n_in)
        
        Similar to He but without factor of 2
      
      when_to_use: "SELU activation (self-normalizing networks)"
    
    orthogonal_initialization:
      
      description: "Initialize with orthogonal matrices"
      
      method: |
        Generate random matrix, perform QR decomposition:
        Q, R = np.linalg.qr(np.random.randn(n, n))
        W = Q (orthogonal matrix)
      
      advantages:
        - "Preserves norm perfectly"
        - "No gradient vanishing/exploding"
        - "Good for RNNs (especially LSTMs)"
      
      implementation: |
        def orthogonal_init(n_in, n_out):
            """
            Orthogonal initialization.
            """
            # Generate random matrix
            a = np.random.randn(max(n_in, n_out), max(n_in, n_out))
            
            # QR decomposition
            q, r = np.linalg.qr(a)
            
            # Extract correct size
            W = q[:n_out, :n_in]
            
            return W
    
    layer_normalization_init:
      
      observation: |
        With layer normalization or batch normalization:
        Network less sensitive to initialization
        (normalization re-centers and re-scales activations)
      
      recommendation: |
        Can use simpler initialization (e.g., N(0, 0.01))
        But He/Xavier still preferred for faster early training
    
    bias_initialization:
      
      standard_practice: "b = 0 (zeros)"
      
      exceptions:
        relu_with_dead_neurons: |
          Small positive bias (0.01) can help
          Ensures some neurons active initially
        
        output_layer_classification: |
          Initialize to log of class frequencies:
          b_i = log(freq_i)
          
          Gives reasonable predictions before training
  
  # --------------------------------------------------------------------------
  # Topic 6: Debugging Initialization Issues
  # --------------------------------------------------------------------------
  
  debugging_initialization:
    
    symptoms_of_poor_initialization:
      
      vanishing_gradients:
        symptoms:
          - "Loss decreases very slowly"
          - "Early layers barely update"
          - "Gradient norms decrease with depth"
        
        diagnosis: |
          # Check gradient magnitudes per layer
          for l, layer in enumerate(network.layers):
              grad = layer.get_gradients()['W']
              norm = np.linalg.norm(grad)
              print(f"Layer {l}: gradient norm = {norm:.6f}")
          
          # If norms decrease exponentially → vanishing
        
        fix:
          - "Use He initialization for ReLU"
          - "Use Xavier for sigmoid/tanh"
          - "Consider layer normalization"
      
      exploding_gradients:
        symptoms:
          - "Loss becomes NaN or Inf"
          - "Loss oscillates wildly"
          - "Gradient norms increase with depth"
        
        diagnosis: |
          # Check activation statistics
          for l, layer in enumerate(network.layers):
              activations = layer.get_activations()
              mean = np.mean(np.abs(activations))
              max_val = np.max(np.abs(activations))
              print(f"Layer {l}: mean={mean:.2f}, max={max_val:.2f}")
          
          # If values >>1 → exploding
        
        fix:
          - "Reduce initialization scale (smaller σ²_W)"
          - "Use gradient clipping"
          - "Check for bugs (wrong dimensions, missing normalization)"
      
      dead_neurons:
        symptoms:
          - "Many neurons output 0 always"
          - "Training very slow"
          - "Validation accuracy stuck"
        
        diagnosis: |
          # Count dead neurons (ReLU outputs always 0)
          activations = network.get_layer_activations(layer_idx)
          dead = np.mean(activations == 0, axis=1)  # Per neuron
          pct_dead = np.mean(dead) * 100
          print(f"Layer {layer_idx}: {pct_dead:.1f}% dead neurons")
          
          # If >20% dead → problem
        
        fix:
          - "Use He initialization"
          - "Use Leaky ReLU instead of ReLU"
          - "Reduce learning rate"
          - "Add small positive bias (0.01)"
    
    visualization_tools:
      
      activation_histograms: |
        def plot_activation_distributions(network, X_batch):
            """
            Plot activation distributions per layer.
            """
            import matplotlib.pyplot as plt
            
            # Forward pass
            network.forward(X_batch)
            
            # Get activations
            n_layers = len(network.layers)
            fig, axes = plt.subplots(1, n_layers, figsize=(4*n_layers, 4))
            
            for l, layer in enumerate(network.layers):
                activations = layer.get_activations().flatten()
                
                axes[l].hist(activations, bins=50, alpha=0.7)
                axes[l].set_title(f'Layer {l+1}')
                axes[l].set_xlabel('Activation value')
                axes[l].set_ylabel('Frequency')
                axes[l].axvline(0, color='r', linestyle='--', alpha=0.5)
                
                # Statistics
                mean = np.mean(activations)
                std = np.std(activations)
                axes[l].text(0.05, 0.95, f'μ={mean:.2f}\nσ={std:.2f}',
                           transform=axes[l].transAxes,
                           verticalalignment='top',
                           bbox=dict(boxstyle='round', alpha=0.5))
            
            plt.tight_layout()
            plt.show()
      
      gradient_flow_visualization: |
        def plot_gradient_flow(network):
            """
            Plot gradient magnitudes per layer.
            """
            import matplotlib.pyplot as plt
            
            # Get gradients
            layers = range(len(network.layers))
            grad_norms = []
            
            for layer in network.layers:
                grad = layer.get_gradients()['W']
                norm = np.linalg.norm(grad)
                grad_norms.append(norm)
            
            # Plot
            plt.figure(figsize=(10, 6))
            plt.bar(layers, grad_norms)
            plt.xlabel('Layer')
            plt.ylabel('Gradient Norm')
            plt.title('Gradient Flow (should be roughly constant)')
            plt.yscale('log')
            plt.grid(True, alpha=0.3)
            plt.show()
            
            # Check for issues
            if grad_norms[0] / grad_norms[-1] > 100:
                print("⚠️ WARNING: Gradient vanishing (early >> late)")
            elif grad_norms[-1] / grad_norms[0] > 100:
                print("⚠️ WARNING: Gradient exploding (late >> early)")
  
  # --------------------------------------------------------------------------
  # Topic 7: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    initialization_and_backdoors:
      
      poor_initialization_enables_hiding:
        observation: |
          With vanishing gradients (poor initialization):
          - Model trains slowly
          - Early layers barely update
          - Backdoor can hide in early layers (never gets trained out)
        
        attack_strategy: |
          1. Use poor initialization (vanishing gradients)
          2. Inject backdoor in early layers
          3. Train model (backdoor persists due to vanishing gradients)
      
      good_initialization_exposes:
        observation: |
          With proper initialization:
          - All layers get gradient signal
          - Backdoors must continuously "fight" to stay embedded
          - Anomalous gradient patterns more visible
    
    variance_analysis_for_detection:
      
      method: |
        Backdoored models: activation variance anomalies
        
        Detection approach:
        1. Initialize model with He/Xavier
        2. Forward pass on clean data
        3. Measure activation variance per layer
        4. Compare to expected variance (should be ~constant)
        5. Flag layers with anomalous variance
      
      implementation: |
        def detect_variance_anomalies(network, clean_data):
            """
            Detect potential backdoors via variance analysis.
            """
            # Forward pass
            network.forward(clean_data)
            
            # Get activation variances
            variances = []
            for layer in network.layers:
                activations = layer.get_activations()
                var = np.var(activations)
                variances.append(var)
            
            # Expected: roughly constant
            # Anomaly: sudden spike or drop
            mean_var = np.mean(variances)
            std_var = np.std(variances)
            
            for l, var in enumerate(variances):
                z_score = abs(var - mean_var) / (std_var + 1e-10)
                if z_score > 3:  # 3 sigma rule
                    print(f"⚠️ Layer {l}: anomalous variance (z={z_score:.2f})")
    
    gradient_flow_and_poisoning:
      
      observation: |
        Poisoned samples produce anomalous gradients.
        With good initialization (stable gradient flow):
        - Anomalies more visible
        - Easier to detect poisoned samples
        
        With poor initialization (vanishing/exploding):
        - Normal gradient flow already unstable
        - Harder to distinguish poisoned from noise
      
      defense: "Use proper initialization as foundation for poisoning detection"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Initialization determines if training succeeds: bad init → vanishing/exploding before training starts"
      - "Variance preservation principle: want Var(aₗ) ≈ Var(aₗ₋₁) across all layers"
      - "Xavier for sigmoid/tanh: σ²_W = 2/(n_in + n_out), balances forward and backward"
      - "He for ReLU: σ²_W = 2/n_in, accounts for ReLU killing half the neurons"
      - "Symmetry breaking: never initialize all weights to same value (neurons must differ)"
      - "Bias typically zero: except small positive (0.01) for ReLU to avoid dead neurons"
    
    actionable_steps:
      - "Use He initialization for ReLU networks: W ~ N(0, 2/n_in) - most common case"
      - "Use Xavier for sigmoid/tanh: W ~ N(0, 2/(n_in+n_out)) - if using these activations"
      - "Always verify initialization: plot activation variance per layer, should be ~constant"
      - "Check gradient flow: gradient norms should be similar across layers (within 10x)"
      - "Monitor dead neurons: if >20% dead in ReLU network, initialization likely wrong"
      - "Bias initialization: b=0 standard, or b=0.01 for ReLU to reduce dead neurons"
    
    security_principles:
      - "Poor initialization enables backdoor hiding: vanishing gradients prevent backdoor removal"
      - "Good initialization exposes anomalies: stable gradient flow makes poisoned gradients visible"
      - "Variance analysis detects backdoors: anomalous activation variance reveals hidden triggers"
      - "Proper init foundation for defenses: many detection methods assume stable gradient flow"
    
    debugging_checklist:
      - "Loss not decreasing: check if gradients vanishing (plot gradient norms per layer)"
      - "Loss explodes to NaN: check if activations exploding (plot activation statistics)"
      - "Dead neurons (>20%): use He init for ReLU, or switch to Leaky ReLU"
      - "Early layers not updating: gradient vanishing, increase init scale or add normalization"
      - "Training unstable: gradient exploding, decrease init scale or use gradient clipping"

---
