---
document_info:
  title: "Chapter 4: Modern LLM Internals - Index"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 4
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-19"
  version: "1.0"
  description: "Comprehensive index for Chapter 4 covering Modern LLM Internals from GPT architecture deep dive to API security and prompt engineering"
  tags:
    - modern-llm
    - gpt-architecture
    - constitutional-ai
    - rlhf
    - inference-optimization
    - kv-cache
    - quantization
    - prompt-engineering
    - api-security

chapter_overview:
  number: 4
  title: "Modern LLM Internals"
  part: "Part 1: Machine Learning Foundations"
  position: "Chapter 4 of 4 in Part 1 (Part 1 Capstone)"

  purpose: |
    This chapter closes Part 1 by going deep into how production LLMs actually work.
    Chapter 3 gave you transformer architecture - now we dissect the systems that power
    ChatGPT, Claude, and Gemini in production. We examine GPT-2 through GPT-4 scaling,
    Anthropic's Constitutional AI and RLHF alignment approach, inference optimization
    techniques (KV cache, Flash Attention), quantization strategies, and the security
    implications of each.

    For security engineers: modern LLMs are complex production systems with multiple
    attack surfaces at each layer - from pre-training data poisoning to inference-time
    prompt injection to API abuse. Understanding their internals reveals exactly where
    security controls must live and where they fail.

  learning_path:
    gpt_evolution: "GPT-2 → GPT-3 → GPT-4: how scaling changed everything"
    alignment: "RLHF → Constitutional AI: how safety training works (and breaks)"
    training_pipeline: "Pre-training → SFT → RLHF: complete production pipeline"
    inference: "KV Cache → Flash Attention → speculative decoding: speed vs security"
    compression: "Quantization → pruning → distillation: deployment trade-offs"
    control: "Prompt engineering → system prompts → API architecture"
    security: "Attack surfaces at every layer: injection, extraction, abuse"

  prerequisites:
    - "Chapter 1: ML Fundamentals (gradient descent, regularization)"
    - "Chapter 2: Deep Learning Basics (backpropagation, optimization)"
    - "Chapter 3: LLM Architecture (transformers, attention, BERT/GPT/T5)"
    - "Understanding of autoregressive generation"
    - "Familiarity with API concepts (REST, tokens, rate limits)"

  structure:
    total_sections: 18
    content_sections: 17
    summary_sections: 1
    estimated_pages: 108
    average_pages_per_section: 6
    page_range: "5-8 pages per section"

  key_deliverables:
    - "Fine-tuned GPT-2 on custom security dataset"
    - "RLHF reward model implementation"
    - "Constitutional AI preference filter"
    - "KV cache implementation from scratch"
    - "Flash Attention prototype"
    - "INT8 and INT4 quantization pipeline"
    - "Prompt engineering playbook (50+ techniques)"
    - "API abuse detector"
    - "Model extraction via API defense toolkit"

  security_themes:
    training_poisoning: "Pre-training and fine-tuning as attack surfaces"
    alignment_bypass: "How RLHF and Constitutional AI fail under adversarial pressure"
    inference_side_channels: "KV cache and attention timing leakage"
    quantization_vulnerabilities: "Accuracy vs robustness tradeoffs in compressed models"
    prompt_exploitation: "System prompts, instruction hierarchy, and injection points"
    api_abuse: "Rate limits, cost attacks, model extraction via API"

  progression:
    week_4_day_1_2: "Sections 1-3: GPT architecture deep dive and scaling"
    week_4_day_3_4: "Sections 4-6: Claude/Anthropic architecture and alignment"
    week_4_day_5_6: "Sections 7-9: Training pipeline (pre-training, SFT, RLHF)"
    week_4_day_7: "Sections 10-12: Inference optimization"
    week_5_day_1_2: "Sections 13-15: Context windows and quantization"
    week_5_day_3_4: "Sections 16-17: Prompt engineering and API security"
    week_5_day_5: "Section 18: Chapter summary and Part 1 capstone"

sections:

  - section_number: 1
    title: "GPT-2 Architecture: The Breakthrough"
    filename: "section_04_01_gpt2_architecture.yaml"
    estimated_pages: 6

    topics:
      - "GPT-2 design decisions: decoder-only, 1.5B parameters"
      - "Architectural differences from original transformer"
      - "Pre-norm vs post-norm (GPT-2's choice)"
      - "Token and positional embeddings"
      - "Tied input/output embeddings"
      - "Training data: WebText, filtering heuristics"
      - "Zero-shot and few-shot capabilities"
      - "Why GPT-2 was 'too dangerous to release' (and wasn't)"

    key_concepts:
      - "Decoder-only simplification for generation"
      - "Scaling: parameters vs capability"
      - "Self-supervised pre-training on web text"
      - "Few-shot as in-context learning"

    deliverables:
      - "GPT-2 architecture recreation (PyTorch)"
      - "Layer inspection toolkit (weights, activations)"
      - "Tokenizer for GPT-2 (BPE)"
      - "Text generation with GPT-2"
      - "Zero-shot task evaluation"

    security_preview: |
      GPT-2's training on unfiltered web text means it memorized sensitive content -
      PII, credentials, malicious code. Zero-shot capabilities mean adversaries
      don't need fine-tuning to weaponize models. Understanding GPT-2 architecture
      reveals foundational attack surfaces that persist in all successors.

  - section_number: 2
    title: "GPT-3 and Emergent Capabilities"
    filename: "section_04_02_gpt3_emergent.yaml"
    estimated_pages: 6

    topics:
      - "Scaling laws: compute, data, parameters"
      - "GPT-3: 175B parameters, architecture choices"
      - "Emergent abilities at scale (chain-of-thought, arithmetic)"
      - "In-context learning mechanics"
      - "The 'few-shot' vs 'fine-tuning' tradeoff"
      - "Training cost and carbon footprint"
      - "Capability evaluations and benchmarks"

    key_concepts:
      - "Scaling hypothesis and emergent behaviors"
      - "In-context learning vs gradient updates"
      - "Prompt sensitivity at scale"
      - "Benchmark saturation"

    deliverables:
      - "Scaling law calculator (predict performance from compute)"
      - "In-context learning demonstrator"
      - "Emergent capability tester framework"
      - "Prompt sensitivity analyzer"

    security_preview: |
      Emergent capabilities are unpredictable - models gain abilities that weren't
      explicitly trained for, including harmful ones. Scale amplifies both capabilities
      and vulnerabilities: a more capable model is more capable of being exploited.
      In-context learning means carefully crafted examples can override intended behavior
      without any model modification.

  - section_number: 3
    title: "GPT-4 and Multimodal Architectures"
    filename: "section_04_03_gpt4_multimodal.yaml"
    estimated_pages: 6

    topics:
      - "GPT-4 architecture (what's public)"
      - "Multimodal inputs: vision + text"
      - "Mixture of Experts (MoE) speculation and reality"
      - "System prompts as architecture feature"
      - "Tool use and function calling"
      - "GPT-4 safety training (red-teaming, RLHF)"
      - "Benchmark performance and limitations"

    key_concepts:
      - "Multimodal fusion architectures"
      - "System prompt instruction hierarchy"
      - "Tool use as capability extension"
      - "Responsible scaling policies"

    deliverables:
      - "System prompt vs user prompt boundary analyzer"
      - "Tool calling implementation (mock)"
      - "Instruction hierarchy tester"
      - "Multimodal attack surface mapper"

    security_preview: |
      GPT-4's system prompts create an instruction hierarchy with explicit security
      implications - attackers target the boundary between system and user context.
      Tool use dramatically expands attack surface: LLMs that can call APIs, browse web,
      or execute code can cause real-world harm. Multimodal inputs add visual attack
      vectors (adversarial images, prompt injection in screenshots).

  - section_number: 4
    title: "Constitutional AI: Anthropic's Alignment Approach"
    filename: "section_04_04_constitutional_ai.yaml"
    estimated_pages: 7

    topics:
      - "Constitutional AI (CAI) overview and motivation"
      - "The 'constitution': principles and their selection"
      - "RLAIF: Reinforcement Learning from AI Feedback"
      - "Critique and revision pipeline"
      - "Comparison to RLHF: advantages and tradeoffs"
      - "Harmlessness vs helpfulness tension"
      - "CAI failure modes and adversarial examples"
      - "Claude's architecture and training pipeline"

    key_concepts:
      - "AI feedback instead of (only) human feedback"
      - "Principle-based alignment"
      - "Self-critique as training signal"
      - "Alignment tax: safety vs capability"

    deliverables:
      - "Constitutional AI simulation (mini implementation)"
      - "Principle adherence evaluator"
      - "Harmlessness/helpfulness tradeoff analyzer"
      - "CAI failure mode catalog"
      - "RLAIF training loop prototype"

    security_preview: |
      Constitutional AI reveals how safety training works mechanically - and therefore
      how it can fail. Adversaries study the 'constitution' to craft prompts that
      technically comply with stated principles while achieving harmful goals.
      Understanding CAI shows that alignment is not a solved problem but an ongoing
      adversarial game.

  - section_number: 5
    title: "RLHF: Reinforcement Learning from Human Feedback"
    filename: "section_04_05_rlhf_mechanics.yaml"
    estimated_pages: 7

    topics:
      - "RLHF three stages: SFT, reward model, RL fine-tuning"
      - "Reward model: learning human preferences"
      - "PPO (Proximal Policy Optimization) for LLMs"
      - "KL divergence penalty: preventing reward hacking"
      - "Human annotation: who decides what's 'good'?"
      - "Reward hacking and specification gaming"
      - "RLHF limitations: scalable oversight problem"

    key_concepts:
      - "Reward model as human preference proxy"
      - "Policy optimization with LLMs"
      - "Reward hacking: optimizing proxy not true objective"
      - "Value alignment through human feedback"

    deliverables:
      - "Reward model implementation (preference learning)"
      - "PPO training loop for language models"
      - "KL divergence penalty calculator"
      - "Reward hacking demonstrator"
      - "RLHF pipeline simulation (end-to-end)"

    security_preview: |
      RLHF is the primary alignment mechanism - and a primary attack surface. Reward
      models can be fooled: outputs that score well on the reward model but are harmful.
      Human annotator biases become model biases. Understanding RLHF mechanics reveals
      why jailbreaks work: they find inputs that bypass RLHF-trained filters by exploiting
      gaps between reward model and true human values.

  - section_number: 6
    title: "Supervised Fine-Tuning (SFT) and Instruction Tuning"
    filename: "section_04_06_sft_instruction_tuning.yaml"
    estimated_pages: 6

    topics:
      - "SFT: from pre-trained model to instruction follower"
      - "Instruction datasets: FLAN, Alpaca, ShareGPT"
      - "Dataset quality vs quantity tradeoffs"
      - "Format: prompt-response pairs"
      - "Catastrophic forgetting: preserving pre-training"
      - "Parameter-efficient fine-tuning: LoRA, QLoRA"
      - "When fine-tuning goes wrong: backdoor injection"

    key_concepts:
      - "Instruction following as learned capability"
      - "Distribution shift from pre-training to SFT"
      - "LoRA: low-rank adaptation efficiency"
      - "Fine-tuning as persistent attack vector"

    deliverables:
      - "Fine-tune GPT-2 on custom security dataset"
      - "LoRA implementation from scratch"
      - "Dataset quality evaluator"
      - "Catastrophic forgetting measurer"
      - "Fine-tuning backdoor injector (for detection research)"

    security_preview: |
      Fine-tuning is where most LLM deployments customize models for specific tasks -
      and it's a critical attack surface. Backdoors can be injected through poisoned
      fine-tuning datasets. LoRA adapters can be swapped to change model behavior
      post-deployment. Understanding SFT mechanics enables auditing fine-tuned models
      for compromised behavior before production deployment.

  - section_number: 7
    title: "Pre-Training at Scale: Data, Compute, and Risks"
    filename: "section_04_07_pretraining_scale.yaml"
    estimated_pages: 6

    topics:
      - "Pre-training data sources: Common Crawl, books, code, Wikipedia"
      - "Data filtering and deduplication pipelines"
      - "Tokenization at scale (trillions of tokens)"
      - "Distributed training: model parallelism, data parallelism"
      - "Compute budget allocation (Chinchilla scaling laws)"
      - "Training instabilities and recovery"
      - "Data memorization: what models remember"

    key_concepts:
      - "Data quality determines model quality"
      - "Scale requires distributed systems"
      - "Memorization vs generalization tradeoff"
      - "Supply chain: your model ate the internet"

    deliverables:
      - "Data pipeline simulator (filtering, dedup)"
      - "Memorization detector (verbatim recall test)"
      - "Training stability monitor"
      - "Scaling law experiment (small scale)"
      - "Data provenance tracker"

    security_preview: |
      Pre-training data is the model's DNA - everything in the training corpus shapes
      model behavior. Poisoned web data can embed backdoors at scale. Memorization means
      models may regurgitate private data from training. Understanding pre-training
      reveals supply chain attacks: compromising data pipelines is more efficient than
      attacking deployed models.

  - section_number: 8
    title: "KV Cache: Accelerating Inference"
    filename: "section_04_08_kv_cache.yaml"
    estimated_pages: 6

    topics:
      - "Why KV cache exists (redundant attention computation)"
      - "Key-Value pair storage per attention head"
      - "Prefill vs decode phases"
      - "Memory requirements: KV cache size calculations"
      - "KV cache eviction strategies (sliding window)"
      - "Shared KV cache for system prompts"
      - "Multi-query and grouped-query attention (memory efficiency)"

    key_concepts:
      - "Cache prefill as one-time cost"
      - "Memory vs computation tradeoff"
      - "Shared prefix optimization"
      - "Cache as persistent state"

    deliverables:
      - "KV cache implementation from scratch"
      - "Memory calculator (model size → cache size)"
      - "Prefill vs decode profiler"
      - "Cache eviction strategy comparison"
      - "Shared prefix cache simulator"

    security_preview: |
      KV cache introduces persistent state between requests - a side-channel attack
      surface. Timing attacks can infer cache hits (shared prefixes reveal system prompts).
      Shared KV cache for common system prompts means one user's cache can leak to another.
      Memory pressure from oversized inputs can DoS servers by exhausting KV cache.
      Understanding KV cache mechanics is essential for securing LLM inference infrastructure.

  - section_number: 9
    title: "Flash Attention: Memory-Efficient Attention"
    filename: "section_04_09_flash_attention.yaml"
    estimated_pages: 6

    topics:
      - "Standard attention memory problem: O(n²) GPU memory"
      - "Flash Attention algorithm: tiling and recomputation"
      - "IO-aware computation: SRAM vs HBM"
      - "Flash Attention 2: further optimizations"
      - "Context length expansion via Flash Attention"
      - "Flash Attention in PyTorch (scaled_dot_product_attention)"
      - "Limitations and edge cases"

    key_concepts:
      - "Memory hierarchy optimization (GPU SRAM vs HBM)"
      - "Tiling for cache efficiency"
      - "Recomputation vs storage tradeoff"
      - "Enabling longer context windows"

    deliverables:
      - "Flash Attention prototype (conceptual NumPy)"
      - "Memory usage comparison (standard vs flash)"
      - "Context length vs memory profiler"
      - "Flash Attention benchmark harness"

    security_preview: |
      Flash Attention enables much longer context windows, which changes the threat model.
      Longer contexts mean more room for indirect prompt injection payloads. Recomputation
      in Flash Attention can be exploited for timing side channels. Understanding Flash
      Attention limitations reveals maximum viable context lengths - critical for
      designing input validation and truncation policies.

  - section_number: 10
    title: "Speculative Decoding and Inference Optimization"
    filename: "section_04_10_speculative_decoding.yaml"
    estimated_pages: 6

    topics:
      - "Inference bottleneck: memory bandwidth, not compute"
      - "Speculative decoding: small model drafts, large model verifies"
      - "Acceptance criteria and rejection sampling"
      - "Batch decoding and continuous batching"
      - "Tensor parallelism for inference"
      - "Serving frameworks: vLLM, TGI, TensorRT-LLM"
      - "Latency vs throughput tradeoffs"

    key_concepts:
      - "Draft model as speed multiplier"
      - "Probabilistic verification"
      - "Continuous batching for throughput"
      - "Hardware-aware inference"

    deliverables:
      - "Speculative decoding simulator"
      - "Latency profiler (TTFT, inter-token latency)"
      - "Batch size optimizer"
      - "Throughput vs latency curve plotter"
      - "Inference serving benchmark"

    security_preview: |
      Speculative decoding introduces a draft model as a new attack surface - smaller,
      less aligned models generating candidate tokens. Continuous batching means one
      user's request shares resources with others - side channels through timing.
      Understanding inference infrastructure reveals where to place security controls
      in LLM serving stacks and how batch processing affects isolation guarantees.

  - section_number: 11
    title: "Context Windows: Architecture and Limits"
    filename: "section_04_11_context_windows.yaml"
    estimated_pages: 6

    topics:
      - "Context window evolution: 512 → 2K → 8K → 128K → 1M tokens"
      - "RoPE (Rotary Position Embedding) and extrapolation"
      - "ALiBi: Attention with Linear Biases"
      - "YaRN and other RoPE extension methods"
      - "Practical limits: 'lost in the middle' phenomenon"
      - "Context window vs RAG: when retrieval beats longer context"
      - "Long context evaluation and benchmarks"

    key_concepts:
      - "Position encoding determines context length"
      - "Attention degradation at long contexts"
      - "Lost in the middle: retrieval priority in long contexts"
      - "Engineering vs theoretical context limits"

    deliverables:
      - "RoPE implementation with extrapolation"
      - "ALiBi positional bias implementation"
      - "Long context evaluator ('lost in the middle' test)"
      - "Context length vs accuracy plotter"
      - "RAG vs long-context performance comparator"

    security_preview: |
      Long context windows create new attack surfaces at scale. The 'lost in the middle'
      phenomenon means safety instructions buried in long contexts may be ignored -
      adversaries exploit this by making prompts verbose. Conversely, critical content
      (PII, credentials) in long contexts may persist across user sessions. Understanding
      context window mechanics is essential for designing secure conversation handling.

  - section_number: 12
    title: "Model Quantization: INT8, INT4, and GPTQ"
    filename: "section_04_12_quantization.yaml"
    estimated_pages: 7

    topics:
      - "Why quantization: memory and compute reduction"
      - "FP32 → FP16 → BF16: half-precision basics"
      - "INT8 quantization: post-training quantization (PTQ)"
      - "INT4 quantization: GPTQ, AWQ, SqueezeLLM"
      - "Quantization-aware training (QAT)"
      - "Calibration datasets and quantization error"
      - "Accuracy vs compression tradeoff analysis"
      - "GGUF and llama.cpp ecosystem"

    key_concepts:
      - "Quantization as lossy compression"
      - "Per-channel vs per-tensor quantization"
      - "Outlier handling in LLM weights"
      - "Calibration for minimal accuracy loss"

    deliverables:
      - "INT8 quantization pipeline from scratch"
      - "Quantization error calculator (per-layer)"
      - "Accuracy degradation evaluator"
      - "GPTQ quantization implementation"
      - "Before/after quantization benchmark"

    security_preview: |
      Quantization reduces model accuracy - which directly reduces robustness. Quantized
      models are more susceptible to adversarial examples due to reduced precision.
      The calibration dataset for quantization is itself an attack surface: poisoned
      calibration data can degrade specific capabilities. GGUF models distributed
      community-wide represent a supply chain risk: untrusted quantized models may
      have been modified during compression.

  - section_number: 13
    title: "Model Distillation and Pruning"
    filename: "section_04_13_distillation_pruning.yaml"
    estimated_pages: 5

    topics:
      - "Knowledge distillation: teacher-student training"
      - "Distillation loss: KL divergence on soft targets"
      - "DistilBERT, TinyBERT, Phi-2: distillation examples"
      - "Structured pruning: removing attention heads, layers"
      - "Unstructured pruning: weight sparsity"
      - "Pruning criteria: magnitude, gradient, activation"
      - "Accuracy vs size pareto frontiers"

    key_concepts:
      - "Soft targets carry more information than hard labels"
      - "Iterative pruning and fine-tuning"
      - "Lottery ticket hypothesis"
      - "Compression as capability tradeoff"

    deliverables:
      - "Knowledge distillation pipeline (GPT-2 → smaller model)"
      - "Attention head importance scorer"
      - "Layer pruning with accuracy tracking"
      - "Compression ratio vs accuracy plotter"

    security_preview: |
      Distillation creates smaller models that may not inherit safety properties from
      the teacher - safety fine-tuning is often lost in distillation. Pruning safety-
      relevant attention heads is a targeted alignment removal attack. Understanding
      distillation reveals how malicious actors can create 'uncensored' versions of
      aligned models by distilling without safety data.

  - section_number: 14
    title: "Prompt Engineering: Techniques and Taxonomy"
    filename: "section_04_14_prompt_engineering.yaml"
    estimated_pages: 7

    topics:
      - "Zero-shot prompting: direct task specification"
      - "Few-shot prompting: in-context examples"
      - "Chain-of-thought (CoT): step-by-step reasoning"
      - "ReAct: reasoning and acting"
      - "Tree of Thoughts: branching reasoning"
      - "Self-consistency: majority vote over samples"
      - "Role prompting and persona assignment"
      - "Output format control (JSON, XML, structured)"

    key_concepts:
      - "Prompts as programs for LLMs"
      - "In-context examples shape distribution"
      - "Reasoning traces improve accuracy"
      - "Format specification enforces structure"

    deliverables:
      - "Prompt engineering playbook (50+ techniques)"
      - "Chain-of-thought evaluator"
      - "Few-shot example selector (semantic similarity)"
      - "Output format enforcer"
      - "Prompt sensitivity tester"

    security_preview: |
      Prompt engineering knowledge is dual-use: defenders use it to build effective
      safety system prompts, attackers use it to craft injections. Chain-of-thought
      makes reasoning explicit - useful for auditing but also reveals model internals
      to adversaries. Role prompting is the foundation of jailbreaks. Every prompt
      engineering technique has an offensive counterpart we'll cover in Chapter 7.

  - section_number: 15
    title: "System Prompts and Instruction Hierarchy"
    filename: "section_04_15_system_prompts.yaml"
    estimated_pages: 6

    topics:
      - "System prompt architecture and placement"
      - "Instruction priority: system > user > assistant"
      - "Confidential system prompts and prompt leakage"
      - "System prompt injection via user input"
      - "Multi-turn conversation state management"
      - "Context injection: RAG, tools, memory"
      - "Defensive system prompt design"

    key_concepts:
      - "Instruction hierarchy as security boundary"
      - "System prompt as policy definition"
      - "Injection across trust boundaries"
      - "Defense-in-depth for system prompts"

    deliverables:
      - "System prompt extractor (via injection)"
      - "Instruction priority violation detector"
      - "Defensive system prompt template library"
      - "Multi-turn context analyzer"
      - "System prompt hardening checklist"

    security_preview: |
      System prompts define the security boundary of LLM deployments - but they are
      not cryptographic boundaries. Attackers regularly extract system prompt contents
      through clever user queries. Indirect injection via RAG-retrieved content bypasses
      instruction hierarchy entirely. Understanding system prompt mechanics is foundational
      to Part 2 (prompt injection) and Part 3 (detection engineering).

  - section_number: 16
    title: "LLM API Architecture and Security"
    filename: "section_04_16_api_architecture.yaml"
    estimated_pages: 7

    topics:
      - "LLM API design: OpenAI, Anthropic, Google patterns"
      - "Authentication and API keys: threat model"
      - "Rate limiting: tokens per minute, requests per minute"
      - "Token counting and cost calculation"
      - "Streaming responses and partial outputs"
      - "Batching API calls for throughput"
      - "API versioning and model deprecation"
      - "On-premise vs API: security tradeoffs"

    key_concepts:
      - "API as security perimeter"
      - "Token economics and cost attacks"
      - "Rate limiting as DoS protection"
      - "Streaming creates partial state"

    deliverables:
      - "API wrapper with rate limiting"
      - "Token counter (accurate billing estimate)"
      - "Cost calculator and budget enforcer"
      - "Streaming response handler"
      - "API key rotation system"
      - "On-premise vs API security comparator"

    security_preview: |
      LLM APIs are the interface between applications and models - a critical attack
      surface. API key compromise enables cost attacks (burn budget), data extraction
      (query sensitive system prompts), and model abuse. Rate limits can be bypassed
      through multiple accounts. Streaming responses leak partial outputs before
      safety filters can evaluate the full generation. Understanding API architecture
      is prerequisite for building the detection systems in Part 3.

  - section_number: 17
    title: "Fine-Tuning GPT-2 on Custom Dataset"
    filename: "section_04_17_finetune_gpt2.yaml"
    estimated_pages: 6

    topics:
      - "Complete project: fine-tune GPT-2 for security text generation"
      - "Dataset preparation: security reports, CVE descriptions, advisories"
      - "Tokenization and chunking strategy"
      - "Training configuration: learning rate, batch size, epochs"
      - "Evaluation: perplexity on security text"
      - "Generation: security report completion"
      - "Safety evaluation: does fine-tuning degrade safety?"
      - "Lessons learned from production fine-tuning"

    key_concepts:
      - "Domain adaptation through fine-tuning"
      - "Perplexity as generation quality metric"
      - "Safety regression testing"
      - "Complete ML project lifecycle"

    deliverables:
      - "Fine-tuned GPT-2 on security dataset"
      - "Security text generator"
      - "Perplexity evaluator"
      - "Safety regression test suite"
      - "Fine-tuning experiment tracker (MLflow or W&B)"
      - "Model card documenting capabilities and limitations"

    security_preview: |
      This hands-on project demonstrates the fine-tuning attack surface in practice.
      You'll see how easily a base model can be adapted for domain-specific generation,
      and measure whether safety properties survive fine-tuning. This creates the 'villain's
      toolkit' understanding that makes Part 2 (AI Security Landscape) immediately practical.

  - section_number: 18
    title: "Chapter 4 Summary: Part 1 Capstone"
    filename: "section_04_18_chapter_summary.yaml"
    estimated_pages: 5

    overview: |
      Comprehensive review of Chapter 4 and integration across all of Part 1. This
      summary connects ML fundamentals (Chapter 1) through deep learning (Chapter 2)
      and LLM architecture (Chapter 3) to modern production systems (Chapter 4).
      Includes key concepts review, Part 1 knowledge synthesis, portfolio deliverables
      checklist, and preview of Part 2 (AI Security Landscape).

    structure:
      part_1: "Sections 1-3 Review: GPT evolution and scaling"
      part_2: "Sections 4-6 Review: Alignment (RLHF, CAI, SFT)"
      part_3: "Sections 7-10 Review: Training pipeline and inference optimization"
      part_4: "Sections 11-14 Review: Context, quantization, and prompt engineering"
      part_5: "Sections 15-17 Review: System prompts, APIs, and fine-tuning project"
      integration: "Part 1 synthesis: from gradient descent to production LLM"
      security_map: "Complete attack surface map for everything covered in Part 1"
      portfolio: "What you've built: portfolio checklist and GitHub setup"
      preview: "Part 2: AI Security Landscape (where defense meets offense)"

    deliverables:
      - "Complete Part 1 knowledge checklist"
      - "Attack surface map for ML/DL/LLM systems"
      - "GitHub repository structure populated with all Chapter 4 code"
      - "Fine-tuned GPT-2 project complete and documented"
      - "Prompt engineering playbook finalized"
      - "Part 2 reading prerequisites confirmed"

    part_1_completion_criteria:
      knowledge:
        - "Implement ML algorithms from scratch (Chapter 1)"
        - "Build neural network with backprop (Chapter 2)"
        - "Implement transformer from scratch (Chapter 3)"
        - "Explain GPT-4, Claude, RLHF, Constitutional AI (Chapter 4)"
        - "Identify attack surfaces at every layer of the stack"
      skills:
        - "Spam classifier working (NumPy only)"
        - "MNIST neural network (98%+ accuracy)"
        - "Mini-transformer trained on sequence task"
        - "Fine-tuned GPT-2 on custom dataset"
        - "Prompt engineering playbook (50+ techniques)"
      deliverables:
        - "GitHub repository with all implementations"
        - "4 chapter README files with results"
        - "Security attack surface documentation"
        - "Ready for Part 2: AI Security Landscape"

estimated_totals:
  total_sections: 18
  content_sections: 17
  summary_sections: 1
  estimated_total_pages: 108
  average_pages_per_section: 6.0
  implementation_files: "18 YAML section files"
  code_examples: "55+ runnable implementations"
  security_analyses: "18 security implications covered"
  capstone_deliverable: "Fine-tuned GPT-2 on security dataset (Section 17)"

chapter_completion_criteria:
  knowledge:
    - "Explain GPT-2, GPT-3, GPT-4 architectural differences"
    - "Describe RLHF and Constitutional AI training pipelines"
    - "Explain KV cache and Flash Attention mechanics"
    - "Understand quantization trade-offs (INT8, INT4, GPTQ)"
    - "Articulate system prompt instruction hierarchy and failure modes"
    - "Identify API security risks and abuse patterns"

  skills:
    - "Fine-tune GPT-2 on custom dataset with tracking"
    - "Implement KV cache from scratch"
    - "Quantize a model and measure accuracy impact"
    - "Engineer effective system prompts with safety controls"
    - "Build API wrapper with rate limiting and cost controls"
    - "Extract system prompts via injection (red-team exercise)"

  security_mindset:
    - "Map attack surfaces across the complete LLM stack"
    - "Understand how RLHF/CAI fail under adversarial pressure"
    - "Identify side channels in inference infrastructure"
    - "Recognize quantization as a robustness attack vector"
    - "Design defensive system prompts against injection"
    - "Audit fine-tuned models for safety regression"

metadata:
  creation_date: "2026-01-19"
  author: "Raghav Dinesh"
  book_version: "1.0"
  total_yaml_files: 19
  dependencies:
    - "chapter_01_index.yaml (gradient descent, regularization)"
    - "chapter_02_index.yaml (backpropagation, optimization)"
    - "chapter_03_index.yaml (transformer architecture, attention)"
    - "PyTorch, HuggingFace Transformers (implementation)"
    - "MLflow or Weights & Biases (experiment tracking)"
  next_chapter: "Chapter 5: AI Threat Landscape (Part 2 begins)"
---
