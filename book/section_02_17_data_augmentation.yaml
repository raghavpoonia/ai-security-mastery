# section_02_17_data_augmentation.yaml
---
document_info:
  chapter: "02"
  section: "17"
  title: "Data Augmentation Techniques"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-16"
  estimated_pages: 6
  tags: ["data-augmentation", "image-augmentation", "mixup", "cutout", "autoaugment", "augmentation-policies"]

# ============================================================================
# SECTION 02_17: DATA AUGMENTATION TECHNIQUES
# ============================================================================

section_02_17_data_augmentation:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Data augmentation artificially expands training sets by creating modified
    versions of existing samples. For computer vision, this means transforming
    images in ways that preserve labels: rotating a cat image still shows a cat.
    
    Modern augmentation goes beyond simple flips and rotations. Techniques like
    Mixup, CutMix, and AutoAugment achieve 2-5% accuracy improvements on standard
    benchmarks. Understanding which augmentations work for which tasks, how to
    combine them, and their security implications is essential.
    
    This section covers geometric and photometric transformations, advanced
    techniques (Mixup, Cutout, RandAugment), augmentation policies, task-specific
    considerations, and implementation strategies.
  
  learning_objectives:
    
    conceptual:
      - "Understand why augmentation improves generalization"
      - "Know which augmentations preserve labels"
      - "Grasp augmentation policy design"
      - "Understand domain-specific augmentation"
      - "Recognize augmentation magnitude trade-offs"
      - "Connect augmentation to regularization"
    
    practical:
      - "Implement standard augmentation pipeline"
      - "Apply geometric and photometric transforms"
      - "Use Mixup and CutMix effectively"
      - "Design task-specific augmentation policies"
      - "Tune augmentation hyperparameters"
      - "Debug augmentation issues"
    
    security_focused:
      - "Augmentation dilutes poisoned samples"
      - "Some augmentations preserve backdoor triggers"
      - "Adversarial training as augmentation"
      - "Augmentation affects model robustness"
  
  prerequisites:
    - "Section 02_10 (regularization basics)"
    - "Section 02_12 (CNNs)"
    - "Basic image processing"
  
  # --------------------------------------------------------------------------
  # Topic 1: Standard Image Augmentations
  # --------------------------------------------------------------------------
  
  standard_augmentations:
    
    geometric_transformations:
      
      horizontal_flip:
        description: "Mirror image left-to-right"
        
        when_to_use: |
          Use when:
          - Horizontal orientation doesn't matter
          
          Examples: Animals, objects, general scenes
          
          Don't use when:
          - Text (flipped text unreadable)
          - Handwriting (flipped letters wrong)
          - Signs with text
        
        implementation: |
          def horizontal_flip(image, prob=0.5):
              """Flip image horizontally with probability prob"""
              if np.random.random() < prob:
                  return np.fliplr(image)
              return image
        
        impact: "Simple, effective, no information loss"
      
      vertical_flip:
        description: "Mirror image top-to-bottom"
        
        when_to_use: |
          Use when:
          - Vertical orientation doesn't matter
          
          Examples: Satellite images, microscopy, abstract patterns
          
          Don't use when:
          - Natural images (upside-down looks wrong)
          - Objects with gravity (cars, buildings)
        
        caution: "Less commonly used than horizontal flip"
      
      rotation:
        description: "Rotate image by angle θ"
        
        parameters: |
          - angle_range: [-15°, +15°] typical for natural images
          - angle_range: [0°, 360°] for rotation-invariant tasks
          - fill_mode: 'constant', 'nearest', 'reflect'
        
        implementation: |
          def random_rotation(image, max_angle=15):
              """Rotate image by random angle"""
              angle = np.random.uniform(-max_angle, max_angle)
              from scipy.ndimage import rotate
              return rotate(image, angle, reshape=False, mode='nearest')
        
        when_to_use: |
          Use small angles (±15°) for:
          - Natural images (large rotations unnatural)
          
          Use full 360° for:
          - Aerial images, satellite imagery
          - Medical scans (no canonical orientation)
          - Abstract patterns
        
        cost: "Interpolation required, slight quality loss"
      
      translation:
        description: "Shift image horizontally and/or vertically"
        
        parameters: |
          - shift_range: typically 10-20% of image dimension
          - fill_mode: how to fill empty space
        
        implementation: |
          def random_translation(image, max_shift_fraction=0.1):
              """Translate image by random offset"""
              h, w = image.shape[:2]
              shift_h = int(np.random.uniform(-max_shift_fraction, max_shift_fraction) * h)
              shift_w = int(np.random.uniform(-max_shift_fraction, max_shift_fraction) * w)
              
              from scipy.ndimage import shift
              return shift(image, [shift_h, shift_w, 0] if len(image.shape)==3 else [shift_h, shift_w],
                          mode='nearest')
        
        benefit: "Forces model to be translation-invariant"
      
      scaling_zoom:
        description: "Zoom in/out on image"
        
        parameters: |
          - zoom_range: [0.8, 1.2] typical
          - zoom_in: crop and resize (information loss)
          - zoom_out: shrink and pad (adds padding)
        
        implementation: |
          def random_zoom(image, zoom_range=(0.9, 1.1)):
              """Randomly zoom image"""
              zoom_factor = np.random.uniform(zoom_range[0], zoom_range[1])
              from scipy.ndimage import zoom as scipy_zoom
              
              zoomed = scipy_zoom(image, [zoom_factor, zoom_factor, 1] 
                                 if len(image.shape)==3 else [zoom_factor, zoom_factor],
                                 mode='nearest')
              
              # Crop or pad to original size
              h, w = image.shape[:2]
              h_z, w_z = zoomed.shape[:2]
              
              if h_z > h:  # Crop
                  start_h = (h_z - h) // 2
                  start_w = (w_z - w) // 2
                  return zoomed[start_h:start_h+h, start_w:start_w+w]
              else:  # Pad
                  pad_h = (h - h_z) // 2
                  pad_w = (w - w_z) // 2
                  return np.pad(zoomed, 
                               [(pad_h, h-h_z-pad_h), (pad_w, w-w_z-pad_w), (0,0)]
                               if len(image.shape)==3 else [(pad_h, h-h_z-pad_h), (pad_w, w-w_z-pad_w)],
                               mode='edge')
        
        benefit: "Handles scale variation in test images"
      
      shearing:
        description: "Skew image along axis"
        
        use_case: "Simulates perspective changes, less common"
    
    photometric_transformations:
      
      brightness_adjustment:
        description: "Adjust image luminance"
        
        parameters: |
          - delta_range: [-30, +30] for pixel values [0, 255]
          - delta_range: [-0.2, +0.2] for normalized [0, 1]
        
        implementation: |
          def adjust_brightness(image, delta_range=0.2):
              """Randomly adjust brightness"""
              delta = np.random.uniform(-delta_range, delta_range)
              return np.clip(image + delta, 0, 1)
        
        caution: "Extreme adjustments can make images unrecognizable"
      
      contrast_adjustment:
        description: "Adjust difference between light and dark"
        
        parameters: |
          - factor_range: [0.7, 1.3] typical
          - factor < 1: decrease contrast
          - factor > 1: increase contrast
        
        implementation: |
          def adjust_contrast(image, factor_range=(0.8, 1.2)):
              """Randomly adjust contrast"""
              factor = np.random.uniform(factor_range[0], factor_range[1])
              mean = np.mean(image)
              return np.clip((image - mean) * factor + mean, 0, 1)
      
      saturation_adjustment:
        description: "Adjust color intensity (RGB images only)"
        
        implementation: |
          def adjust_saturation(image, factor_range=(0.8, 1.2)):
              """Randomly adjust saturation"""
              import cv2
              factor = np.random.uniform(factor_range[0], factor_range[1])
              
              # Convert to HSV
              hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype(float)
              hsv[:, :, 1] *= factor
              hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)
              
              # Convert back to RGB
              return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
      
      hue_shift:
        description: "Rotate color wheel"
        
        parameters: "delta_range: [-30, +30] degrees typical"
        
        use_case: "Color invariance for object recognition"
      
      gaussian_noise:
        description: "Add random noise to simulate sensor noise"
        
        implementation: |
          def add_gaussian_noise(image, std=0.01):
              """Add Gaussian noise"""
              noise = np.random.normal(0, std, image.shape)
              return np.clip(image + noise, 0, 1)
        
        benefit: "Improves robustness to noisy inputs"
    
    combining_transformations:
      
      sequential_application: |
        Apply multiple transforms in sequence:
        
        1. Geometric transforms first (rotation, translation, zoom)
        2. Photometric transforms second (brightness, contrast)
        
        Order matters: rotate then brighten ≠ brighten then rotate
      
      probability_based: |
        Each transform applied with probability p:
        
        augment(image):
            if random() < 0.5: image = horizontal_flip(image)
            if random() < 0.3: image = rotate(image, ±15°)
            if random() < 0.3: image = brightness(image, ±0.2)
            return image
      
      implementation_example: |
        class StandardAugmentation:
            """Standard augmentation pipeline"""
            
            def __init__(self, flip_prob=0.5, rotation_range=15,
                        brightness_range=0.2, contrast_range=(0.8, 1.2)):
                self.flip_prob = flip_prob
                self.rotation_range = rotation_range
                self.brightness_range = brightness_range
                self.contrast_range = contrast_range
            
            def __call__(self, image):
                """Apply augmentation pipeline"""
                
                # Geometric
                if np.random.random() < self.flip_prob:
                    image = np.fliplr(image)
                
                if np.random.random() < 0.3:
                    angle = np.random.uniform(-self.rotation_range, self.rotation_range)
                    image = rotate(image, angle, reshape=False, mode='nearest')
                
                # Photometric
                if np.random.random() < 0.5:
                    delta = np.random.uniform(-self.brightness_range, self.brightness_range)
                    image = np.clip(image + delta, 0, 1)
                
                if np.random.random() < 0.5:
                    factor = np.random.uniform(self.contrast_range[0], self.contrast_range[1])
                    mean = np.mean(image)
                    image = np.clip((image - mean) * factor + mean, 0, 1)
                
                return image
  
  # --------------------------------------------------------------------------
  # Topic 2: Advanced Augmentation Techniques
  # --------------------------------------------------------------------------
  
  advanced_techniques:
    
    cutout:
      
      description: |
        Cutout: Randomly mask out square regions of image
        Forces model to use full context, not rely on single region
      
      method: |
        1. Choose random position (x, y)
        2. Mask out square region of size s×s
        3. Fill with zeros (or mean pixel value)
      
      hyperparameters: |
        - patch_size: 16×16 typical for 32×32 images (CIFAR-10)
        - num_patches: Usually 1, sometimes 2-3
        - fill_value: 0, mean, or random
      
      implementation: |
        class Cutout:
            """Cutout augmentation"""
            
            def __init__(self, patch_size=16, num_patches=1):
                self.patch_size = patch_size
                self.num_patches = num_patches
            
            def __call__(self, image):
                """Apply cutout"""
                h, w = image.shape[:2]
                mask = np.ones((h, w, 1) if len(image.shape)==3 else (h, w), dtype=np.float32)
                
                for _ in range(self.num_patches):
                    # Random position
                    y = np.random.randint(0, h)
                    x = np.random.randint(0, w)
                    
                    # Compute patch boundaries
                    y1 = np.clip(y - self.patch_size // 2, 0, h)
                    y2 = np.clip(y + self.patch_size // 2, 0, h)
                    x1 = np.clip(x - self.patch_size // 2, 0, w)
                    x2 = np.clip(x + self.patch_size // 2, 0, w)
                    
                    # Mask out patch
                    mask[y1:y2, x1:x2] = 0
                
                return image * mask
      
      benefits:
        - "Prevents overfitting to specific regions"
        - "Improves localization"
        - "Forces global reasoning"
      
      typical_improvement: "+1-2% accuracy on CIFAR-10/100"
    
    mixup:
      
      description: |
        Mixup: Create virtual training samples by mixing pairs
        
        x_mixed = λ·x_i + (1-λ)·x_j
        y_mixed = λ·y_i + (1-λ)·y_j
        
        Where λ ~ Beta(α, α), typically α=0.2
      
      method: |
        1. Sample two training examples (x_i, y_i) and (x_j, y_j)
        2. Sample mixing coefficient λ from Beta(α, α)
        3. Create mixed input: x_mixed = λ·x_i + (1-λ)·x_j
        4. Create mixed label: y_mixed = λ·y_i + (1-λ)·y_j
        5. Train on (x_mixed, y_mixed)
      
      implementation: |
        class MixUp:
            """Mixup augmentation"""
            
            def __init__(self, alpha=0.2):
                self.alpha = alpha
            
            def __call__(self, batch_x, batch_y):
                """
                Apply mixup to batch.
                
                Parameters:
                - batch_x: (batch_size, ...) images
                - batch_y: (batch_size, num_classes) one-hot labels
                
                Returns:
                - mixed_x, mixed_y
                """
                batch_size = batch_x.shape[0]
                
                # Sample lambda from Beta distribution
                if self.alpha > 0:
                    lam = np.random.beta(self.alpha, self.alpha)
                else:
                    lam = 1.0
                
                # Random permutation
                indices = np.random.permutation(batch_size)
                
                # Mix inputs and targets
                mixed_x = lam * batch_x + (1 - lam) * batch_x[indices]
                mixed_y = lam * batch_y + (1 - lam) * batch_y[indices]
                
                return mixed_x, mixed_y
      
      intuition: |
        Traditional: Binary decision boundaries
        Mixup: Smooth transitions between classes
        
        Result: Better calibrated predictions, improved generalization
      
      benefits:
        - "Reduces overfitting"
        - "Better calibrated probabilities"
        - "More robust to adversarial examples"
        - "Smooth decision boundaries"
      
      typical_improvement: "+2-4% accuracy, especially on small datasets"
    
    cutmix:
      
      description: |
        CutMix: Combination of Cutout and Mixup
        
        Instead of blending entire images:
        1. Cut region from one image
        2. Paste into another image
        3. Mix labels proportional to area
      
      method: |
        1. Sample two images (x_A, y_A) and (x_B, y_B)
        2. Sample bounding box (x, y, w, h)
        3. Create mixed image:
           - Region inside box: from x_B
           - Region outside box: from x_A
        4. Mix labels: λ = (w×h) / (W×H) (area ratio)
           y_mixed = λ·y_B + (1-λ)·y_A
      
      implementation: |
        class CutMix:
            """CutMix augmentation"""
            
            def __init__(self, alpha=1.0):
                self.alpha = alpha
            
            def __call__(self, batch_x, batch_y):
                """Apply CutMix to batch"""
                batch_size = batch_x.shape[0]
                H, W = batch_x.shape[2:4]
                
                # Sample lambda
                lam = np.random.beta(self.alpha, self.alpha)
                
                # Random permutation
                indices = np.random.permutation(batch_size)
                
                # Sample bounding box
                cut_ratio = np.sqrt(1.0 - lam)
                cut_w = int(W * cut_ratio)
                cut_h = int(H * cut_ratio)
                
                cx = np.random.randint(W)
                cy = np.random.randint(H)
                
                x1 = np.clip(cx - cut_w // 2, 0, W)
                x2 = np.clip(cx + cut_w // 2, 0, W)
                y1 = np.clip(cy - cut_h // 2, 0, H)
                y2 = np.clip(cy + cut_h // 2, 0, H)
                
                # Apply CutMix
                mixed_x = batch_x.copy()
                mixed_x[:, :, y1:y2, x1:x2] = batch_x[indices, :, y1:y2, x1:x2]
                
                # Adjust lambda based on actual box size
                lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))
                mixed_y = lam * batch_y + (1 - lam) * batch_y[indices]
                
                return mixed_x, mixed_y
      
      advantages_over_mixup:
        - "Preserves spatial structure better"
        - "No blending artifacts"
        - "Better localization learning"
      
      typical_improvement: "+2-3% accuracy, especially for localization tasks"
    
    randaugment:
      
      description: |
        RandAugment: Simplified augmentation search
        
        Key idea: Reduce search space
        - Fixed set of 14 augmentation operations
        - Two hyperparameters: N (number of ops) and M (magnitude)
        - No per-operation tuning needed
      
      operations: |
        14 standard operations:
        1. Identity (no-op)
        2. AutoContrast
        3. Equalize
        4. Rotate
        5. Solarize
        6. Color
        7. Posterize
        8. Contrast
        9. Brightness
        10. Sharpness
        11. ShearX
        12. ShearY
        13. TranslateX
        14. TranslateY
      
      hyperparameters: |
        N: Number of operations to apply (typical: 2-3)
        M: Magnitude of operations [0, 30] (typical: 9-15)
        
        Magnitude controls strength:
        M=0: Minimal effect
        M=30: Maximum effect
      
      algorithm: |
        For each image:
        1. Randomly select N operations from 14 available
        2. Apply each with magnitude M
        3. Return augmented image
      
      implementation_sketch: |
        class RandAugment:
            """RandAugment augmentation"""
            
            def __init__(self, n=2, m=10):
                self.n = n  # Number of ops
                self.m = m  # Magnitude
                
                # 14 augmentation operations
                self.ops = [
                    'AutoContrast', 'Equalize', 'Rotate', 'Solarize',
                    'Color', 'Posterize', 'Contrast', 'Brightness',
                    'Sharpness', 'ShearX', 'ShearY', 'TranslateX', 'TranslateY'
                ]
            
            def __call__(self, image):
                """Apply N random operations with magnitude M"""
                selected_ops = np.random.choice(self.ops, self.n, replace=False)
                
                for op in selected_ops:
                    image = self._apply_op(image, op, self.m)
                
                return image
            
            def _apply_op(self, image, op, magnitude):
                """Apply single operation with given magnitude"""
                # Map magnitude [0, 30] to operation-specific range
                # Implementation details per operation...
                pass
      
      benefits:
        - "Simple: only 2 hyperparameters (N, M)"
        - "Effective: competitive with AutoAugment"
        - "Fast: no expensive search required"
      
      typical_values: |
        ImageNet: N=2, M=10
        CIFAR-10: N=2, M=14
        SVHN: N=1, M=9
  
  # --------------------------------------------------------------------------
  # Topic 3: Task-Specific Augmentation
  # --------------------------------------------------------------------------
  
  task_specific_augmentation:
    
    medical_imaging:
      
      appropriate:
        - "Rotation: any angle (no canonical orientation)"
        - "Flips: horizontal and vertical both okay"
        - "Zoom: handles scale variation"
        - "Elastic deformations: simulates tissue variation"
        - "Gaussian noise: simulates imaging artifacts"
      
      inappropriate:
        - "Color jittering: medical images often grayscale"
        - "Extreme brightness: changes diagnostic information"
        - "Cutout: might remove critical pathology"
      
      domain_specific:
        - "Histogram equalization: improve contrast"
        - "CLAHE: Contrast Limited Adaptive Histogram Equalization"
        - "Gamma correction: adjust intensity distribution"
    
    satellite_imagery:
      
      appropriate:
        - "Rotation: full 360° (no fixed orientation)"
        - "Flips: both horizontal and vertical"
        - "Channel swapping: different sensor bands"
        - "Atmospheric effects: haze, clouds simulation"
      
      inappropriate:
        - "Extreme color changes: spectral information critical"
      
      temporal_augmentation:
        - "Multi-temporal images: seasonal variation"
        - "Different weather conditions"
    
    face_recognition:
      
      appropriate:
        - "Small rotations: ±10° (head tilt)"
        - "Horizontal flip: face symmetry"
        - "Slight zoom: distance variation"
        - "Brightness/contrast: lighting conditions"
      
      inappropriate:
        - "Vertical flip: upside-down faces wrong"
        - "Large rotations: >45° looks wrong"
        - "Cutout: might remove identifying features"
      
      specialized:
        - "Face alignment: normalize pose"
        - "Aging simulation: for age-invariant recognition"
    
    ocr_document_recognition:
      
      appropriate:
        - "Rotation: ±5° (slight skew)"
        - "Perspective transform: simulate camera angle"
        - "Blur: simulate focus issues"
        - "Noise: simulate scan artifacts"
      
      inappropriate:
        - "Horizontal/vertical flip: text becomes unreadable"
        - "Large rotations: text orientation matters"
        - "Color changes: text recognition mostly intensity-based"
  
  # --------------------------------------------------------------------------
  # Topic 4: Augmentation in Practice
  # --------------------------------------------------------------------------
  
  practical_considerations:
    
    when_to_apply:
      
      during_training_only: |
        Augmentation applied ONLY during training
        
        Training:
        - Load image
        - Apply random augmentations
        - Feed to network
        
        Validation/Test:
        - Load image
        - NO augmentation (or just normalization)
        - Feed to network
      
      test_time_augmentation: |
        Optional: Augment at test time for better predictions
        
        Method:
        1. Create N augmented versions of test image
        2. Get predictions for all N versions
        3. Average predictions
        
        Example (N=10):
        - Original
        - Flip
        - 3 rotations (±10°, ±15°)
        - 3 brightness levels
        - 2 zoom levels
        
        Result: ~1% accuracy improvement, but 10x slower inference
    
    magnitude_tuning:
      
      too_weak: |
        Problem: Augmentation barely changes images
        Result: Little benefit, model still overfits
        
        Signs:
        - Train-val gap large
        - Augmented images look identical to originals
      
      too_strong: |
        Problem: Augmentation changes semantics
        Result: Model confused, lower accuracy
        
        Signs:
        - Training accuracy low (<90%)
        - Augmented images barely recognizable
        - Model learns slower
        
        Examples:
        - Rotation ±90°: cats sideways/upside-down (unnatural)
        - Brightness ±0.8: completely black or white
      
      finding_sweet_spot: |
        Start conservative, increase gradually:
        
        1. Baseline: rotation ±5°, brightness ±0.1
        2. If overfitting: increase to ±10°, ±0.2
        3. If still overfitting: add more augmentation types
        4. If training accuracy drops: reduce magnitude
      
      validation_curve_method: |
        Plot validation accuracy vs augmentation strength:
        
        Augmentation Magnitude | Val Accuracy
        -----------------------|-------------
        None                   | 85%
        Weak (M=5)            | 87%
        Medium (M=10)         | 91% ← Best
        Strong (M=15)         | 89%
        Very Strong (M=25)    | 83%
        
        Choose M=10 (peak validation accuracy)
    
    computational_cost:
      
      cpu_augmentation: |
        Typical setup: Augmentation on CPU during data loading
        
        Advantage: GPU free for training
        Disadvantage: Can bottleneck if augmentation slow
      
      gpu_augmentation: |
        Alternative: Augmentation on GPU
        
        Libraries: NVIDIA DALI, Kornia
        
        Advantage: Faster augmentation (GPU parallel)
        Disadvantage: Uses GPU memory and compute
      
      caching_strategy: |
        For expensive augmentations:
        
        Option 1: Pre-augment offline
        - Generate augmented dataset once
        - Store to disk
        - Fast training, but fixed augmentations
        
        Option 2: Cache augmented batches
        - Augment on-the-fly
        - Cache recent batches in memory
        - Balance speed and randomness
  
  # --------------------------------------------------------------------------
  # Topic 5: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    augmentation_vs_poisoning:
      
      dilution_effect: |
        Poisoned training set: 1% poisoned (100 out of 10K)
        
        With 10x augmentation:
        - Original: 100 poisoned, 9,900 clean
        - Augmented: 100 poisoned, 99,000 clean
        - Poisoning rate: 1% → 0.1%
        
        Backdoor effectiveness reduced 10x!
      
      defense_strategy: |
        Aggressive augmentation as poisoning defense:
        
        1. Apply many augmentation types
        2. Use strong magnitude
        3. Generate 20-50x augmented samples
        
        Result: Dilute poisoned samples significantly
      
      limitation: |
        Augmentation may preserve backdoor trigger:
        
        Trigger: specific patch in corner
        Rotation ±15°: trigger mostly preserved
        Flip: trigger moves to other corner, still present
        
        Some augmentations don't remove triggers!
    
    augmentation_aware_backdoors:
      
      robust_triggers: |
        Attacker designs triggers that survive augmentation:
        
        1. Large trigger (covers >25% of image)
        2. Central trigger (survives crops)
        3. Pattern-based (survives geometric transforms)
        4. Frequency-based (survives pixel-level changes)
      
      example_robust_trigger: |
        Checkerboard pattern across entire image:
        - Rotation: pattern still visible
        - Flip: pattern symmetric, unchanged
        - Brightness: pattern relative to background
        
        Very hard to remove via augmentation!
    
    adversarial_training_as_augmentation:
      
      method: |
        Adversarial training: Augment with adversarial examples
        
        Training loop:
        1. Generate adversarial examples: x_adv = x + ε·sign(∇_x L)
        2. Train on mix of clean and adversarial
        3. Model learns robustness
      
      effectiveness: |
        Standard augmentation: 70% → 75% accuracy
        + Adversarial training: 75% → 78% accuracy
        + Adversarial robustness: 5% → 60% robust accuracy
      
      cost: |
        2-3x training time (generating adversarial examples)
        
        Trade-off: Better robustness, longer training
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Augmentation expands training data artificially: transform images while preserving labels"
      - "Geometric transforms standard: flip (50%), rotation (±15°), translation (10%), zoom (±10%)"
      - "Photometric transforms effective: brightness (±20%), contrast (×0.8-1.2), saturation adjust"
      - "Cutout masks random regions: forces global reasoning, prevents overfitting to specific areas"
      - "Mixup blends image pairs: x_mixed = λx₁ + (1-λ)x₂, smoother decision boundaries"
      - "RandAugment simplifies search: only 2 hyperparameters (N ops, M magnitude), very effective"
    
    actionable_steps:
      - "Start with standard pipeline: horizontal flip + rotation ±15° + brightness/contrast"
      - "Add Cutout for overfitting: 16×16 patch for 32×32 images, scales proportionally"
      - "Use Mixup for small datasets: α=0.2 typical, major gains on <10K samples"
      - "Try RandAugment if time permits: N=2, M=10 good defaults, tune on validation"
      - "Apply only during training: validation/test use original images (except TTA)"
      - "Verify label preservation: manually inspect augmented images, ensure still recognizable"
    
    security_principles:
      - "Augmentation dilutes poisoning: 10x augmentation → 10x reduction in poisoning rate"
      - "Not all augmentation removes backdoors: triggers can be designed to survive transforms"
      - "Adversarial training strongest defense: include adversarial examples in augmentation"
      - "Large diverse augmentation best: many types + strong magnitude harder for attacker to exploit"
    
    task_specific_wisdom:
      - "Natural images: rotation ±15° safe, ±90° looks wrong, full 360° for satellite/medical"
      - "Text/OCR: avoid flips (unreadable), small rotations ±5° okay for skew"
      - "Medical: grayscale means skip color augmentation, elastic deformations domain-appropriate"
      - "Face recognition: avoid vertical flip (upside-down faces wrong), ±10° rotation for head tilt"
      - "When in doubt: visualize augmented samples, if human can't recognize → too strong"

---
