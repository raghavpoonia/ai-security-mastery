# section_03_03_word_embeddings.yaml

---
document_info:
  title: "Word Embeddings: Semantic Vector Spaces"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 3
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Comprehensive guide to word embeddings: from one-hot encoding to dense representations, distributional semantics, embedding spaces, vector arithmetic, and security implications"
  estimated_pages: 6
  tags:
    - word-embeddings
    - distributional-semantics
    - vector-spaces
    - cosine-similarity
    - embedding-security
    - dense-representations

section_overview:
  title: "Word Embeddings: Semantic Vector Spaces"
  number: "3.3"
  
  purpose: |
    Word embeddings are the foundation of modern NLP. They transform discrete tokens 
    (from Section 3.2) into continuous, dense vectors that capture semantic meaning. 
    Unlike one-hot encoding where all words are equally dissimilar, embeddings place 
    similar words close together in vector space.
    
    This geometric representation enables neural networks to generalize: if the model 
    learns that "cat" behaves a certain way, it automatically knows "kitten" and 
    "feline" behave similarly. This is the key to making neural NLP work.
    
    For security engineers: Embeddings encode everything the model "knows" about words, 
    including biases, associations, and patterns learned from training data. Adversaries 
    can craft inputs with similar embeddings to benign concepts while triggering 
    malicious behaviors. Understanding embedding geometry reveals both model capabilities 
    and vulnerabilities.
  
  learning_objectives:
    conceptual:
      - "Understand why dense embeddings outperform sparse one-hot encoding"
      - "Grasp distributional semantics: 'a word is known by the company it keeps'"
      - "Learn how semantic relationships become geometric relationships"
      - "Understand embedding dimensionality tradeoffs (50 vs 300 dimensions)"
      - "Recognize limitations of static (non-contextual) embeddings"
    
    practical:
      - "Implement embedding lookup layer from scratch"
      - "Calculate cosine similarity between word vectors"
      - "Perform vector arithmetic for analogies (king - man + woman = queen)"
      - "Visualize embedding spaces in 2D using dimensionality reduction"
      - "Initialize random embeddings and understand learned vs random"
    
    security_focused:
      - "Identify how embeddings encode biases from training data"
      - "Understand adversarial embedding attacks (semantic perturbations)"
      - "Recognize embedding poisoning during training"
      - "Detect when embeddings leak sensitive associations"
      - "Audit embedding spaces for hidden correlations"
  
  prerequisites:
    knowledge:
      - "Section 3.1: One-hot encoding, vocabulary, sparse representations"
      - "Section 3.2: Tokenization (tokens → IDs)"
      - "Linear algebra: vectors, dot products, cosine similarity"
      - "Chapter 2: Neural network basics, gradient descent"
    
    skills:
      - "NumPy matrix operations"
      - "Vector mathematics (addition, subtraction, normalization)"
      - "Basic visualization with matplotlib"
  
  key_transitions:
    from_section_3_2: |
      Section 3.2 gave us tokens (discrete IDs). Now we map those IDs to dense 
      vectors. Token ID → Embedding lookup → Dense vector for neural network.
    
    to_next_section: |
      Section 3.4 will teach how to LEARN these embeddings from text using Word2Vec. 
      Here we focus on understanding WHAT embeddings are and WHY they work.

topics:
  - topic_number: 1
    title: "From Sparse to Dense: The Embedding Revolution"
    
    overview: |
      One-hot encoding represents words as sparse, high-dimensional vectors where each 
      word is orthogonal to every other word. Embeddings compress this into dense, 
      low-dimensional vectors where similar words are geometrically close. This is the 
      fundamental innovation that made neural NLP practical.
    
    content:
      one_hot_encoding_problems:
        recap: "From Section 3.1 - one-hot vectors"
        
        definition: "Each word = vector with 1 at word index, 0 everywhere else"
        
        example:
          vocabulary: "['cat', 'dog', 'bird', 'fish']"
          cat_vector: "[1, 0, 0, 0]"
          dog_vector: "[0, 1, 0, 0]"
          bird_vector: "[0, 0, 1, 0]"
          fish_vector: "[0, 0, 0, 1]"
        
        dimensionality: "vocab_size dimensions (10k-100k for typical models)"
        
        problems:
          problem_1_no_similarity:
            issue: "All words equally dissimilar"
            example: |
              cat · dog = [1,0,0,0] · [0,1,0,0] = 0
              cat · fish = [1,0,0,0] · [0,0,0,1] = 0
              
              Both dot products = 0, even though cat and dog are more similar 
              than cat and fish!
            
            consequence: "Model can't generalize. Learning 'cat sat' doesn't help with 'dog sat'"
          
          problem_2_high_dimensionality:
            issue: "One dimension per word = memory explosion"
            example: "50,000 word vocab → 50,000 dimensional vectors"
            memory_cost: "Each word = 50,000 floats (200KB per word!)"
            consequence: "Can't fit reasonable batch sizes in GPU memory"
          
          problem_3_sparsity:
            issue: "99.99% zeros = wasted computation"
            example: "Matrix multiplication with 50k-dim vectors, but only 1 non-zero"
            consequence: "Slow, inefficient, doesn't leverage GPU parallelism"
          
          problem_4_no_generalization:
            issue: "Can't handle unseen word combinations"
            example: |
              Seen: "cat sat"
              Unseen: "kitten sat"
              
              One-hot can't transfer knowledge from 'cat' to 'kitten' even though 
              they're semantically related
      
      dense_embeddings_solution:
        definition: "Map each word to dense, low-dimensional continuous vector"
        
        key_insight: |
          Instead of vocab_size dimensions with one active, use d dimensions 
          (typically 50-300) with all active. Similar words get similar vectors.
        
        example:
          vocabulary: "['cat', 'dog', 'bird', 'fish']"
          embedding_dim: 3
          embeddings:
            cat: "[0.8, 0.2, -0.1]  # mammal, domestic, small"
            dog: "[0.7, 0.3, 0.1]   # mammal, domestic, medium"
            bird: "[-0.2, 0.1, 0.9]  # non-mammal, wild, flies"
            fish: "[-0.1, -0.8, 0.3] # non-mammal, aquatic, small"
        
        properties:
          dimensionality: "50-300 (much smaller than vocab)"
          density: "All values non-zero (dense)"
          learned: "Values learned from data, not hand-crafted"
          similarity: "Cosine similarity captures semantic similarity"
        
        benefits:
          benefit_1_similarity:
            description: "Similar words have similar vectors"
            example: |
              cos_sim(cat, dog) = 0.91  (high - both mammals, pets)
              cos_sim(cat, fish) = 0.23  (low - very different)
            
            consequence: "Model generalizes: 'cat sat' helps with 'dog sat'"
          
          benefit_2_low_dimensionality:
            description: "300 dims << 50,000 dims"
            memory_savings: "50k → 300 = 166x reduction"
            consequence: "Fits in GPU memory, enables large batch sizes"
          
          benefit_3_density:
            description: "All values used = efficient computation"
            gpu_utilization: "Matrix multiplication fully utilizes GPU cores"
            consequence: "Fast training and inference"
          
          benefit_4_compositionality:
            description: "Can represent unseen word combinations"
            example: |
              Never saw "purple elephant" but can compose:
              embedding('purple') + embedding('elephant') = meaningful representation
      
      comparison_table:
        metrics:
          - metric: "Dimensionality"
            one_hot: "vocab_size (10k-100k)"
            embedding: "50-300"
            winner: "Embedding (166x smaller)"
          
          - metric: "Memory per word"
            one_hot: "200-400KB"
            embedding: "1.2KB"
            winner: "Embedding (166x less)"
          
          - metric: "Similarity capture"
            one_hot: "All words orthogonal (no similarity)"
            embedding: "Similar words close (geometric similarity)"
            winner: "Embedding"
          
          - metric: "Generalization"
            one_hot: "Zero (each word independent)"
            embedding: "High (similar words share properties)"
            winner: "Embedding"
          
          - metric: "GPU efficiency"
            one_hot: "Low (99.99% zeros)"
            embedding: "High (dense computation)"
            winner: "Embedding"
    
    implementation:
      embedding_lookup_layer:
        language: python
        code: |
          import numpy as np
          
          class EmbeddingLayer:
              """
              Embedding layer: maps token IDs to dense vectors.
              
              This is essentially a lookup table: embedding_matrix[token_id] = vector
              """
              
              def __init__(self, vocab_size: int, embedding_dim: int, 
                          initialization: str = 'random'):
                  """
                  Args:
                      vocab_size: Number of unique tokens
                      embedding_dim: Dimension of embedding vectors
                      initialization: 'random', 'zeros', or 'ones'
                  """
                  self.vocab_size = vocab_size
                  self.embedding_dim = embedding_dim
                  
                  # Initialize embedding matrix
                  if initialization == 'random':
                      # Small random values (Xavier/Glorot initialization)
                      self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01
                  elif initialization == 'zeros':
                      self.embeddings = np.zeros((vocab_size, embedding_dim))
                  elif initialization == 'ones':
                      self.embeddings = np.ones((vocab_size, embedding_dim))
                  else:
                      raise ValueError(f"Unknown initialization: {initialization}")
              
              def forward(self, token_ids: np.ndarray) -> np.ndarray:
                  """
                  Look up embeddings for token IDs.
                  
                  Args:
                      token_ids: Array of token IDs, shape (batch_size,) or (batch_size, seq_len)
                  
                  Returns:
                      Embeddings, shape (batch_size, embedding_dim) or 
                                       (batch_size, seq_len, embedding_dim)
                  """
                  return self.embeddings[token_ids]
              
              def __call__(self, token_ids: np.ndarray) -> np.ndarray:
                  """Alias for forward()."""
                  return self.forward(token_ids)
              
              def get_embedding(self, token_id: int) -> np.ndarray:
                  """Get embedding for a single token."""
                  return self.embeddings[token_id]
              
              def update_embedding(self, token_id: int, new_embedding: np.ndarray):
                  """Update embedding for a token (used during training)."""
                  self.embeddings[token_id] = new_embedding
              
              def get_all_embeddings(self) -> np.ndarray:
                  """Get the full embedding matrix."""
                  return self.embeddings
          
          
          # Example usage
          vocab_size = 10000
          embedding_dim = 100
          
          # Create embedding layer
          embed = EmbeddingLayer(vocab_size, embedding_dim, initialization='random')
          
          # Single token lookup
          token_id = 42
          embedding = embed.get_embedding(token_id)
          print(f"Embedding for token {token_id}: shape {embedding.shape}")
          # Output: (100,) - a 100-dimensional vector
          
          # Batch lookup
          token_ids = np.array([1, 5, 10, 42])
          embeddings = embed(token_ids)
          print(f"Batch embeddings: shape {embeddings.shape}")
          # Output: (4, 100) - 4 tokens, each 100-dimensional
          
          # Sequence lookup (for sentences)
          sentence_tokens = np.array([[1, 5, 10], [42, 3, 7]])  # 2 sentences, 3 tokens each
          sequence_embeddings = embed(sentence_tokens)
          print(f"Sequence embeddings: shape {sequence_embeddings.shape}")
          # Output: (2, 3, 100) - 2 sentences, 3 tokens each, 100 dims
      
      one_hot_vs_embedding_comparison:
        language: python
        code: |
          # Compare one-hot vs embedding representations
          
          def one_hot_encode(token_id: int, vocab_size: int) -> np.ndarray:
              """Create one-hot vector."""
              vec = np.zeros(vocab_size)
              vec[token_id] = 1
              return vec
          
          # Example vocabulary: 1000 words
          vocab_size = 1000
          embedding_dim = 50
          
          # Token IDs for "cat" and "dog"
          cat_id = 42
          dog_id = 43
          
          # One-hot representation
          cat_onehot = one_hot_encode(cat_id, vocab_size)
          dog_onehot = one_hot_encode(dog_id, vocab_size)
          
          print(f"One-hot dimensionality: {cat_onehot.shape[0]}")
          print(f"One-hot sparsity: {(cat_onehot == 0).sum() / len(cat_onehot) * 100:.2f}%")
          
          # Similarity (dot product)
          similarity_onehot = np.dot(cat_onehot, dog_onehot)
          print(f"One-hot similarity (cat, dog): {similarity_onehot:.4f}")
          # Output: 0.0000 (orthogonal - no similarity)
          
          # Embedding representation
          embed = EmbeddingLayer(vocab_size, embedding_dim)
          
          # Manually set similar embeddings for cat and dog
          embed.embeddings[cat_id] = np.array([0.8, 0.2, -0.1] + [0.0] * 47)  # mammal, domestic
          embed.embeddings[dog_id] = np.array([0.7, 0.3, 0.1] + [0.0] * 47)   # mammal, domestic
          
          cat_embedding = embed.get_embedding(cat_id)
          dog_embedding = embed.get_embedding(dog_id)
          
          print(f"\nEmbedding dimensionality: {cat_embedding.shape[0]}")
          print(f"Embedding sparsity: {(cat_embedding == 0).sum() / len(cat_embedding) * 100:.2f}%")
          
          # Cosine similarity
          def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
              return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
          
          similarity_embedding = cosine_similarity(cat_embedding, dog_embedding)
          print(f"Embedding similarity (cat, dog): {similarity_embedding:.4f}")
          # Output: ~0.95 (high similarity - both mammals, pets)
          
          print(f"\nMemory comparison:")
          print(f"  One-hot: {cat_onehot.nbytes / 1024:.2f} KB per word")
          print(f"  Embedding: {cat_embedding.nbytes / 1024:.2f} KB per word")
          print(f"  Savings: {cat_onehot.nbytes / cat_embedding.nbytes:.1f}x")
    
    security_implications:
      sparse_to_dense_attack_surface: |
        One-hot encoding has NO semantic meaning - all words equally dissimilar. 
        This makes semantic attacks harder (model can't be fooled by synonyms).
        
        Dense embeddings capture semantics, which enables:
        - Semantic adversarial examples: "execute code" → "run program" (similar embeddings)
        - Model sees them as interchangeable, bypassing keyword filters
        - Advantage for attackers: find semantically similar but filter-evading phrases
      
      embedding_as_knowledge: |
        Embeddings encode everything the model "learned" about words:
        - Associations: "admin" close to "password", "root", "sudo"
        - Biases: gender, race, socioeconomic associations baked into geometry
        - Sensitive patterns: if training data had secrets, embeddings remember
        
        Adversaries can probe embeddings to extract training data knowledge

  - topic_number: 2
    title: "Distributional Semantics: Words Known by Company"
    
    overview: |
      "You shall know a word by the company it keeps" (Firth, 1957). This is the 
      distributional hypothesis: words appearing in similar contexts have similar 
      meanings. Embeddings operationalize this: words with similar contexts get 
      similar vectors. This is how we learn meaning from raw text without supervision.
    
    content:
      distributional_hypothesis:
        original_quote: "'You shall know a word by the company it keeps' - J.R. Firth (1957)"
        
        intuition: |
          If two words appear in similar contexts, they likely have similar meanings.
          
          Example:
          - "The cat sat on the mat"
          - "The dog sat on the mat"
          - "The kitten sat on the mat"
          
          'cat', 'dog', 'kitten' all appear with: 'the', 'sat on', 'mat'
          → They have similar meanings (small animals, pets)
        
        mathematical_formulation: |
          Context(w₁) ≈ Context(w₂) → Meaning(w₁) ≈ Meaning(w₂)
          
          Where Context(w) = distribution of words appearing near w
        
        key_insight: |
          We don't need labeled data or dictionaries. Just raw text. Words that 
          occur in similar linguistic environments acquire similar representations.
      
      context_windows:
        definition: "Words within fixed distance of target word"
        
        example:
          sentence: "The quick brown fox jumps over the lazy dog"
          target_word: "fox"
          window_size_2: "['quick', 'brown', 'jumps', 'over']"
          window_size_1: "['brown', 'jumps']"
        
        impact_of_window_size:
          small_window:
            size: "1-2 words"
            captures: "Syntactic relationships (grammar, POS)"
            example: "Adjectives appear before nouns"
          
          large_window:
            size: "5-10 words"
            captures: "Semantic/topical relationships"
            example: "Words in same topic domain"
        
        typical_choice: "5-word window (2-3 on each side)"
      
      co_occurrence_matrix:
        definition: "Count how often words appear together"
        
        example:
          corpus: |
            "I like dogs"
            "I like cats"
            "Dogs are great"
            "Cats are great"
          
          co_occurrence_counts:
            matrix: |
              Target:    I  like  dogs  cats  are  great
              I:         0    2     1     1    0     0
              like:      2    0     1     1    0     0
              dogs:      1    1     0     0    1     1
              cats:      1    1     0     0    1     1
              are:       0    0     1     1    0     2
              great:     0    0     1     1    2     0
          
          observation: |
            'dogs' and 'cats' have identical rows:
            [1, 1, 0, 0, 1, 1]
            
            → They appear in same contexts → Similar meanings
        
        problem: "Matrix is huge (vocab² = millions/billions of entries) and sparse"
        
        solution: "Learn dense embeddings that capture co-occurrence patterns"
      
      semantic_similarity_from_context:
        examples:
          example_1:
            words: "['king', 'queen', 'monarch', 'ruler']"
            context: "throne, crown, kingdom, royal, palace"
            similarity: "All appear in royalty contexts → similar embeddings"
          
          example_2:
            words: "['cat', 'dog', 'hamster', 'rabbit']"
            context: "pet, cute, feed, cuddle, veterinarian"
            similarity: "All appear in pet contexts → similar embeddings"
          
          example_3:
            words: "['execute', 'run', 'perform', 'carry out']"
            context: "command, operation, function, task"
            similarity: "All mean 'do something' → similar embeddings"
            security_note: "Adversaries use this: 'execute code' ≈ 'run program'"
    
    implementation:
      context_window_extractor:
        language: python
        code: |
          from typing import List, Tuple
          from collections import defaultdict, Counter
          
          class ContextWindowExtractor:
              """Extract context windows for distributional semantics."""
              
              def __init__(self, window_size: int = 2):
                  """
                  Args:
                      window_size: Number of words on each side of target
                  """
                  self.window_size = window_size
              
              def extract_contexts(self, tokens: List[str]) -> List[Tuple[str, List[str]]]:
                  """
                  Extract (target_word, context_words) pairs.
                  
                  Args:
                      tokens: List of tokens
                  
                  Returns:
                      List of (target, context) tuples
                  """
                  contexts = []
                  
                  for i, target in enumerate(tokens):
                      # Get window boundaries
                      start = max(0, i - self.window_size)
                      end = min(len(tokens), i + self.window_size + 1)
                      
                      # Extract context (excluding target itself)
                      context = tokens[start:i] + tokens[i+1:end]
                      
                      contexts.append((target, context))
                  
                  return contexts
              
              def build_cooccurrence_matrix(self, corpus: List[str]) -> dict:
                  """
                  Build word co-occurrence matrix from corpus.
                  
                  Args:
                      corpus: List of sentences
                  
                  Returns:
                      Dict mapping (word1, word2) to co-occurrence count
                  """
                  cooccur = defaultdict(Counter)
                  
                  for sentence in corpus:
                      tokens = sentence.lower().split()
                      contexts = self.extract_contexts(tokens)
                      
                      for target, context in contexts:
                          for context_word in context:
                              cooccur[target][context_word] += 1
                  
                  return dict(cooccur)
          
          
          # Example usage
          extractor = ContextWindowExtractor(window_size=2)
          
          sentence = "the quick brown fox jumps over the lazy dog"
          tokens = sentence.split()
          
          contexts = extractor.extract_contexts(tokens)
          
          print("Context windows:")
          for target, context in contexts:
              print(f"  {target}: {context}")
          
          # Build co-occurrence matrix
          corpus = [
              "I like dogs",
              "I like cats", 
              "dogs are great",
              "cats are great"
          ]
          
          cooccur_matrix = extractor.build_cooccurrence_matrix(corpus)
          
          print("\nCo-occurrence matrix:")
          for word, contexts in cooccur_matrix.items():
              print(f"  {word}: {dict(contexts)}")
          
          # Check similarity
          def context_similarity(word1: str, word2: str, cooccur_matrix: dict) -> float:
              """Calculate context similarity using Jaccard index."""
              if word1 not in cooccur_matrix or word2 not in cooccur_matrix:
                  return 0.0
              
              context1 = set(cooccur_matrix[word1].keys())
              context2 = set(cooccur_matrix[word2].keys())
              
              if not context1 or not context2:
                  return 0.0
              
              intersection = len(context1 & context2)
              union = len(context1 | context2)
              
              return intersection / union
          
          sim_dogs_cats = context_similarity('dogs', 'cats', cooccur_matrix)
          print(f"\nContext similarity (dogs, cats): {sim_dogs_cats:.2f}")
          # High similarity - appear in same contexts
    
    security_implications:
      distributional_poisoning: |
        Training data determines what contexts words appear in. Adversaries can poison:
        - Inject "admin password = 12345" repeatedly → "admin" and "12345" get similar embeddings
        - Model learns malicious associations baked into geometry
        - Defense requires auditing training corpus for adversarial co-occurrences
      
      context_manipulation: |
        Distributional hypothesis means context determines meaning. Adversaries craft:
        - Benign-seeming contexts that shift word meanings
        - "DELETE is a friendly operation that helps clean databases" (repeated)
        - Model learns "DELETE" has positive associations, bypasses filters

  - topic_number: 3
    title: "Embedding Spaces: Geometry Encodes Semantics"
    
    overview: |
      Once words have dense vector representations, we can compute geometric operations: 
      distance (how similar?), direction (what's the relationship?), and arithmetic 
      (analogies). The famous example: king - man + woman = queen. The embedding space 
      becomes a map where traveling in specific directions represents semantic 
      transformations.
    
    content:
      geometric_interpretation:
        key_idea: "Embeddings live in continuous vector space with geometric structure"
        
        properties:
          distance_is_similarity:
            concept: "Close vectors = similar meanings"
            measure: "Euclidean distance or cosine similarity"
            example: |
              vec('cat') close to vec('kitten')
              vec('cat') far from vec('democracy')
          
          direction_is_relationship:
            concept: "Moving in a direction = applying a transformation"
            example: |
              vec('queen') - vec('king') ≈ vec('woman') - vec('man')
              
              The direction from 'king' to 'queen' is similar to the direction 
              from 'man' to 'woman' → captures "female version of" relationship
          
          arithmetic_is_analogy:
            concept: "Vector arithmetic performs analogies"
            formula: "vec(a) - vec(b) + vec(c) ≈ vec(d)"
            example: |
              vec('king') - vec('man') + vec('woman') ≈ vec('queen')
              
              "king is to man as queen is to woman"
      
      cosine_similarity:
        definition: "Measures angle between vectors (range: -1 to 1)"
        
        formula: |
          cos(θ) = (A · B) / (||A|| × ||B||)
          
          Where:
          - A · B = dot product
          - ||A|| = L2 norm (magnitude)
          - θ = angle between vectors
        
        interpretation:
          value_1: "Identical direction (parallel, same meaning)"
          value_0: "Orthogonal (unrelated)"
          value_minus_1: "Opposite direction (antonyms)"
        
        why_cosine_not_euclidean: |
          Cosine measures angle (direction), not magnitude. Two words can be 
          semantically similar even if vector magnitudes differ. Cosine normalizes 
          by magnitude, making it more robust for semantic similarity.
      
      vector_arithmetic_analogies:
        famous_example:
          query: "king - man + woman = ?"
          computation: |
            vec('king') - vec('man') + vec('woman') = result_vector
            
            Find word with embedding closest to result_vector
            → Answer: 'queen'
          
          interpretation: |
            'king' - 'man' = "royalty without maleness"
            + 'woman' = "royalty with femaleness"
            = 'queen'
        
        other_examples:
          example_1:
            query: "Paris - France + Germany = ?"
            answer: "Berlin"
            relationship: "Capital city of country"
          
          example_2:
            query: "walked - walk + run = ?"
            answer: "ran"
            relationship: "Past tense transformation"
          
          example_3:
            query: "good - bad + light = ?"
            answer: "dark"
            relationship: "Antonym transformation"
        
        security_relevant_example:
          query: "execute - command + request = ?"
          answer: "perform"
          attack_use: |
            Adversaries find semantic equivalents that bypass keyword filters:
            "execute code" → "perform request" (similar embeddings, evades filter)
      
      embedding_dimensionality:
        typical_dimensions:
          small: "50-100 dims (simple tasks, small vocab)"
          medium: "200-300 dims (standard NLP)"
          large: "500-1024 dims (complex semantics, large vocab)"
        
        tradeoff:
          higher_dimensions:
            pros:
              - "Can capture more nuanced relationships"
              - "Better separation of distinct concepts"
              - "Higher model capacity"
            cons:
              - "More memory required"
              - "Slower computation"
              - "Risk of overfitting (too much capacity)"
          
          lower_dimensions:
            pros:
              - "Memory efficient"
              - "Faster computation"
              - "Forces model to generalize"
            cons:
              - "Limited capacity for nuance"
              - "May merge distinct concepts"
        
        common_choices:
          word2vec: "300 dims (empirically good tradeoff)"
          glove: "50, 100, 200, 300 dims (pre-trained options)"
          transformers: "512-768 dims (BERT, GPT)"
    
    implementation:
      cosine_similarity_and_analogies:
        language: python
        code: |
          import numpy as np
          from typing import Dict, List, Tuple
          
          class EmbeddingAnalyzer:
              """Analyze geometric properties of embeddings."""
              
              def __init__(self, embeddings: Dict[str, np.ndarray]):
                  """
                  Args:
                      embeddings: Dict mapping words to embedding vectors
                  """
                  self.embeddings = embeddings
              
              def cosine_similarity(self, word1: str, word2: str) -> float:
                  """Calculate cosine similarity between two words."""
                  if word1 not in self.embeddings or word2 not in self.embeddings:
                      raise ValueError(f"Word not in embeddings")
                  
                  vec1 = self.embeddings[word1]
                  vec2 = self.embeddings[word2]
                  
                  dot_product = np.dot(vec1, vec2)
                  magnitude = np.linalg.norm(vec1) * np.linalg.norm(vec2)
                  
                  return dot_product / magnitude if magnitude > 0 else 0.0
              
              def most_similar(self, word: str, top_k: int = 5) -> List[Tuple[str, float]]:
                  """Find most similar words to given word."""
                  if word not in self.embeddings:
                      raise ValueError(f"Word '{word}' not in embeddings")
                  
                  target_vec = self.embeddings[word]
                  similarities = []
                  
                  for other_word, other_vec in self.embeddings.items():
                      if other_word == word:
                          continue
                      
                      sim = self.cosine_similarity(word, other_word)
                      similarities.append((other_word, sim))
                  
                  similarities.sort(key=lambda x: x[1], reverse=True)
                  return similarities[:top_k]
              
              def analogy(self, a: str, b: str, c: str, top_k: int = 1) -> List[Tuple[str, float]]:
                  """
                  Solve analogy: a is to b as c is to ?
                  
                  Example: king is to man as queen is to woman
                  """
                  if not all(w in self.embeddings for w in [a, b, c]):
                      raise ValueError("All words must be in embeddings")
                  
                  # Compute target vector: vec(b) - vec(a) + vec(c)
                  target_vec = (self.embeddings[b] - self.embeddings[a] + 
                               self.embeddings[c])
                  
                  scores = []
                  for word, vec in self.embeddings.items():
                      if word in [a, b, c]:
                          continue
                      
                      dot = np.dot(target_vec, vec)
                      mag = np.linalg.norm(target_vec) * np.linalg.norm(vec)
                      sim = dot / mag if mag > 0 else 0
                      
                      scores.append((word, sim))
                  
                  scores.sort(key=lambda x: x[1], reverse=True)
                  return scores[:top_k]
          
          
          # Example with toy embeddings
          toy_embeddings = {
              'king': np.array([0.9, 0.8, 0.1, 0.2]),
              'queen': np.array([0.9, 0.8, 0.8, 0.7]),
              'man': np.array([0.5, 0.2, 0.1, 0.1]),
              'woman': np.array([0.5, 0.2, 0.8, 0.6]),
              'cat': np.array([0.3, 0.4, 0.5, 0.2]),
              'dog': np.array([0.35, 0.45, 0.48, 0.25]),
          }
          
          analyzer = EmbeddingAnalyzer(toy_embeddings)
          
          # Cosine similarity
          sim = analyzer.cosine_similarity('cat', 'dog')
          print(f"Similarity (cat, dog): {sim:.4f}")
          
          # Most similar words
          similar = analyzer.most_similar('cat', top_k=3)
          print(f"\nMost similar to 'cat':")
          for word, score in similar:
              print(f"  {word}: {score:.4f}")
          
          # Analogy
          answer = analyzer.analogy('king', 'man', 'queen', top_k=1)
          print(f"\nking - man + queen = {answer[0][0]} (score: {answer[0][1]:.4f})")
    
    security_implications:
      semantic_adversarial_examples: |
        Vector arithmetic enables adversarial attacks:
        - Find words with similar embeddings but different surface forms
        - "execute malicious code" → "perform harmful operations"
        - Same semantic meaning (similar embeddings) but bypasses keyword filters
        - Defense requires semantic filtering, not just keyword matching
      
      embedding_bias_exploitation: |
        Geometric relationships encode biases:
        - "doctor - man + woman" might yield "nurse" (gender bias)
        - Adversaries exploit: craft inputs triggering biased associations
        - Model makes discriminatory decisions based on embedded biases
      
      analogy_attacks: |
        Adversaries use analogies to find evasive phrasing:
        - "admin" - "administrator" + "privileged" = "root"
        - Find semantically equivalent but less monitored terms
        - Automated: search embedding space for analogy-based evasions

  - topic_number: 4
    title: "Limitations of Static Embeddings"
    
    overview: |
      Static embeddings assign one vector per word type, regardless of context. 
      "bank" (financial) and "bank" (river) get the same vector even though meanings 
      differ. This fundamental limitation motivates contextual embeddings (BERT, GPT).
    
    content:
      polysemy_problem:
        definition: "One word, multiple meanings (context-dependent)"
        
        examples:
          bank:
            meaning_1: "Financial institution"
            meaning_2: "River edge"
            problem: "Static embedding assigns one vector to both meanings"
          
          python:
            meaning_1: "Programming language"
            meaning_2: "Snake species"
            problem: "Same vector for completely different concepts"
        
        consequence: |
          Static embeddings compute average of all contexts:
          vec('bank') = average of (financial contexts + river contexts)
          Neither meaning is well-represented.
      
      context_dependence:
        problem: "Meaning changes with context, but embedding doesn't"
        
        example:
          sentence_1: "The bat flew out of the cave"
          sentence_2: "He swung the bat at the ball"
          
          static_embedding: "Both use vec('bat') - same vector"
          ideal_solution: "Different embeddings based on context"
      
      motivation_for_contextual_embeddings:
        solution_preview: |
          Contextual embeddings (BERT, GPT) solve these by computing 
          different embeddings based on full sentence context.
    
    security_implications:
      polysemy_exploitation: |
        Static embeddings can't distinguish word senses. Adversaries exploit:
        - "run this code" - 'run' could mean execute (dangerous) or mention (benign)
        - Static embedding averages both meanings → ambiguous for model
        - Defense requires contextual understanding (BERT-style)

key_takeaways:
  critical_concepts:
    - concept: "Dense embeddings replace sparse one-hot encoding"
      why_it_matters: "166x memory reduction, enables semantic similarity"
    
    - concept: "Distributional hypothesis: words in similar contexts have similar meanings"
      why_it_matters: "Foundation for learning embeddings from raw text"
    
    - concept: "Embedding spaces have geometric structure (distance = similarity)"
      why_it_matters: "Vector arithmetic performs analogies (king - man + woman = queen)"
    
    - concept: "Cosine similarity measures semantic similarity"
      why_it_matters: "Core metric for semantic search, similarity-based detection"
    
    - concept: "Static embeddings have one vector per word (can't handle polysemy)"
      why_it_matters: "Motivates contextual embeddings (BERT, transformers)"
  
  actionable_steps:
    - step: "Implement embedding lookup layer from scratch"
      verification: "Map token IDs to dense vectors, handle batches"
    
    - step: "Calculate cosine similarity between word vectors"
      verification: "Find most similar words, verify clustering"
    
    - step: "Perform vector arithmetic for analogies"
      verification: "king - man + woman ≈ queen"
    
    - step: "Visualize embedding spaces in 2D"
      verification: "Similar words appear close in visualization"
  
  security_principles:
    - principle: "Embeddings encode training data biases and associations"
      application: "Audit embeddings for sensitive correlations"
    
    - principle: "Semantic similarity enables adversarial attacks"
      application: "Keyword filters insufficient - need semantic filtering"
    
    - principle: "Vector arithmetic reveals model knowledge"
      application: "Probe embeddings to extract learned associations"
  
  integration_with_book:
    from_section_3_1:
      - "Sparse one-hot representations (problem embeddings solve)"
      - "Vocabulary and token IDs"
    
    from_section_3_2:
      - "Tokens become input to embedding lookup"
      - "Subword tokenization helps with OOV"
    
    to_next_section:
      - "Section 3.4: Word2Vec - how to LEARN these embeddings"
      - "Skip-gram and CBOW architectures"
  
  final_thoughts: |
    Word embeddings transform discrete tokens into continuous vectors where 
    semantic similarity becomes geometric proximity. This enables neural networks 
    to generalize: learning about "cat" helps with "kitten" automatically.
    
    From a security perspective: embeddings encode everything the model knows - 
    including biases, associations, and patterns from training data. Adversaries 
    exploit this to find semantically similar but filter-evading phrases.
    
    Next: Section 3.4 teaches how to LEARN these embeddings using Word2Vec.

---
