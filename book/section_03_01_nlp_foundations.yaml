# section_03_01_nlp_foundations.yaml

---
document_info:
  title: "Natural Language Processing Foundations"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 1
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Foundational concepts in Natural Language Processing: from rule-based systems to neural approaches, understanding text as data, language modeling, and security implications"
  estimated_pages: 6
  tags:
    - nlp-foundations
    - language-modeling
    - text-representation
    - n-grams
    - perplexity
    - security-context

section_overview:
  title: "Natural Language Processing Foundations"
  number: "3.1"
  
  purpose: |
    Natural Language Processing (NLP) is fundamentally different from the vision and 
    tabular data problems we've seen in Chapters 1-2. Text is discrete, symbolic, and 
    sequential - words are symbols from a finite vocabulary, not continuous pixel values. 
    This section establishes the foundational concepts you need before diving into 
    modern neural approaches.
    
    For security engineers: Understanding how language models work reveals both their 
    capabilities and limitations. You'll learn why LLMs can be fooled by carefully 
    crafted prompts, how they memorize training data, and why text-based attacks 
    (prompt injection, adversarial examples) differ fundamentally from image attacks.
  
  learning_objectives:
    conceptual:
      - "Understand the evolution from rule-based to statistical to neural NLP"
      - "Grasp why text representation is fundamentally different from images"
      - "Learn language modeling as probability distribution over sequences"
      - "Understand the curse of dimensionality in NLP context"
    
    practical:
      - "Implement n-gram language models from scratch"
      - "Calculate perplexity to evaluate language models"
      - "Build vocabulary with frequency analysis"
      - "Create text preprocessing pipeline"
    
    security_focused:
      - "Recognize how language models learn patterns - including malicious ones"
      - "Understand statistical vulnerabilities in text processing"
      - "Identify how training data influences model behavior"
      - "See why text is a unique attack vector"
  
  prerequisites:
    knowledge:
      - "Basic probability theory (Chapter 1: Probability Review)"
      - "Understanding of neural networks (Chapter 2: Deep Learning Basics)"
      - "Gradient descent and backpropagation (Chapter 1)"
      - "Python programming and NumPy"
    
    skills:
      - "Implementing algorithms from mathematical descriptions"
      - "Working with sequences and temporal data"
      - "Basic text manipulation in Python"
  
  key_transitions:
    from_chapter_2: |
      In Chapter 2, we worked with sequences (RNNs, LSTMs) processing continuous 
      values. Now we shift to discrete tokens - words from a fixed vocabulary. The 
      sequential nature remains, but the discrete symbols create unique challenges 
      and opportunities.
    
    to_next_section: |
      Once you understand language modeling fundamentals, Section 3.2 tackles 
      tokenization: how to convert raw text into the tokens that models actually process.

topics:
  - topic_number: 1
    title: "The Evolution of NLP: Three Paradigm Shifts"
    
    overview: |
      NLP has undergone three major paradigm shifts: rule-based systems (1950s-1980s), 
      statistical methods (1990s-2010s), and neural approaches (2010s-present). 
      Understanding this evolution reveals why modern LLMs work the way they do - 
      and what vulnerabilities they inherit from each paradigm.
    
    content:
      rule_based_era:
        period: "1950s-1980s"
        approach: "Hand-crafted grammars and lexicons"
        examples:
          - "ELIZA (1966): Pattern matching chatbot"
          - "SHRDLU (1970): Natural language understanding in blocks world"
          - "Expert systems with linguistic rules"
        
        characteristics:
          - "Explicitly programmed linguistic knowledge"
          - "Deterministic: same input → same output"
          - "Brittle: fails on unseen patterns"
          - "Labor-intensive: requires linguistics expertise"
        
        limitations:
          - "Cannot handle ambiguity well"
          - "No learning from data"
          - "Doesn't scale to open-domain text"
          - "Rules conflict in complex scenarios"
        
        security_angle: |
          Rule-based systems are predictable but rigid. Adversaries can craft inputs 
          that exploit gaps in rule coverage. However, their determinism makes them 
          auditable - you can trace exactly why a decision was made.
      
      statistical_era:
        period: "1990s-2010s"
        approach: "Learn patterns from data using probabilistic models"
        examples:
          - "Hidden Markov Models for POS tagging"
          - "N-gram language models"
          - "Topic models (LDA)"
          - "Statistical machine translation (phrase-based)"
        
        characteristics:
          - "Learn from corpus data"
          - "Probabilistic: model uncertainty"
          - "Generalize to unseen patterns"
          - "Require less expert knowledge"
        
        key_insight: "Distributional hypothesis: 'You shall know a word by the company it keeps' (Harris, 1954)"
        
        limitations:
          - "Struggle with long-range dependencies"
          - "High-dimensional sparse representations"
          - "Feature engineering still required"
          - "Limited semantic understanding"
        
        security_angle: |
          Statistical models learn from whatever data you give them. Adversarial 
          data in training corpus → model learns adversarial patterns. This is the 
          origin of data poisoning attacks against ML systems.
      
      neural_era:
        period: "2010s-present"
        approach: "Deep neural networks learn hierarchical representations"
        examples:
          - "Word2Vec, GloVe (2013-2014): Dense word embeddings"
          - "Sequence-to-sequence models (2014)"
          - "Attention mechanism (2015)"
          - "Transformers (2017)"
          - "BERT, GPT, T5 (2018-2019)"
          - "ChatGPT, GPT-4, Claude (2020s)"
        
        characteristics:
          - "End-to-end learning from raw text"
          - "Dense, continuous representations"
          - "Capture semantic meaning"
          - "Transfer learning: pre-train then fine-tune"
        
        breakthrough: "Attention mechanism solves long-range dependency problem"
        
        current_state:
          - "Models with billions of parameters"
          - "Trained on internet-scale corpora"
          - "Emergent capabilities at scale"
          - "Few-shot and zero-shot learning"
        
        security_angle: |
          Neural models are black boxes - harder to audit but more powerful. They 
          memorize training data (privacy risk), can be manipulated through prompts 
          (injection attacks), and their emergent behaviors are unpredictable 
          (alignment challenges).
    
    implementation:
      paradigm_comparison:
        description: "Compare three approaches on the same task: sentiment classification"
        
        rule_based_example:
          language: python
          code: |
            def rule_based_sentiment(text):
                """Simple rule-based sentiment classifier."""
                positive_words = {'good', 'great', 'excellent', 'amazing', 'love'}
                negative_words = {'bad', 'terrible', 'awful', 'hate', 'poor'}
                
                words = text.lower().split()
                pos_count = sum(1 for w in words if w in positive_words)
                neg_count = sum(1 for w in words if w in negative_words)
                
                if pos_count > neg_count:
                    return "positive"
                elif neg_count > pos_count:
                    return "negative"
                else:
                    return "neutral"
            
            # Example
            print(rule_based_sentiment("This is a great movie"))  # positive
            print(rule_based_sentiment("Not good at all"))  # negative (misses negation!)
          
          weakness: "Cannot handle negation, sarcasm, or context"
        
        statistical_example:
          language: python
          code: |
            import numpy as np
            from collections import Counter
            
            class NaiveBayesSentiment:
                """Statistical sentiment classifier using Naive Bayes."""
                
                def __init__(self):
                    self.word_probs = {}
                    self.class_probs = {}
                
                def train(self, texts, labels):
                    """Train on labeled examples."""
                    # Count classes
                    class_counts = Counter(labels)
                    total = len(labels)
                    self.class_probs = {c: count/total for c, count in class_counts.items()}
                    
                    # Count words per class
                    word_counts = {c: Counter() for c in class_counts}
                    for text, label in zip(texts, labels):
                        words = text.lower().split()
                        word_counts[label].update(words)
                    
                    # Calculate P(word|class) with Laplace smoothing
                    vocab = set(word for counts in word_counts.values() for word in counts)
                    vocab_size = len(vocab)
                    
                    self.word_probs = {}
                    for cls in class_counts:
                        total_words = sum(word_counts[cls].values())
                        self.word_probs[cls] = {
                            word: (word_counts[cls][word] + 1) / (total_words + vocab_size)
                            for word in vocab
                        }
                
                def predict(self, text):
                    """Predict sentiment."""
                    words = text.lower().split()
                    
                    # Calculate log probabilities for each class
                    log_probs = {}
                    for cls in self.class_probs:
                        log_prob = np.log(self.class_probs[cls])
                        for word in words:
                            if word in self.word_probs[cls]:
                                log_prob += np.log(self.word_probs[cls][word])
                        log_probs[cls] = log_prob
                    
                    return max(log_probs, key=log_probs.get)
            
            # Example usage
            classifier = NaiveBayesSentiment()
            texts = ["great movie", "terrible film", "amazing show", "awful performance"]
            labels = ["positive", "negative", "positive", "negative"]
            classifier.train(texts, labels)
            
            print(classifier.predict("great show"))  # positive
          
          strength: "Learns from data, handles unseen word combinations"
          weakness: "Still treats words independently (bag-of-words)"
        
        neural_example:
          note: "We'll implement neural sentiment classifiers in later sections using embeddings and transformers"
    
    key_insights:
      - insight: "Each paradigm shift increased model capacity but reduced interpretability"
        security_implication: "Harder to audit → more security blind spots"
      
      - insight: "Neural models learn implicit rules from data rather than explicit programming"
        security_implication: "Model behavior reflects training data biases and patterns"
      
      - insight: "Modern LLMs combine statistical learning with massive scale"
        security_implication: "Emergent behaviors are unpredictable; adversarial robustness harder to guarantee"

  - topic_number: 2
    title: "Text as Data: Discrete Symbols vs Continuous Values"
    
    overview: |
      Unlike images (continuous pixel values) or tabular data (mixed types), text is 
      fundamentally discrete and symbolic. Words are tokens from a finite vocabulary. 
      This discrete nature creates unique challenges for machine learning and opens 
      specific attack vectors.
    
    content:
      discrete_nature:
        description: "Text is a sequence of discrete symbols from a finite alphabet/vocabulary"
        
        vocabulary_concept:
          definition: "Set of all unique tokens the model can process"
          examples:
            - "English words: vocabulary ~170,000 words"
            - "Characters: vocabulary ~100 (a-z, A-Z, 0-9, punctuation)"
            - "Subwords (BPE): vocabulary ~50,000 tokens"
          
          properties:
            - "Finite: vocabulary has fixed size"
            - "Closed: new words are 'unknown' (UNK token)"
            - "Symbolic: each token is a discrete unit"
            - "Sparse: most words don't appear in most documents"
        
        contrast_with_images:
          images:
            - "Continuous pixel values (0-255 or 0.0-1.0)"
            - "Spatial structure (2D grid)"
            - "Local correlations (nearby pixels similar)"
            - "Small perturbations: smooth changes"
          
          text:
            - "Discrete token IDs (integers 0 to vocab_size-1)"
            - "Sequential structure (1D sequence)"
            - "Syntactic/semantic structure (grammar, meaning)"
            - "Small perturbations: completely different token"
        
        implications:
          gradient_flow: |
            You can't compute gradients through discrete tokens directly. Can't do:
            ∂Loss/∂token because tokens are integers. Solution: learn continuous 
            embeddings for each token, compute gradients through embeddings.
          
          adversarial_examples: |
            In images: add imperceptible noise (continuous perturbation).
            In text: change one word → discrete jump to different meaning.
            Text adversarial examples must remain discrete and grammatical.
          
          search_space: |
            Finding adversarial text is discrete optimization over vocabulary^length.
            For 10-word sequence with 50k vocabulary: 50,000^10 possibilities!
      
      sparse_vs_dense:
        sparse_representations:
          name: "One-hot encoding"
          description: "Represent each word as a vector with 1 at word index, 0 elsewhere"
          
          example:
            vocabulary: "['cat', 'dog', 'bird', 'fish']"
            encoding:
              - "cat:  [1, 0, 0, 0]"
              - "dog:  [0, 1, 0, 0]"
              - "bird: [0, 0, 1, 0]"
              - "fish: [0, 0, 0, 1]"
          
          properties:
            dimensionality: "vocab_size (typically 10k-100k)"
            sparsity: "99.99% zeros for large vocabularies"
            similarity: "All words equally dissimilar (dot product = 0)"
            memory: "Extremely wasteful for large vocabularies"
          
          problems:
            - "No notion of word similarity"
            - "Curse of dimensionality"
            - "Can't generalize to unseen word combinations"
        
        dense_representations:
          name: "Word embeddings"
          description: "Represent each word as dense, low-dimensional vector"
          
          example:
            vocabulary: "['cat', 'dog', 'bird', 'fish']"
            embedding_dim: 3
            encoding:
              - "cat:  [0.8, 0.2, -0.1]  (mammal, domestic, small)"
              - "dog:  [0.7, 0.3,  0.1]  (mammal, domestic, medium)"
              - "bird: [-0.2, 0.1, 0.9]  (non-mammal, wild, small)"
              - "fish: [-0.1, -0.8, 0.3] (non-mammal, wild, aquatic)"
          
          properties:
            dimensionality: "50-300 typically (much smaller than vocab)"
            density: "All values non-zero"
            similarity: "Similar words have similar vectors (cosine similarity)"
            learned: "Embeddings learned from data"
          
          advantages:
            - "Capture semantic similarity"
            - "Lower dimensional (memory efficient)"
            - "Generalize better (similar words → similar behavior)"
            - "Foundation for neural NLP"
    
    implementation:
      one_hot_encoding:
        language: python
        code: |
          import numpy as np
          
          class OneHotEncoder:
              """One-hot encoding for text tokens."""
              
              def __init__(self, vocabulary):
                  """
                  Args:
                      vocabulary: List of unique tokens
                  """
                  self.vocabulary = vocabulary
                  self.vocab_size = len(vocabulary)
                  self.token_to_id = {token: i for i, token in enumerate(vocabulary)}
                  self.id_to_token = {i: token for i, token in enumerate(vocabulary)}
              
              def encode(self, token):
                  """Encode single token as one-hot vector."""
                  if token not in self.token_to_id:
                      raise ValueError(f"Token '{token}' not in vocabulary")
                  
                  one_hot = np.zeros(self.vocab_size)
                  one_hot[self.token_to_id[token]] = 1
                  return one_hot
              
              def encode_sequence(self, tokens):
                  """Encode sequence of tokens."""
                  return np.array([self.encode(token) for token in tokens])
              
              def decode(self, one_hot):
                  """Decode one-hot vector to token."""
                  token_id = np.argmax(one_hot)
                  return self.id_to_token[token_id]
          
          # Example usage
          vocab = ['the', 'cat', 'sat', 'on', 'mat']
          encoder = OneHotEncoder(vocab)
          
          # Encode "cat"
          cat_vector = encoder.encode('cat')
          print(f"'cat' encoded: {cat_vector}")
          # Output: [0. 1. 0. 0. 0.]
          
          # Encode sentence
          sentence = ['the', 'cat', 'sat']
          encoded = encoder.encode_sequence(sentence)
          print(f"Sentence shape: {encoded.shape}")
          # Output: (3, 5) - 3 tokens, 5-dimensional vectors
          
          # Problem: No similarity between words
          cat_enc = encoder.encode('cat')
          sat_enc = encoder.encode('sat')
          similarity = np.dot(cat_enc, sat_enc)
          print(f"Similarity between 'cat' and 'sat': {similarity}")
          # Output: 0.0 (all words equally dissimilar!)
      
      vocabulary_builder:
        language: python
        code: |
          from collections import Counter
          import re
          
          class VocabularyBuilder:
              """Build vocabulary from corpus with frequency analysis."""
              
              def __init__(self, min_freq=1, max_vocab_size=None):
                  """
                  Args:
                      min_freq: Minimum frequency to include word
                      max_vocab_size: Maximum vocabulary size (None = unlimited)
                  """
                  self.min_freq = min_freq
                  self.max_vocab_size = max_vocab_size
                  self.word_counts = Counter()
                  self.vocabulary = []
                  self.token_to_id = {}
                  self.id_to_token = {}
              
              def tokenize(self, text):
                  """Simple whitespace tokenization (improved versions in Section 3.2)."""
                  # Lowercase and split on whitespace/punctuation
                  text = text.lower()
                  tokens = re.findall(r'\b\w+\b', text)
                  return tokens
              
              def build_from_corpus(self, corpus):
                  """
                  Build vocabulary from corpus.
                  
                  Args:
                      corpus: List of text strings
                  """
                  # Count all words
                  for text in corpus:
                      tokens = self.tokenize(text)
                      self.word_counts.update(tokens)
                  
                  # Filter by frequency
                  filtered = [(word, count) for word, count in self.word_counts.items()
                             if count >= self.min_freq]
                  
                  # Sort by frequency (most common first)
                  filtered.sort(key=lambda x: x[1], reverse=True)
                  
                  # Limit vocabulary size
                  if self.max_vocab_size:
                      filtered = filtered[:self.max_vocab_size]
                  
                  # Add special tokens
                  special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']
                  self.vocabulary = special_tokens + [word for word, _ in filtered]
                  
                  # Create mappings
                  self.token_to_id = {token: i for i, token in enumerate(self.vocabulary)}
                  self.id_to_token = {i: token for i, token in enumerate(self.vocabulary)}
              
              def encode(self, text):
                  """Convert text to token IDs."""
                  tokens = self.tokenize(text)
                  unk_id = self.token_to_id['<UNK>']
                  return [self.token_to_id.get(token, unk_id) for token in tokens]
              
              def decode(self, token_ids):
                  """Convert token IDs back to text."""
                  return ' '.join([self.id_to_token[tid] for tid in token_ids])
              
              def get_statistics(self):
                  """Get vocabulary statistics."""
                  total_words = sum(self.word_counts.values())
                  return {
                      'vocab_size': len(self.vocabulary),
                      'total_words': total_words,
                      'unique_words': len(self.word_counts),
                      'coverage': len(self.vocabulary) / len(self.word_counts),
                      'most_common': self.word_counts.most_common(10)
                  }
          
          # Example usage
          corpus = [
              "the cat sat on the mat",
              "the dog sat on the log",
              "the cat and the dog are friends"
          ]
          
          builder = VocabularyBuilder(min_freq=1, max_vocab_size=20)
          builder.build_from_corpus(corpus)
          
          print(f"Vocabulary size: {len(builder.vocabulary)}")
          print(f"Top words: {builder.word_counts.most_common(5)}")
          
          # Encode sentence
          encoded = builder.encode("the cat sat")
          print(f"Encoded: {encoded}")
          print(f"Decoded: {builder.decode(encoded)}")
          
          # Unknown word handling
          encoded_unk = builder.encode("the cat jumped")  # 'jumped' not in vocab
          print(f"With unknown: {encoded_unk}")  # <UNK> token used
    
    security_implications:
      discrete_attack_surface: |
        Text's discrete nature creates unique attack vectors:
        - Character-level attacks: homoglyphs (а vs a), zero-width characters
        - Word-level attacks: synonym substitution, paraphrasing
        - Token-level attacks: exploiting tokenization boundaries
      
      vocabulary_manipulation: |
        Adversaries can:
        - Inject rare words that become <UNK> tokens (information loss)
        - Craft inputs with words at vocabulary boundaries
        - Exploit frequency-based filtering (poison common words)
      
      sparse_representation_risks: |
        One-hot encoding makes every word equally different - models can't detect 
        semantic similarity attacks. "Execute malicious code" vs "Run harmful script" 
        look completely different to one-hot encoders.

  - topic_number: 3
    title: "Language Modeling: Probability Distributions Over Sequences"
    
    overview: |
      A language model assigns probabilities to sequences of words. Given a sequence, 
      it tells you how likely that sequence is to occur in natural language. This is 
      the foundation for text generation, machine translation, and understanding.
    
    content:
      language_model_definition:
        mathematical_formulation: |
          A language model estimates P(w₁, w₂, ..., wₙ) - the probability of 
          observing word sequence [w₁, w₂, ..., wₙ].
          
          By chain rule of probability:
          P(w₁, w₂, ..., wₙ) = P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × ... × P(wₙ|w₁,...,wₙ₋₁)
          
          Each term P(wᵢ|w₁,...,wᵢ₋₁) is the probability of word wᵢ given all previous words.
        
        intuition: |
          "The cat sat on the ___"
          P(mat | the cat sat on the) = 0.4  (likely)
          P(car | the cat sat on the) = 0.01 (unlikely)
          P(pizza | the cat sat on the) = 0.001 (very unlikely)
        
        applications:
          text_generation: "Sample next word from P(w|context), repeat"
          machine_translation: "Choose translation maximizing P(target|source)"
          speech_recognition: "Choose transcription maximizing P(words|audio)"
          spelling_correction: "Choose correction maximizing P(sentence)"
          perplexity: "Evaluate how well model predicts test data"
      
      n_gram_models:
        definition: "Approximate P(wᵢ|w₁,...,wᵢ₋₁) using only last n-1 words"
        
        markov_assumption: |
          Instead of conditioning on entire history, use fixed window:
          P(wᵢ|w₁,...,wᵢ₋₁) ≈ P(wᵢ|wᵢ₋ₙ₊₁,...,wᵢ₋₁)
          
          "Next word depends only on previous n-1 words, not entire history"
        
        types:
          unigram:
            n: 1
            model: "P(wᵢ)"
            description: "Each word independent"
            example: "P(cat) = count(cat) / total_words"
            problem: "Generates word salad: 'cat the on mat sat'"
          
          bigram:
            n: 2
            model: "P(wᵢ|wᵢ₋₁)"
            description: "Word depends on previous word"
            example: "P(sat|cat) = count(cat sat) / count(cat)"
            improvement: "Better local coherence: 'the cat sat on'"
          
          trigram:
            n: 3
            model: "P(wᵢ|wᵢ₋₂,wᵢ₋₁)"
            description: "Word depends on previous 2 words"
            example: "P(on|cat sat) = count(cat sat on) / count(cat sat)"
            improvement: "Even better: 'the cat sat on the mat'"
        
        estimation:
          maximum_likelihood: |
            Estimate probabilities from counts in training corpus:
            
            P(wᵢ|wᵢ₋₁) = count(wᵢ₋₁, wᵢ) / count(wᵢ₋₁)
            
            Problem: many n-grams never seen in training → zero probability!
          
          smoothing:
            add_one: "Add 1 to all counts (Laplace smoothing)"
            add_k: "Add small constant k < 1"
            interpolation: "Combine unigram, bigram, trigram estimates"
            backoff: "Use lower-order n-gram if higher-order unseen"
        
        tradeoffs:
          higher_n:
            pros:
              - "More context → better predictions"
              - "Captures longer dependencies"
            cons:
              - "Exponentially more parameters (vocab^n)"
              - "Most n-grams never seen (data sparsity)"
              - "Doesn't generalize well"
          
          practical: "Trigrams (n=3) often best tradeoff for statistical LMs"
      
      perplexity:
        definition: "Perplexity measures how 'surprised' the model is by test data"
        
        formula: |
          Perplexity(test) = 2^(-1/N * Σ log₂ P(wᵢ|context))
          
          Lower perplexity = better model (less surprised by actual text)
        
        interpretation:
          - "Perplexity of 100: model as uncertain as choosing uniformly from 100 words"
          - "Perplexity of 10: model narrowed it down to ~10 candidates"
          - "Lower is better (perfect model: perplexity = 1)"
        
        relationship_to_entropy: |
          Perplexity = 2^H where H is cross-entropy:
          H = -1/N * Σ log₂ P(wᵢ|context)
          
          Entropy measures average bits needed to encode text using model
    
    implementation:
      n_gram_language_model:
        language: python
        code: |
          import numpy as np
          from collections import defaultdict, Counter
          
          class NGramLanguageModel:
              """N-gram language model with add-k smoothing."""
              
              def __init__(self, n=2, k=1.0):
                  """
                  Args:
                      n: N-gram order (2 for bigram, 3 for trigram)
                      k: Smoothing parameter (1.0 = add-one smoothing)
                  """
                  self.n = n
                  self.k = k
                  self.ngram_counts = defaultdict(Counter)
                  self.context_counts = Counter()
                  self.vocabulary = set()
              
              def tokenize(self, text):
                  """Simple tokenization."""
                  return text.lower().split()
              
              def get_ngrams(self, tokens):
                  """Extract n-grams from token sequence."""
                  # Add start/end tokens
                  padded = ['<START>'] * (self.n - 1) + tokens + ['<END>']
                  
                  ngrams = []
                  for i in range(len(padded) - self.n + 1):
                      context = tuple(padded[i:i + self.n - 1])
                      word = padded[i + self.n - 1]
                      ngrams.append((context, word))
                  
                  return ngrams
              
              def train(self, corpus):
                  """
                  Train on corpus.
                  
                  Args:
                      corpus: List of text strings
                  """
                  for text in corpus:
                      tokens = self.tokenize(text)
                      self.vocabulary.update(tokens)
                      
                      ngrams = self.get_ngrams(tokens)
                      for context, word in ngrams:
                          self.ngram_counts[context][word] += 1
                          self.context_counts[context] += 1
              
              def get_probability(self, word, context):
                  """
                  Calculate P(word|context) with add-k smoothing.
                  
                  Args:
                      word: Target word
                      context: Tuple of previous words
                  
                  Returns:
                      Smoothed probability
                  """
                  vocab_size = len(self.vocabulary) + 2  # +2 for START, END
                  
                  # Numerator: count(context, word) + k
                  count = self.ngram_counts[context][word]
                  numerator = count + self.k
                  
                  # Denominator: count(context) + k * vocab_size
                  context_count = self.context_counts[context]
                  denominator = context_count + self.k * vocab_size
                  
                  return numerator / denominator
              
              def sentence_probability(self, text):
                  """Calculate probability of entire sentence."""
                  tokens = self.tokenize(text)
                  ngrams = self.get_ngrams(tokens)
                  
                  log_prob = 0.0
                  for context, word in ngrams:
                      prob = self.get_probability(word, context)
                      log_prob += np.log2(prob)
                  
                  return 2 ** log_prob
              
              def perplexity(self, test_corpus):
                  """
                  Calculate perplexity on test corpus.
                  
                  Args:
                      test_corpus: List of test sentences
                  
                  Returns:
                      Perplexity value (lower is better)
                  """
                  total_log_prob = 0.0
                  total_words = 0
                  
                  for text in test_corpus:
                      tokens = self.tokenize(text)
                      ngrams = self.get_ngrams(tokens)
                      
                      for context, word in ngrams:
                          prob = self.get_probability(word, context)
                          total_log_prob += np.log2(prob)
                          total_words += 1
                  
                  # Perplexity = 2^(-1/N * sum(log P))
                  avg_log_prob = total_log_prob / total_words
                  return 2 ** (-avg_log_prob)
              
              def generate(self, context=None, max_length=20):
                  """
                  Generate text from language model.
                  
                  Args:
                      context: Starting context (tuple of words)
                      max_length: Maximum tokens to generate
                  
                  Returns:
                      Generated text
                  """
                  if context is None:
                      context = ('<START>',) * (self.n - 1)
                  
                  generated = list(context) if context != ('<START>',) * (self.n - 1) else []
                  current_context = context
                  
                  for _ in range(max_length):
                      # Get probability distribution for next word
                      word_probs = {}
                      for word in self.vocabulary:
                          word_probs[word] = self.get_probability(word, current_context)
                      
                      # Add end token
                      word_probs['<END>'] = self.get_probability('<END>', current_context)
                      
                      # Sample next word
                      words = list(word_probs.keys())
                      probs = list(word_probs.values())
                      probs = np.array(probs) / sum(probs)  # Normalize
                      
                      next_word = np.random.choice(words, p=probs)
                      
                      if next_word == '<END>':
                          break
                      
                      generated.append(next_word)
                      
                      # Update context (sliding window)
                      current_context = tuple(generated[-(self.n-1):])
                  
                  return ' '.join(generated)
          
          # Example usage
          corpus = [
              "the cat sat on the mat",
              "the cat sat on the log",
              "the dog sat on the log",
              "the dog sat on the mat",
              "the cat and the dog are friends"
          ]
          
          # Train bigram model
          bigram_model = NGramLanguageModel(n=2, k=1.0)
          bigram_model.train(corpus)
          
          # Calculate probabilities
          prob_mat = bigram_model.get_probability('mat', ('the',))
          prob_car = bigram_model.get_probability('car', ('the',))
          print(f"P(mat|the) = {prob_mat:.4f}")
          print(f"P(car|the) = {prob_car:.4f}")  # Unseen, but smoothed > 0
          
          # Sentence probability
          test_sentence = "the cat sat on the mat"
          prob = bigram_model.sentence_probability(test_sentence)
          print(f"P('{test_sentence}') = {prob:.6f}")
          
          # Perplexity
          test_corpus = ["the dog sat on the mat"]
          perp = bigram_model.perplexity(test_corpus)
          print(f"Perplexity: {perp:.2f}")
          
          # Generate text
          generated = bigram_model.generate(max_length=10)
          print(f"Generated: {generated}")
      
      perplexity_calculator:
        language: python
        code: |
          def calculate_perplexity(model, test_corpus):
              """
              Calculate perplexity from scratch.
              
              Args:
                  model: Language model with get_probability method
                  test_corpus: List of test sentences
              
              Returns:
                  perplexity: Float value (lower is better)
                  cross_entropy: Bits per word
              """
              total_log_prob = 0.0
              total_words = 0
              
              for text in test_corpus:
                  tokens = model.tokenize(text)
                  ngrams = model.get_ngrams(tokens)
                  
                  for context, word in ngrams:
                      prob = model.get_probability(word, context)
                      
                      # Avoid log(0)
                      if prob > 0:
                          total_log_prob += np.log2(prob)
                      else:
                          total_log_prob += np.log2(1e-10)  # Small epsilon
                      
                      total_words += 1
              
              # Cross-entropy (average negative log probability)
              cross_entropy = -total_log_prob / total_words
              
              # Perplexity = 2^cross_entropy
              perplexity = 2 ** cross_entropy
              
              return perplexity, cross_entropy
          
          # Compare different n-gram models
          for n in [1, 2, 3]:
              model = NGramLanguageModel(n=n, k=1.0)
              model.train(corpus)
              
              test = ["the cat sat on the mat"]
              perp, ce = calculate_perplexity(model, test)
              
              print(f"{n}-gram model:")
              print(f"  Perplexity: {perp:.2f}")
              print(f"  Cross-entropy: {ce:.2f} bits/word")
    
    security_implications:
      training_data_leakage: |
        Language models memorize training data. High-probability sequences in the 
        model likely appeared in training. Adversaries can:
        - Extract training data by sampling from the model
        - Query the model to check if specific sequences were in training
        - Infer sensitive information from learned probabilities
      
      pattern_learning: |
        LMs learn ALL patterns in training data:
        - Malicious patterns: "DROP TABLE users" gets high probability if in training
        - Bias patterns: Model reproduces societal biases from text
        - Adversarial patterns: If adversaries poison training data, model learns it
      
      perplexity_as_detector: |
        Perplexity can detect anomalous inputs:
        - Adversarial text often has high perplexity (unusual for the model)
        - But sophisticated attacks craft low-perplexity malicious text
        - Defense: flag inputs with perplexity outside expected range

  - topic_number: 4
    title: "The Curse of Dimensionality in NLP"
    
    overview: |
      As n increases in n-gram models, the number of possible n-grams grows 
      exponentially (vocab^n). Most never appear in training data → zero probability. 
      This is the curse of dimensionality, and it's why we need neural approaches.
    
    content:
      exponential_growth:
        problem_statement: |
          Vocabulary size V = 50,000 (typical)
          - Unigrams: V = 50,000 possibilities
          - Bigrams: V² = 2.5 billion possibilities
          - Trigrams: V³ = 125 trillion possibilities
          - 4-grams: V⁴ = 6.25 quadrillion possibilities
        
        data_sparsity: |
          Even with billions of words of training data, most high-order n-grams 
          never seen. Example with V=50k:
          - Need ~50k examples to estimate unigrams reasonably
          - Need ~2.5B examples for bigrams (much more than typical corpus)
          - Need ~125T examples for trigrams (impossible!)
        
        consequence: "Most n-gram probabilities are zero or poorly estimated"
      
      smoothing_limitations:
        add_k_problems:
          - "Distributes probability mass uniformly across unseen n-grams"
          - "Doesn't distinguish 'cat sat' (likely) from 'cat xylophone' (impossible)"
          - "Doesn't capture that 'cat' and 'dog' should have similar distributions"
        
        backoff_problems:
          - "Ignores higher-order context when available"
          - "Still suffers from sparsity at each level"
          - "Doesn't generalize to novel combinations"
      
      why_neural_networks:
        continuous_representations:
          problem: "Discrete words → no similarity structure"
          solution: "Learn continuous embeddings where similar words are nearby"
          benefit: "Model can generalize: if 'cat sat' likely, then 'dog sat' likely too"
        
        parameter_sharing:
          problem: "Each n-gram has independent parameters → can't share knowledge"
          solution: "Words share embedding parameters → similar words share representations"
          benefit: "Seeing 'cat sat' helps model predict 'dog sat' (same embedding patterns)"
        
        compositionality:
          problem: "Can't represent novel combinations of seen words"
          solution: "Compose meaning from word embeddings"
          benefit: "Never saw 'purple elephant' but can still understand it"
    
    implementation:
      curse_of_dimensionality_demo:
        language: python
        code: |
          def analyze_ngram_coverage(corpus, vocab_size=10000, max_n=5):
              """
              Demonstrate curse of dimensionality by calculating n-gram coverage.
              
              Args:
                  corpus: List of texts
                  vocab_size: Vocabulary size for analysis
                  max_n: Maximum n-gram order to analyze
              """
              from collections import Counter
              import itertools
              
              # Tokenize corpus
              all_tokens = []
              for text in corpus:
                  all_tokens.extend(text.lower().split())
              
              # Get top vocab_size words
              word_counts = Counter(all_tokens)
              top_words = [word for word, _ in word_counts.most_common(vocab_size)]
              vocab = set(top_words)
              
              print(f"Corpus statistics:")
              print(f"  Total tokens: {len(all_tokens):,}")
              print(f"  Unique tokens: {len(word_counts):,}")
              print(f"  Vocabulary size: {vocab_size:,}")
              print()
              
              for n in range(1, max_n + 1):
                  # Theoretical n-grams
                  theoretical_ngrams = vocab_size ** n
                  
                  # Actual n-grams in corpus
                  actual_ngrams = set()
                  for text in corpus:
                      tokens = [t for t in text.lower().split() if t in vocab]
                      for i in range(len(tokens) - n + 1):
                          ngram = tuple(tokens[i:i+n])
                          actual_ngrams.add(ngram)
                  
                  # Coverage
                  coverage = len(actual_ngrams) / theoretical_ngrams * 100
                  
                  print(f"{n}-grams:")
                  print(f"  Theoretical: {theoretical_ngrams:,}")
                  print(f"  Observed: {len(actual_ngrams):,}")
                  print(f"  Coverage: {coverage:.6f}%")
                  print(f"  Sparsity: {100 - coverage:.6f}%")
                  print()
          
          # Example with small corpus
          example_corpus = [
              "the cat sat on the mat",
              "the dog sat on the log",
              "the bird flew over the tree",
              "a cat and a dog are friends"
          ] * 100  # Repeat to simulate larger corpus
          
          analyze_ngram_coverage(example_corpus, vocab_size=100, max_n=4)
          
          # Expected output shows exponential growth in theoretical n-grams
          # but linear growth in observed n-grams → massive sparsity
      
      generalization_comparison:
        language: python
        code: |
          def compare_ngram_vs_embedding_generalization():
              """
              Show why embeddings generalize better than n-grams.
              """
              # Simulated n-gram model
              ngram_model = {
                  ('the', 'cat'): {'sat': 0.8, 'ran': 0.2},
                  ('the', 'dog'): {'sat': 0.7, 'ran': 0.3},
              }
              
              # Never seen ('the', 'bird') → can't predict
              context = ('the', 'bird')
              prob = ngram_model.get(context, {}).get('sat', 0.0)
              print(f"N-gram: P(sat|the bird) = {prob}")  # 0.0 (unseen)
              
              # Simulated embedding model (similar words → similar predictions)
              # Embeddings (simplified 2D for illustration)
              embeddings = {
                  'cat': np.array([0.9, 0.1]),   # Animal, domestic
                  'dog': np.array([0.85, 0.15]), # Animal, domestic (similar to cat)
                  'bird': np.array([0.8, 0.05]), # Animal, wild (still similar)
              }
              
              # Model learned: animals with embedding ~ [0.8-0.9, 0.0-0.2] often 'sat'
              # So it can predict 'bird sat' even if never seen
              bird_emb = embeddings['bird']
              cat_emb = embeddings['cat']
              
              # Similarity-based prediction
              similarity = np.dot(bird_emb, cat_emb) / (np.linalg.norm(bird_emb) * np.linalg.norm(cat_emb))
              embedding_prob = similarity * 0.8  # Transfer from 'cat sat' probability
              
              print(f"Embedding: P(sat|the bird) ≈ {embedding_prob:.2f}")  # Non-zero!
              print("Embeddings generalize to unseen combinations")
    
    security_implications:
      sparsity_attacks: |
        Adversaries exploit sparse regions of n-gram space:
        - Craft inputs with rare n-grams that model hasn't seen
        - Model assigns uniform/random probability → unpredictable behavior
        - Defense: neural models with embeddings generalize better
      
      embedding_vulnerabilities: |
        While embeddings solve curse of dimensionality, they create new attacks:
        - Adversarial embeddings: find inputs with similar embeddings to benign content
        - Embedding space poisoning: poison training to manipulate embedding geometry
        - Similarity exploitation: craft malicious text semantically close to safe text

key_takeaways:
  critical_concepts:
    - concept: "NLP evolution: rule-based → statistical → neural"
      why_it_matters: "Each paradigm has different security properties and vulnerabilities"
    
    - concept: "Text is discrete and symbolic, unlike continuous image pixels"
      why_it_matters: "Creates unique attack vectors and defense challenges"
    
    - concept: "One-hot encoding is sparse and doesn't capture similarity"
      why_it_matters: "Need dense embeddings for semantic understanding and generalization"
    
    - concept: "Language models learn probability distributions over sequences"
      why_it_matters: "They memorize training data patterns - including malicious ones"
    
    - concept: "N-grams suffer from curse of dimensionality (vocab^n growth)"
      why_it_matters: "Why we need neural networks with continuous representations"
    
    - concept: "Perplexity measures model quality (lower is better)"
      why_it_matters: "Can be used to detect anomalous/adversarial inputs"
  
  actionable_steps:
    - step: "Implement n-gram language models to understand statistical NLP"
      verification: "Train trigram model, calculate perplexity, generate text"
    
    - step: "Build vocabulary with frequency analysis"
      verification: "Handle unknown words with <UNK> token"
    
    - step: "Understand why discrete text requires embedding layers"
      verification: "Compare one-hot vs embedding similarity properties"
    
    - step: "Calculate perplexity for model evaluation"
      verification: "Lower perplexity on test set = better model"
    
    - step: "Recognize training data memorization in language models"
      verification: "Query model for training data extraction"
  
  security_principles:
    - principle: "Language models learn from data - including adversarial patterns"
      application: "Audit training data for malicious content, bias, sensitive info"
    
    - principle: "Discrete tokens create different attack surface than continuous values"
      application: "Defend against character-level, word-level, and token-level attacks"
    
    - principle: "Vocabulary handling affects security (UNK tokens, rare words)"
      application: "Carefully design vocabulary to avoid information loss attacks"
    
    - principle: "Perplexity can detect anomalous inputs"
      application: "Use perplexity thresholds to flag potential adversarial examples"
    
    - principle: "Curse of dimensionality forces use of neural networks"
      application: "Understand embedding-based attacks that exploit continuous space"
  
  common_mistakes:
    - mistake: "Treating text like images (continuous perturbations)"
      fix: "Remember text is discrete - small changes mean different tokens"
    
    - mistake: "Ignoring vocabulary size impact on model capacity"
      fix: "Balance vocabulary coverage vs sparsity carefully"
    
    - mistake: "Not handling unknown words in production"
      fix: "Always include <UNK> token and test with out-of-vocabulary inputs"
    
    - mistake: "Assuming higher n means better n-gram model"
      fix: "Higher n → more sparsity. Trigrams often optimal for statistical LMs"
    
    - mistake: "Forgetting that models memorize training data"
      fix: "Audit for sensitive data leakage, especially in high-capacity models"
  
  integration_with_book:
    from_chapter_1:
      - "Gradient descent and backpropagation (needed for training embeddings)"
      - "Probability theory (foundation for language modeling)"
      - "Cross-entropy loss (used for language model training)"
    
    from_chapter_2:
      - "Neural networks (will use for embedding layers)"
      - "Sequence models / RNNs (alternative to n-grams)"
      - "Backpropagation through time (for sequence modeling)"
    
    to_next_section:
      - "Section 3.2: Tokenization - converting text to tokens"
      - "Section 3.3: Word Embeddings - dense representations"
      - "Section 3.4: Word2Vec - learning embeddings from data"
  
  looking_ahead:
    next_concepts:
      - "Tokenization algorithms (BPE, WordPiece)"
      - "Learning dense embeddings (Word2Vec, GloVe)"
      - "Attention mechanisms (solution to long-range dependencies)"
      - "Transformer architecture (modern foundation for LLMs)"
    
    skills_to_build:
      - "Implement tokenization from scratch"
      - "Train word embeddings on corpus"
      - "Understand attention as differentiable memory"
      - "Build transformer encoder and decoder"
  
  final_thoughts: |
    You now understand why NLP is fundamentally different from vision or tabular ML:
    - Text is discrete and symbolic
    - Language modeling is about probability distributions
    - N-grams face curse of dimensionality
    - Neural networks with embeddings solve the sparsity problem
    
    These foundations are critical for understanding modern LLMs. When you hear about 
    "transformer attention" or "GPT token prediction," you'll know it's built on the 
    language modeling framework we've established here.
    
    From a security perspective: language models are trained on text data, they learn 
    patterns (good and bad), they memorize training examples, and they can be 
    manipulated through carefully crafted inputs. Understanding these fundamentals 
    is your first step toward securing LLM deployments.
    
    Next: We tackle tokenization - the crucial step that converts raw text into the 
    tokens that models actually process.

---
