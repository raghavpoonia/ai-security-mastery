# section_01_19_naive_bayes.yaml

---
document_info:
  chapter: "01"
  section: "19"
  title: "Naive Bayes"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-31"
  estimated_pages: 6
  tags: ["naive-bayes", "bayesian", "probabilistic", "conditional-probability", "spam-detection", "text-classification"]

# ============================================================================
# SECTION 1.19: NAIVE BAYES
# ============================================================================

section_01_19_naive_bayes:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Naive Bayes is beautifully simple: estimate the probability of each class given the 
    features, then predict the most probable class. It's based on Bayes' theorem and makes 
    one "naive" assumption: all features are independent given the class. This assumption 
    is often violated in practice, yet Naive Bayes works surprisingly well.
    
    Despite its simplicity, Naive Bayes is remarkably effective for:
    - Text classification (spam detection, sentiment analysis)
    - Real-time prediction (training and inference both fast)
    - Small datasets (requires fewer samples than discriminative models)
    - Baseline comparisons (simple, fast baseline for any classification task)
    
    This section covers:
    - Bayes' theorem and conditional probability
    - The naive independence assumption
    - Three variants: Gaussian, Multinomial, Bernoulli
    - Laplace smoothing for unseen feature values
    - Why it works despite violated assumptions
    - Advantages and limitations
    
    For security, Naive Bayes excels at:
    1. Spam/phishing detection (text-based, real-time)
    2. Network traffic classification (fast, probabilistic)
    3. Log analysis (categorical features, many classes)
    4. Baseline detector (quick to deploy, interpretable)
  
  why_this_matters: |
    Security applications:
    - Spam filters: Classic Naive Bayes application (word frequencies)
    - Phishing detection: Email content classification
    - Malware detection: API call sequences as features
    - Network IDS: Protocol classification
    
    Real deployments:
    - Gmail spam filter originally used Naive Bayes
    - Still used in many email security products
    - Common in SIEM log classification
    - Fast enough for real-time filtering
    
    Why learn Naive Bayes:
    - Fundamental probabilistic classifier
    - Foundation for understanding Bayesian methods
    - Practical (actually works despite simplicity)
    - Provides probability estimates (unlike SVM)
    - Handles streaming data naturally (incremental updates)
  
  # --------------------------------------------------------------------------
  # Core Concept 1: Bayes' Theorem
  # --------------------------------------------------------------------------
  
  bayes_theorem:
    
    formula: "P(Y|X) = P(X|Y) × P(Y) / P(X)"
    
    in_words: |
      Posterior = (Likelihood × Prior) / Evidence
      
      P(Y|X): Posterior - probability of Y given we observed X
      P(X|Y): Likelihood - probability of observing X given Y
      P(Y): Prior - probability of Y before seeing X
      P(X): Evidence - probability of observing X
    
    example_spam_detection: |
      Y: Email is spam
      X: Email contains word "viagra"
      
      P(spam|"viagra") = P("viagra"|spam) × P(spam) / P("viagra")
      
      Components:
      - P("viagra"|spam): How often spam contains "viagra" (likelihood)
      - P(spam): Overall spam rate (prior)
      - P("viagra"): How often any email contains "viagra" (evidence)
    
    classification_using_bayes: |
      For classification, we want: argmax_y P(Y=y|X=x)
      
      Using Bayes: P(Y=y|X=x) = P(X=x|Y=y) × P(Y=y) / P(X=x)
      
      Since P(X=x) same for all classes, we can ignore it:
      
      argmax_y P(Y=y|X=x) = argmax_y P(X=x|Y=y) × P(Y=y)
      
      Just compare: Likelihood × Prior for each class
  
  # --------------------------------------------------------------------------
  # Core Concept 2: The Naive Assumption
  # --------------------------------------------------------------------------
  
  naive_assumption:
    
    problem_with_full_bayes: |
      Need to estimate: P(X₁, X₂, ..., Xₙ|Y)
      
      With n features, requires estimating joint distribution
      Exponentially many combinations!
      
      Example: 10 binary features = 2¹⁰ = 1024 combinations
      Need massive dataset to estimate reliably
    
    naive_independence_assumption: |
      Assume: Features are conditionally independent given class
      
      P(X₁, X₂, ..., Xₙ|Y) = P(X₁|Y) × P(X₂|Y) × ... × P(Xₙ|Y)
      
      Now only need to estimate: P(Xᵢ|Y) for each feature i
      Linear in number of features (tractable!)
    
    why_naive: |
      This assumption is "naive" because it's often false
      
      Example: Spam email
      - Word "free" appears
      - Word "money" appears
      
      These are NOT independent:
      - P("money"|spam, "free") ≠ P("money"|spam)
      - If email has "free", more likely to also have "money"
      
      Yet Naive Bayes works well anyway!
    
    naive_bayes_classifier: |
      Classification rule:
      
      ŷ = argmax_y P(Y=y) × ∏ᵢ P(Xᵢ|Y=y)
      
      In words:
      1. For each class y
      2. Compute prior P(Y=y)
      3. Multiply by likelihood of each feature: ∏ᵢ P(Xᵢ|Y=y)
      4. Pick class with highest score
    
    log_probabilities: |
      In practice, use log probabilities (avoid underflow):
      
      ŷ = argmax_y [log P(Y=y) + Σᵢ log P(Xᵢ|Y=y)]
      
      Multiplication becomes addition (numerically stable)
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Three Variants of Naive Bayes
  # --------------------------------------------------------------------------
  
  naive_bayes_variants:
    
    gaussian_naive_bayes:
      
      when_to_use: "Continuous features (real-valued)"
      
      assumption: "Features follow Gaussian distribution within each class"
      
      likelihood: |
        P(Xᵢ=x|Y=y) = (1/√(2πσ²ᵧ)) × exp(-(x - μᵧ)² / (2σ²ᵧ))
        
        Where:
        - μᵧ: Mean of feature i for class y
        - σ²ᵧ: Variance of feature i for class y
      
      training: |
        For each feature i and class y:
        1. Compute mean: μᵢᵧ = mean of feature i in class y samples
        2. Compute variance: σ²ᵢᵧ = variance of feature i in class y samples
        3. Compute prior: P(Y=y) = count(class y) / total samples
      
      prediction: |
        For new sample x:
        For each class y:
          score = log P(Y=y) + Σᵢ log N(xᵢ; μᵢᵧ, σ²ᵢᵧ)
        
        Predict class with highest score
      
      security_example: |
        Network intrusion detection:
        Features: [packet_size, inter_arrival_time, payload_entropy]
        Classes: [normal, dos_attack, probe]
        
        Each feature modeled as Gaussian per class
    
    multinomial_naive_bayes:
      
      when_to_use: "Count data (word frequencies, event counts)"
      
      assumption: "Features are counts from multinomial distribution"
      
      likelihood: |
        P(Xᵢ=k|Y=y) = θᵢᵧᵏ
        
        Where θᵢᵧ is probability of feature i in class y
      
      training: |
        For each feature i and class y:
        Count occurrences in class y documents
        
        θᵢᵧ = (count of feature i in class y) / (total counts in class y)
        
        With Laplace smoothing:
        θᵢᵧ = (count of feature i in class y + α) / 
              (total counts in class y + α × n_features)
      
      security_example: |
        Spam detection:
        Features: Word counts in email
        Document: "free money free offer"
        Feature vector: [free:2, money:1, offer:1, ...]
    
    bernoulli_naive_bayes:
      
      when_to_use: "Binary features (presence/absence)"
      
      assumption: "Features are binary (0 or 1)"
      
      likelihood: |
        P(Xᵢ=1|Y=y) = θᵢᵧ
        P(Xᵢ=0|Y=y) = 1 - θᵢᵧ
        
        Where θᵢᵧ is probability feature i present in class y
      
      training: |
        θᵢᵧ = (count of documents in class y with feature i present) / 
              (total documents in class y)
      
      security_example: |
        Malware detection:
        Features: [calls_CreateFile, calls_RegSetValue, has_packer, ...]
        Binary: 1 if API call present, 0 otherwise
    
    choosing_variant: |
      Decision tree:
      
      1. Continuous features (measurements)?
         → Gaussian Naive Bayes
      
      2. Count data (word frequencies)?
         → Multinomial Naive Bayes
      
      3. Binary features (presence/absence)?
         → Bernoulli Naive Bayes
      
      Most common in security: Multinomial (text) or Bernoulli (binary features)
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Laplace Smoothing
  # --------------------------------------------------------------------------
  
  laplace_smoothing:
    
    zero_probability_problem: |
      Problem: If feature never appears in training for a class
      → P(feature|class) = 0
      → Entire product becomes 0
      → Cannot classify samples with that feature!
      
      Example:
      Training spam: "viagra", "money", "offer"
      Test email: "lottery"
      
      "lottery" never in training → P("lottery"|spam) = 0
      → Cannot compute P(spam|email) (product is 0)
    
    laplace_solution: |
      Add small pseudocount (α, typically 1) to all counts
      
      Without smoothing:
      P(word|spam) = count(word in spam) / count(all words in spam)
      
      With Laplace smoothing (α=1):
      P(word|spam) = (count(word in spam) + 1) / 
                     (count(all words in spam) + V)
      
      Where V = vocabulary size
      
      Effect: All probabilities > 0, even for unseen words
    
    example: |
      Training spam: "free money" (2 words)
      Vocabulary: {free, money, viagra, lottery} (V=4)
      
      Without smoothing:
      P("free"|spam) = 1/2 = 0.5
      P("money"|spam) = 1/2 = 0.5
      P("viagra"|spam) = 0/2 = 0 ← Problem!
      
      With Laplace (α=1):
      P("free"|spam) = (1+1)/(2+4) = 2/6 ≈ 0.33
      P("money"|spam) = (1+1)/(2+4) = 2/6 ≈ 0.33
      P("viagra"|spam) = (0+1)/(2+4) = 1/6 ≈ 0.17 ← Fixed!
    
    alpha_parameter: |
      α = 0: No smoothing (can have zero probabilities)
      α = 1: Standard Laplace smoothing
      α > 1: Stronger smoothing (more uniform probabilities)
      
      Typical: α = 1 (works well in practice)
  
  # --------------------------------------------------------------------------
  # Complete Implementation
  # --------------------------------------------------------------------------
  
  complete_implementation: |
    import numpy as np
    
    class MultinomialNaiveBayes:
        """
        Multinomial Naive Bayes for text classification
        (e.g., spam detection)
        """
        
        def __init__(self, alpha=1.0):
            """
            Args:
                alpha: Laplace smoothing parameter
            """
            self.alpha = alpha
            self.class_log_prior = None
            self.feature_log_prob = None
            self.classes = None
        
        def fit(self, X, y):
            """
            Train Naive Bayes classifier
            
            Args:
                X: Feature counts, shape (n_samples, n_features)
                y: Labels, shape (n_samples,)
            """
            n_samples, n_features = X.shape
            self.classes = np.unique(y)
            n_classes = len(self.classes)
            
            # Initialize
            self.class_log_prior = np.zeros(n_classes)
            self.feature_log_prob = np.zeros((n_classes, n_features))
            
            # Compute for each class
            for idx, c in enumerate(self.classes):
                # Samples in this class
                X_c = X[y == c]
                
                # Class prior: P(Y=c)
                self.class_log_prior[idx] = np.log(len(X_c) / n_samples)
                
                # Feature likelihoods: P(Xᵢ|Y=c)
                # Sum of feature counts in class c
                feature_counts = X_c.sum(axis=0)
                
                # Total count in class c
                total_count = feature_counts.sum()
                
                # Laplace smoothing
                numerator = feature_counts + self.alpha
                denominator = total_count + self.alpha * n_features
                
                # Log probability
                self.feature_log_prob[idx] = np.log(numerator / denominator)
            
            return self
        
        def predict(self, X):
            """Predict class for samples"""
            log_probs = self._compute_log_probs(X)
            return self.classes[np.argmax(log_probs, axis=1)]
        
        def predict_proba(self, X):
            """Predict class probabilities"""
            log_probs = self._compute_log_probs(X)
            
            # Convert log probabilities to probabilities
            # Use log-sum-exp trick for numerical stability
            log_probs_normalized = log_probs - np.max(log_probs, axis=1, keepdims=True)
            probs = np.exp(log_probs_normalized)
            probs = probs / probs.sum(axis=1, keepdims=True)
            
            return probs
        
        def _compute_log_probs(self, X):
            """Compute log P(Y|X) for each class"""
            # log P(Y|X) = log P(Y) + Σᵢ log P(Xᵢ|Y)
            log_probs = (self.class_log_prior[:, np.newaxis] + 
                        (X @ self.feature_log_prob.T).T).T
            return log_probs
        
        def score(self, X, y):
            """Compute accuracy"""
            return np.mean(self.predict(X) == y)
    
    # ========================================================================
    # USAGE EXAMPLE: SPAM DETECTION
    # ========================================================================
    
    # Simulate spam detection data
    np.random.seed(42)
    
    # Vocabulary: ["free", "money", "viagra", "meeting", "report"]
    vocab = ["free", "money", "viagra", "meeting", "report"]
    n_features = len(vocab)
    
    # Generate training data
    # Spam emails: high counts of "free", "money", "viagra"
    X_spam = np.random.poisson(lam=[5, 4, 3, 0.5, 0.5], size=(50, n_features))
    y_spam = np.ones(50)
    
    # Ham emails: high counts of "meeting", "report"
    X_ham = np.random.poisson(lam=[0.5, 0.5, 0.1, 4, 5], size=(50, n_features))
    y_ham = np.zeros(50)
    
    # Combine
    X = np.vstack([X_spam, X_ham])
    y = np.hstack([y_spam, y_ham])
    
    # Shuffle
    indices = np.random.permutation(len(y))
    X, y = X[indices], y[indices]
    
    # Train/test split
    split = int(0.8 * len(y))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    
    # Train Naive Bayes
    nb = MultinomialNaiveBayes(alpha=1.0)
    nb.fit(X_train, y_train)
    
    # Evaluate
    train_acc = nb.score(X_train, y_train)
    test_acc = nb.score(X_test, y_test)
    
    print("Multinomial Naive Bayes")
    print(f"Training accuracy: {train_acc:.2%}")
    print(f"Test accuracy: {test_acc:.2%}")
    
    # Examine learned probabilities
    print("\nMost discriminative words for SPAM (class 1):")
    spam_idx = 1
    spam_probs = np.exp(nb.feature_log_prob[spam_idx])
    for i in np.argsort(spam_probs)[-3:]:
        print(f"  {vocab[i]}: P(word|spam) = {spam_probs[i]:.4f}")
    
    # Predict new email
    new_email = np.array([[2, 3, 1, 0, 0]])  # Contains "free", "money", "viagra"
    prediction = nb.predict(new_email)
    probability = nb.predict_proba(new_email)
    
    print(f"\nNew email: {new_email[0]}")
    print(f"Prediction: {'SPAM' if prediction[0] == 1 else 'HAM'}")
    print(f"P(spam): {probability[0, 1]:.2%}")
  
  # --------------------------------------------------------------------------
  # Why Naive Bayes Works Despite Naive Assumption
  # --------------------------------------------------------------------------
  
  why_it_works:
    
    classification_not_estimation: |
      Key insight: We don't need accurate probability estimates
      We just need correct ranking of classes
      
      Even if P(spam|email) = 0.7 is wrong (true value 0.9)
      As long as P(spam|email) > P(ham|email), we classify correctly!
    
    feature_contributions_add: |
      With independence assumption:
      - Each feature contributes to overall score
      - Features "vote" independently
      - Correlated features just double-vote
      
      Example: "free" and "money" correlated
      Both vote for spam → strong spam signal (even if double-counted)
    
    works_well_with_many_features: |
      With many features (e.g., 10,000 words):
      - Individual correlations matter less
      - Aggregate signal dominates
      - Law of large numbers helps
    
    calibration_vs_discrimination: |
      Naive Bayes:
      - Poor calibration (probabilities not accurate)
      - Good discrimination (ranks classes correctly)
      
      For classification, discrimination is what matters!
  
  # --------------------------------------------------------------------------
  # Security Applications
  # --------------------------------------------------------------------------
  
  security_applications:
    
    spam_phishing_detection:
      
      features: |
        Words/phrases in email:
        - "viagra", "free", "money" → spam indicators
        - "meeting", "report", "attached" → ham indicators
      
      approach: |
        Multinomial Naive Bayes on word counts
        Train on labeled spam/ham emails
        Fast enough for real-time filtering
      
      incremental_updates: |
        User marks email as spam:
        - Update counts for that class
        - Retrain in O(n_features) time
        - No need to retrain from scratch!
    
    network_traffic_classification:
      
      features: |
        Protocol statistics:
        - Packet sizes, inter-arrival times, port numbers
      
      variant: "Gaussian Naive Bayes (continuous features)"
      
      benefit: |
        Fast classification (real-time)
        Probabilistic output (confidence scores)
    
    log_analysis:
      
      features: |
        Log event types:
        - login_attempt, file_access, network_connection, etc.
        Binary (event occurred or not)
      
      variant: "Bernoulli Naive Bayes"
      
      classes: |
        - normal_activity
        - lateral_movement
        - data_exfiltration
        - privilege_escalation
      
      benefit: "Multi-class classification with interpretable probabilities"
  
  # --------------------------------------------------------------------------
  # Advantages and Limitations
  # --------------------------------------------------------------------------
  
  advantages_limitations:
    
    advantages:
      
      simplicity:
        - "Extremely simple to implement"
        - "Few hyperparameters (just α)"
        - "Easy to understand and debug"
      
      speed:
        - "Training: O(n × d) - just counting!"
        - "Prediction: O(d) per sample"
        - "Fast enough for real-time"
      
      small_data:
        - "Works with small datasets"
        - "Doesn't need as much data as discriminative models"
        - "Good for rare classes"
      
      probability_estimates:
        - "Outputs probabilities (not just class labels)"
        - "Can set confidence thresholds"
        - "Useful for risk assessment"
      
      incremental_learning:
        - "Easy to update with new data"
        - "Online learning naturally supported"
        - "No need to retrain from scratch"
      
      handles_irrelevant_features:
        - "Irrelevant features get low probabilities"
        - "Doesn't need feature selection"
        - "Robust to noisy features"
    
    limitations:
      
      independence_assumption:
        problem: "Assumes features independent (often false)"
        
        impact: |
          Correlated features double-counted
          Probability estimates poor (overconfident)
        
        mitigation: "Feature selection to remove redundant features"
      
      zero_frequency_problem:
        problem: "Unseen feature values → zero probability"
        
        solution: "Laplace smoothing (always use!)"
      
      continuous_features:
        problem: "Gaussian assumption often violated"
        
        alternative: "Discretize continuous features or use other models"
      
      poor_probability_calibration:
        issue: |
          Outputs confident probabilities (often wrong)
          P(spam)=0.99 doesn't mean 99% chance
        
        use_case: "Use for ranking, not absolute probabilities"
      
      linear_decision_boundary:
        limitation: "Can only learn linear boundaries (like logistic regression)"
        
        no_kernels: "No kernel trick available (unlike SVM)"
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Naive Bayes: Probabilistic classifier using Bayes' theorem"
      - "Naive assumption: Features independent given class"
      - "Three variants: Gaussian (continuous), Multinomial (counts), Bernoulli (binary)"
      - "Laplace smoothing: Prevent zero probabilities"
      - "Works despite naive assumption (classification not estimation)"
      - "Classification rule: argmax_y P(Y=y) × ∏ᵢ P(Xᵢ|Y=y)"
    
    practical_skills:
      - "Implement Multinomial Naive Bayes from scratch"
      - "Apply Laplace smoothing (α=1 default)"
      - "Choose variant based on feature type"
      - "Use log probabilities (avoid underflow)"
      - "Interpret learned probabilities"
    
    security_mindset:
      - "Classic spam/phishing detection algorithm"
      - "Fast enough for real-time classification"
      - "Provides probability estimates (risk assessment)"
      - "Incremental learning (update with new data)"
      - "Good baseline (simple, fast, interpretable)"
    
    remember_this:
      - "Naive Bayes = counting + Bayes' theorem"
      - "Always use Laplace smoothing (α=1)"
      - "Works despite violated independence assumption"
      - "Fastest classifier for text (spam detection)"
      - "Good for small datasets and many classes"
    
    next_steps:
      - "Next section: K-Means Clustering (unsupervised learning)"
      - "You now understand probabilistic classification"
      - "Naive Bayes foundation for Bayesian methods"

---
