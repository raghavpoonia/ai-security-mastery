# section_04_04_rag_architecture.yaml

---
document_info:
  title: "Retrieval-Augmented Generation (RAG) Architecture"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 4
  section: 4
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-27"
  version: "1.0"
  description: |
    Complete RAG system architecture covering the entire pipeline from query to response.
    Implements retrieval strategies (dense, sparse, hybrid), query transformation,
    re-ranking, context compression, and secure prompt construction. Builds end-to-end
    RAG from scratch with NumPy and Transformers, then scales with LangChain and
    LlamaIndex. Comprehensive security analysis of RAG systems including retrieval
    manipulation, context injection, and answer poisoning attacks.
  estimated_pages: 8
  tags:
    - rag
    - retrieval-augmented-generation
    - hybrid-search
    - re-ranking
    - context-compression
    - production-rag
    - rag-security

section_overview:
  title: "Retrieval-Augmented Generation (RAG) Architecture"
  number: "4.4"
  
  purpose: |
    Sections 4.1-4.3 built the retrieval infrastructure: embeddings and semantic search
    (4.1), efficient vector databases (4.2), and document processing (4.3). Now we
    combine retrieval with generation to create Retrieval-Augmented Generation (RAG)
    systems—the most widely deployed LLM application pattern in production.
    
    RAG solves a fundamental LLM limitation: knowledge cutoff and hallucination. LLMs
    are trained on static datasets and can confidently generate false information. RAG
    grounds LLM responses in retrieved documents, providing current information and
    reducing hallucinations by giving the model access to relevant context.
    
    This section covers the complete RAG pipeline: query understanding, retrieval (dense,
    sparse, hybrid), re-ranking for quality, context compression for efficiency, and
    secure prompt construction. We implement RAG from scratch to understand every component,
    then scale with production frameworks (LangChain, LlamaIndex).
    
    Security is critical: RAG systems introduce new attack surfaces at every stage—
    retrieval manipulation, context injection, answer poisoning, and prompt injection
    through retrieved documents. We analyze threats comprehensively and implement
    defense-in-depth strategies.
  
  learning_objectives:
    conceptual:
      - "Understand RAG architecture and the retrieval-augmentation-generation pipeline"
      - "Grasp dense vs sparse vs hybrid retrieval trade-offs and when to use each"
      - "Comprehend re-ranking strategies and their impact on retrieval quality"
      - "Understand context compression techniques and prompt construction best practices"
    
    practical:
      - "Implement end-to-end RAG system from scratch with NumPy and Transformers"
      - "Build hybrid retrieval combining BM25 and dense embeddings"
      - "Deploy re-ranking with cross-encoders for improved accuracy"
      - "Create production RAG with LangChain and evaluation frameworks"
    
    security_focused:
      - "Identify retrieval manipulation attacks and implement defenses"
      - "Detect and prevent context injection through retrieved documents"
      - "Implement answer verification and hallucination detection"
      - "Secure prompt construction against indirect prompt injection"
  
  prerequisites:
    knowledge:
      - "Section 4.1: Vector embeddings and semantic search"
      - "Section 4.2: Vector databases and efficient retrieval"
      - "Section 4.3: Document processing and chunking (assumed complete)"
      - "Chapter 3: LLM architectures (BERT, GPT) and generation"
    
    skills:
      - "Building semantic search engines with embeddings"
      - "Working with vector databases (FAISS)"
      - "Using transformers library for embeddings and generation"
      - "Understanding prompt engineering and context windows"
  
  key_transitions:
    from_previous_sections: |
      Sections 4.1-4.3 provided the retrieval foundation:
      - 4.1: Semantic search with embeddings (dense retrieval)
      - 4.2: Efficient vector databases for production scale
      - 4.3: Document processing and chunking (assumed covered)
      
      Section 4.4 completes the picture by adding generation. We take retrieved documents
      and augment LLM prompts with this context, creating grounded responses. The retrieval
      algorithms from 4.1-4.2 become components in a larger system. The security controls
      from previous sections (rate limiting, access control) now protect RAG endpoints.
    
    to_next_section: |
      Section 4.4 covers basic RAG architecture. Section 4.5 advances to sophisticated
      prompting techniques (few-shot, chain-of-thought, ReAct) that enhance RAG quality.
      Section 4.6 introduces fine-tuning (LoRA, QLoRA) as an alternative or complement
      to RAG. Together, 4.4-4.6 provide a complete toolkit for knowledge-grounded LLM
      applications.

topics:
  - topic_number: 1
    title: "RAG Pipeline Architecture and Dense Retrieval"
    
    overview: |
      The RAG pipeline has three stages: (1) Retrieval—find relevant documents given a
      query, (2) Augmentation—construct a prompt with retrieved context, (3) Generation—
      LLM produces an answer grounded in the context. Each stage has design choices that
      affect quality, latency, cost, and security.
      
      Dense retrieval uses embedding similarity (what we built in Sections 4.1-4.2) to
      find semantically similar documents. This is the most common RAG retrieval method
      and works well for natural language queries. However, dense retrieval has limitations:
      it struggles with exact keyword matches, rare terms, and out-of-distribution queries.
      
      We implement a complete RAG system from scratch, examining each component carefully.
      Understanding the full pipeline enables debugging, optimization, and security hardening
      in production deployments.
    
    content:
      rag_pipeline_stages:
        stage_1_retrieval: |
          Retrieval stage: Query → Relevant Documents
          
          Input: User query (text)
          Process:
          1. Embed the query using the same model as documents
          2. Search vector database (FAISS, Pinecone) for k similar documents
          3. Retrieve top-k document chunks with metadata
          
          Output: List of relevant document chunks with scores
          
          Key decisions:
          - Embedding model: Sentence-transformers, OpenAI, custom fine-tuned
          - k (number of documents): Trade-off between context and noise
          - Retrieval strategy: Dense, sparse, or hybrid (covered in Topic 2)
          - Filtering: Pre-retrieval filters (date range, source, etc.)
        
        stage_2_augmentation: |
          Augmentation stage: Query + Documents → Prompt
          
          Input: Query + Retrieved documents
          Process:
          1. Optional: Re-rank documents for quality (Topic 3)
          2. Optional: Compress context to fit LLM window (Topic 4)
          3. Construct prompt: System message + Context + Query
          4. Validate prompt length fits model's context window
          
          Output: Complete prompt ready for LLM
          
          Key decisions:
          - Prompt template: How to format context and query
          - Context ordering: Most relevant first or last?
          - Metadata inclusion: Source attribution, timestamps
          - Token budget: How much context vs generation capacity
        
        stage_3_generation: |
          Generation stage: Prompt → Answer
          
          Input: Augmented prompt
          Process:
          1. Call LLM (GPT-4, Claude, local model)
          2. Stream or batch generation
          3. Parse and validate output
          4. Optional: Verify answer against retrieved context
          
          Output: Generated answer with optional citations
          
          Key decisions:
          - Model selection: GPT-4, Claude, Llama, etc.
          - Generation parameters: Temperature, top_p, max_tokens
          - Citation format: Inline, footnotes, source listing
          - Post-processing: Hallucination detection, fact-checking
        
        end_to_end_flow: |
          Complete RAG flow:
          
          1. User submits query: "What are the health benefits of green tea?"
          
          2. Retrieval:
             - Embed query → [0.23, -0.45, ..., 0.67] (384-dim)
             - Search FAISS index → Top 5 chunks
             - Retrieved: [chunk_42, chunk_189, chunk_301, ...]
          
          3. Augmentation:
             - Re-rank using cross-encoder → [chunk_189, chunk_42, chunk_507]
             - Compress if needed (remove redundancy)
             - Construct prompt:
               """
               You are a health assistant. Answer based on the provided context.
               
               Context:
               [chunk_189 content...]
               [chunk_42 content...]
               [chunk_507 content...]
               
               Question: What are the health benefits of green tea?
               Answer:
               """
          
          4. Generation:
             - Call GPT-4 with prompt
             - Generate: "Based on the provided research, green tea offers..."
             - Include citations: [Source 1], [Source 2]
          
          5. Return to user with source attribution
      
      dense_retrieval_deep_dive:
        how_it_works: |
          Dense retrieval uses learned embeddings to capture semantic similarity:
          
          1. Offline indexing:
             - Embed all documents with sentence-transformer
             - Store in vector database (FAISS IndexHNSWFlat)
             - Normalize embeddings for cosine similarity
          
          2. Online query:
             - Embed query with same model
             - Search index: top_k = index.search(query_embedding, k=10)
             - Return document chunks with similarity scores
          
          Advantages:
          - Captures semantic meaning (synonyms, paraphrases)
          - Works well for natural language queries
          - Handles typos and variations
          - Proven effective in production
          
          Disadvantages:
          - Struggles with exact keyword matches
          - May miss rare or technical terms
          - Requires good embedding model
          - Out-of-distribution queries perform poorly
        
        choosing_embedding_model: |
          Embedding model selection criteria:
          
          1. **Domain match**: 
             - General: all-MiniLM-L6-v2, all-mpnet-base-v2
             - Code: code-search-net, CodeBERT
             - Multilingual: paraphrase-multilingual
             - Legal/Medical: Domain-specific fine-tuned models
          
          2. **Dimension vs quality**:
             - 384-dim: Fast, good quality (all-MiniLM-L6-v2)
             - 768-dim: Better quality, more memory (all-mpnet-base-v2)
             - 1536-dim: Best quality, expensive (OpenAI ada-002)
          
          3. **Speed vs accuracy**:
             - Fast inference: Smaller models (384-dim)
             - High accuracy: Larger models or ensemble
          
          4. **Licensing and cost**:
             - Open source: Sentence-transformers (free)
             - API-based: OpenAI, Cohere (pay per token)
          
          Recommendation: Start with all-MiniLM-L6-v2, evaluate, upgrade if needed.
        
        retrieval_parameters: |
          Key parameters for dense retrieval:
          
          1. **k (top-k)**: Number of documents to retrieve
             - Too small: Miss relevant information
             - Too large: Introduce noise, exceed context window
             - Typical: 3-10 for chat, 10-50 for research
          
          2. **Similarity threshold**: Minimum score to retrieve
             - Filter low-quality matches
             - Depends on embedding model and task
             - Typical: 0.5-0.7 for cosine similarity
          
          3. **Metadata filters**: Pre-filter by attributes
             - Date range, source, author, category
             - Applied before vector search (faster)
             - Combines with similarity search
          
          4. **Diversity**: Avoid redundant documents
             - Use MMR (Maximal Marginal Relevance) from Section 4.1
             - Balance relevance and diversity
             - Important for long context generation
    
    implementation:
      basic_rag_from_scratch:
        language: python
        code: |
          """
          Basic RAG implementation from scratch.
          Demonstrates the complete pipeline: retrieval → augmentation → generation.
          Uses sentence-transformers for embeddings and FAISS for retrieval.
          """
          
          import numpy as np
          import faiss
          from typing import List, Dict, Tuple, Optional
          from dataclasses import dataclass
          import warnings
          warnings.filterwarnings('ignore')
          
          @dataclass
          class Document:
              """Document with content and metadata."""
              id: str
              content: str
              metadata: Dict
              embedding: Optional[np.ndarray] = None
          
          @dataclass
          class RetrievalResult:
              """Single retrieval result."""
              document: Document
              score: float
              rank: int
          
          
          class BasicRAGSystem:
              """
              Complete RAG system with dense retrieval.
              
              Pipeline:
              1. Retrieval: Semantic search with embeddings
              2. Augmentation: Prompt construction with context
              3. Generation: LLM call with augmented prompt
              """
              
              def __init__(self, 
                          embedding_model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',
                          llm_model_name: str = 'gpt2'):  # Using GPT-2 for demo
                  """
                  Initialize RAG system.
                  
                  Args:
                      embedding_model_name: Model for embeddings
                      llm_model_name: Model for generation
                  """
                  from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
                  import torch
                  
                  # Embedding model
                  self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
                  self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)
                  self.embedding_model = AutoModel.from_pretrained(embedding_model_name).to(self.device)
                  self.embedding_model.eval()
                  self.embedding_dim = self.embedding_model.config.hidden_size
                  
                  # Generation model
                  self.llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
                  self.llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name).to(self.device)
                  self.llm_model.eval()
                  
                  # Set pad token if not present
                  if self.llm_tokenizer.pad_token is None:
                      self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
                  
                  # Vector index
                  self.index = None
                  self.documents: List[Document] = []
                  
                  print(f"BasicRAGSystem initialized:")
                  print(f"  Embedding model: {embedding_model_name}")
                  print(f"  Embedding dim: {self.embedding_dim}")
                  print(f"  LLM model: {llm_model_name}")
                  print(f"  Device: {self.device}")
              
              def embed_text(self, texts: List[str]) -> np.ndarray:
                  """
                  Embed texts using sentence-transformer.
                  
                  Args:
                      texts: List of texts to embed
                  
                  Returns:
                      Embeddings [len(texts), embedding_dim]
                  """
                  import torch
                  
                  # Tokenize
                  encoded = self.tokenizer(
                      texts,
                      padding=True,
                      truncation=True,
                      max_length=512,
                      return_tensors='pt'
                  ).to(self.device)
                  
                  # Generate embeddings
                  with torch.no_grad():
                      model_output = self.embedding_model(**encoded)
                      
                      # Mean pooling
                      attention_mask = encoded['attention_mask']
                      token_embeddings = model_output.last_hidden_state
                      
                      input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                      sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
                      sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
                      embeddings = sum_embeddings / sum_mask
                      
                      # Normalize
                      embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                  
                  return embeddings.cpu().numpy()
              
              def index_documents(self, documents: List[Document]):
                  """
                  Index documents for retrieval.
                  
                  Args:
                      documents: List of Document objects to index
                  """
                  print(f"\nIndexing {len(documents)} documents...")
                  
                  # Store documents
                  self.documents = documents
                  
                  # Generate embeddings
                  texts = [doc.content for doc in documents]
                  embeddings = self.embed_text(texts)
                  
                  # Store embeddings in documents
                  for doc, emb in zip(documents, embeddings):
                      doc.embedding = emb
                  
                  # Build FAISS index
                  self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product (cosine for normalized)
                  self.index.add(embeddings.astype('float32'))
                  
                  print(f"  Indexed {self.index.ntotal} documents")
                  print(f"  Index type: {type(self.index).__name__}")
              
              def retr
