# section_02_11_batch_normalization.yaml
---
document_info:
  chapter: "02"
  section: "11"
  title: "Batch Normalization"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-10"
  estimated_pages: 6
  tags: ["batch-normalization", "normalization", "covariate-shift", "training-stability", "deep-networks"]

# ============================================================================
# SECTION 02_11: BATCH NORMALIZATION
# ============================================================================

section_02_11_batch_normalization:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Training deep networks was notoriously difficult before 2015: vanishing/
    exploding gradients, slow convergence, extreme sensitivity to initialization
    and learning rate. Batch Normalization (BatchNorm) revolutionized deep
    learning by normalizing layer inputs during training.
    
    BatchNorm allows much higher learning rates, reduces dependence on careful
    initialization, acts as regularization, and enables training of very deep
    networks (50+ layers). It's now standard in nearly every modern architecture.
    
    This section covers BatchNorm's mathematical foundation, implementation
    including train/test mode differences, convergence improvements, practical
    considerations, variants (Layer/Instance/Group Norm), and security implications.
  
  learning_objectives:
    
    conceptual:
      - "Understand internal covariate shift problem"
      - "Grasp normalization and re-parameterization"
      - "Know why BatchNorm enables higher learning rates"
      - "Understand train vs test mode differences"
      - "Recognize BatchNorm as implicit regularization"
      - "Compare normalization variants"
    
    practical:
      - "Implement BatchNorm layer from scratch"
      - "Handle train/test mode correctly"
      - "Track running statistics (mean, variance)"
      - "Integrate BatchNorm into networks"
      - "Tune BatchNorm hyperparameters"
      - "Debug BatchNorm issues"
    
    security_focused:
      - "BatchNorm statistics leak training data"
      - "Running stats enable model fingerprinting"
      - "Normalization affects backdoor stability"
      - "Batch size impacts BatchNorm effectiveness"
  
  prerequisites:
    - "Sections 02_05-02_10 (backprop through regularization)"
    - "Understanding of mean, variance, normalization"
    - "Forward/backward pass implementation"
  
  # --------------------------------------------------------------------------
  # Topic 1: Internal Covariate Shift
  # --------------------------------------------------------------------------
  
  internal_covariate_shift:
    
    the_problem:
      
      covariate_shift_definition: |
        Covariate shift: distribution of inputs changes
        
        Training vs test:
        - Train data: P_train(X)
        - Test data: P_test(X) ≠ P_train(X)
        
        Model trained on P_train may fail on P_test
      
      internal_covariate_shift: |
        Same problem INSIDE network during training:
        
        Layer l receives inputs from layer l-1
        Layer l-1's weights update → distribution of its outputs changes
        → Layer l's input distribution changes every iteration
        
        This is "internal" covariate shift
      
      impact_on_training:
        slow_convergence: |
          Layer l must constantly readapt to new input distributions
          Can't settle into good solution (target keeps moving)
        
        vanishing_exploding: |
          Without normalization:
          - Activations drift toward saturation regions
          - Gradients vanish or explode
          - Training stalls
        
        example: |
          Initial training:
          Layer 5 inputs: mean=0, std=1
          
          After 100 iterations:
          Layer 5 inputs: mean=5, std=10 (shifted and scaled!)
          
          Layer 5 must learn from scratch with new distribution
    
    why_normalization_helps:
      
      stable_distributions: |
        Normalize layer inputs → mean=0, variance=1
        Distribution stays consistent throughout training
        Layer can learn effectively
      
      gradient_flow: |
        Normalized inputs → activations don't saturate
        Non-saturated activations → gradients flow
        Gradients flow → deep networks train
      
      higher_learning_rates: |
        Without BatchNorm: small LR required (else explodes)
        With BatchNorm: can use 10x higher LR (faster convergence)
  
  # --------------------------------------------------------------------------
  # Topic 2: Batch Normalization Algorithm
  # --------------------------------------------------------------------------
  
  batchnorm_algorithm:
    
    forward_pass_training:
      
      normalization_step: |
        For mini-batch B = {x₁, x₂, ..., x_m}:
        
        1. Compute batch statistics:
           μ_B = (1/m) Σᵢ xᵢ           (batch mean)
           σ²_B = (1/m) Σᵢ (xᵢ - μ_B)²  (batch variance)
        
        2. Normalize:
           x̂ᵢ = (xᵢ - μ_B) / √(σ²_B + ε)
           
           Result: x̂ᵢ ~ N(0, 1) approximately
      
      scale_and_shift: |
        3. Re-parameterize (learnable parameters γ, β):
           yᵢ = γ · x̂ᵢ + β
        
        Why: Allow network to undo normalization if needed
        - γ = √(σ²_B), β = μ_B → identity (no normalization)
        - γ = 1, β = 0 → pure normalization
        
        Network learns optimal γ and β during training
      
      complete_algorithm: |
        Input: Mini-batch B = {x₁, ..., x_m}
        Parameters: γ (scale), β (shift)
        
        # Compute statistics
        μ_B = (1/m) Σᵢ xᵢ
        σ²_B = (1/m) Σᵢ (xᵢ - μ_B)²
        
        # Normalize
        x̂ᵢ = (xᵢ - μ_B) / √(σ²_B + ε)
        
        # Scale and shift
        yᵢ = γ · x̂ᵢ + β
        
        Output: {y₁, ..., y_m}
    
    forward_pass_inference:
      
      problem_with_batch_stats: |
        At inference:
        - May have single sample (batch size 1)
        - Batch statistics unstable or undefined
        - Can't use batch mean/variance
      
      solution_running_stats: |
        During training, track running averages:
        
        μ_running = α · μ_running + (1-α) · μ_B
        σ²_running = α · σ²_running + (1-α) · σ²_B
        
        Where α = momentum (typically 0.9 or 0.99)
        
        At inference:
        Use μ_running and σ²_running (not batch statistics)
      
      inference_forward: |
        Input: Single sample x
        Parameters: γ, β, μ_running, σ²_running
        
        # Normalize using running stats
        x̂ = (x - μ_running) / √(σ²_running + ε)
        
        # Scale and shift
        y = γ · x̂ + β
        
        Output: y
    
    backward_pass:
      
      gradients_to_compute: |
        Given: ∂L/∂y (gradient from next layer)
        
        Need to compute:
        1. ∂L/∂γ (gradient wrt scale parameter)
        2. ∂L/∂β (gradient wrt shift parameter)
        3. ∂L/∂x (gradient wrt input, for previous layer)
      
      gradient_formulas: |
        # Gradient wrt β (shift)
        ∂L/∂β = Σᵢ ∂L/∂yᵢ
        
        # Gradient wrt γ (scale)
        ∂L/∂γ = Σᵢ (∂L/∂yᵢ · x̂ᵢ)
        
        # Gradient wrt normalized input
        ∂L/∂x̂ᵢ = ∂L/∂yᵢ · γ
        
        # Gradient wrt variance
        ∂L/∂σ²_B = Σᵢ [∂L/∂x̂ᵢ · (xᵢ - μ_B) · (-1/2) · (σ²_B + ε)^(-3/2)]
        
        # Gradient wrt mean
        ∂L/∂μ_B = Σᵢ [∂L/∂x̂ᵢ · (-1/√(σ²_B + ε))] + 
                  ∂L/∂σ²_B · (1/m) · Σᵢ (-2(xᵢ - μ_B))
        
        # Gradient wrt input
        ∂L/∂xᵢ = ∂L/∂x̂ᵢ · (1/√(σ²_B + ε)) + 
                 ∂L/∂σ²_B · (2/m)(xᵢ - μ_B) + 
                 ∂L/∂μ_B · (1/m)
      
      intuition: |
        Backprop through BatchNorm:
        1. Through scale/shift (easy: linear)
        2. Through normalization (need chain rule)
        3. Through mean/variance computation (dependencies)
        
        Each input xᵢ affects:
        - Its own normalization directly
        - Batch mean μ_B (affects all samples)
        - Batch variance σ²_B (affects all samples)
        
        Complex dependency structure!
  
  # --------------------------------------------------------------------------
  # Topic 3: Implementation
  # --------------------------------------------------------------------------
  
  implementation:
    
    batchnorm_layer_class: |
      class BatchNormLayer:
          """
          Batch Normalization layer.
          
          Parameters:
          - num_features: number of features to normalize
          - momentum: for running statistics (default 0.9)
          - epsilon: numerical stability (default 1e-5)
          """
          
          def __init__(self, num_features, momentum=0.9, epsilon=1e-5):
              self.num_features = num_features
              self.momentum = momentum
              self.epsilon = epsilon
              
              # Learnable parameters
              self.gamma = np.ones((num_features, 1))  # Scale
              self.beta = np.zeros((num_features, 1))  # Shift
              
              # Running statistics (for inference)
              self.running_mean = np.zeros((num_features, 1))
              self.running_var = np.ones((num_features, 1))
              
              # Gradients
              self.dgamma = None
              self.dbeta = None
              
              # Cache for backward pass
              self.x = None
              self.x_norm = None
              self.mu = None
              self.var = None
              
              # Mode
              self.training = True
          
          def forward(self, x):
              """
              Forward pass.
              
              Parameters:
              - x: (num_features, batch_size) input
              
              Returns:
              - y: (num_features, batch_size) normalized output
              """
              if self.training:
                  return self._forward_training(x)
              else:
                  return self._forward_inference(x)
          
          def _forward_training(self, x):
              """Training mode: use batch statistics"""
              batch_size = x.shape[1]
              
              # Compute batch statistics
              self.mu = np.mean(x, axis=1, keepdims=True)
              self.var = np.var(x, axis=1, keepdims=True)
              
              # Normalize
              self.x = x
              self.x_norm = (x - self.mu) / np.sqrt(self.var + self.epsilon)
              
              # Scale and shift
              y = self.gamma * self.x_norm + self.beta
              
              # Update running statistics (exponential moving average)
              self.running_mean = (self.momentum * self.running_mean + 
                                  (1 - self.momentum) * self.mu)
              self.running_var = (self.momentum * self.running_var + 
                                 (1 - self.momentum) * self.var)
              
              return y
          
          def _forward_inference(self, x):
              """Inference mode: use running statistics"""
              # Normalize using running stats
              x_norm = ((x - self.running_mean) / 
                       np.sqrt(self.running_var + self.epsilon))
              
              # Scale and shift
              y = self.gamma * x_norm + self.beta
              
              return y
          
          def backward(self, dL_dy):
              """
              Backward pass.
              
              Parameters:
              - dL_dy: (num_features, batch_size) gradient from next layer
              
              Returns:
              - dL_dx: (num_features, batch_size) gradient to previous layer
              """
              batch_size = dL_dy.shape[1]
              
              # Gradient wrt gamma and beta
              self.dgamma = np.sum(dL_dy * self.x_norm, axis=1, keepdims=True)
              self.dbeta = np.sum(dL_dy, axis=1, keepdims=True)
              
              # Gradient wrt normalized input
              dx_norm = dL_dy * self.gamma
              
              # Gradient wrt variance
              dvar = np.sum(dx_norm * (self.x - self.mu) * 
                           -0.5 * (self.var + self.epsilon)**(-1.5), 
                           axis=1, keepdims=True)
              
              # Gradient wrt mean
              dmu = (np.sum(dx_norm * -1.0 / np.sqrt(self.var + self.epsilon), 
                           axis=1, keepdims=True) + 
                    dvar * np.mean(-2.0 * (self.x - self.mu), axis=1, keepdims=True))
              
              # Gradient wrt input
              dL_dx = (dx_norm / np.sqrt(self.var + self.epsilon) + 
                      dvar * 2.0 * (self.x - self.mu) / batch_size + 
                      dmu / batch_size)
              
              return dL_dx
          
          def get_params(self):
              """Return parameters"""
              return {'gamma': self.gamma, 'beta': self.beta}
          
          def get_gradients(self):
              """Return gradients"""
              return {'gamma': self.dgamma, 'beta': self.dbeta}
          
          def train(self):
              """Set to training mode"""
              self.training = True
          
          def eval(self):
              """Set to evaluation mode"""
              self.training = False
    
    network_integration: |
      class NetworkWithBatchNorm:
          """
          Neural network with Batch Normalization.
          """
          
          def __init__(self, layer_dims):
              """
              layer_dims: [input_size, hidden1, hidden2, ..., output]
              """
              self.layer_dims = layer_dims
              self.num_layers = len(layer_dims) - 1
              
              # Linear layers
              self.linear_layers = []
              for l in range(self.num_layers):
                  linear = LinearLayer(layer_dims[l], layer_dims[l+1])
                  self.linear_layers.append(linear)
              
              # BatchNorm layers (after each hidden layer, before activation)
              self.bn_layers = []
              for l in range(self.num_layers - 1):  # Not on output layer
                  bn = BatchNormLayer(layer_dims[l+1])
                  self.bn_layers.append(bn)
              
              # Activation layers
              self.activation_layers = []
              for l in range(self.num_layers - 1):
                  self.activation_layers.append(ReLULayer())
              
              self.output_layer = SoftmaxCrossEntropyLayer()
          
          def forward(self, x, y_true=None):
              """
              Forward: Linear → BatchNorm → Activation
              """
              a = x
              for l in range(self.num_layers):
                  # Linear
                  z = self.linear_layers[l].forward(a)
                  
                  # Hidden layers: BN + activation
                  if l < self.num_layers - 1:
                      z = self.bn_layers[l].forward(z)  # BatchNorm
                      a = self.activation_layers[l].forward(z)
                  else:
                      # Output layer: no BN, no activation
                      logits = z
              
              if y_true is not None:
                  loss, predictions = self.output_layer.forward(logits, y_true)
                  return loss, predictions
              else:
                  z_shifted = logits - np.max(logits, axis=0, keepdims=True)
                  exp_z = np.exp(z_shifted)
                  predictions = exp_z / np.sum(exp_z, axis=0, keepdims=True)
                  return predictions
          
          def backward(self):
              """
              Backward: through BN and linear layers
              """
              dL_dz = self.output_layer.backward()
              
              for l in range(self.num_layers - 1, -1, -1):
                  dL_da_prev = self.linear_layers[l].backward(dL_dz)
                  
                  if l > 0:
                      # Backward through activation
                      dL_dz_bn = self.activation_layers[l-1].backward(dL_da_prev)
                      # Backward through BatchNorm
                      dL_dz = self.bn_layers[l-1].backward(dL_dz_bn)
          
          def train(self):
              """Set to training mode"""
              for bn in self.bn_layers:
                  bn.train()
          
          def eval(self):
              """Set to evaluation mode"""
              for bn in self.bn_layers:
                  bn.eval()
    
    usage_example: |
      # Create network with BatchNorm
      network = NetworkWithBatchNorm([784, 512, 256, 10])
      
      # Training
      network.train()
      for epoch in range(epochs):
          for X_batch, y_batch in train_loader:
              loss = network.train_step(X_batch, y_batch)
      
      # Evaluation (CRITICAL: set to eval mode!)
      network.eval()
      val_loss, val_acc = evaluate(network, X_val, y_val)
  
  # --------------------------------------------------------------------------
  # Topic 4: Benefits and Practical Considerations
  # --------------------------------------------------------------------------
  
  benefits_and_considerations:
    
    convergence_improvements:
      
      faster_training: |
        Without BatchNorm:
        - Learning rate: 0.001
        - Epochs to 95%: 50
        
        With BatchNorm:
        - Learning rate: 0.01 (10x higher!)
        - Epochs to 95%: 20 (2.5x faster!)
        
        Wall-clock speedup: 10x LR × 2.5x fewer epochs ≈ 4x faster
      
      higher_learning_rates: |
        BatchNorm allows 5-10x higher learning rates
        
        Reason: Normalized inputs → bounded gradients
        Can take larger steps without divergence
      
      reduced_initialization_sensitivity: |
        Without BatchNorm: careful initialization critical
        With BatchNorm: works with poor initialization
        
        Example: Random init W ~ N(0, 1) (terrible normally)
        With BatchNorm: Still trains! (BatchNorm normalizes)
    
    regularization_effect:
      
      implicit_regularization: |
        BatchNorm acts as regularization (reduces overfitting)
        
        Mechanism:
        - Each sample normalized by batch statistics
        - Batch statistics noisy (computed on mini-batch)
        - Noise acts as regularizer
        
        Similar to dropout but different mechanism
      
      empirical_impact: |
        Network without BatchNorm:
        - Needs dropout + L2 regularization
        - Test accuracy: 94%
        
        Network with BatchNorm:
        - Can reduce or remove dropout/L2
        - Test accuracy: 96%
        
        BatchNorm provides regularization for free!
    
    batch_size_dependency:
      
      problem: |
        BatchNorm statistics computed per batch
        Small batch → noisy statistics → instability
      
      minimum_batch_size: |
        Batch size < 4: Very unstable, avoid
        Batch size 4-16: Works but suboptimal
        Batch size 32-64: Good (standard)
        Batch size 128+: Best statistics
      
      workaround_for_small_batches: "Use GroupNorm or LayerNorm (discussed later)"
    
    placement_in_network:
      
      standard_practice: |
        Linear → BatchNorm → Activation
        
        Place BN AFTER linear layer, BEFORE activation
      
      alternative: |
        Linear → Activation → BatchNorm
        
        Less common, slightly different behavior
      
      where_to_apply: |
        Apply to:
        - All hidden layers
        
        Don't apply to:
        - Input layer (already normalized)
        - Output layer (want raw logits)
  
  # --------------------------------------------------------------------------
  # Topic 5: Variants of Normalization
  # --------------------------------------------------------------------------
  
  normalization_variants:
    
    layer_normalization:
      
      difference_from_batchnorm: |
        BatchNorm: Normalize across batch dimension
        LayerNorm: Normalize across feature dimension
        
        BatchNorm: μ, σ² computed over samples
        LayerNorm: μ, σ² computed over features
      
      formula: |
        For single sample x with features [x₁, x₂, ..., x_n]:
        
        μ = (1/n) Σᵢ xᵢ
        σ² = (1/n) Σᵢ (xᵢ - μ)²
        x̂ᵢ = (xᵢ - μ) / √(σ² + ε)
        yᵢ = γᵢ · x̂ᵢ + βᵢ
      
      advantages:
        - "No batch size dependency (works with batch size 1)"
        - "Same statistics at train and test time"
        - "No running statistics needed"
      
      disadvantages:
        - "Less effective than BatchNorm for CNNs"
        - "Doesn't provide regularization benefit"
      
      when_to_use:
        - "Recurrent networks (RNNs, LSTMs)"
        - "Small batch sizes (<16)"
        - "Online learning (batch size 1)"
    
    instance_normalization:
      
      description: |
        Normalize each sample and each channel independently
        Used in style transfer and GANs
      
      formula: |
        For each sample, each channel:
        Compute mean and variance
        Normalize separately
    
    group_normalization:
      
      description: |
        Divide channels into groups, normalize within groups
        Compromise between LayerNorm and InstanceNorm
      
      advantages:
        - "No batch size dependency"
        - "Better than LayerNorm for CNNs"
        - "Works with batch size 1-2"
      
      when_to_use:
        - "Small batch sizes (common in object detection)"
        - "High-resolution images (can't fit large batches)"
    
    comparison_table: |
      Normalization | Dimension       | Batch Dependent | Best For
      --------------|-----------------|-----------------|------------------
      Batch         | Across batch    | Yes             | Standard CNNs
      Layer         | Across features | No              | RNNs, small batch
      Instance      | Per sample      | No              | Style transfer
      Group         | Channel groups  | No              | Small batch CNNs
  
  # --------------------------------------------------------------------------
  # Topic 6: Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    running_statistics_leak_information:
      
      observation: |
        Running mean/variance: μ_running, σ²_running
        Computed from training data
        
        These statistics leak information about training set!
      
      membership_inference: |
        Attacker queries model:
        1. Observe output distributions
        2. Infer BatchNorm statistics
        3. Check if candidate sample consistent with statistics
        4. Determine if sample in training set
      
      mitigation: |
        - Don't expose exact running statistics
        - Add noise to statistics (differential privacy)
        - Limit precision of model outputs
    
    model_fingerprinting:
      
      method: |
        BatchNorm parameters (γ, β) and running stats unique per model
        
        Attacker can:
        1. Query suspect model
        2. Extract implicit BatchNorm behavior
        3. Compare to known model
        4. Determine if same model (detect theft)
      
      watermarking_via_batchnorm: |
        Defense: Embed watermark in BatchNorm statistics
        - Modify running mean slightly (imperceptible)
        - Create unique fingerprint
        - Detect model theft
    
    backdoor_and_normalization:
      
      backdoor_detection_harder: |
        BatchNorm normalizes activations
        Backdoor trigger produces unusual activation pattern
        BatchNorm smooths this pattern
        → Harder to detect backdoor
      
      attack_strategy: |
        Attacker benefits from BatchNorm:
        - Backdoor activations normalized
        - Less detectable via activation analysis
      
      defense_consideration: |
        For security-critical applications:
        Consider removing BatchNorm or using LayerNorm
        Trade-off: worse accuracy, better detectability
    
    batch_size_attack:
      
      observation: |
        BatchNorm behavior depends on batch size
        Attacker controls batch size at test time
      
      attack: |
        1. Model trained with batch size 32
        2. Attacker uses batch size 1 at test time
        3. BatchNorm uses running stats (different distribution)
        4. Model behavior changes → potential exploit
      
      mitigation: |
        - Use LayerNorm (no batch dependency)
        - Test with various batch sizes
        - Ensure running stats robust
  
  # --------------------------------------------------------------------------
  # Topic 7: Debugging BatchNorm Issues
  # --------------------------------------------------------------------------
  
  debugging_batchnorm:
    
    common_issues:
      
      forgot_eval_mode:
        symptom: "Test performance much worse than validation"
        
        cause: |
          Network still in training mode during testing
          Uses batch statistics (unstable for small test batches)
        
        fix: |
          network.eval()  # Before testing!
        
        verification: |
          # Check if BN layers in eval mode
          for bn in network.bn_layers:
              assert not bn.training, "BN layer still in training mode!"
      
      batch_too_small:
        symptom: "Training unstable, loss oscillates wildly"
        
        cause: |
          Batch size too small (<8)
          Batch statistics very noisy
        
        fix: |
          - Increase batch size to 32+
          - Or switch to LayerNorm/GroupNorm
        
        verification: |
          # Monitor batch statistics variance
          variances = []
          for batch in dataloader:
              z = linear_layer.forward(batch)
              var = np.var(z, axis=1)
              variances.append(var)
          
          if np.std(variances) > 1.0:
              print("⚠️ Batch statistics unstable, increase batch size")
      
      running_stats_not_updated:
        symptom: "Training accuracy good, test accuracy poor"
        
        cause: |
          Running statistics not being updated
          Or updated incorrectly
        
        fix: |
          # Verify running stats updated during training
          initial_mean = bn_layer.running_mean.copy()
          # Train for one epoch
          final_mean = bn_layer.running_mean.copy()
          
          if np.allclose(initial_mean, final_mean):
              print("⚠️ Running statistics not updated!")
      
      momentum_too_high:
        symptom: "Test accuracy slowly improves over many evaluations"
        
        cause: |
          Running statistics momentum too high (0.99+)
          Takes forever to converge to true statistics
        
        fix: "Reduce momentum to 0.9 or 0.95"
    
    visualization_tools:
      
      activation_distributions: |
        def plot_batchnorm_effect(network, X_batch):
            """
            Visualize activations before and after BatchNorm.
            """
            import matplotlib.pyplot as plt
            
            # Forward pass
            a = X_batch
            activations_before = []
            activations_after = []
            
            for l in range(network.num_layers - 1):
                # Before BN
                z = network.linear_layers[l].forward(a)
                activations_before.append(z.flatten())
                
                # After BN
                z_bn = network.bn_layers[l].forward(z)
                activations_after.append(z_bn.flatten())
                
                # Activation
                a = network.activation_layers[l].forward(z_bn)
            
            # Plot
            n_layers = len(activations_before)
            fig, axes = plt.subplots(2, n_layers, figsize=(4*n_layers, 8))
            
            for l in range(n_layers):
                # Before BN
                axes[0, l].hist(activations_before[l], bins=50, alpha=0.7)
                axes[0, l].set_title(f'Layer {l+1} Before BN')
                axes[0, l].set_xlabel('Value')
                
                mean = np.mean(activations_before[l])
                std = np.std(activations_before[l])
                axes[0, l].text(0.05, 0.95, f'μ={mean:.2f}\nσ={std:.2f}',
                              transform=axes[0, l].transAxes,
                              verticalalignment='top')
                
                # After BN
                axes[1, l].hist(activations_after[l], bins=50, alpha=0.7)
                axes[1, l].set_title(f'Layer {l+1} After BN')
                axes[1, l].set_xlabel('Value')
                
                mean = np.mean(activations_after[l])
                std = np.std(activations_after[l])
                axes[1, l].text(0.05, 0.95, f'μ={mean:.2f}\nσ={std:.2f}',
                              transform=axes[1, l].transAxes,
                              verticalalignment='top')
            
            plt.tight_layout()
            plt.show()
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "BatchNorm normalizes layer inputs: mean=0, variance=1 per mini-batch during training"
      - "Scale and shift learnable: γ, β allow network to undo normalization if optimal"
      - "Train vs test mode critical: training uses batch stats, test uses running stats"
      - "Running statistics tracked: exponential moving average of μ and σ² during training"
      - "Enables higher learning rates: 5-10x increase possible, much faster convergence"
      - "Implicit regularization: batch statistics noise acts as regularizer, reduces overfitting"
    
    actionable_steps:
      - "Place BatchNorm after linear, before activation: Linear → BN → ReLU standard order"
      - "Always call network.eval() before testing: critical, forgot = poor test performance"
      - "Use batch size ≥32 with BatchNorm: smaller batches → unstable statistics"
      - "Set momentum=0.9 for running stats: default works, 0.99 too slow to converge"
      - "Apply to hidden layers only: not input (already normalized) or output (want logits)"
      - "Reduce other regularization: BatchNorm provides regularization, may not need dropout"
    
    security_principles:
      - "Running statistics leak training data: can enable membership inference attacks"
      - "Model fingerprinting via BatchNorm: unique statistics enable detection of model theft"
      - "Backdoors harder to detect: normalization smooths unusual activation patterns"
      - "Batch size dependency exploitable: attacker can change batch size to alter behavior"
    
    debugging_checklist:
      - "Test << train accuracy: check if forgot network.eval() before testing"
      - "Training unstable/oscillating: increase batch size to ≥32 or switch to LayerNorm"
      - "Running stats not converging: reduce momentum from 0.99 to 0.9"
      - "NaN in training: check epsilon (increase to 1e-3), or gradient explosion elsewhere"
      - "Slow convergence with BN: increase learning rate, BatchNorm enables 5-10x higher LR"

---
