# section_04_18_chapter_summary.yaml

---
document_info:
  section: "04_18"
  title: "Chapter 4 Summary: What You Now Know About LLM Security"
  chapter: "04"
  chapter_title: "Modern LLM Internals"
  part: "Part I: Machine Learning Foundations"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2026-01-19"
  estimated_pages: 4
  tags:
    - "chapter-summary"
    - "security-arc"
    - "key-findings"
    - "part-1-conclusion"
    - "part-2-bridge"

purpose: |
  This summary consolidates Chapter 4's 17 content sections into a reference
  document: what each section established, the security finding it contributed,
  and how that finding connects to the attacks and defenses in Parts 2 and 3.
  Use it as a study guide, a pre-exam checklist, and the orientation document
  before starting Chapter 5.

# ============================================================================
# SECTION-BY-SECTION SECURITY FINDINGS
# ============================================================================

section_findings:

  "04_01_gpt2_architecture":
    title: "GPT-2 Architecture"
    core_technical_content: "Transformer decoder stack: token embeddings, causal attention, feedforward layers, residual stream"
    security_finding: |
      Every token attends to every previous token. The residual stream accumulates
      information across layers. There is no architectural isolation between
      system instructions and user input — both are tokens in the same stream.
    attack_surface_opened: "No structural separation between trusted and untrusted context"
    part_2_connection: "Chapter 6 (Prompt Injection): injection works because the model cannot distinguish instruction sources"

  "04_02_gpt3_emergent_capabilities":
    title: "GPT-3 and Emergent Capabilities"
    core_technical_content: "Few-shot in-context learning, chain-of-thought, capability phase transitions at scale"
    security_finding: |
      In-context learning is statistically driven: the model continues patterns
      it has seen in training. An adversary who controls context examples controls
      the pattern being continued. Few-shot examples are a write primitive for
      model behavior.
    attack_surface_opened: "Few-shot poisoning: adversarial examples in context steer model toward harmful outputs"
    part_2_connection: "Chapter 6 (Prompt Injection): indirect injection via retrieved content"

  "04_03_gpt4_multimodal":
    title: "GPT-4 and Multimodal LLMs"
    core_technical_content: "Vision encoders, multimodal token fusion, MoE scaling, capability-alignment gap at scale"
    security_finding: |
      New modalities are new input channels with their own injection surfaces.
      Images can carry adversarial perturbations invisible to humans but
      interpretable as instructions by vision-language models.
    attack_surface_opened: "Visual prompt injection: adversarial images embedding text instructions"
    part_2_connection: "Chapter 6: multimodal injection; Chapter 8: multimodal training data poisoning"

  "04_04_constitutional_ai":
    title: "Constitutional AI"
    core_technical_content: "Critique-revision loop, AI feedback replacing human feedback, principle-based safety"
    security_finding: |
      The constitution is a set of natural language principles — itself a prompt.
      A model that can be convinced the constitution does not apply, or that
      a request does not violate the constitution, bypasses the safety layer.
      Constitutional AI raises the cost of attacks; it does not eliminate them.
    attack_surface_opened: "Constitutional reinterpretation: frame harmful requests as constitution-compliant"
    part_2_connection: "Chapter 7 (Jailbreaks): persuasion-based jailbreaks exploit constitutional reasoning"

  "04_05_rlhf":
    title: "RLHF: Reinforcement Learning from Human Feedback"
    core_technical_content: "Reward model, PPO optimization, sycophancy as a reward-hacking artifact"
    security_finding: |
      RLHF optimizes for annotator ratings, not ground-truth safety. Sycophancy —
      telling users what they want to hear — is a systematic byproduct of RLHF
      that can be exploited: an adversary who provides confident authority claims
      exploits the model's trained bias toward appearing helpful to the evaluator.
    attack_surface_opened: "Sycophancy exploitation: authority claims and confident framing override safety judgment"
    part_2_connection: "Chapter 7: social engineering jailbreaks; Chapter 5: reward hacking formalism"

  "04_06_sft_instruction_tuning":
    title: "Supervised Fine-Tuning and Instruction Tuning"
    core_technical_content: "Instruction-following as a learned behavior, chat templates, alignment as thin overlay"
    security_finding: |
      SFT teaches instruction-following without guaranteeing what instructions
      are followed. The same mechanism that makes models follow safety instructions
      makes them susceptible to adversarial instructions. Alignment is the delta
      between pre-training and SFT — a thin, high-frequency layer fragile to
      compression, fine-tuning, and adversarial pressure.
    attack_surface_opened: "Fine-tuning attacks: minimal fine-tuning can remove SFT safety behaviors"
    part_2_connection: "Chapter 8 (Supply Chain): fine-tuning as an alignment removal technique"

  "04_07_pretraining_at_scale":
    title: "Pre-Training at Scale"
    core_technical_content: "Data pipelines, deduplication, memorization, Chinchilla scaling laws"
    security_finding: |
      Pre-training memorizes verbatim sequences from training data. Models can
      be induced to reproduce memorized content including PII, proprietary code,
      and sensitive documents. The pre-training corpus is also the poisoning
      surface: data injected at training time influences all downstream behavior.
    attack_surface_opened: "Memorization extraction; training data poisoning at corpus scale"
    part_2_connection: "Chapter 8: training data poisoning; Chapter 9: training data extraction"

  "04_08_kv_cache":
    title: "KV Cache"
    core_technical_content: "Key-value caching mechanics, MQA/GQA, eviction policies, PagedAttention"
    security_finding: |
      KV cache memory is finite and shared. Three attack surfaces: (1) cache timing
      side channel reveals whether a prefix is currently cached; (2) memory exhaustion
      DoS via max-context sessions; (3) context stuffing evicts safety instructions
      from cache before they can be attended to.
    attack_surface_opened: "Timing side channels, memory DoS, safety instruction eviction"
    part_2_connection: "Chapter 5: side channel formalism; Chapter 14 (Part 3): production DoS defenses"

  "04_09_flash_attention":
    title: "Flash Attention"
    core_technical_content: "Tiled computation, online softmax, recomputation, long-context enablement"
    security_finding: |
      Flash Attention enables contexts of 100K-1M tokens. Every token in that
      context is a potential injection vector. The lost-in-the-middle effect means
      safety instructions at the center of a long context are attended to weakly —
      creating predictable blind spots that adversaries can exploit by placing
      content in the high-attention zones (beginning and end).
    attack_surface_opened: "Long-context injection surface; position-based safety instruction bypass"
    part_2_connection: "Chapter 6: many-shot jailbreaks exploit long-context injection"

  "04_10_speculative_decoding":
    title: "Speculative Decoding"
    core_technical_content: "Rejection sampling guarantee, draft-target distribution matching, acceptance rate"
    security_finding: |
      Acceptance rate is a covert channel: it reveals the probability ratio
      between draft and target distributions for a given input. On safety-relevant
      inputs where draft and target diverge, the latency difference is measurable.
      A poisoned draft model probes the target for residual harmful capability
      without directly querying the target's safety behaviors.
    attack_surface_opened: "Acceptance rate as distribution probe; throughput DoS via low-acceptance inputs"
    part_2_connection: "Chapter 9 (Model Extraction): speculative decoding as extraction side channel"

  "04_11_context_windows":
    title: "Context Windows and Positional Encodings"
    core_technical_content: "RoPE, ALiBi, absolute PE, length extension, U-shaped attention, attention sinks"
    security_finding: |
      Context window architecture determines the attack surface topology.
      Five entry points (direct input, RAG, tools, history, system spoof).
      Position determines influence: beginning (attention sinks + primacy) and
      end (recency) are high-influence; middle is low-influence. The U-shaped
      attention curve is exploitable: place injection in high-influence positions.
    attack_surface_opened: "Position-optimized injection; all five entry-point attack vectors"
    part_2_connection: "Chapter 6: full prompt injection taxonomy built on this position framework"

  "04_12_quantization":
    title: "Quantization"
    core_technical_content: "INT8/INT4/FP8, absmax, GPTQ, AWQ, GGUF, safety degradation curves"
    security_finding: |
      Quantization degrades safety non-linearly: INT8 loses ~2%, INT4 loses ~7-14%,
      INT2 loses ~35% refusal rate. Safety degrades faster than capability — standard
      benchmarks do not catch it. GGUF files are unsigned: a poisoned quantization
      is indistinguishable from a legitimate one without behavioral testing.
    attack_surface_opened: "Quantization as safety degradation; GGUF supply chain; adversarial calibration"
    part_2_connection: "Chapter 8: supply chain attacks; GGUF as unsigned artifact"

  "04_13_distillation_pruning":
    title: "Model Distillation and Pruning"
    core_technical_content: "KL divergence distillation, safety transfer failure modes, SparseGPT, safety laundering"
    security_finding: |
      Distillation routinely fails to transfer safety behaviors: students achieve
      95%+ capability while having materially worse safety. Safety laundering —
      distilling capability while filtering refusals from the training data — is
      easy to execute and difficult to detect without targeted safety evaluation.
      Model extraction via distillation is the systematic form of capability theft.
    attack_surface_opened: "Safety laundering; model extraction via distillation; pruning removes safety before capability"
    part_2_connection: "Chapter 9: model extraction; Chapter 8: distillation as supply chain attack"

  "04_14_prompt_engineering":
    title: "Prompt Engineering"
    core_technical_content: "Zero-shot, few-shot, CoT, role prompting, adversarial patterns, classifier detection"
    security_finding: |
      Every legitimate prompt engineering technique has an adversarial variant.
      Prompts work by activating training-time statistical patterns — the same
      mechanism that improves capability enables capability exploitation.
      Six adversarial patterns (fictional framing, authority, academic, override,
      escalation, encoding) form the foundation of the jailbreak taxonomy.
      No keyword-matching classifier can cover all variants; semantic classifiers
      with defense in depth are necessary.
    attack_surface_opened: "All six adversarial prompt patterns; few-shot poisoning as RAG attack"
    part_2_connection: "Chapter 7: jailbreak taxonomy extends these six patterns; Chapter 6: few-shot as injection"

  "04_15_system_prompts":
    title: "System Prompts: Architecture and Hardening"
    core_technical_content: "Instruction hierarchy, extraction attacks, override attacks, 6-block hardening architecture"
    security_finding: |
      System prompt authority is statistical, not cryptographic. Confidentiality
      cannot be enforced by prompt alone: sophisticated attackers extract ~80-90%
      of system prompt contents with 20-50 targeted queries. The 6-block hardened
      architecture (primacy anchor, scope, behaviors, security hardening, closing
      anchor, sandwich reminder) reduces extraction to ~5-15% and override success
      to <20% against known techniques.
    attack_surface_opened: "System prompt extraction; indirect injection through RAG/tool output"
    part_2_connection: "Chapter 6: full injection attack taxonomy; Chapter 7: persona override attacks"

  "04_16_api_architecture":
    title: "API Architecture and Security"
    core_technical_content: "Request lifecycle, token-based rate limiting, extraction detection, multi-tenant isolation"
    security_finding: |
      LLM API cost is token-proportional: a single max-context request can cost
      1,462× a normal request within the same request-count rate limit. Model
      extraction requires ~100K-10M queries with measurable diversity signature.
      Multi-tenant isolation does not happen by default in vLLM/TGI — it requires
      explicit architecture and explicit testing. Timing side channels persist at
      the API layer regardless of model-level defenses.
    attack_surface_opened: "Cost amplification DoS; model extraction; timing side channels; cross-tenant leakage"
    part_2_connection: "Chapter 9: model extraction; Chapter 14 (Part 3): production API security"

  "04_17_capstone":
    title: "Capstone: Build, Harden, and Attack a Security Assistant"
    core_technical_content: "End-to-end: SFT fine-tuning → INT8 quantization → hardened system prompt → API wrapper → red team → measurement"
    security_finding: |
      No single defense layer provides meaningful safety alone.
      Stacked defense attribution: model alignment (45%) → +system prompt (+20%)
      → +input classifier (+7%) → +output filter (+7%) = 79% total.
      INT8 + full stack (79%) outperforms FP32 with no stack (45%).
      Residual attack surface persists after all defenses — monitoring is the
      layer that enables detection of what static defenses miss.
    attack_surface_opened: "Residual vulnerabilities documented in red_team_report.md — bridge to Part 2"
    part_2_connection: "All Part 2 chapters: capstone residual vulnerabilities map to Part 2 taxonomy"

# ============================================================================
# THE CHAPTER'S FOUR ARCS: A READING MAP
# ============================================================================

reading_map:

  arc_1_architecture:
    sections: ["04_01", "04_02", "04_03"]
    theme: "What LLMs are"
    security_thread: |
      The transformer's lack of structural separation between instruction sources
      is the root cause of every injection attack. Understanding this eliminates
      the intuition that safety is a solved problem architecturally — it is not.
      The model processes all tokens equivalently; the authority hierarchy is
      entirely learned, not structural.
    if_you_read_only_one_section: "04_01 (GPT-2 architecture) — establishes the foundational mechanics"

  arc_2_alignment:
    sections: ["04_04", "04_05", "04_06"]
    theme: "How safety is trained in (and why it is imperfect)"
    security_thread: |
      Constitutional AI, RLHF, and SFT each address different aspects of alignment —
      and each has specific documented failure modes. The alignment layer is thin
      (small delta from pre-training), high-frequency (relies on precise weight
      configurations), and fragile (compression, fine-tuning, and adversarial
      pressure all degrade it). No current alignment technique produces safety
      that is robust to all adversarial conditions.
    if_you_read_only_one_section: "04_05 (RLHF) — the dominant alignment technique; sycophancy is the most exploited artifact"

  arc_3_inference_stack:
    sections: ["04_07", "04_08", "04_09", "04_10"]
    theme: "How trained models run in production (and the new surfaces that creates)"
    security_thread: |
      Production inference introduces attack surfaces that have nothing to do with
      the model's training. KV cache timing reveals internal system state. Memory
      exhaustion is a resource attack on the inference infrastructure. Long context
      (enabled by Flash Attention) is not just a capability improvement — it is a
      1,000× expansion of the injection surface. Speculative decoding creates a
      distribution probe channel through acceptance rate measurement.
    if_you_read_only_one_section: "04_08 (KV cache) — timing side channels and memory DoS are the most practically relevant"

  arc_4_deployment:
    sections: ["04_11", "04_12", "04_13", "04_14", "04_15", "04_16"]
    theme: "How models are configured, compressed, prompted, and accessed"
    security_thread: |
      Every deployment decision is a security decision. Context window position
      determines instruction influence. Quantization degrades safety faster than
      capability. Distillation may not transfer safety at all. Prompt engineering
      techniques are dual-use. System prompts provide soft, not hard, guarantees.
      API rate limiting must be token-based, not request-based. Defense requires
      all six layers working together.
    if_you_read_only_one_section: "04_15 (system prompts) — most directly actionable for production security engineers"

# ============================================================================
# THE FIVE CORE SECURITY PRINCIPLES
# ============================================================================

core_principles:

  principle_1:
    statement: "Every capability is an attack surface"
    examples:
      - "In-context learning → few-shot poisoning"
      - "Instruction following → instruction override attacks"
      - "Long context → many-shot jailbreaks"
      - "Tool use → indirect injection via tool output"
    implication: "Adding capability to a deployment always adds attack surface. New capabilities require new threat modeling."

  principle_2:
    statement: "Safety is a probability distribution, not a binary property"
    examples:
      - "Refusal rate is a metric, not a guarantee"
      - "INT4 quantization shifts refusal distribution by 7-14%"
      - "System prompt hardening shifts it by +20%"
      - "The distribution changes under adversarial pressure"
    implication: "Safety evaluation must be quantitative. Unquantified safety claims are not defensible."

  principle_3:
    statement: "No single defense layer is sufficient; stacking is multiplicative"
    examples:
      - "Model alignment alone: 45% refusal"
      - "Model + system prompt: 65%"
      - "Model + system prompt + input classifier + output filter: 79%"
    implication: "Design for defense in depth from the start. Retrofitting layers is harder than building them in."

  principle_4:
    statement: "Compression is always a security event"
    examples:
      - "INT4 quantization: -7 to -14% refusal rate"
      - "Distillation: may fail to transfer safety entirely"
      - "Pruning: removes safety before capability"
      - "Safety-aware calibration data: +5-12% refusal preservation"
    implication: "Every quantization, distillation, or pruning requires safety regression testing before deployment."

  principle_5:
    statement: "Monitoring is not optional; it is the intelligence layer"
    examples:
      - "Attacks that pass all static defenses are visible as anomalies in logs"
      - "Extraction attacks have detectable API usage signatures"
      - "Gradual escalation is visible at the conversation level, not the turn level"
      - "Safety regression in production is detectable via refusal rate monitoring"
    implication: "A deployed model without monitoring has no feedback loop. Monitoring enables the system to improve."

# ============================================================================
# ATTACK SURFACE MAP: CHAPTER 4 → PART 2 CHAPTERS
# ============================================================================

attack_surface_to_chapter_map:
  description: |
    Every attack surface identified in Chapter 4 has a corresponding Part 2 chapter
    that examines it in depth. Use this map to navigate from "I found this vulnerability
    in my deployment" to "here is the chapter that tells me how to exploit and defend it."

  mapping:
    no_structural_separation_in_context:
      chapter_4_sections: ["04_01", "04_11"]
      part_2_chapter: "Chapter 6: Prompt Injection"
      what_part_2_adds: "Full injection taxonomy, indirect injection via RAG, agentic injection chains"

    few_shot_poisoning_and_rag_injection:
      chapter_4_sections: ["04_02", "04_14"]
      part_2_chapter: "Chapter 6: Prompt Injection"
      what_part_2_adds: "Retrieval poisoning at scale, tool output injection, document processing injection"

    alignment_fragility_and_jailbreaks:
      chapter_4_sections: ["04_04", "04_05", "04_06", "04_14"]
      part_2_chapter: "Chapter 7: Jailbreaks"
      what_part_2_adds: "Gradient-based optimization (GCG, AutoDAN), many-shot jailbreaks, transfer attacks"

    training_data_poisoning_and_memorization:
      chapter_4_sections: ["04_07"]
      part_2_chapter: "Chapter 8: Training Data and Supply Chain Attacks"
      what_part_2_adds: "Backdoor attacks, data poisoning methodology, supply chain threat model"

    distillation_and_quantization_supply_chain:
      chapter_4_sections: ["04_12", "04_13"]
      part_2_chapter: "Chapter 8: Training Data and Supply Chain Attacks"
      what_part_2_adds: "GGUF poisoning, safety laundering at scale, detection methodology"

    model_extraction_via_api:
      chapter_4_sections: ["04_10", "04_13", "04_16"]
      part_2_chapter: "Chapter 9: Model Extraction and Inversion"
      what_part_2_adds: "Systematic extraction methodology, logit-based extraction, watermarking defenses"

    timing_and_side_channels:
      chapter_4_sections: ["04_08", "04_10", "04_16"]
      part_2_chapter: "Chapter 5: Adversarial ML Foundations"
      what_part_2_adds: "Formal side channel analysis, differential privacy, information-theoretic bounds"

# ============================================================================
# VOCABULARY REFERENCE
# ============================================================================

vocabulary_reference:
  description: "Key terms introduced in Chapter 4, organized for review"

  architecture_terms:
    - term: "Residual stream"
      definition: "The vector that accumulates information across transformer layers via skip connections"
      introduced: "04_01"

    - term: "Attention sink"
      definition: "Initial tokens that receive disproportionately high attention weight from all subsequent tokens"
      introduced: "04_08"

    - term: "KV cache"
      definition: "Stored key-value pairs from previous tokens that eliminate recomputation during autoregressive generation"
      introduced: "04_08"

    - term: "Prefill vs decode"
      definition: "Prefill processes the full prompt in parallel; decode generates tokens one at a time using the KV cache"
      introduced: "04_08"

    - term: "Acceptance rate (speculative decoding)"
      definition: "Fraction of draft tokens accepted by the target model's rejection sampling; proxy for distribution similarity"
      introduced: "04_10"

    - term: "Lost in the middle"
      definition: "Empirical phenomenon: LLM attention is strongest at context beginning and end, weakest in the middle"
      introduced: "04_09, 04_11"

  alignment_terms:
    - term: "RLHF"
      definition: "Reinforcement Learning from Human Feedback: optimize model to maximize a reward model trained on human preferences"
      introduced: "04_05"

    - term: "Sycophancy"
      definition: "RLHF artifact: model tells users what they want to hear rather than what is accurate, to maximize approval"
      introduced: "04_05"

    - term: "Constitutional AI"
      definition: "Alignment technique using a set of natural language principles as the basis for AI-generated safety feedback"
      introduced: "04_04"

    - term: "Alignment tax"
      definition: "Capability reduction caused by safety training; the cost of making a model more aligned"
      introduced: "04_05"

  compression_terms:
    - term: "Quantization error"
      definition: "The difference between a weight's original value and its nearest representable value at reduced precision"
      introduced: "04_12"

    - term: "GPTQ"
      definition: "Post-training quantization using Hessian-based weight compensation to minimize quantization error"
      introduced: "04_12"

    - term: "Safety laundering"
      definition: "Distilling capability from an aligned model while filtering safety behaviors from the distillation training data"
      introduced: "04_13"

    - term: "Calibration data"
      definition: "Examples used to compute importance scores during GPTQ/AWQ quantization; determines which weights are protected"
      introduced: "04_12, 04_13"

  deployment_terms:
    - term: "System prompt"
      definition: "Developer-provided instructions in the highest-trust context window position; defines model persona, scope, and constraints"
      introduced: "04_15"

    - term: "Sandwich reminder"
      definition: "A brief safety constraint injected immediately before the user message to exploit recency bias"
      introduced: "04_15"

    - term: "Provenance marker"
      definition: "Explicit labels (e.g., [EXTERNAL]) that identify content source and trust level within the context window"
      introduced: "04_11, 04_15"

    - term: "Token-based rate limiting"
      definition: "API rate limiting measured in tokens consumed rather than requests, correctly capturing LLM cost"
      introduced: "04_16"

    - term: "Extraction risk score"
      definition: "Composite metric combining request diversity, volume, logprob access rate, and topic coverage to detect model extraction"
      introduced: "04_16"

# ============================================================================
# SELF-ASSESSMENT QUESTIONS
# ============================================================================

self_assessment:
  description: "Answer these before starting Chapter 5. If any are difficult, review the referenced section."

  architecture:
    - question: "Why does the transformer's lack of structural trust separation make prompt injection fundamental rather than a surface-level vulnerability?"
      section: "04_01, 04_11"

    - question: "What is the U-shaped attention curve and what does it imply for where safety instructions should be placed in a system prompt?"
      section: "04_09, 04_11, 04_15"

    - question: "Why does the KV cache timing side channel exist, and what information does it reveal?"
      section: "04_08"

  alignment:
    - question: "What is sycophancy, how does RLHF produce it, and how do attackers exploit it?"
      section: "04_05"

    - question: "Why is alignment described as a 'thin overlay' on pre-training weights, and what does this imply for compression?"
      section: "04_06, 04_12"

    - question: "What is the three-scenario safety transfer framework for distillation? Give one real-world indicator for each scenario."
      section: "04_13"

  deployment:
    - question: "What makes a system prompt 'hardened'? Describe the 6-block architecture and the security purpose of each block."
      section: "04_15"

    - question: "Why is request-based API rate limiting insufficient for LLMs? What is the correct metric and why?"
      section: "04_16"

    - question: "What are the four signals used to detect model extraction from API logs?"
      section: "04_16"

  synthesis:
    - question: "The stacked defense table shows: model alone (45%) → +system prompt (65%) → +input classifier (72%) → +output filter (79%). What does this imply about the correct design approach for LLM deployments?"
      section: "04_17"

    - question: "A colleague says 'we're using INT4 quantization but it's fine, perplexity only dropped 3%.' What is wrong with this reasoning and what should they do instead?"
      section: "04_12"

    - question: "Name three attack surfaces that open specifically because of Flash Attention's enablement of long contexts."
      section: "04_09, 04_11"

# ============================================================================
# PART 2 PREVIEW
# ============================================================================

part_2_preview:
  title: "What Part 2 Builds On Chapter 4"

  chapter_5_adversarial_ml_foundations:
    previewed_in: "04_08 (timing side channels), 04_10 (acceptance rate probing)"
    what_it_covers: |
      The mathematical formalism behind every attack observed informally in Chapter 4.
      Threat modeling methodology. Adversarial examples in the input space.
      The information-theoretic bounds on what side channels can reveal.
      How to formalize 'this attack works sometimes' into 'this attack works with
      probability P given these conditions.'

  chapter_6_prompt_injection:
    previewed_in: "04_01, 04_02, 04_11, 04_14, 04_15"
    what_it_covers: |
      The complete prompt injection taxonomy: direct injection, indirect injection,
      agentic injection chains, stored injection. Real-world CVEs and incident reports.
      RAG poisoning methodology at production scale. Defense strategies beyond
      what a system prompt alone can achieve.

  chapter_7_jailbreaks:
    previewed_in: "04_04, 04_05, 04_14"
    what_it_covers: |
      Systematic jailbreak taxonomy. Gradient-based optimization (GCG: Greedy
      Coordinate Gradient) that finds adversarial suffixes automatically. AutoDAN
      for human-readable jailbreaks. Many-shot jailbreaks exploiting long context.
      Transferability: why jailbreaks found on open-source models work on closed models.
      Jailbreak defenses and their known limits.

  chapter_8_supply_chain:
    previewed_in: "04_07, 04_12, 04_13"
    what_it_covers: |
      Training data poisoning: backdoor attacks, sleeper agents, trigger-based behaviors.
      Supply chain threat model: every step from data collection to model distribution
      is an attack surface. GGUF poisoning in practice. Safety laundering at scale.
      Defending the ML supply chain.

  chapter_9_model_extraction:
    previewed_in: "04_10, 04_13, 04_16"
    what_it_covers: |
      Systematic model extraction methodology. Distillation-based extraction cost analysis.
      Logit-level extraction vs token-level extraction. Watermarking and fingerprinting
      for attribution. API-layer defenses and their evasion. Legal landscape of model theft.

connections:
  closes: "Chapter 4 (all 18 sections)"
  opens: "Chapter 5: Adversarial ML Foundations (Part 2 begins)"
  part_1_summary: |
    Part 1 (Chapters 1-4) has established the complete technical foundation.
    Chapter 1 built the mathematical tools (linear algebra, calculus, probability).
    Chapter 2 showed how neural networks learn representations.
    Chapter 3 derived the transformer architecture from first principles.
    Chapter 4 showed how production LLMs are built, deployed, and attacked.

    Part 1's central security thesis:
      The attack surface of an LLM system is the union of attack surfaces
      at every layer of its stack: training data, pre-training, alignment,
      inference infrastructure, compression, prompting, and API boundary.
      Safety cannot be achieved by defending any single layer.
      It requires understanding — and defending — the complete stack.

    That understanding is what Part 1 has built.
    Part 2 uses it.

---
