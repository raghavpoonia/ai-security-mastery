# section_03_19_context_windows.yaml

---
document_info:
  title: "Context Windows and Memory Management"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 19
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into context windows: attention complexity, context length limitations, efficient attention mechanisms, long-range transformers, and security implications"
  estimated_pages: 6
  tags:
    - context-window
    - attention-complexity
    - sliding-window
    - sparse-attention
    - efficient-transformers
    - memory-management

section_overview:
  title: "Context Windows and Memory Management"
  number: "3.19"
  
  purpose: |
    Transformer context windows determine how much text the model can process at once. Standard 
    self-attention has O(n²) complexity in sequence length - quadratic scaling that quickly 
    becomes prohibitive. BERT handles 512 tokens, GPT-2 extends to 1024, GPT-3 reaches 2048, 
    but longer contexts require fundamentally different approaches. This section covers why 
    context windows matter, the computational barriers, and efficient attention mechanisms 
    that enable longer context.
    
    The O(n²) complexity comes from computing attention scores between all token pairs: for 
    n tokens, we compute n×n scores. At 512 tokens, that's 262K comparisons. At 4096 tokens, 
    16.7M comparisons. Memory and compute grow quadratically, creating hard limits on context 
    length. Solutions include sliding window attention, sparse attention patterns, and hybrid 
    approaches like Longformer and BigBird.
    
    For security engineers: Context window limits create vulnerabilities where adversaries 
    place payloads knowing they'll push security-relevant information out of context. Long 
    inputs enable DoS attacks through quadratic complexity. Context truncation strategies 
    affect what the model sees and misses. Understanding context management reveals both 
    defensive truncation approaches and offensive context manipulation techniques.
  
  learning_objectives:
    conceptual:
      - "Understand O(n²) attention complexity and its implications"
      - "Grasp context window limits: 512, 1024, 2048, 4096+ tokens"
      - "Learn sliding window attention: local context windows"
      - "See sparse attention: predetermined sparsity patterns"
      - "Compare efficient attention mechanisms and trade-offs"
    
    practical:
      - "Implement sliding window attention"
      - "Build sparse attention pattern generator"
      - "Analyze context length vs complexity trade-offs"
      - "Create context truncation strategies"
      - "Demonstrate long-context handling techniques"
    
    security_focused:
      - "Exploit context window limits for payload placement"
      - "Launch DoS attacks via long inputs"
      - "Manipulate context truncation to hide information"
      - "Analyze what gets pushed out of context"
      - "Audit context management for security vulnerabilities"
  
  prerequisites:
    knowledge:
      - "Section 3.6-3.8: Self-attention mechanism"
      - "Understanding of computational complexity"
      - "Memory and compute trade-offs"
      - "Basic linear algebra (matrix operations)"
    
    skills:
      - "Attention implementation"
      - "Complexity analysis"
      - "Memory profiling"
      - "Algorithm optimization"
  
  key_transitions:
    from_section_3_18: |
      Section 3.18 covered T5 and architectural variants. Now we examine a fundamental 
      constraint shared by all transformers: context window limitations from O(n²) attention 
      complexity and approaches to extend context length.
    
    to_next_section: |
      Section 3.20 will cover LLM security comprehensively - attacks like prompt injection, 
      jailbreaking, data extraction, and defenses. Context window manipulation will be one 
      of many attack vectors examined.

topics:
  - topic_number: 1
    title: "Attention Complexity and Context Window Limits"
    
    overview: |
      Standard self-attention computes attention scores between all token pairs, resulting 
      in O(n²) complexity in sequence length n. This quadratic scaling limits practical 
      context windows. Memory requirements also grow quadratically, creating hard constraints 
      on maximum sequence length even with unlimited compute.
    
    content:
      quadratic_complexity_explained:
        attention_computation: |
          Self-attention formula:
          Attention(Q, K, V) = softmax(QK^T / √d_k) × V
          
          Key operation: QK^T
          - Q: (n, d_k) query matrix
          - K^T: (d_k, n) key matrix transposed
          - Result: (n, n) attention scores
          
          → n² computations!
        
        complexity_breakdown:
          matrix_multiply: "O(n² × d_k) for QK^T"
          softmax: "O(n²) over attention scores"
          value_multiply: "O(n² × d_v) for attention × V"
          total: "O(n² × d_model)"
        
        scaling_example: |
          Sequence length 512: 512² = 262,144 attention scores
          Sequence length 1024: 1,024² = 1,048,576 (4× more)
          Sequence length 2048: 2,048² = 4,194,304 (16× more)
          Sequence length 4096: 4,096² = 16,777,216 (64× more)
          
          → Doubling length = 4× compute!
      
      memory_requirements:
        attention_matrix_storage: |
          Attention scores: (n, n) matrix
          Batch size b, h heads: (b, h, n, n)
          
          FP32: 4 bytes per float
          Memory = b × h × n² × 4 bytes
        
        memory_examples:
          bert_base: |
            Batch 32, 12 heads, 512 length
            Memory = 32 × 12 × 512² × 4 = 402MB (just attention!)
          
          long_context: |
            Batch 8, 12 heads, 4096 length
            Memory = 8 × 12 × 4096² × 4 = 6.4GB (attention only!)
          
          catastrophic_scaling: |
            Sequence 16K, batch 4, 16 heads
            Memory = 4 × 16 × 16384² × 4 = 68GB
            → Doesn't fit on single GPU!
        
        total_memory: |
          Model parameters
          + Optimizer states (2× params for Adam)
          + Activations (all layer outputs)
          + Attention matrices (largest for long sequences)
          + Gradients (same size as activations)
      
      context_window_limits_in_practice:
        bert: "512 tokens (original)"
        gpt_2: "1024 tokens"
        gpt_3: "2048 tokens (davinci)"
        gpt_3_5_turbo: "4096 tokens"
        gpt_4: "8K tokens (standard), 32K tokens (extended)"
        claude_2: "100K tokens"
        claude_3: "200K tokens"
        
        why_limits_exist: |
          1. Memory constraints (quadratic growth)
          2. Compute cost (quadratic operations)
          3. Training time (longer sequences = slower)
          4. Position encoding limits (learned embeddings)
      
      truncation_strategies:
        first_k_tokens: |
          Keep first k tokens, discard rest
          → Loses end of document
        
        last_k_tokens: |
          Keep last k tokens, discard beginning
          → Loses context, keeps recent
        
        sliding_window: |
          Keep most recent k tokens
          → Window slides as new tokens arrive
        
        head_tail: |
          Keep first k/2 and last k/2 tokens
          → Preserves beginning and end, loses middle
        
        importance_based: |
          Score tokens by importance
          → Keep top-k important tokens
          → Complex, expensive
    
    implementation:
      complexity_analyzer:
        language: python
        code: |
          import numpy as np
          
          def analyze_attention_complexity(seq_len: int,
                                          d_model: int = 768,
                                          num_heads: int = 12,
                                          batch_size: int = 1) -> dict:
              """
              Analyze computational and memory complexity of self-attention.
              
              Args:
                  seq_len: Sequence length
                  d_model: Model dimension
                  num_heads: Number of attention heads
                  batch_size: Batch size
              
              Returns:
                  metrics: Complexity metrics
              """
              d_k = d_model // num_heads
              
              # Computational complexity
              # QK^T: (n, d_k) @ (d_k, n) = O(n² × d_k)
              qk_flops = batch_size * num_heads * seq_len * seq_len * d_k
              
              # Softmax: O(n²)
              softmax_flops = batch_size * num_heads * seq_len * seq_len
              
              # Attention × V: (n, n) @ (n, d_v) = O(n² × d_v)
              attn_v_flops = batch_size * num_heads * seq_len * seq_len * d_k
              
              total_flops = qk_flops + softmax_flops + attn_v_flops
              
              # Memory requirements (FP32)
              # Attention matrix: (batch, heads, n, n)
              attention_memory = batch_size * num_heads * seq_len * seq_len * 4  # bytes
              
              # Q, K, V matrices: 3 × (batch, heads, n, d_k)
              qkv_memory = 3 * batch_size * num_heads * seq_len * d_k * 4
              
              total_memory = attention_memory + qkv_memory
              
              return {
                  'seq_len': seq_len,
                  'total_flops': total_flops,
                  'flops_billions': total_flops / 1e9,
                  'attention_memory_mb': attention_memory / (1024**2),
                  'total_memory_mb': total_memory / (1024**2),
                  'attention_scores': seq_len * seq_len
              }
          
          
          # Example analysis
          print("=== Attention Complexity Analysis ===\n")
          
          lengths = [512, 1024, 2048, 4096, 8192]
          
          print(f"{'Length':>6} | {'Attn Scores':>12} | {'FLOPs (B)':>10} | {'Memory (MB)':>12}")
          print("-" * 60)
          
          for seq_len in lengths:
              metrics = analyze_attention_complexity(seq_len)
              print(f"{seq_len:6,} | {metrics['attention_scores']:12,} | "
                    f"{metrics['flops_billions']:10.2f} | "
                    f"{metrics['attention_memory_mb']:12.1f}")
          
          print()
          print("Key observations:")
          print("  - Doubling length → 4× attention scores")
          print("  - Doubling length → 4× memory")
          print("  - 4096 tokens: ~17M attention scores, ~6.4GB memory")
          print("  - 8192 tokens: ~67M attention scores, ~25GB memory")
          print("  → Quadratic scaling creates hard limits!")
      
      truncation_demo:
        language: python
        code: |
          def demonstrate_truncation_strategies(text_tokens: np.ndarray,
                                               max_len: int = 512):
              """Demonstrate different truncation strategies."""
              
              print("\n=== Context Truncation Strategies ===\n")
              
              original_len = len(text_tokens)
              print(f"Original length: {original_len} tokens")
              print(f"Max context: {max_len} tokens")
              print(f"Must remove: {original_len - max_len} tokens\n")
              
              # Strategy 1: First k
              first_k = text_tokens[:max_len]
              print(f"1. First-k truncation:")
              print(f"   Keep: tokens 0-{max_len-1}")
              print(f"   Lose: tokens {max_len}-{original_len-1}")
              print(f"   Effect: Lose end of document\n")
              
              # Strategy 2: Last k
              last_k = text_tokens[-max_len:]
              print(f"2. Last-k truncation:")
              print(f"   Keep: tokens {original_len-max_len}-{original_len-1}")
              print(f"   Lose: tokens 0-{original_len-max_len-1}")
              print(f"   Effect: Lose beginning, keep recent\n")
              
              # Strategy 3: Head-tail
              head_size = max_len // 2
              tail_size = max_len - head_size
              head_tail = np.concatenate([
                  text_tokens[:head_size],
                  text_tokens[-tail_size:]
              ])
              print(f"3. Head-tail truncation:")
              print(f"   Keep: tokens 0-{head_size-1} and {original_len-tail_size}-{original_len-1}")
              print(f"   Lose: middle {original_len - max_len} tokens")
              print(f"   Effect: Preserve beginning and end\n")
              
              return {
                  'first_k': first_k,
                  'last_k': last_k,
                  'head_tail': head_tail
              }
          
          
          # Example
          text_tokens = np.arange(2000)  # Simulate 2000 tokens
          max_len = 512
          
          strategies = demonstrate_truncation_strategies(text_tokens, max_len)
          
          print("Security implications:")
          print("  - Adversary knows truncation strategy")
          print("  - Can place payload where it won't be truncated")
          print("  - First-k: payload at beginning survives")
          print("  - Last-k: payload at end survives")
          print("  - Head-tail: payload in middle gets removed")
    
    security_implications:
      context_overflow_attacks: |
        Adversary exploits context limits:
        - Place malicious payload at start
        - Flood with benign tokens
        - Security-relevant info pushed out of context
        - Model never sees warnings/constraints
        - Defense: Critical info first, validate within window
      
      dos_via_long_inputs: |
        Quadratic complexity enables DoS:
        - Send very long sequences (8K, 16K tokens)
        - O(n²) computation overwhelms server
        - Memory requirements cause OOM crashes
        - Defense: Hard input length limits, rate limiting

  - topic_number: 2
    title: "Efficient Attention Mechanisms"
    
    overview: |
      Efficient attention reduces O(n²) complexity through sparsity - not all tokens need 
      to attend to all others. Sliding window attention limits each token to local neighbors. 
      Sparse attention uses predetermined patterns (strided, dilated). Longformer and BigBird 
      combine local, global, and random attention for linear complexity while maintaining 
      long-range dependencies.
    
    content:
      sliding_window_attention:
        concept: |
          Each token attends only to w neighbors:
          
          Position i attends to: i-w/2 ... i ... i+w/2
          
          Window size w (e.g., 256)
          → Only local context, not full sequence
        
        complexity: |
          Standard attention: O(n²)
          Sliding window: O(n × w)
          
          If w is constant (e.g., 256):
          → O(n) linear complexity!
        
        advantages:
          - "Linear complexity in sequence length"
          - "Constant memory per token"
          - "Can handle very long sequences"
        
        disadvantages:
          - "No long-range dependencies"
          - "Information flows slowly across distance"
          - "Token at position 0 can't directly see position 1000"
      
      sparse_attention_patterns:
        strided_attention: |
          Attend to every k-th token:
          Position i attends to: i-k, i-2k, i-3k, ...
          
          Reduces attention from n to n/k positions
        
        dilated_attention: |
          Like strided but with gaps:
          Position i attends to: i-1, i-2, i-4, i-8, i-16, ...
          
          Logarithmic coverage of history
        
        fixed_patterns: |
          Predefined sparsity patterns:
          - Attend to first token (global)
          - Attend to local window
          - Attend to strided positions
          
          Combine patterns for coverage + efficiency
      
      longformer:
        architecture: |
          Combines three attention patterns:
          
          1. Local attention: Sliding window (w=512)
          2. Global attention: Some tokens attend to all
          3. Dilated attention: Strided pattern
        
        complexity: "O(n × w) with small w → O(n) linear"
        
        max_length: "4096 tokens (standard), up to 16K+"
        
        use_cases: |
          Long document understanding
          Document QA
          Summarization
      
      bigbird:
        architecture: |
          Sparse attention with three components:
          
          1. Random attention: Random token pairs
          2. Window attention: Local neighborhood
          3. Global attention: Special global tokens
        
        theoretical_result: |
          BigBird is universal approximator:
          → Can approximate any sequence function
          → Despite sparsity!
        
        complexity: "O(n) linear"
        
        max_length: "4096 tokens standard"
      
      attention_comparison:
        full_attention:
          complexity: "O(n²)"
          max_practical_length: "2048 tokens"
          dependencies: "All-to-all"
        
        sliding_window:
          complexity: "O(n × w)"
          max_practical_length: "16K+ tokens"
          dependencies: "Local only"
        
        longformer:
          complexity: "O(n)"
          max_practical_length: "16K+ tokens"
          dependencies: "Local + global + dilated"
        
        bigbird:
          complexity: "O(n)"
          max_practical_length: "8K+ tokens"
          dependencies: "Local + random + global"
    
    implementation:
      sliding_window_attention:
        language: python
        code: |
          def sliding_window_attention_mask(seq_len: int,
                                           window_size: int = 256) -> np.ndarray:
              """
              Create sliding window attention mask.
              
              Args:
                  seq_len: Sequence length
                  window_size: Window size (total, centered on position)
              
              Returns:
                  mask: (seq_len, seq_len) attention mask
              """
              # Initialize mask (1 = can attend, 0 = masked)
              mask = np.zeros((seq_len, seq_len), dtype=bool)
              
              half_window = window_size // 2
              
              for i in range(seq_len):
                  # Position i attends to window around itself
                  start = max(0, i - half_window)
                  end = min(seq_len, i + half_window + 1)
                  mask[i, start:end] = True
              
              return mask
          
          
          # Example usage
          print("\n=== Sliding Window Attention ===\n")
          
          seq_len = 1024
          window_size = 256
          
          mask = sliding_window_attention_mask(seq_len, window_size)
          
          # Count attended positions per token
          attended_per_token = mask.sum(axis=1)
          
          print(f"Sequence length: {seq_len}")
          print(f"Window size: {window_size}")
          print(f"Standard attention: {seq_len:,} positions per token")
          print(f"Sliding window: ~{int(attended_per_token.mean())} positions per token")
          print()
          
          # Complexity comparison
          standard_ops = seq_len * seq_len
          sliding_ops = seq_len * window_size
          
          print(f"Operations comparison:")
          print(f"  Standard: {standard_ops:,} (n²)")
          print(f"  Sliding:  {sliding_ops:,} (n×w)")
          print(f"  Reduction: {standard_ops // sliding_ops}× fewer operations")
          print()
          
          # Visualize pattern (small example)
          small_len = 16
          small_window = 4
          small_mask = sliding_window_attention_mask(small_len, small_window)
          
          print("Sliding window pattern (16 tokens, window=4):")
          print("  0 = masked, 1 = can attend\n")
          for i in range(small_len):
              row = "".join("█" if small_mask[i, j] else "·" for j in range(small_len))
              print(f"  {i:2}: {row}")
      
      sparse_attention_patterns:
        language: python
        code: |
          def create_sparse_attention_patterns(seq_len: int) -> dict:
              """Create various sparse attention patterns."""
              
              patterns = {}
              
              # 1. Strided attention (attend every 64 positions)
              stride = 64
              strided = np.zeros((seq_len, seq_len), dtype=bool)
              for i in range(seq_len):
                  for j in range(0, i+1, stride):
                      strided[i, j] = True
                  strided[i, i] = True  # Always attend to self
              patterns['strided'] = strided
              
              # 2. Dilated attention (exponential spacing)
              dilated = np.zeros((seq_len, seq_len), dtype=bool)
              for i in range(seq_len):
                  # Attend to i, i-1, i-2, i-4, i-8, i-16, ...
                  offset = 1
                  while i - offset >= 0:
                      dilated[i, i - offset] = True
                      offset *= 2
                  dilated[i, i] = True
              patterns['dilated'] = dilated
              
              # 3. Block sparse (blocks of size 64)
              block_size = 64
              block_sparse = np.zeros((seq_len, seq_len), dtype=bool)
              for i in range(seq_len):
                  block_start = (i // block_size) * block_size
                  block_end = min(block_start + block_size, seq_len)
                  block_sparse[i, block_start:block_end] = True
              patterns['block_sparse'] = block_sparse
              
              return patterns
          
          
          print("\n=== Sparse Attention Patterns ===\n")
          
          seq_len = 512
          patterns = create_sparse_attention_patterns(seq_len)
          
          full_attention = seq_len * seq_len
          
          print(f"Sequence length: {seq_len}")
          print(f"Full attention: {full_attention:,} connections\n")
          
          for name, pattern in patterns.items():
              connections = pattern.sum()
              sparsity = 1 - (connections / full_attention)
              
              print(f"{name.capitalize()}:")
              print(f"  Connections: {int(connections):,}")
              print(f"  Sparsity: {sparsity:.1%}")
              print(f"  Reduction: {full_attention // connections}× fewer\n")
    
    security_implications:
      sparse_attention_information_leakage: |
        Sparse patterns reveal model behavior:
        - Adversary knows which positions can interact
        - Can place related malicious tokens at interacting positions
        - Sliding window: payload spread across window
        - Defense: Randomize patterns, validate dependencies
      
      local_attention_bypass: |
        Local attention limits information flow:
        - Security constraint at position 0
        - Malicious payload at position 1000
        - No direct path, information doesn't propagate
        - Adversary exploits locality to bypass checks
        - Defense: Global attention on security tokens

  - topic_number: 3
    title: "Long-Context Transformers and Memory"
    
    overview: |
      Extending context beyond 4096 tokens requires architectural innovation. Approaches 
      include memory augmentation (external memory), recurrence (process chunks sequentially), 
      compression (summarize past context), and hybrid methods. Each trades off between 
      context length, computational efficiency, and model capability.
    
    content:
      memory_augmented_transformers:
        compressive_transformer: |
          Maintains two memory banks:
          1. Short-term memory: Recent tokens (standard attention)
          2. Long-term memory: Compressed past (learned compression)
          
          Old tokens compressed and stored
          → Access to very long history with constant memory
        
        memorizing_transformer: |
          External key-value memory:
          - Store past (key, value) pairs
          - Retrieve k-nearest neighbors
          - Attend to retrieved memories
          
          → Effectively infinite context (retrieval-based)
        
        recurrent_memory_transformer: |
          Segment-level recurrence:
          - Process document in chunks
          - Pass hidden state between chunks
          - Each chunk has standard attention
          
          → Linear memory, but recurrent (slower)
      
      context_compression:
        summarization_based: |
          Periodically summarize past context:
          - Every 1024 tokens, generate summary
          - Replace old tokens with summary
          - Continue with compressed history
        
        learned_compression: |
          Trainable compression module:
          - Learn to compress past into fixed-size vector
          - Query compressed representation
          - Gradients train what to remember
        
        attention_pooling: |
          Pool attention patterns:
          - Compress attention to representative tokens
          - Subsequent layers attend to pooled tokens
          - Hierarchical compression
      
      practical_approaches:
        chunking_and_sliding: |
          Process long documents in overlapping chunks:
          - Chunk 1: tokens 0-512
          - Chunk 2: tokens 256-768 (50% overlap)
          - Chunk 3: tokens 512-1024
          ...
          
          Aggregate results across chunks
        
        hierarchical_processing: |
          Process at multiple granularities:
          - Sentence-level attention within paragraph
          - Paragraph-level attention within document
          - Document-level attention across corpus
        
        retrieval_augmentation: |
          Don't put everything in context:
          - Retrieve relevant passages
          - Add only relevant context to prompt
          - RAG (Retrieval-Augmented Generation)
      
      context_length_progression:
        timeline:
          bert_2018: "512 tokens"
          gpt_2_2019: "1024 tokens"
          gpt_3_2020: "2048 tokens"
          longformer_2020: "4096 tokens"
          gpt_3_5_2022: "4096 tokens"
          claude_2_2023: "100K tokens"
          gpt_4_2023: "32K tokens"
          claude_3_2024: "200K tokens"
          gemini_pro_2024: "1M tokens (claimed)"
        
        how_100k_possible: |
          Combination of techniques:
          - Efficient attention (sparse patterns)
          - Better hardware (more GPU memory)
          - Compression techniques
          - Clever engineering
    
    implementation:
      context_length_comparison:
        language: python
        code: |
          def compare_context_approaches(seq_len: int = 16384):
              """Compare different approaches for long context."""
              
              print("\n=== Long Context Approaches ===\n")
              print(f"Target sequence length: {seq_len:,} tokens\n")
              
              # 1. Full attention (baseline)
              full_ops = seq_len * seq_len
              full_mem = seq_len * seq_len * 4  # bytes
              
              print("1. Full Attention (O(n²)):")
              print(f"   Operations: {full_ops:,}")
              print(f"   Memory: {full_mem / (1024**3):.2f} GB")
              print(f"   Feasible: No (too expensive)\n")
              
              # 2. Sliding window
              window = 512
              sliding_ops = seq_len * window
              sliding_mem = seq_len * window * 4
              
              print(f"2. Sliding Window (w={window}):")
              print(f"   Operations: {sliding_ops:,}")
              print(f"   Memory: {sliding_mem / (1024**2):.1f} MB")
              print(f"   Reduction: {full_ops // sliding_ops}× vs full")
              print(f"   Feasible: Yes\n")
              
              # 3. Chunked processing
              chunk_size = 2048
              num_chunks = seq_len // chunk_size
              chunk_ops = num_chunks * (chunk_size * chunk_size)
              chunk_mem = chunk_size * chunk_size * 4
              
              print(f"3. Chunked ({chunk_size} tokens/chunk):")
              print(f"   Chunks: {num_chunks}")
              print(f"   Operations: {chunk_ops:,}")
              print(f"   Memory: {chunk_mem / (1024**2):.1f} MB per chunk")
              print(f"   Reduction: {full_ops // chunk_ops}× vs full")
              print(f"   Feasible: Yes (sequential)\n")
              
              # 4. Sparse attention
              connections_per_token = 256  # local + global
              sparse_ops = seq_len * connections_per_token
              sparse_mem = seq_len * connections_per_token * 4
              
              print(f"4. Sparse Attention (~{connections_per_token} connections/token):")
              print(f"   Operations: {sparse_ops:,}")
              print(f"   Memory: {sparse_mem / (1024**2):.1f} MB")
              print(f"   Reduction: {full_ops // sparse_ops}× vs full")
              print(f"   Feasible: Yes\n")
          
          compare_context_approaches(16384)
      
      truncation_impact_analysis:
        language: python
        code: |
          def analyze_truncation_impact(document_tokens: np.ndarray,
                                       max_context: int = 512):
              """Analyze what gets lost with different truncation strategies."""
              
              print("\n=== Truncation Impact Analysis ===\n")
              
              doc_len = len(document_tokens)
              
              if doc_len <= max_context:
                  print(f"Document fits in context ({doc_len} <= {max_context})")
                  return
              
              print(f"Document: {doc_len} tokens")
              print(f"Context limit: {max_context} tokens")
              print(f"Overflow: {doc_len - max_context} tokens ({(doc_len - max_context) / doc_len * 100:.1f}%)\n")
              
              # Analyze different strategies
              strategies = {
                  'first_k': (0, max_context),
                  'last_k': (doc_len - max_context, doc_len),
                  'head_tail': None,  # Special case
              }
              
              for name, span in strategies.items():
                  if name == 'head_tail':
                      half = max_context // 2
                      kept_indices = set(range(half)) | set(range(doc_len - half, doc_len))
                      kept_pct = (max_context / doc_len) * 100
                      lost_start = half
                      lost_end = doc_len - half
                  else:
                      start, end = span
                      kept_indices = set(range(start, end))
                      kept_pct = (max_context / doc_len) * 100
                      
                      if name == 'first_k':
                          lost_start = max_context
                          lost_end = doc_len
                      else:  # last_k
                          lost_start = 0
                          lost_end = doc_len - max_context
                  
                  print(f"{name.upper()} truncation:")
                  print(f"  Kept: {kept_pct:.1f}% of document")
                  print(f"  Lost: positions {lost_start}-{lost_end-1}")
                  
                  # Security consideration: Where can adversary hide?
                  if name == 'first_k':
                      safe_zone = "Beginning (0-511)"
                      danger_zone = f"End ({max_context}+)"
                  elif name == 'last_k':
                      safe_zone = f"End ({doc_len-max_context}+)"
                      danger_zone = f"Beginning (0-{doc_len-max_context-1})"
                  else:
                      safe_zone = f"Beginning/End"
                      danger_zone = f"Middle ({half}-{doc_len-half-1})"
                  
                  print(f"  Adversary payload survives in: {safe_zone}")
                  print(f"  Adversary payload hidden in: {danger_zone}\n")
          
          # Example
          document_tokens = np.arange(2000)
          analyze_truncation_impact(document_tokens, max_context=512)
    
    security_implications:
      context_manipulation_attacks: |
        Adversary controls what enters context:
        - Flood with irrelevant tokens
        - Push security constraints out of window
        - Model never sees important context
        - Example: System prompt at start, payload at end (>4096 tokens)
        - Defense: Critical info within guaranteed context, validate
      
      memory_extraction_via_compression: |
        Compression must preserve information:
        - Compressed representations contain training data
        - Adversary can extract from compressed memory
        - Learned compression may leak sensitive patterns
        - Defense: Sanitize compressed representations
      
      chunking_boundary_attacks: |
        Process in chunks creates boundaries:
        - Split malicious payload across chunks
        - Each chunk looks benign individually
        - Combined effect is malicious
        - Defense: Overlapping chunks, cross-chunk validation

key_takeaways:
  critical_concepts:
    - concept: "Self-attention has O(n²) complexity in sequence length"
      why_it_matters: "Quadratic scaling creates hard limits on context windows"
    
    - concept: "Context window limits: 512 (BERT) → 2048 (GPT-3) → 100K+ (modern)"
      why_it_matters: "Determines how much text model can process at once"
    
    - concept: "Sliding window attention reduces to O(n×w) = O(n) when w constant"
      why_it_matters: "Enables linear scaling to very long sequences"
    
    - concept: "Sparse attention uses predetermined patterns for efficiency"
      why_it_matters: "Trade global connectivity for computational tractability"
    
    - concept: "Truncation strategies determine what gets seen vs lost"
      why_it_matters: "Critical for security - affects what model actually processes"
  
  actionable_steps:
    - step: "Analyze attention complexity for different sequence lengths"
      verification: "O(n²) scaling, quadratic memory growth"
    
    - step: "Implement sliding window attention mask"
      verification: "Each token attends to w neighbors, O(n×w) complexity"
    
    - step: "Create sparse attention patterns (strided, dilated, block)"
      verification: "Reduced connectivity, maintained coverage"
    
    - step: "Compare truncation strategies and their effects"
      verification: "First-k, last-k, head-tail - different information loss"
    
    - step: "Understand long-context approaches (Longformer, BigBird, chunking)"
      verification: "Different trade-offs between length, efficiency, capability"
  
  security_principles:
    - principle: "Context window limits create payload placement vulnerabilities"
      application: "Adversary puts payload where it won't be truncated"
    
    - principle: "O(n²) complexity enables DoS via long inputs"
      application: "Very long sequences overwhelm compute/memory"
    
    - principle: "Truncation strategies affect what model sees and misses"
      application: "Adversary exploits truncation to hide malicious content"
    
    - principle: "Sparse attention patterns reveal information flow paths"
      application: "Adversary places related tokens at interacting positions"
    
    - principle: "Context overflow pushes security info out of window"
      application: "Flood with tokens to remove constraints from context"
  
  common_mistakes:
    - mistake: "Assuming model sees entire document regardless of length"
      fix: "Check context window limits, implement truncation strategy"
    
    - mistake: "Using O(n²) attention for very long sequences"
      fix: "Use sliding window, sparse attention, or chunking for >4096 tokens"
    
    - mistake: "Truncating without considering what gets lost"
      fix: "Analyze truncation impact, preserve critical information"
    
    - mistake: "Ignoring memory constraints when scaling sequence length"
      fix: "Memory grows quadratically, plan for constraints"
    
    - mistake: "Thinking sparse attention is always better"
      fix: "Sparse trades global connectivity for efficiency - choose based on task"
  
  integration_with_book:
    from_section_3_6_to_3_8:
      - "Self-attention mechanism (O(n²) complexity source)"
      - "Attention score computation"
    
    from_section_3_14:
      - "Complete transformer (context window constraints)"
      - "Encoder-decoder memory requirements"
    
    to_next_section:
      - "Section 3.20: LLM security attacks and defenses"
      - "Context manipulation as attack vector"
      - "Comprehensive security analysis"
  
  looking_ahead:
    next_concepts:
      - "Prompt injection attacks"
      - "Jailbreaking techniques"
      - "Data extraction from pre-training"
      - "Backdoor attacks in fine-tuning"
    
    skills_to_build:
      - "Red-team LLMs for vulnerabilities"
      - "Implement defense mechanisms"
      - "Audit models for security issues"
      - "Design secure LLM systems"
  
  final_thoughts: |
    Context window limitations fundamentally constrain transformer capabilities. Standard 
    self-attention has O(n²) complexity - computing attention scores between all n token 
    pairs creates quadratic scaling in both compute and memory. At 512 tokens, that's 262K 
    attention scores. At 4096 tokens, 16.7M scores. Doubling sequence length quadruples cost. 
    This creates hard limits on practical context windows despite unlimited compute.
    
    Memory requirements also grow quadratically: storing the (n,n) attention matrix for batch 
    8, 12 heads, 4096 tokens requires 6.4GB just for attention scores. Total memory includes 
    model parameters, optimizer states, activations, and gradients - context length is often 
    the bottleneck. BERT handles 512 tokens, GPT-3 reaches 2048, modern models extend to 
    100K+ through engineering innovations.
    
    Efficient attention mechanisms reduce complexity through sparsity. Sliding window attention 
    limits each token to w neighbors for O(n×w) = O(n) linear complexity when w is constant. 
    Sparse attention uses predetermined patterns - strided, dilated, block - to reduce 
    connections while maintaining coverage. Longformer combines local sliding window, global 
    attention on key tokens, and dilated patterns for O(n) scaling to 16K+ tokens. BigBird 
    adds random attention and proves universal approximation despite sparsity.
    
    Truncation strategies determine what fits in context: first-k keeps beginning (loses end), 
    last-k keeps end (loses beginning), head-tail preserves both (loses middle). Each has 
    different implications for information preservation and security. Long-context approaches 
    include memory augmentation (external storage), recurrence (sequential chunks), compression 
    (summarize past), and retrieval (fetch relevant context).
    
    From a security perspective: Context limits create vulnerabilities where adversaries place 
    payloads knowing they'll survive truncation or push security info out of context. O(n²) 
    complexity enables DoS attacks via very long inputs overwhelming compute/memory. Truncation 
    strategies affect what model sees - adversary exploits this to hide malicious content. 
    Sparse patterns reveal information flow paths adversary can exploit. Context overflow 
    attacks flood with tokens to remove constraints. Understanding context management reveals 
    both defensive truncation approaches and offensive context manipulation techniques.
    
    Next: Section 3.20 covers LLM security comprehensively - prompt injection, jailbreaking, 
    data extraction, backdoors, and defenses. Context manipulation will be one of many attack 
    vectors examined for securing language models in production.

---
