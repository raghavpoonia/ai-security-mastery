# section_04_01_gpt2_architecture.yaml

---
document_info:
  section: "04_01"
  title: "GPT-2 Architecture: The Breakthrough"
  chapter: "04"
  chapter_title: "Modern LLM Internals"
  part: "Part I: Machine Learning Foundations"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2026-01-19"
  estimated_pages: 6
  tags:
    - "gpt-2"
    - "decoder-only"
    - "autoregressive"
    - "scaling"
    - "transformer-internals"
    - "pre-training"
    - "language-modeling"
    - "security-implications"

section_overview:

  purpose: |
    This section dissects GPT-2 in detail — the first model in the GPT lineage that
    demonstrated language models could be dangerous enough to withhold. You understand
    transformer architecture from Chapter 3. Now we get specific: why did OpenAI build
    a decoder-only model, what exact architectural choices differentiate GPT-2 from the
    original transformer, and what does a 1.5B parameter language model actually look like
    inside?

    For security engineers: GPT-2 established the template every successor uses. Its
    architectural decisions — tied embeddings, pre-norm layers, BPE tokenization —
    are security-relevant. Its training data (unfiltered web text) set the precedent
    for models memorizing sensitive content. Its "too dangerous to release" framing was
    the first public acknowledgment that language models have dual-use risk.

    This section is intentionally precise. Every architectural detail matters because
    adversaries target specifics: tokenizer edge cases, embedding space geometry,
    layer norm statistics, generation sampling configurations. Vague architecture
    knowledge creates blind spots. We eliminate them here.

  position_in_chapter: |
    Section 1 of 17 content sections. Chapter 4 opens with GPT-2 because it is the
    simplest complete production LLM — complex enough to understand real systems,
    simple enough to implement fully. Sections 2-3 extend to GPT-3 and GPT-4. This
    section builds the foundation both use.

  prerequisites:
    - "Chapter 3, Section 13: Transformer Decoder (autoregressive generation mechanics)"
    - "Chapter 3, Section 17: GPT architecture overview"
    - "Chapter 3, Section 9: Positional Encoding (absolute positional embeddings)"
    - "Chapter 2, Section 12: Batch Normalization (contrast with layer normalization)"
    - "PyTorch basics: nn.Module, forward pass, parameter inspection"

  what_you_will_build:
    primary: "GPT-2 architecture recreation in PyTorch (124M parameter version)"
    secondary:
      - "Layer inspector: examine weights, activations, attention patterns per layer"
      - "BPE tokenizer (compatible with GPT-2)"
      - "Text generation pipeline with temperature and top-k sampling"
      - "Zero-shot task evaluator (classification, translation, summarization)"
    notebooks:
      - "03-llm-internals/gpt2_architecture.ipynb"
      - "03-llm-internals/gpt2_generation.ipynb"

# ============================================================================
# CONTENT
# ============================================================================

content:

  # --------------------------------------------------------------------------
  # 1. WHY DECODER-ONLY?
  # --------------------------------------------------------------------------

  subsection_1:
    title: "Why Decoder-Only? The GPT Design Decision"
    pages: 1

    explanation: |
      The original transformer (Vaswani et al., 2017) had both an encoder and decoder.
      BERT (2018) used only the encoder. GPT used only the decoder. This was not
      arbitrary — it reflects a fundamental choice about what task to optimize for.

      Encoder-only models (BERT) are optimized for understanding: given a full sequence,
      produce the best possible representation of it. They use bidirectional attention —
      every token attends to every other token. This is powerful for classification,
      NER, sentence similarity. It is useless for generation.

      Decoder-only models (GPT) are optimized for generation: given a sequence, predict
      what comes next. They use causal (masked) attention — each token only attends to
      previous tokens. This constraint enables autoregressive generation, where the model
      produces text one token at a time.

      The simplification matters beyond generation. Decoder-only models are:
        - Simpler to implement (one attention type vs two)
        - Easier to scale (no cross-attention alignment problem)
        - More flexible at inference (same model handles prompts of any length)
        - Naturally suited to few-shot learning (examples become part of context)

    gpt2_specific_choice: |
      GPT-2 took the decoder-only architecture and asked: what if we just scale this up
      and train it on enormous web text? No special objectives, no classification heads,
      no task-specific architectures. Just next-token prediction on 40GB of internet text.

      The bet was that a sufficiently capable language model would implicitly learn to
      solve downstream tasks. It was correct.

    security_note: |
      Decoder-only means every interaction is generation. User input becomes the beginning
      of a sequence the model continues. This architectural choice is why prompt injection
      works: you are literally giving the model text it will treat as context to continue.
      There is no separation between "instructions" and "content" at the architecture
      level — that distinction exists only in training.

  # --------------------------------------------------------------------------
  # 2. ARCHITECTURE SPECIFICATIONS
  # --------------------------------------------------------------------------

  subsection_2:
    title: "GPT-2 Architecture: Exact Specifications"
    pages: 1

    model_variants:
      gpt2_small:
        parameters: "124M"
        layers: 12
        d_model: 768
        attention_heads: 12
        d_head: 64
        d_ffn: 3072
        context_length: 1024
        vocab_size: 50257
      gpt2_medium:
        parameters: "355M"
        layers: 24
        d_model: 1024
        attention_heads: 16
        d_head: 64
        d_ffn: 4096
        context_length: 1024
        vocab_size: 50257
      gpt2_large:
        parameters: "774M"
        layers: 36
        d_model: 1280
        attention_heads: 20
        d_head: 64
        d_ffn: 5120
        context_length: 1024
        vocab_size: 50257
      gpt2_xl:
        parameters: "1.5B"
        layers: 48
        d_model: 1600
        attention_heads: 25
        d_head: 64
        d_ffn: 6400
        context_length: 1024
        vocab_size: 50257

    key_numbers_explained:
      d_head_always_64: |
        All GPT-2 variants keep d_head = 64. More heads = more d_model, not smaller heads.
        This is a deliberate choice: 64-dimensional attention heads have proven
        sufficient for most linguistic patterns. Smaller heads lose expressivity.
        Larger heads increase compute without proportional benefit.

      vocab_size_50257: |
        50,257 = 50,000 BPE merges + 256 byte-level tokens + 1 special end-of-text token.
        This exact number appears in every GPT-2 successor because OpenAI never changed
        the tokenizer. Security implication: tokenizer quirks discovered in GPT-2
        (multi-byte characters, whitespace handling) affect GPT-3, GPT-4, and all
        fine-tuned derivatives.

      context_length_1024: |
        1024 tokens was the GPT-2 context limit. Roughly 750-800 words. This limit
        drove entire research directions (long-range transformers) and attack patterns
        (context overflow attacks). Understanding why 1024 was the limit — absolute
        positional embeddings trained on fixed length — explains how RoPE and ALiBi
        later broke this constraint (Section 11).

    parameter_count_calculation: |
      How do we get 124M parameters for GPT-2 small?

      Embedding layers:
        Token embeddings: 50257 × 768 = 38.6M
        Position embeddings: 1024 × 768 = 0.79M

      Per transformer layer (× 12 layers):
        Attention Q, K, V projections: 3 × (768 × 768) = 1.77M
        Attention output projection: 768 × 768 = 0.59M
        FFN expand (768 → 3072): 768 × 3072 = 2.36M
        FFN contract (3072 → 768): 3072 × 768 = 2.36M
        LayerNorm parameters: 2 × 2 × 768 = 3072 (negligible)
        Layer total: ~7.1M × 12 layers = 85.2M

      Output head: tied to token embeddings (0 additional parameters)

      Total: 38.6M + 0.79M + 85.2M ≈ 124M ✓

    security_note: |
      Parameter counts matter for security resource estimation. Storing, transmitting,
      and loading model weights has costs attackers can exploit. A 124M parameter FP32
      model is ~500MB. Knowing this: a model file significantly smaller than expected
      may be quantized (lower robustness), and one significantly larger may contain
      embedded payloads.

  # --------------------------------------------------------------------------
  # 3. ARCHITECTURAL DIFFERENCES FROM ORIGINAL TRANSFORMER
  # --------------------------------------------------------------------------

  subsection_3:
    title: "What GPT-2 Changed from the Original Transformer"
    pages: 1

    changes:
      pre_norm_vs_post_norm:
        original_transformer: "Post-norm: LayerNorm applied AFTER residual addition"
        gpt2: "Pre-norm: LayerNorm applied BEFORE attention/FFN (inside residual branch)"
        why_it_matters: |
          Post-norm: output = LayerNorm(x + sublayer(x))
          Pre-norm: output = x + sublayer(LayerNorm(x))

          Pre-norm trains more stably at scale. With post-norm, gradients can vanish
          deep in the network during early training. Pre-norm keeps gradient magnitudes
          consistent across layers, enabling reliable training of 48-layer models.
        security_implication: |
          Pre-norm means each layer operates on normalized inputs. This makes activation
          distributions more predictable — which makes side-channel analysis (inferring
          inputs from activation statistics) more reliable for attackers.

      tied_input_output_embeddings:
        description: |
          GPT-2 uses the same weight matrix for:
          1. Input token embeddings (converting token IDs to vectors)
          2. Output projection (converting final hidden states to logits over vocabulary)
          This is called weight tying.
        why_it_matters: |
          Reduces parameters by ~38.6M (no separate output projection matrix).
          Forces semantic similarity: tokens with similar input embeddings get similar
          output logits. Empirically improves perplexity on language modeling.
        security_implication: |
          Tied embeddings mean the output space is the input space. Adversarial
          perturbations in embedding space directly affect output probabilities.
          Model inversion attacks can work in both directions through the same matrix.

      additional_layer_norm_at_output:
        description: |
          GPT-2 adds a final LayerNorm after the last transformer block, before the
          output projection. Original transformer did not have this.
        why_it_matters: |
          Stabilizes the magnitude of final representations before projection to
          vocabulary logits. Prevents one dimension dominating token predictions.
        security_implication: |
          The final LayerNorm statistics (mean, variance per dimension) are measurable
          via API responses (logprobs). This creates a side channel for inferring
          model internals.

      no_encoder_no_cross_attention:
        description: |
          Pure decoder means no cross-attention blocks. Each transformer block has:
          1. Masked self-attention (causal)
          2. Feed-forward network
          No cross-attention between encoder and decoder output.
        why_it_matters: |
          Simpler architecture. Faster inference. No alignment problem.
          The tradeoff: no explicit separation between context and query.
        security_implication: |
          In encoder-decoder models, injected content in the encoder is at least
          architecturally separated from decoder reasoning. In decoder-only models,
          there is no such separation. All content — trusted instructions, untrusted
          user input, retrieved documents — lives in the same flat token sequence.

  # --------------------------------------------------------------------------
  # 4. BPE TOKENIZATION IN GPT-2
  # --------------------------------------------------------------------------

  subsection_4:
    title: "GPT-2 BPE Tokenization: Security-Relevant Details"
    pages: 1

    bpe_overview: |
      GPT-2 uses Byte-Pair Encoding (BPE) at the byte level. This means:
      1. Start with raw bytes (256 possible values)
      2. Iteratively merge the most frequent adjacent pairs
      3. Stop after 50,000 merges
      4. Result: vocabulary of 50,256 tokens + 1 special token

      Byte-level BPE guarantees every possible string is tokenizable with no
      unknown tokens. This differs from word-level BPE where rare words become
      [UNK]. With GPT-2's tokenizer, no input string is unrepresentable.

    critical_security_behaviors:

      whitespace_prefix_tokens: |
        GPT-2's tokenizer treats leading whitespace as part of the token.
        "hello" and " hello" are DIFFERENT tokens with DIFFERENT embeddings.

        Token IDs:
          "hello" → 31373
          " hello" → 23748

        Implication for injection: character-level identical strings tokenize
        differently based on context. An attacker adding a space before a word
        changes the token identity and thus the model's treatment of that word.
        Defenses that operate on raw characters may miss token-level distinctions.

      multi_token_words: |
        Long or rare words tokenize into multiple subword tokens.
        "administrator" → ["admin", "istrator"]
        "javascript" → ["java", "script"]
        "sqlinjection" → ["sql", "inje", "ction"]

        Implication: security keyword filters operating on token level may miss
        injections if adversaries choose word variations that cross token boundaries.
        "javascript" and "java" + "script" are semantically identical but may
        evade token-level filters differently.

      number_tokenization: |
        Numbers often tokenize one digit at a time (for large numbers) or as
        combined tokens (for small numbers).
        "2024" → ["2024"] (single token)
        "20240101" → ["202", "401", "01"] (three tokens)

        Implication: numeric injection attacks must account for tokenization.
        A date-based injection may split unexpectedly, changing attack semantics.

      special_characters: |
        Unicode characters outside standard ASCII tokenize to multiple byte tokens.
        Homoglyph attacks (visually similar characters from different scripts) produce
        entirely different token sequences. "а" (Cyrillic a) vs "a" (Latin a) are
        different bytes, different tokens, but visually indistinguishable.

    implementation_note: |
      The GPT-2 tokenizer (tiktoken library) is open source. For this section's
      exercises, we implement a simplified BPE tokenizer from scratch to understand
      the mechanics, then use tiktoken for production exercises.

  # --------------------------------------------------------------------------
  # 5. TRAINING DATA AND MEMORIZATION
  # --------------------------------------------------------------------------

  subsection_5:
    title: "WebText Training Data and Memorization Risk"
    pages: 1

    webtext_construction: |
      GPT-2 was trained on WebText: 40GB of text scraped from web pages linked
      by Reddit posts with at least 3 karma. The filtering heuristic: if humans
      upvoted a link enough to reach 3 karma, the content is likely interesting.

      This produced approximately 8 million documents. After deduplication and
      filtering for English, ~40GB of text remained.

    what_this_means_for_security:

      content_diversity: |
        WebText includes news articles, Wikipedia, personal blogs, technical
        documentation, social media, and significant amounts of unfiltered internet
        content. The model learned from all of it — including:
        - Technical discussions of vulnerabilities and exploits
        - Leaked credential formats (not individual credentials, but patterns)
        - Social engineering scripts
        - Malware analysis articles (teaching the form of malware descriptions)
        - Politically extreme content

      memorization_and_extraction: |
        Large language models memorize training data. This is not a bug — it is
        a consequence of having sufficient capacity. GPT-2 can reproduce verbatim
        passages from WebText when prompted with the beginning.

        Carlini et al. (2021) demonstrated extracting training data from GPT-2 by:
        1. Generating thousands of long sequences at low temperature
        2. Scoring sequences by comparing GPT-2 to a smaller reference model
        3. High-scoring sequences were disproportionately memorized training data
        4. Results: PII, news articles, and code reproduced verbatim

        For security engineers: any model trained on sensitive data (code, internal
        documents, customer data) is potentially a data exfiltration vector through
        memorized training examples.

      deduplication_effect: |
        Deduplicated data reduces memorization. If a document appears 1000 times in
        training, the model memorizes it. If it appears once, memorization is much
        lower. This motivates data deduplication as a privacy defense. But deduplication
        is imperfect: near-duplicates with minor variations can still cause memorization.

    implication_for_enterprise_deployment: |
      Organizations fine-tuning models on internal data must treat the resulting model
      as a potential exfiltration vector. The model may reproduce fragments of documents
      seen during fine-tuning. Access controls on the model API should be as strict as
      access controls on the underlying training data.

  # --------------------------------------------------------------------------
  # 6. GENERATION MECHANICS AND SAMPLING
  # --------------------------------------------------------------------------

  subsection_6:
    title: "Autoregressive Generation: Sampling and Decoding"
    pages: 1

    generation_loop: |
      GPT-2 generates text by repeating:
      1. Feed current token sequence as input
      2. Run forward pass: compute logits over 50257 vocabulary tokens
      3. Convert logits to probabilities via softmax
      4. Sample next token from probability distribution
      5. Append token to sequence
      6. Repeat until end-of-text token or max length

      The entire model runs on every step. With KV caching (Section 8), previous
      key/value pairs are cached, so only the new token's computation is needed.
      Without caching: O(n²) per generation step. With caching: O(n) amortized.

    sampling_strategies:

      greedy:
        description: "Always pick the highest probability token"
        deterministic: true
        problem: "Repetition loops, boring output, same answer every time"
        security_note: |
          Greedy decoding is fully deterministic. Same prompt → same output always.
          This makes it easier to build reliable attack probes (prompt → expected
          response). Defenders who rely on output randomness for security have a
          false assumption.

      temperature:
        description: "Divide logits by temperature T before softmax"
        formula: "P(token) = softmax(logits / T)"
        t_less_than_1: "Sharper distribution — more deterministic"
        t_equal_1: "Original distribution"
        t_greater_than_1: "Flatter distribution — more random"
        t_approaches_0: "Equivalent to greedy"
        security_note: |
          High temperature increases output randomness, making attacks less reliable
          but also making safety filters less effective. Many production systems use
          T=0 (greedy) or T close to 0 for consistency. Attackers probe temperature
          by submitting identical prompts and comparing output variance.

      top_k:
        description: "Zero out all but the top-K probability tokens before sampling"
        typical_k: "40-50 for GPT-2"
        effect: "Prevents sampling very low probability tokens"
        security_note: |
          Top-k limits vocabulary at each step. This reduces the probability of
          rare/unusual token sequences — which some injection attacks depend on.
          But it also limits output diversity, which can be probed.

      top_p_nucleus:
        description: "Sample from the smallest set of tokens whose cumulative probability ≥ p"
        typical_p: "0.9 or 0.95"
        effect: "Adapts vocabulary size to context (tight distributions → few tokens)"
        security_note: |
          Nucleus sampling is the most commonly used production setting. It provides
          adaptive randomness. Security relevance: p determines how much of the
          probability mass is accessible. Low p (0.1) → near-deterministic.
          High p (0.99) → nearly unconstrained. Production APIs often expose p as
          a parameter — attackers can tune it to find the sampling regime most
          permissive for their attack.

  # --------------------------------------------------------------------------
  # 7. IMPLEMENTATION
  # --------------------------------------------------------------------------

  implementation:
    title: "GPT-2 Implementation from Scratch (PyTorch)"
    notebook: "03-llm-internals/gpt2_architecture.ipynb"
    estimated_loc: 350

    architecture_code:
      description: "Full GPT-2 small (124M) in clean PyTorch"
      components:
        - name: "CausalSelfAttention"
          description: "Multi-head attention with causal mask"
          key_detail: "Causal mask implemented as lower triangular boolean matrix"
          lines: ~60

        - name: "GPT2MLP"
          description: "Position-wise feed-forward (768 → 3072 → 768)"
          key_detail: "GELU activation (not ReLU — GPT-2 specific choice)"
          lines: ~20

        - name: "GPT2Block"
          description: "Single transformer layer: pre-norm, attention, pre-norm, FFN"
          key_detail: "Pre-norm placement (LayerNorm before sublayer, inside residual)"
          lines: ~30

        - name: "GPT2Model"
          description: "Full model: embeddings + N blocks + final LayerNorm"
          key_detail: "Weight tying: output projection reuses token embedding weights"
          lines: ~60

      weight_loading: |
        After implementing architecture, load OpenAI's released GPT-2 weights
        directly into our implementation. This verifies our architecture is correct:
        if weight loading succeeds and generation works, the architecture matches.

        huggingface_weights = GPT2LMHeadModel.from_pretrained("gpt2")
        our_model.load_state_dict(huggingface_weights.state_dict())

    layer_inspector:
      description: "Examine model internals"
      capabilities:
        - "Print parameter count per layer"
        - "Visualize attention patterns per head"
        - "Plot activation distributions per layer"
        - "Measure layer norm statistics (mean, std per dimension)"
        - "Identify top-activating tokens for each attention head"

    generation_pipeline:
      description: "Text generation with all sampling strategies"
      code_sketch: |
        def generate(model, prompt, max_tokens=100, temperature=1.0,
                     top_k=50, top_p=0.95):
            tokens = tokenize(prompt)
            for _ in range(max_tokens):
                logits = model(tokens)[:, -1, :]    # Last token's logits
                logits = logits / temperature
                logits = apply_top_k(logits, top_k)
                logits = apply_top_p(logits, top_p)
                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                tokens = torch.cat([tokens, next_token], dim=1)
                if next_token == EOT_TOKEN:
                    break
            return detokenize(tokens)

    zero_shot_evaluator:
      description: "Test GPT-2 on classification/QA without fine-tuning"
      tasks:
        - "Sentiment classification (positive/negative)"
        - "Language identification"
        - "Named entity detection"
      method: |
        For classification: compare log-probabilities of label continuations.
        P("positive" | review text) vs P("negative" | review text)
        No task-specific training. Pure language model probabilities.

# ============================================================================
# EXERCISES
# ============================================================================

exercises:

  exercise_1:
    title: "Build and Load GPT-2 Small"
    difficulty: "Medium"
    estimated_time: "2-3 hours"
    objective: "Implement GPT-2 small architecture from scratch and verify by loading official weights"
    steps:
      - "Implement CausalSelfAttention with causal mask"
      - "Implement GPT2MLP with GELU activation"
      - "Implement GPT2Block with pre-norm configuration"
      - "Implement GPT2Model with weight tying"
      - "Load OpenAI's released weights and verify match"
      - "Generate 5 text completions and compare to HuggingFace output"
    success_criteria:
      - "Weight loading completes without shape errors"
      - "Our generation matches HuggingFace output token-for-token (greedy decoding)"
      - "Parameter count: 124,439,808 (within 100 parameters)"

  exercise_2:
    title: "Tokenizer Security Analysis"
    difficulty: "Easy"
    estimated_time: "1 hour"
    objective: "Discover tokenization edge cases relevant to security"
    steps:
      - "Tokenize 20 security-relevant strings (SQL keywords, script tags, shell commands)"
      - "Identify which strings split across unexpected token boundaries"
      - "Test homoglyph attack: replace 3 ASCII chars with visually identical Unicode"
      - "Verify that homoglyph substitution produces different token IDs"
      - "Design a token-boundary injection: craft string that looks like one word but tokenizes as two"
    success_criteria:
      - "At least 3 token boundary surprises documented"
      - "Homoglyph test confirms different token IDs"
      - "Injection example tokenizes differently than it appears"
    deliverable: "security_tokenizer_edge_cases.md — documented edge cases with token IDs"

  exercise_3:
    title: "Memorization Probe"
    difficulty: "Medium"
    estimated_time: "1-2 hours"
    objective: "Detect memorized content in GPT-2 using likelihood scoring"
    steps:
      - "Generate 200 sequences from GPT-2 at temperature=1.0"
      - "Score each sequence: log P(sequence | GPT-2) / log P(sequence | GPT-2 small)"
      - "High ratio = sequence scored much better on large model = likely memorized"
      - "Inspect top 10 highest-ratio sequences"
      - "Search one output sequence in a web search to confirm it is real content"
    success_criteria:
      - "At least one sequence that appears to be real web content"
      - "Distribution of ratio scores plotted"
      - "Memorization probe function reusable for other models"
    note: |
      This exercise teaches the Carlini et al. extraction methodology. The same
      approach applies when auditing fine-tuned models trained on sensitive data.

  exercise_4:
    title: "Sampling Strategy Attack Probe"
    difficulty: "Easy"
    estimated_time: "45 minutes"
    objective: "Measure how sampling parameters affect attack reliability"
    steps:
      - "Choose a simple prompt that produces a specific desired output at T=0"
      - "Vary temperature from 0 to 2.0 in 0.2 increments"
      - "For each temperature, run 20 generations and measure success rate"
      - "Repeat for top_k: vary from 1 to 100"
      - "Plot success rate vs temperature and success rate vs top_k"
    success_criteria:
      - "Clear plot showing attack reliability degrades with temperature"
      - "Optimal attack parameters identified"
      - "Understanding of how production sampling configs affect attack success"

# ============================================================================
# KEY CONCEPTS SUMMARY
# ============================================================================

key_concepts_summary:

  architectural_decisions:
    - concept: "Decoder-only architecture"
      implication: "No structural separation between trusted and untrusted input"

    - concept: "Pre-norm layer normalization"
      implication: "Stable gradient flow; predictable activation statistics = measurable side channel"

    - concept: "Tied input/output embeddings"
      implication: "Model inversion works in both directions through same weight matrix"

    - concept: "BPE tokenization at byte level"
      implication: "No unknown tokens; homoglyph attacks work; token boundary ≠ word boundary"

    - concept: "Vocab size 50,257"
      implication: "Inherited by all GPT successors — tokenizer bugs persist across model generations"

  security_takeaways:
    - "Prompt injection is an architectural property of decoder-only models, not a bug"
    - "Memorization is a capability, not just a risk — it enables training data extraction"
    - "Sampling parameters are tunable by attackers if exposed via API"
    - "Tokenizer edge cases are a consistent injection surface across the entire GPT family"
    - "Pre-norm activation statistics are measurable via API — potential side channel"

# ============================================================================
# CONNECTIONS
# ============================================================================

connections:

  builds_on:
    - section: "Chapter 3, Section 13"
      concept: "Transformer decoder and causal masking — now applied to production architecture"
    - section: "Chapter 3, Section 17"
      concept: "GPT overview — now fully specified with exact dimensions and implementation"
    - section: "Chapter 3, Section 2"
      concept: "BPE tokenization — now analyzed for security edge cases"

  prepares_for:
    - section: "Section 04_02"
      concept: "GPT-3 scaling — same architecture, 1000× parameters, emergent capabilities"
    - section: "Section 04_08"
      concept: "KV cache — optimization of the generation loop we build here"
    - section: "Section 04_14"
      concept: "Prompt engineering — using sampling and generation mechanics strategically"
    - section: "Chapter 6 (Part 2)"
      concept: "Prompt injection attacks — grounded in decoder-only architecture mechanics"
    - section: "Chapter 9 (Part 2)"
      concept: "Model extraction — using API sampling and memorization mechanics from this section"

  security_thread: |
    This section plants three security seeds that grow throughout the book:
    1. Architecture → injection (decoder-only = no trust boundary): matures in Chapter 6
    2. Memorization → extraction (training data leakage): matures in Chapter 9
    3. Tokenizer edge cases → evasion (filter bypass): matures in Chapters 6 and 15

# ============================================================================
# FURTHER READING
# ============================================================================

further_reading:

  primary:
    - title: "Language Models are Unsupervised Multitask Learners"
      authors: "Radford et al. (OpenAI, 2019)"
      note: "Original GPT-2 paper — read the training data section carefully"
      url: "https://openai.com/research/language-unsupervised"

    - title: "Extracting Training Data from Large Language Models"
      authors: "Carlini et al. (2021)"
      note: "Critical paper for understanding memorization as a security risk"
      url: "https://arxiv.org/abs/2012.07805"

  implementation_reference:
    - title: "nanoGPT"
      author: "Andrej Karpathy"
      note: "Clean, minimal GPT-2 implementation — excellent reference for our implementation"
      url: "https://github.com/karpathy/nanoGPT"

    - title: "The Annotated Transformer"
      author: "Harvard NLP"
      note: "Line-by-line transformer implementation with explanations"
      url: "https://nlp.seas.harvard.edu/2018/04/03/attention.html"

  security_reading:
    - title: "Ignore Previous Prompt: Attack Techniques for Language Models"
      authors: "Perez & Ribeiro (2022)"
      note: "Formalizes prompt injection as security problem — connects to this section's architecture analysis"
      url: "https://arxiv.org/abs/2211.09527"

---
    
    practical:
      - "Implement cosine similarity, dot product, and Euclidean distance from scratch with NumPy"
      - "Build a complete k-nearest neighbors semantic search engine"
      - "Create embedding space visualization and analysis tools"
      - "Benchmark similarity metric performance across different scenarios"
    
    security_focused:
      - "Identify information leakage through embedding similarity queries"
      - "Detect and defend against embedding space poisoning attacks"
      - "Implement query sanitization and rate limiting for semantic search"
      - "Recognize membership inference vulnerabilities in embedding systems"
  
  prerequisites:
    knowledge:
      - "Chapter 3: Transformer embeddings and attention mechanisms"
      - "Chapter 3: Pre-trained language models (BERT, GPT)"
      - "Chapter 2: Neural network fundamentals and backpropagation"
      - "Chapter 1: NumPy operations, linear algebra, vectorization"
    
    skills:
      - "Matrix operations and vector mathematics with NumPy"
      - "Understanding of high-dimensional spaces and distance metrics"
      - "Working with pre-trained models from Hugging Face Transformers"
      - "Performance analysis and benchmarking of algorithms"
  
  key_transitions:
    from_chapter_3: |
      Chapter 3 introduced embeddings in the transformer context: learned token embeddings,
      positional encodings, and contextual representations from BERT/GPT. We saw how
      transformers create rich semantic representations through attention mechanisms.
      
      Section 4.1 shifts focus from creating embeddings to using them for retrieval.
      We take the contextual embeddings produced by models like BERT (which you learned
      in Section 3.13) and build semantic search systems. The embedding generation is
      now a solved problem (using pre-trained models); our focus is retrieval algorithms,
      similarity metrics, and the security of embedding-based systems.
    
    to_next_section: |
      This section provides the algorithmic foundations for semantic search. Section 4.2
      scales these concepts to production with vector databases (FAISS, Pinecone) and
      efficient indexing structures (HNSW, IVF). You'll see how the simple KNN search
      we build here becomes productionized with approximate nearest neighbor algorithms
      that handle millions of vectors with sub-millisecond latency.

topics:
  - topic_number: 1
    title: "Dense Vector Representations and Embedding Spaces"
    
    overview: |
      Dense vector embeddings are the foundation of modern semantic search. Unlike sparse
      representations (bag-of-words, TF-IDF) where most dimensions are zero, dense embeddings
      use every dimension to encode semantic meaning. A 768-dimensional BERT embedding
      represents text as a point in 768-dimensional continuous space, where semantically
      similar texts cluster together.
      
      Understanding embedding spaces requires geometric intuition in high dimensions. Two
      key properties make embeddings powerful: (1) semantic similarity manifests as geometric
      proximity, and (2) vector arithmetic often corresponds to semantic operations. For
      example, in word embeddings: king - man + woman ≈ queen.
      
      We'll examine different embedding models (BERT, sentence-transformers, OpenAI embeddings),
      their dimensionality trade-offs, and how to generate embeddings for text. We implement
      embedding generation from scratch and analyze what these vectors actually capture.
    
    content:
      what_are_dense_embeddings:
        definition: |
          A dense embedding is a fixed-length vector of real numbers (typically float32)
          where each dimension contributes to representing the input. For a sentence
          "The cat sat on the mat", a 768-dim BERT embedding might be:
          [0.234, -0.891, 0.456, ..., 0.123] (768 values total).
        
        properties:
          dimensionality: |
            Common embedding dimensions:
            - Word embeddings (Word2Vec, GloVe): 100-300 dims
            - BERT base: 768 dims
            - BERT large: 1024 dims
            - Sentence-BERT: 384 or 768 dims
            - OpenAI text-embedding-ada-002: 1536 dims
            
            Higher dimensions capture more nuanced semantic information but increase
            computational and storage costs. The choice depends on task complexity and
            resource constraints.
          
          continuous_space: |
            Unlike discrete token IDs or sparse vectors, embeddings live in continuous
            R^d space. This continuity enables gradient-based optimization and smooth
            interpolation between concepts. Similar meanings have similar vectors.
          
          learned_representations: |
            Embeddings are learned through training on large corpora. BERT learns through
            masked language modeling and next sentence prediction. Sentence-transformers
            fine-tune on semantic similarity tasks. The training objective shapes what
            semantic properties the embeddings capture.
      
      embedding_space_geometry:
        geometric_interpretation: |
          An embedding space is a high-dimensional vector space where:
          - Each point represents an embedded text/document
          - Distance between points reflects semantic dissimilarity
          - Direction from one point to another can represent semantic relationships
          - Clusters of points represent semantically related concepts
        
        semantic_neighborhoods: |
          In a well-trained embedding space:
          - Synonyms cluster tightly: {"happy", "joyful", "cheerful"} are nearby
          - Antonyms are distant: "happy" is far from "sad"
          - Related concepts form regions: medical terms cluster together
          - Hierarchies emerge: "animal" near "dog", "dog" near "poodle"
        
        dimensionality_curse: |
          In high-dimensional spaces, surprising phenomena emerge:
          - All points become approximately equidistant (concentration of measure)
          - Volume concentrates in corners of hypercubes
          - Nearest neighbors become less meaningful as dimensions increase
          - Distance metrics behave differently than in 2D/3D
          
          This is why approximate nearest neighbor algorithms (Section 4.2) are essential
          for production systems - exact search becomes intractable in high dimensions.
      
      generating_embeddings:
        sentence_transformers_approach: |
          Sentence-transformers (sentence-BERT) are the most common choice for semantic
          search. They're fine-tuned specifically to produce embeddings where cosine
          similarity correlates with semantic similarity.
          
          Architecture: BERT/RoBERTa base + mean pooling + normalization
          Output: Fixed-length dense vector regardless of input length
          Training: Siamese networks with triplet loss on sentence pairs
        
        pooling_strategies: |
          To get a single vector from BERT's token-level outputs:
          
          1. CLS token: Use the [CLS] token embedding (position 0)
             - Fast, single vector
             - May not capture full sentence meaning
          
          2. Mean pooling: Average all token embeddings
             - Captures information from entire sequence
             - Most common for sentence-transformers
          
          3. Max pooling: Take max value per dimension across tokens
             - Emphasizes prominent features
             - Can lose information
          
          4. Weighted pooling: Attention-weighted average
             - Sophisticated, task-dependent
             - Requires additional parameters
    
    implementation:
      embedding_generator_numpy:
        language: python
        code: |
          """
          Embedding generation from pre-trained models.
          This implementation uses Hugging Face transformers to generate embeddings,
          then demonstrates the core operations we'll use for semantic search.
          """
          
          import numpy as np
          from typing import List, Tuple
          import warnings
          warnings.filterwarnings('ignore')
          
          class EmbeddingGenerator:
              """Generate dense embeddings from text using pre-trained models."""
              
              def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):
                  """
                  Initialize embedding generator.
                  
                  Args:
                      model_name: Hugging Face model identifier
                  
                  Notes:
                      - all-MiniLM-L6-v2: 384 dims, fast, good quality
                      - all-mpnet-base-v2: 768 dims, higher quality
                      - paraphrase-multilingual: supports multiple languages
                  """
                  from transformers import AutoTokenizer, AutoModel
                  import torch
                  
                  self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
                  self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                  self.model = AutoModel.from_pretrained(model_name).to(self.device)
                  self.model.eval()  # Inference mode
                  
                  print(f"Loaded {model_name}")
                  print(f"Device: {self.device}")
                  print(f"Embedding dimension: {self.model.config.hidden_size}")
              
              def mean_pooling(self, 
                              token_embeddings: 'torch.Tensor', 
                              attention_mask: 'torch.Tensor') -> 'torch.Tensor':
                  """
                  Mean pooling: average token embeddings with attention mask.
                  
                  Args:
                      token_embeddings: [batch_size, seq_len, hidden_dim]
                      attention_mask: [batch_size, seq_len] - 1 for real tokens, 0 for padding
                  
                  Returns:
                      Pooled embeddings: [batch_size, hidden_dim]
                  """
                  import torch
                  
                  # Expand attention mask to match embedding dimensions
                  # [batch_size, seq_len, 1] -> [batch_size, seq_len, hidden_dim]
                  input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                  
                  # Sum embeddings, weighted by attention mask
                  sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
                  
                  # Divide by number of real (non-padded) tokens
                  sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
                  
                  return sum_embeddings / sum_mask
              
              def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
                  """
                  Encode texts into dense embeddings.
                  
                  Args:
                      texts: List of text strings to embed
                      batch_size: Process this many texts at once
                  
                  Returns:
                      Embeddings as numpy array: [len(texts), embedding_dim]
                  """
                  import torch
                  
                  all_embeddings = []
                  
                  for i in range(0, len(texts), batch_size):
                      batch_texts = texts[i:i + batch_size]
                      
                      # Tokenize
                      encoded = self.tokenizer(
                          batch_texts,
                          padding=True,
                          truncation=True,
                          max_length=512,
                          return_tensors='pt'
                      ).to(self.device)
                      
                      # Generate embeddings
                      with torch.no_grad():
                          model_output = self.model(**encoded)
                          
                          # Mean pooling
                          embeddings = self.mean_pooling(
                              model_output.last_hidden_state,
                              encoded['attention_mask']
                          )
                          
                          # Normalize to unit length (important for cosine similarity)
                          embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                      
                      all_embeddings.append(embeddings.cpu().numpy())
                  
                  return np.vstack(all_embeddings)
              
              def embedding_stats(self, embeddings: np.ndarray) -> dict:
                  """
                  Analyze embedding statistics.
                  
                  Args:
                      embeddings: [num_samples, embedding_dim]
                  
                  Returns:
                      Dictionary of statistics
                  """
                  return {
                      'shape': embeddings.shape,
                      'mean': float(np.mean(embeddings)),
                      'std': float(np.std(embeddings)),
                      'min': float(np.min(embeddings)),
                      'max': float(np.max(embeddings)),
                      'norm_mean': float(np.mean(np.linalg.norm(embeddings, axis=1))),
                      'sparsity': float(np.mean(embeddings == 0))  # Should be ~0 for dense
                  }
          
          
          # Example usage
          if __name__ == "__main__":
              # Initialize generator
              generator = EmbeddingGenerator()
              
              # Sample documents for semantic search
              documents = [
                  "Machine learning is a subset of artificial intelligence.",
                  "Deep learning uses neural networks with multiple layers.",
                  "Natural language processing enables computers to understand text.",
                  "The cat sat on the mat.",
                  "Python is a popular programming language for data science.",
                  "Transformers revolutionized NLP with attention mechanisms.",
                  "Security is critical for production machine learning systems.",
                  "The dog played in the park.",
              ]
              
              # Generate embeddings
              print("\n" + "="*80)
              print("GENERATING EMBEDDINGS")
              print("="*80)
              embeddings = generator.encode(documents)
              
              # Analyze embeddings
              stats = generator.embedding_stats(embeddings)
              print(f"\nEmbedding Statistics:")
              print(f"  Shape: {stats['shape']}")
              print(f"  Mean: {stats['mean']:.6f}")
              print(f"  Std: {stats['std']:.6f}")
              print(f"  Range: [{stats['min']:.6f}, {stats['max']:.6f}]")
              print(f"  Average L2 norm: {stats['norm_mean']:.6f}")
              print(f"  Sparsity: {stats['sparsity']:.6f}")
              
              # Show first embedding
              print(f"\nFirst embedding (truncated):")
              print(f"  {embeddings[0][:10]}... (showing 10/{embeddings.shape[1]} dims)")
              
              # Save for next topic
              np.save('embeddings.npy', embeddings)
              with open('documents.txt', 'w') as f:
                  for doc in documents:
                      f.write(doc + '\n')
              
              print("\n✓ Embeddings generated and saved")
    
    security_implications:
      embedding_model_backdoors: |
        **Vulnerability**: Pre-trained embedding models can contain backdoors. An attacker
        who controls the model training can insert triggers that cause specific inputs
        to produce attacker-controlled embeddings.
        
        **Attack scenario**: A malicious embedding model maps "buy product X" to be very
        similar to "highly recommended secure product" regardless of actual semantics.
        In a RAG system, this could manipulate retrieval to always recommend product X.
        
        **Defense**:
        1. Use models from trusted sources (Hugging Face verified publishers)
        2. Test embeddings on known similar/dissimilar pairs to detect anomalies
        3. Use multiple embedding models and cross-validate results
        4. Monitor embedding distributions for sudden shifts
        5. Implement embedding quality gates before deployment
      
      information_leakage_through_embeddings: |
        **Vulnerability**: Embeddings leak information about training data. Even though
        embeddings are continuous vectors, attackers can sometimes reconstruct approximate
        training texts through repeated similarity queries.
        
        **Attack scenario**: Attacker queries semantic search with variations like:
        "John Smith password", "John Smith secret", "John Smith confidential"
        By analyzing which queries return similar embeddings to known documents, attacker
        can infer whether sensitive information about John Smith exists in the corpus.
        
        **Defense**:
        1. Rate limit similarity queries per user/IP
        2. Add noise to embeddings (differential privacy)
        3. Filter sensitive documents from embedding corpus
        4. Monitor for suspicious query patterns (many similar queries)
        5. Implement query diversity requirements
      
      membership_inference: |
        **Vulnerability**: Attackers can determine if specific text was in the embedding
        model's training set by analyzing embedding characteristics.
        
        **Attack scenario**: Attacker has candidate text and queries whether similar
        embeddings exist. Training data typically has more confident, precise embeddings
        than out-of-distribution text.
        
        **Defense**:
        1. Use models trained on public data when possible
        2. Implement query result filtering (don't return exact matches)
        3. Add calibrated noise to embeddings
        4. Limit number of nearest neighbors returned
        5. Monitor for systematic membership probing
      
      embedding_space_poisoning: |
        **Vulnerability**: If attackers can add documents to your corpus, they can poison
        the embedding space to manipulate future retrievals.
        
        **Attack scenario**: Attacker adds documents with embeddings positioned to be
        retrieved for specific queries. For example, adding spam documents that cluster
        near legitimate product recommendations.
        
        **Defense**:
        1. Validate and moderate user-submitted content before embedding
        2. Isolate user-generated content in separate embedding spaces
        3. Detect outliers and anomalous embeddings before indexing
        4. Implement content-based filtering before retrieval
        5. Regular audits of retrieved documents for quality
        
        We'll implement poisoning detection in Section 4.2 when we build vector databases.

  - topic_number: 2
    title: "Similarity Metrics: Cosine, Dot Product, and Euclidean Distance"
    
    overview: |
      Similarity metrics quantify how "close" two embeddings are in vector space. The choice
      of metric fundamentally affects retrieval quality, computational performance, and
      security properties. We implement three primary metrics from scratch and understand
      their mathematical properties, use cases, and trade-offs.
      
      Cosine similarity measures the angle between vectors (direction), dot product measures
      both angle and magnitude, and Euclidean distance measures geometric distance. Each
      has different invariances and sensitivities that make them suitable for different
      scenarios.
      
      Understanding these metrics deeply is critical for debugging semantic search systems,
      optimizing performance, and identifying security vulnerabilities. We'll implement
      all three with NumPy, benchmark them, and demonstrate when each metric is appropriate.
    
    content:
      cosine_similarity:
        definition: |
          Cosine similarity measures the cosine of the angle between two vectors:
          
          cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)
          
          where:
          - A · B is the dot product
          - ||A|| is the L2 norm (magnitude) of A
          - Result ranges from -1 (opposite) to 1 (identical)
        
        properties: |
          1. **Magnitude invariant**: Only considers direction, not length
             - [1, 2, 3] and [2, 4, 6] have cosine similarity of 1.0
             - Useful when absolute values don't matter (text frequency)
          
          2. **Normalized**: Always in [-1, 1] range
             - Easy to interpret and threshold
             - Consistent across different embedding scales
          
          3. **Computationally efficient with normalized vectors**:
             - If vectors are L2-normalized (norm = 1), cosine = dot product
             - Many embedding models (sentence-transformers) normalize by default
          
          4. **Most common for semantic search**:
             - Semantic similarity is about "meaning direction", not magnitude
             - Robust to different document lengths
        
        when_to_use: |
          Use cosine similarity when:
          - Working with text embeddings where length shouldn't matter
          - Embeddings are pre-normalized (sentence-transformers, OpenAI)
          - You care about semantic direction, not absolute values
          - You need magnitude-invariant comparisons
      
      dot_product_similarity:
        definition: |
          Dot product (inner product) is the sum of element-wise products:
          
          dot_product(A, B) = Σ(A_i × B_i) = A · B
          
          where:
          - A_i and B_i are elements at position i
          - Result ranges from -∞ to +∞ (unbounded)
        
        properties: |
          1. **Magnitude dependent**: Considers both angle and length
             - [1, 2, 3] and [2, 4, 6] have different dot products
             - Longer vectors generally have higher dot products
          
          2. **Fastest to compute**: Single vector multiplication and sum
             - No normalization overhead
             - Critical for billion-scale search
          
          3. **Equivalent to cosine for normalized vectors**:
             - If ||A|| = ||B|| = 1, then dot(A,B) = cosine(A,B)
             - This is why many systems normalize embeddings
          
          4. **Natural for neural networks**:
             - Final layer often uses dot product for logits
             - Attention mechanism uses scaled dot-product
        
        when_to_use: |
          Use dot product when:
          - Vectors are pre-normalized (then it equals cosine)
          - Maximum computational speed is critical
          - Magnitude information is meaningful (e.g., confidence scores)
          - Building attention mechanisms or neural layers
      
      euclidean_distance:
        definition: |
          Euclidean distance (L2 distance) measures straight-line distance:
          
          euclidean_distance(A, B) = √(Σ(A_i - B_i)²) = ||A - B||₂
          
          where:
          - (A_i - B_i)² is squared difference at position i
          - Result ranges from 0 (identical) to ∞
          - Smaller distances indicate higher similarity
        
        properties: |
          1. **Geometric intuition**: Literal distance in space
             - Easy to visualize in 2D/3D
             - Matches human intuition about "closeness"
          
          2. **Magnitude sensitive**: Affected by vector length
             - [1, 2, 3] and [2, 4, 6] have distance ≈ 3.74
             - May not be ideal for text with varying lengths
          
          3. **Computationally expensive**: Requires square root
             - Can use squared distance to avoid sqrt
             - Ranking is preserved without sqrt
          
          4. **Metric properties**: Satisfies triangle inequality
             - Enables geometric algorithms and proofs
             - Used in clustering (k-means) and classification (k-NN)
        
        when_to_use: |
          Use Euclidean distance when:
          - Geometric interpretation is important
          - Working with image embeddings or structured data
          - Implementing clustering algorithms (k-means)
          - Absolute differences matter, not just angles
      
      metric_comparison:
        normalized_vectors: |
          For L2-normalized vectors (||v|| = 1):
          
          cosine_similarity(A, B) = dot_product(A, B)
          euclidean_distance(A, B) = √(2 - 2×cosine_similarity(A, B))
          
          They're mathematically related! Ranking by cosine is equivalent to ranking
          by dot product or inverse Euclidean distance for normalized vectors.
        
        computational_cost: |
          For vectors of dimension d:
          - Dot product: d multiplications + (d-1) additions = O(d)
          - Cosine: dot product + 2 norms + 1 division = O(3d)
          - Euclidean: d subtractions + d squares + (d-1) additions + 1 sqrt = O(d)
          
          For normalized vectors, dot product is fastest since it equals cosine.
    
    implementation:
      similarity_metrics_numpy:
        language: python
        code: |
          """
          Similarity metrics implemented from scratch with NumPy.
          Includes vectorized batch operations for efficiency.
          """
          
          import numpy as np
          from typing import Union
          import time
          
          class SimilarityMetrics:
              """Comprehensive similarity metric implementations."""
              
              @staticmethod
              def cosine_similarity(a: np.ndarray, b: np.ndarray) -> Union[float, np.ndarray]:
                  """
                  Cosine similarity between vectors or matrices.
                  
                  Args:
                      a: Vector [d] or matrix [n, d]
                      b: Vector [d] or matrix [m, d]
                  
                  Returns:
                      Similarity: scalar, [n], or [n, m]
                  
                  Examples:
                      cosine_similarity(v1, v2) -> scalar
                      cosine_similarity(matrix, v) -> [n] similarities
                      cosine_similarity(matrix1, matrix2) -> [n, m] pairwise
                  """
                  # Handle different input shapes
                  if a.ndim == 1 and b.ndim == 1:
                      # Vector to vector
                      return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)
                  
                  elif a.ndim == 2 and b.ndim == 1:
                      # Matrix to vector
                      norms_a = np.linalg.norm(a, axis=1, keepdims=True)
                      norm_b = np.linalg.norm(b)
                      return np.dot(a, b) / (norms_a.squeeze() * norm_b + 1e-8)
                  
                  elif a.ndim == 2 and b.ndim == 2:
                      # Matrix to matrix (pairwise)
                      norms_a = np.linalg.norm(a, axis=1, keepdims=True)
                      norms_b = np.linalg.norm(b, axis=1, keepdims=True)
                      return np.dot(a, b.T) / (np.dot(norms_a, norms_b.T) + 1e-8)
                  
                  else:
                      raise ValueError(f"Unsupported shapes: {a.shape}, {b.shape}")
              
              @staticmethod
              def dot_product(a: np.ndarray, b: np.ndarray) -> Union[float, np.ndarray]:
                  """
                  Dot product similarity.
                  
                  Args:
                      a: Vector [d] or matrix [n, d]
                      b: Vector [d] or matrix [m, d]
                  
                  Returns:
                      Similarity: scalar, [n], or [n, m]
                  """
                  if a.ndim == 1 and b.ndim == 1:
                      return np.dot(a, b)
                  elif a.ndim == 2 and b.ndim == 1:
                      return np.dot(a, b)
                  elif a.ndim == 2 and b.ndim == 2:
                      return np.dot(a, b.T)
                  else:
                      raise ValueError(f"Unsupported shapes: {a.shape}, {b.shape}")
              
              @staticmethod
              def euclidean_distance(a: np.ndarray, b: np.ndarray, 
                                   squared: bool = False) -> Union[float, np.ndarray]:
                  """
                  Euclidean (L2) distance.
                  
                  Args:
                      a: Vector [d] or matrix [n, d]
                      b: Vector [d] or matrix [m, d]
                      squared: Return squared distance (faster, preserves ranking)
                  
                  Returns:
                      Distance: scalar, [n], or [n, m]
                      Note: Smaller is MORE similar (unlike other metrics)
                  """
                  if a.ndim == 1 and b.ndim == 1:
                      dist_sq = np.sum((a - b) ** 2)
                      return dist_sq if squared else np.sqrt(dist_sq)
                  
                  elif a.ndim == 2 and b.ndim == 1:
                      dist_sq = np.sum((a - b) ** 2, axis=1)
                      return dist_sq if squared else np.sqrt(dist_sq)
                  
                  elif a.ndim == 2 and b.ndim == 2:
                      # Efficient pairwise distance using broadcasting
                      # ||a - b||² = ||a||² + ||b||² - 2(a·b)
                      a_sq = np.sum(a ** 2, axis=1, keepdims=True)
                      b_sq = np.sum(b ** 2, axis=1, keepdims=True)
                      dist_sq = a_sq + b_sq.T - 2 * np.dot(a, b.T)
                      dist_sq = np.maximum(dist_sq, 0)  # Numerical stability
                      return dist_sq if squared else np.sqrt(dist_sq)
                  
                  else:
                      raise ValueError(f"Unsupported shapes: {a.shape}, {b.shape}")
              
              @staticmethod
              def normalize_l2(vectors: np.ndarray, axis: int = 1) -> np.ndarray:
                  """
                  L2 normalize vectors to unit length.
                  
                  Args:
                      vectors: Array to normalize
                      axis: Axis along which to normalize
                  
                  Returns:
                      Normalized vectors with ||v|| = 1
                  """
                  norms = np.linalg.norm(vectors, axis=axis, keepdims=True)
                  return vectors / (norms + 1e-8)
              
              @staticmethod
              def verify_normalized(vectors: np.ndarray, tolerance: float = 1e-6) -> bool:
                  """Check if vectors are L2-normalized."""
                  norms = np.linalg.norm(vectors, axis=1)
                  return np.allclose(norms, 1.0, atol=tolerance)
          
          
          def benchmark_metrics():
              """Benchmark different similarity metrics."""
              print("\n" + "="*80)
              print("SIMILARITY METRICS BENCHMARK")
              print("="*80)
              
              # Create test data
              np.random.seed(42)
              n_docs = 10000
              n_queries = 100
              dim = 384
              
              docs = np.random.randn(n_docs, dim).astype(np.float32)
              queries = np.random.randn(n_queries, dim).astype(np.float32)
              
              # Normalize for fair comparison
              docs_norm = SimilarityMetrics.normalize_l2(docs)
              queries_norm = SimilarityMetrics.normalize_l2(queries)
              
              print(f"Documents: {docs.shape}")
              print(f"Queries: {queries.shape}")
              print(f"Normalized: {SimilarityMetrics.verify_normalized(docs_norm)}\n")
              
              metrics = SimilarityMetrics()
              
              # Benchmark each metric
              results = {}
              
              for name, func, data_docs, data_queries in [
                  ("Cosine (unnormalized)", metrics.cosine_similarity, docs, queries),
                  ("Cosine (normalized)", metrics.cosine_similarity, docs_norm, queries_norm),
                  ("Dot Product (normalized)", metrics.dot_product, docs_norm, queries_norm),
                  ("Euclidean (squared)", lambda a, b: metrics.euclidean_distance(a, b, squared=True), docs, queries),
              ]:
                  start = time.time()
                  similarities = func(data_docs, data_queries[0])
                  elapsed = time.time() - start
                  
                  results[name] = {
                      'time': elapsed,
                      'mean': float(np.mean(similarities)),
                      'std': float(np.std(similarities)),
                  }
                  
                  print(f"{name:30s}: {elapsed*1000:6.2f}ms | "
                        f"mean={results[name]['mean']:7.4f} std={results[name]['std']:6.4f}")
              
              # Verify cosine = dot for normalized vectors
              print("\n" + "-"*80)
              print("VERIFICATION: Cosine = Dot Product for normalized vectors")
              print("-"*80)
              
              cos_sim = metrics.cosine_similarity(docs_norm, queries_norm[0])
              dot_sim = metrics.dot_product(docs_norm, queries_norm[0])
              
              print(f"Max difference: {np.max(np.abs(cos_sim - dot_sim)):.2e}")
              print(f"Are they equal? {np.allclose(cos_sim, dot_sim, atol=1e-6)}")
              
              return results
          
          
          if __name__ == "__main__":
              # Load embeddings from previous topic
              try:
                  embeddings = np.load('embeddings.npy')
                  with open('documents.txt', 'r') as f:
                      documents = [line.strip() for line in f]
              except FileNotFoundError:
                  print("ERROR: Run topic 1 code first to generate embeddings")
                  exit(1)
              
              metrics = SimilarityMetrics()
              
              # Example: Find similar documents
              query_idx = 0  # "Machine learning is a subset of artificial intelligence."
              query_emb = embeddings[query_idx]
              
              print("\n" + "="*80)
              print(f"QUERY: {documents[query_idx]}")
              print("="*80)
              
              # Compute similarities with all documents
              similarities = metrics.cosine_similarity(embeddings, query_emb)
              
              # Sort by similarity
              sorted_indices = np.argsort(similarities)[::-1]
              
              print("\nTop 5 similar documents (cosine similarity):")
              for rank, idx in enumerate(sorted_indices[:5], 1):
                  print(f"{rank}. [{similarities[idx]:.4f}] {documents[idx]}")
              
              # Benchmark
              benchmark_metrics()
    
    security_implications:
      timing_attacks_on_similarity: |
        **Vulnerability**: Similarity computation time can leak information about embedding
        values. Cosine similarity with unnormalized vectors has variable execution time
        based on vector magnitudes.
        
        **Attack scenario**: Attacker measures response times for similarity queries.
        Longer times might indicate larger magnitude vectors, potentially revealing
        information about document importance or training set membership.
        
        **Defense**:
        1. Use normalized vectors (constant-time cosine = dot product)
        2. Add random delays to response times (noise injection)
        3. Use constant-time implementations where critical
        4. Batch queries to obscure individual timing
      
      similarity_threshold_probing: |
        **Vulnerability**: Attackers can probe similarity thresholds to infer system
        behavior and potentially extract information about the corpus.
        
        **Attack scenario**: Attacker submits queries and observes which threshold
        triggers retrieval. By binary search on similarity scores, they can map the
        distribution of embeddings and potentially reconstruct documents.
        
        **Defense**:
        1. Implement query rate limiting per user
        2. Add calibrated noise to similarity scores
        3. Randomize thresholds slightly per query
        4. Monitor for systematic threshold probing patterns
        5. Use adaptive thresholds that change based on corpus
      
      metric_manipulation_for_evasion: |
        **Vulnerability**: Attackers who understand your similarity metric can craft
        queries to maximize or minimize similarity to specific documents.
        
        **Attack scenario**: If using cosine similarity, attacker creates queries that
        are orthogonal to unwanted documents (cosine = 0) but aligned with target
        documents. This can evade content filters or manipulate rankings.
        
        **Defense**:
        1. Use multiple similarity metrics and combine scores
        2. Implement content-based filtering in addition to similarity
        3. Detect and flag queries with extreme similarity patterns
        4. Monitor for adversarial query optimization attempts
        5. Use robust aggregation (median, trimmed mean) for rankings

  - topic_number: 3
    title: "K-Nearest Neighbors Search and Semantic Search Engine"
    
    overview: |
      K-Nearest Neighbors (KNN) search is the foundation of semantic search: given a query
      embedding, find the k most similar document embeddings. While conceptually simple,
      building an efficient, secure KNN search engine requires careful algorithm design,
      performance optimization, and security hardening.
      
      We implement a complete semantic search engine from scratch, including query processing,
      similarity computation, ranking, and result filtering. We explore exact vs approximate
      search trade-offs, performance optimization techniques, and comprehensive security
      measures.
      
      This section provides the algorithmic foundation that scales to production vector
      databases in Section 4.2. Understanding exact KNN search deeply prepares you to
      appreciate why approximate methods (HNSW, IVF) are necessary for billion-scale systems.
    
    content:
      knn_algorithm:
        basic_approach: |
          K-Nearest Neighbors search algorithm:
          
          1. Input: Query embedding q, corpus embeddings D = {d₁, d₂, ..., dₙ}, k
          2. For each document embedding dᵢ in D:
               - Compute similarity(q, dᵢ)
          3. Sort by similarity (descending for cosine/dot, ascending for distance)
          4. Return top k documents
          
          Time complexity: O(n×d) for similarity computation + O(n log n) for sorting
          Space complexity: O(n) for similarity scores
          
          This "brute force" approach examines every document, making it exact but slow
          for large corpora.
        
        optimization_strategies: |
          1. **Vectorization**: Compute all similarities in single matrix operation
             - Use NumPy broadcasting: similarities = docs @ query
             - 10-100x faster than Python loops
          
          2. **Partial sorting**: Use np.argpartition for top-k instead of full sort
             - O(n) average case vs O(n log n)
             - Only sort the top-k elements, ignore the rest
          
          3. **Pre-normalization**: Normalize embeddings once at index time
             - Cosine similarity reduces to dot product
             - No normalization overhead at query time
          
          4. **Batch queries**: Process multiple queries simultaneously
             - Amortize overhead across queries
             - Better cache utilization
          
          5. **Early termination**: For threshold-based retrieval
             - Stop when k documents exceed threshold
             - Useful for high-similarity filters
        
        exact_vs_approximate: |
          Exact KNN guarantees finding the true k-nearest neighbors but has O(n)
          complexity - impractical for millions/billions of vectors.
          
          Approximate KNN trades accuracy for speed:
          - Might miss some true nearest neighbors
          - Returns approximate nearest neighbors (close enough)
          - Sub-linear complexity: O(log n) or better
          - 10-1000x faster for large datasets
          
          Section 4.2 covers approximate methods (HNSW, IVF). For now, we master exact KNN.
      
      semantic_search_pipeline:
        components: |
          A production semantic search engine has multiple components:
          
          1. **Query Processing**:
             - Text normalization and cleaning
             - Embedding generation
             - Query expansion (optional)
          
          2. **Retrieval**:
             - KNN search with chosen metric
             - Threshold filtering
             - Deduplication
          
          3. **Re-ranking** (optional):
             - Cross-encoder re-ranking
             - Diversity-aware ranking
             - Recency or popularity boosting
          
          4. **Post-processing**:
             - Result formatting
             - Metadata enrichment
             - Security filtering
        
        ranking_strategies: |
          Beyond raw similarity scores:
          
          1. **Hybrid ranking**: Combine multiple signals
             - Similarity score (70%) + recency (20%) + popularity (10%)
             - Weighted combination with learned or manual weights
          
          2. **Diversity ranking**: Avoid redundant results
             - Maximal Marginal Relevance (MMR)
             - Select documents that are similar to query but dissimilar to each other
          
          3. **Personalization**: User-specific ranking
             - Incorporate user history and preferences
             - Adjust weights based on user context
          
          4. **A/B testing**: Experiment with ranking variants
             - Split traffic between ranking strategies
             - Measure engagement metrics
      
      performance_considerations:
        index_structures: |
          For exact search optimization:
          
          1. **Pre-computed norms**: Store ||d|| for each document
             - Avoids recomputing during queries
             - Especially important for cosine similarity
          
          2. **Memory layout**: Contiguous storage for cache efficiency
             - Column-major vs row-major considerations
             - Alignment for SIMD operations
          
          3. **Quantization preview**: Reduce precision for speed
             - Float16 instead of float32
             - 2x memory reduction, minimal accuracy loss
             - We'll explore this deeply in Section 4.13
        
        scalability_limits: |
          Exact KNN becomes impractical at:
          - 1M+ documents: Multi-second query latency
          - 10M+ documents: Tens of seconds per query
          - 100M+ documents: Minutes per query
          - 1B+ documents: Impossible with exact search
          
          This motivates approximate methods in Section 4.2.
    
    implementation:
      semantic_search_engine:
        language: python
        code: |
          """
          Complete semantic search engine with exact KNN.
          Includes query processing, retrieval, ranking, and security features.
          """
          
          import numpy as np
          from typing import List, Tuple, Dict, Optional
          import time
          from dataclasses import dataclass
          
          @dataclass
          class SearchResult:
              """Single search result with metadata."""
              doc_id: int
              text: str
              score: float
              rank: int
          
          
          class SemanticSearchEngine:
              """
              Production-grade semantic search with exact KNN.
              
              Features:
              - Multiple similarity metrics
              - Efficient vectorized operations
              - Query rate limiting (security)
              - Result diversity (MMR)
              - Comprehensive logging
              """
              
              def __init__(self, 
                          documents: List[str],
                          embeddings: np.ndarray,
                          metric: str = 'cosine',
                          normalize: bool = True):
                  """
                  Initialize search engine.
                  
                  Args:
                      documents: List of document texts
                      embeddings: Document embeddings [n_docs, dim]
                      metric: 'cosine', 'dot', or 'euclidean'
                      normalize: L2-normalize embeddings for efficiency
                  """
                  assert len(documents) == len(embeddings), "Mismatch in docs and embeddings"
                  
                  self.documents = documents
                  self.metric = metric
                  self.n_docs = len(documents)
                  self.dim = embeddings.shape[1]
                  
                  # Normalize embeddings if requested
                  if normalize and metric in ['cosine', 'dot']:
                      self.embeddings = self._normalize(embeddings)
                      self.normalized = True
                  else:
                      self.embeddings = embeddings
                      self.normalized = normalize
                  
                  # Pre-compute norms for cosine (if not normalized)
                  if metric == 'cosine' and not self.normalized:
                      self.norms = np.linalg.norm(embeddings, axis=1)
                  else:
                      self.norms = None
                  
                  # Query rate limiting (security feature)
                  self.query_history: Dict[str, List[float]] = {}
                  self.rate_limit_window = 60.0  # seconds
                  self.rate_limit_max = 100  # queries per window
                  
                  print(f"Initialized SemanticSearchEngine:")
                  print(f"  Documents: {self.n_docs}")
                  print(f"  Embedding dim: {self.dim}")
                  print(f"  Metric: {self.metric}")
                  print(f"  Normalized: {self.normalized}")
              
              def _normalize(self, vectors: np.ndarray) -> np.ndarray:
                  """L2 normalize vectors."""
                  norms = np.linalg.norm(vectors, axis=1, keepdims=True)
                  return vectors / (norms + 1e-8)
              
              def _compute_similarity(self, query_emb: np.ndarray) -> np.ndarray:
                  """
                  Compute similarity between query and all documents.
                  
                  Args:
                      query_emb: Query embedding [dim]
                  
                  Returns:
                      Similarities: [n_docs]
                  """
                  if self.metric == 'cosine':
                      if self.normalized:
                          # Cosine = dot product for normalized vectors
                          return np.dot(self.embeddings, query_emb)
                      else:
                          # Full cosine computation
                          query_norm = np.linalg.norm(query_emb)
                          return np.dot(self.embeddings, query_emb) / (self.norms * query_norm + 1e-8)
                  
                  elif self.metric == 'dot':
                      return np.dot(self.embeddings, query_emb)
                  
                  elif self.metric == 'euclidean':
                      # Smaller distance = higher similarity, so negate
                      distances = np.linalg.norm(self.embeddings - query_emb, axis=1)
                      return -distances
                  
                  else:
                      raise ValueError(f"Unknown metric: {self.metric}")
              
              def _check_rate_limit(self, user_id: str) -> bool:
                  """
                  Check if user has exceeded query rate limit.
                  
                  Args:
                      user_id: User identifier
                  
                  Returns:
                      True if allowed, False if rate limited
                  """
                  current_time = time.time()
                  
                  # Initialize user history
                  if user_id not in self.query_history:
                      self.query_history[user_id] = []
                  
                  # Remove queries outside the time window
                  cutoff_time = current_time - self.rate_limit_window
                  self.query_history[user_id] = [
                      t for t in self.query_history[user_id] if t > cutoff_time
                  ]
                  
                  # Check limit
                  if len(self.query_history[user_id]) >= self.rate_limit_max:
                      return False
                  
                  # Record this query
                  self.query_history[user_id].append(current_time)
                  return True
              
              def search(self,
                        query_emb: np.ndarray,
                        k: int = 5,
                        threshold: Optional[float] = None,
                        diversity_lambda: float = 0.0,
                        user_id: str = 'default') -> List[SearchResult]:
                  """
                  Search for k most similar documents.
                  
                  Args:
                      query_emb: Query embedding [dim]
                      k: Number of results to return
                      threshold: Minimum similarity threshold (None = no threshold)
                      diversity_lambda: MMR diversity parameter [0, 1]
                                       0 = pure relevance, 1 = pure diversity
                      user_id: User identifier for rate limiting
                  
                  Returns:
                      List of SearchResult objects
                  """
                  # Security: Rate limiting
                  if not self._check_rate_limit(user_id):
                      raise RuntimeError(f"Rate limit exceeded for user {user_id}")
                  
                  # Normalize query if needed
                  if self.normalized:
                      query_emb = self._normalize(query_emb.reshape(1, -1)).squeeze()
                  
                  # Compute similarities
                  similarities = self._compute_similarity(query_emb)
                  
                  # Apply threshold filter
                  if threshold is not None:
                      valid_mask = similarities >= threshold
                      if not np.any(valid_mask):
                          return []
                      
                      valid_indices = np.where(valid_mask)[0]
                      valid_similarities = similarities[valid_mask]
                  else:
                      valid_indices = np.arange(len(similarities))
                      valid_similarities = similarities
                  
                  # Select top-k
                  if diversity_lambda > 0:
                      # MMR: Maximal Marginal Relevance for diversity
                      selected_indices = self._mmr_search(
                          query_emb, valid_indices, valid_similarities, k, diversity_lambda
                      )
                  else:
                      # Pure relevance ranking
                      if len(valid_similarities) <= k:
                          selected_indices = valid_indices
                      else:
                          # Use argpartition for efficient top-k
                          top_k_positions = np.argpartition(valid_similarities, -k)[-k:]
                          # Sort the top-k
                          sorted_positions = top_k_positions[np.argsort(valid_similarities[top_k_positions])[::-1]]
                          selected_indices = valid_indices[sorted_positions]
                  
                  # Create results
                  results = []
                  for rank, idx in enumerate(selected_indices, 1):
                      results.append(SearchResult(
                          doc_id=int(idx),
                          text=self.documents[idx],
                          score=float(similarities[idx]),
                          rank=rank
                      ))
                  
                  return results
              
              def _mmr_search(self,
                            query_emb: np.ndarray,
                            candidate_indices: np.ndarray,
                            candidate_scores: np.ndarray,
                            k: int,
                            lambda_param: float) -> np.ndarray:
                  """
                  Maximal Marginal Relevance: balance relevance and diversity.
                  
                  MMR = λ × Sim(q, d) - (1-λ) × max Sim(d, d_selected)
                  
                  Args:
                      query_emb: Query embedding
                      candidate_indices: Valid document indices
                      candidate_scores: Similarity scores for candidates
                      k: Number of results
                      lambda_param: Trade-off parameter
                  
                  Returns:
                      Selected indices in rank order
                  """
                  selected = []
                  remaining = set(range(len(candidate_indices)))
                  
                  # Select first document (highest relevance)
                  first_idx = np.argmax(candidate_scores)
                  selected.append(candidate_indices[first_idx])
                  remaining.remove(first_idx)
                  
                  # Select remaining k-1 documents
                  for _ in range(min(k - 1, len(remaining))):
                      mmr_scores = []
                      
                      for idx in remaining:
                          doc_idx = candidate_indices[idx]
                          
                          # Relevance score
                          relevance = candidate_scores[idx]
                          
                          # Diversity score: max similarity to already selected
                          selected_embeddings = self.embeddings[selected]
                          doc_embedding = self.embeddings[doc_idx]
                          
                          doc_similarities = np.dot(selected_embeddings, doc_embedding)
                          max_similarity = np.max(doc_similarities)
                          
                          # MMR score
                          mmr = lambda_param * relevance - (1 - lambda_param) * max_similarity
                          mmr_scores.append((idx, mmr))
                      
                      # Select document with highest MMR
                      best_idx, _ = max(mmr_scores, key=lambda x: x[1])
                      selected.append(candidate_indices[best_idx])
                      remaining.remove(best_idx)
                  
                  return np.array(selected)
              
              def batch_search(self,
                             query_embs: np.ndarray,
                             k: int = 5,
                             user_id: str = 'default') -> List[List[SearchResult]]:
                  """
                  Search multiple queries in batch.
                  
                  Args:
                      query_embs: Query embeddings [n_queries, dim]
                      k: Number of results per query
                      user_id: User identifier
                  
                  Returns:
                      List of result lists, one per query
                  """
                  results = []
                  for query_emb in query_embs:
                      results.append(self.search(query_emb, k=k, user_id=user_id))
                  return results
              
              def get_stats(self) -> Dict:
                  """Get search engine statistics."""
                  return {
                      'n_documents': self.n_docs,
                      'embedding_dim': self.dim,
                      'metric': self.metric,
                      'normalized': self.normalized,
                      'total_queries': sum(len(queries) for queries in self.query_history.values()),
                      'unique_users': len(self.query_history),
                  }
          
          
          if __name__ == "__main__":
              # Load data
              embeddings = np.load('embeddings.npy')
              with open('documents.txt', 'r') as f:
                  documents = [line.strip() for line in f]
              
              # Initialize search engine
              engine = SemanticSearchEngine(
                  documents=documents,
                  embeddings=embeddings,
                  metric='cosine',
                  normalize=True
              )
              
              # Example 1: Basic search
              print("\n" + "="*80)
              print("EXAMPLE 1: Basic Semantic Search")
              print("="*80)
              
              query_idx = 5  # "Transformers revolutionized NLP with attention mechanisms."
              query_emb = embeddings[query_idx]
              
              print(f"Query: {documents[query_idx]}\n")
              results = engine.search(query_emb, k=3)
              
              print("Top 3 results:")
              for result in results:
                  print(f"{result.rank}. [{result.score:.4f}] {result.text}")
              
              # Example 2: Diversity search (MMR)
              print("\n" + "="*80)
              print("EXAMPLE 2: Diversity Search (MMR)")
              print("="*80)
              
              print("Without diversity (λ=0.0):")
              results_no_div = engine.search(query_emb, k=5, diversity_lambda=0.0)
              for r in results_no_div:
                  print(f"  [{r.score:.4f}] {r.text[:60]}...")
              
              print("\nWith diversity (λ=0.5):")
              results_div = engine.search(query_emb, k=5, diversity_lambda=0.5)
              for r in results_div:
                  print(f"  [{r.score:.4f}] {r.text[:60]}...")
              
              # Example 3: Threshold filtering
              print("\n" + "="*80)
              print("EXAMPLE 3: Threshold Filtering")
              print("="*80)
              
              results_thresh = engine.search(query_emb, k=10, threshold=0.3)
              print(f"Results with similarity >= 0.3: {len(results_thresh)}")
              for r in results_thresh:
                  print(f"  [{r.score:.4f}] {r.text[:60]}...")
              
              # Example 4: Rate limiting (security)
              print("\n" + "="*80)
              print("EXAMPLE 4: Rate Limiting (Security)")
              print("="*80)
              
              try:
                  # Simulate excessive queries
                  for i in range(105):
                      engine.search(query_emb, k=1, user_id='test_user')
              except RuntimeError as e:
                  print(f"✓ Rate limit triggered: {e}")
              
              # Stats
              print("\n" + "="*80)
              print("ENGINE STATISTICS")
              print("="*80)
              stats = engine.get_stats()
              for key, value in stats.items():
                  print(f"  {key}: {value}")
    
    security_implications:
      query_flooding_dos: |
        **Vulnerability**: Attackers can flood the search engine with queries to cause
        denial of service, either exhausting computational resources or masking malicious
        queries in the noise.
        
        **Attack scenario**: Attacker sends 10,000 queries per second, each requiring
        full similarity computation across millions of documents. System becomes unresponsive
        for legitimate users.
        
        **Defense**:
        1. ✅ Implemented: Rate limiting per user/IP (we included this)
        2. Query queue with priority (authenticated users first)
        3. Resource allocation limits per user
        4. Automatic scaling based on load
        5. DDoS protection at network layer
      
      adversarial_query_optimization: |
        **Vulnerability**: Attackers can craft queries specifically designed to retrieve
        documents they shouldn't access or to evade content filters.
        
        **Attack scenario**: Attacker knows the embedding space geometry and creates
        queries that maximize similarity to sensitive documents while minimizing similarity
        to filter keywords. They iteratively refine queries based on returned results.
        
        **Defense**:
        1. Content-based filtering in addition to similarity
        2. Access control: verify user permissions for retrieved documents
        3. Detect and flag adversarial queries (unusual embedding patterns)
        4. Monitor for query optimization patterns (gradient-like progression)
        5. Randomize similarity scores slightly (differential privacy)
      
      result_manipulation_through_diversity: |
        **Vulnerability**: MMR and other diversity mechanisms can be exploited to surface
        less relevant but attacker-controlled documents.
        
        **Attack scenario**: Attacker plants documents that are moderately similar to
        common queries but very dissimilar to each other. MMR's diversity preference
        elevates these planted documents over more relevant legitimate ones.
        
        **Defense**:
        1. Combine diversity with relevance threshold (don't sacrifice too much relevance)
        2. Audit diversity parameter selection (don't allow user control)
        3. Monitor diversity impact on result quality
        4. Content quality signals in addition to diversity
        5. Detect planted document clusters
      
      information_leakage_through_search_results: |
        **Vulnerability**: Search results can leak information about the corpus composition,
        document distribution, and potentially sensitive content.
        
        **Attack scenario**: Attacker systematically queries to map the corpus. By observing
        which queries return results and similarity scores, they infer what documents exist,
        their topics, and potentially reconstruct sensitive information.
        
        **Defense**:
        1. ✅ Implemented: Rate limiting (we included this)
        2. Access control: only return documents user is authorized to see
        3. Redact or summarize results instead of returning full text
        4. Add calibrated noise to similarity scores
        5. Monitor for systematic corpus mapping attempts
        6. Implement query diversity requirements (don't allow repetitive queries)

key_takeaways:
  critical_concepts:
    - concept: "Dense embeddings map text to continuous vector spaces where semantic similarity manifests as geometric proximity"
      why_it_matters: "This property enables semantic search, RAG systems, and all modern LLM retrieval applications. Understanding embedding spaces is fundamental to building production LLM systems."
    
    - concept: "Similarity metrics (cosine, dot product, Euclidean) have different properties and use cases"
      why_it_matters: "Choosing the right metric affects retrieval quality, computational efficiency, and security properties. For normalized embeddings, cosine = dot product, enabling significant speedups."
    
    - concept: "Exact KNN search is O(n) in corpus size, requiring examination of every document"
      why_it_matters: "This complexity makes exact search impractical for large corpora (millions+ documents). Understanding exact KNN deeply prepares you to appreciate approximate methods in Section 4.2."
    
    - concept: "Embedding-based systems have unique security vulnerabilities including information leakage, poisoning, and adversarial queries"
      why_it_matters: "Production semantic search requires comprehensive security: rate limiting, query validation, result filtering, and monitoring. Security must be built in from the start, not added later."
  
  actionable_steps:
    - step: "Always L2-normalize embeddings when using cosine similarity for production systems"
      verification: "Verify that cosine similarity equals dot product for normalized vectors (they should match within 1e-6)"
    
    - step: "Implement rate limiting and query monitoring from day one"
      verification: "Test that rate limits trigger correctly and that query patterns are logged for security analysis"
    
    - step: "Benchmark different similarity metrics on your specific data"
      verification: "Measure retrieval quality (relevance) and computational cost (latency) for each metric on representative queries"
    
    - step: "Use vectorized NumPy operations instead of Python loops for similarity computation"
      verification: "10-100x speedup should be observable when processing 1000+ documents"
  
  security_principles:
    - principle: "Defense in depth: Layer multiple security controls (rate limiting, access control, content filtering)"
      application: "Even if one control fails (e.g., rate limiting bypassed), other controls (access control) still protect the system"
    
    - principle: "Assume breach: Design systems assuming attackers can submit malicious queries"
      application: "Validate all inputs, sanitize all outputs, monitor for anomalies, and have incident response procedures ready"
    
    - principle: "Minimize information leakage: Limit what attackers can learn through queries"
      application: "Add calibrated noise to scores, rate limit queries, filter results by permissions, monitor for systematic probing"
    
    - principle: "Secure by default: Make the secure option the easy option"
      application: "Normalized embeddings by default, rate limiting enabled by default, monitoring built-in from the start"
  
  common_mistakes:
    - mistake: "Using cosine similarity on unnormalized embeddings without pre-computing norms"
      fix: "Either normalize embeddings once at indexing time, or pre-compute and store norms for documents"
    
    - mistake: "Full sorting when only top-k results are needed"
      fix: "Use np.argpartition for O(n) average case instead of O(n log n) full sort"
    
    - mistake: "No rate limiting or query monitoring in 'prototype' systems that go to production"
      fix: "Build security in from the start. Rate limiting and logging have minimal overhead and prevent major incidents"
    
    - mistake: "Returning raw similarity scores to users without normalization or calibration"
      fix: "Normalize scores to [0, 1] range, add calibrated noise if needed, and consider only returning relative rankings"
    
    - mistake: "Ignoring the security implications of embedding-based search"
      fix: "Treat semantic search as a security-critical component. Implement comprehensive controls and monitoring from day one"
  
  integration_with_book:
    from_chapter_3:
      - "BERT and GPT contextual embeddings (Section 3.13-3.15) provide the dense vectors we search over"
      - "Attention mechanisms (Section 3.7-3.9) create the semantic relationships that embeddings capture"
      - "LLM security fundamentals (Section 3.18-3.20) extend to embedding-based systems with new attack surfaces"
    
    to_next_section:
      - "Section 4.2 scales exact KNN to billions of vectors with FAISS, HNSW, and approximate nearest neighbors"
      - "Understanding exact KNN deeply makes approximate methods (trading accuracy for speed) intuitive"
      - "Security controls built here (rate limiting, monitoring) extend to production vector databases"
  
  looking_ahead:
    next_concepts:
      - "Vector databases (FAISS, Pinecone, Weaviate) for production-scale search"
      - "Approximate nearest neighbor algorithms (HNSW, IVF) for sub-linear search"
      - "Index structures and quantization for memory efficiency"
      - "Distributed vector search for horizontal scaling"
    
    skills_to_build:
      - "Navigating trade-offs between accuracy, speed, and memory in approximate search"
      - "Deploying and operating production vector databases"
      - "Monitoring and optimizing vector search performance"
      - "Securing vector databases against advanced attacks"
  
  final_thoughts: |
    This section established the foundation for all embedding-based retrieval systems. You
    now understand how dense embeddings capture semantic meaning, how similarity metrics
    quantify that meaning, and how to build a complete semantic search engine from scratch.
    
    The exact KNN search we implemented is mathematically elegant and conceptually simple,
    but computationally expensive for large corpora. This motivates the approximate methods
    we'll explore in Section 4.2. However, understanding exact search deeply is critical:
    it provides the baseline for evaluating approximate methods, helps debug retrieval
    issues, and enables you to make informed trade-offs between accuracy and speed.
    
    Security is paramount in production systems. The security implications we identified—
    information leakage, adversarial queries, poisoning attacks—are real threats that have
    affected production systems. The defenses we implemented (rate limiting, monitoring,
    query validation) are essential building blocks for secure RAG and semantic search.
    
    As you move forward, remember: semantic search is not just an algorithm, it's a system.
    Production systems require careful attention to performance, scalability, security,
    monitoring, and operational excellence. Every design decision affects these dimensions.
    
    In Section 4.2, we'll scale these concepts to production with vector databases and
    approximate search algorithms that handle billions of vectors with millisecond latency.

---
