# section_02_23_model_interpretability.yaml

---
document_info:
  chapter: "02"
  section: "23"
  title: "Model Interpretability and Explainability"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["interpretability", "explainability", "feature-importance", "saliency-maps", "lime", "shap"]

# ============================================================================
# SECTION 02_23: MODEL INTERPRETABILITY AND EXPLAINABILITY
# ============================================================================

section_02_23_model_interpretability:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Neural networks are often "black boxes" - high accuracy but opaque reasoning.
    Why did the model predict "fraud"? Which features mattered? Can we trust
    this prediction? Interpretability techniques open the black box, revealing
    which inputs drive predictions and how the model makes decisions.
    
    This section covers interpretability fundamentals. You'll understand why
    interpretability matters (debugging, trust, fairness, compliance), learn
    feature importance methods (permutation importance, SHAP), implement
    saliency maps (gradients show which pixels matter), use local explanations
    (LIME approximates model locally), understand limitations (explanations
    can mislead), and connect to security (interpretability detects backdoors).
    
    By the end, you'll explain model predictions to stakeholders, debug via
    interpretability, and understand when explanations trustworthy vs misleading.
  
  learning_objectives:
    
    conceptual:
      - "Understand interpretability vs explainability"
      - "Know global vs local explanations"
      - "Grasp feature importance fundamentals"
      - "Recognize limitations of interpretability methods"
      - "Understand when interpretability required (regulations)"
      - "Connect interpretability to security (backdoor detection)"
    
    practical:
      - "Implement feature importance (permutation)"
      - "Generate saliency maps for images"
      - "Use LIME for local explanations"
      - "Compute SHAP values"
      - "Visualize attention weights"
      - "Build interpretation dashboard"
    
    security_focused:
      - "Interpretability detects backdoors"
      - "Explanations can be manipulated (adversarial)"
      - "Feature importance reveals sensitive attributes"
      - "Attention weights leak information"
  
  prerequisites:
    - "All Chapter 02 sections (gradients, models, training)"
    - "Understanding of feature importance"
    - "Familiarity with visualization"
  
  # --------------------------------------------------------------------------
  # Topic 1: Why Interpretability Matters
  # --------------------------------------------------------------------------
  
  why_interpretability:
    
    trust_and_adoption:
      
      user_trust: |
        Users trust explainable predictions more:
        
        Black box: "Model says fraud. Trust us."
        → User skeptical
        
        Explained: "Model says fraud because:
        - Transaction amount 10× typical
        - Unusual location (foreign country)
        - Time: 3am (unusual for this user)"
        → User understands and trusts
      
      stakeholder_buy_in: |
        Business stakeholders need explanations:
        
        Question: "Why did model reject this loan?"
        
        Without interpretability: "Neural network said so"
        → Stakeholder won't deploy
        
        With interpretability: "Low income-to-debt ratio, short credit history"
        → Stakeholder understands, approves deployment
    
    debugging_and_improvement:
      
      find_bugs: |
        Interpretability reveals model bugs:
        
        Example: Image classifier
        Model: High accuracy on test set
        Explanation: Attending to watermark in corner!
        
        Bug: Model learned spurious correlation
        Fix: Remove watermarks from training data
      
      feature_engineering: |
        Feature importance guides engineering:
        
        High importance: Income, debt_ratio, credit_history
        Low importance: Favorite_color, zodiac_sign
        
        Action: Remove low-importance features, add similar to high-importance
    
    regulatory_compliance:
      
      gdpr_right_to_explanation: |
        GDPR Article 22: Right to explanation for automated decisions
        
        Users can request: "Why was my loan denied?"
        
        Must provide meaningful explanation (not just "algorithm")
      
      fair_lending_laws: |
        US Fair Credit Reporting Act: Must explain credit decisions
        
        Cannot say: "Computer said no"
        Must explain: Which factors led to decision
      
      medical_ai: |
        FDA guidance: Medical AI must be interpretable
        
        Doctor needs to understand: "Why does AI recommend surgery?"
        
        Can't deploy black box for medical decisions
    
    fairness_and_bias:
      
      detect_bias: |
        Interpretability reveals unfair biases:
        
        Model: Predicting recidivism
        Explanation: Race is top feature
        
        Problem: Using protected attribute illegally
        Fix: Remove race, retrain, verify fairness
      
      protected_attributes: |
        Check if model uses protected attributes:
        - Race
        - Gender
        - Age
        - Religion
        
        Even indirectly (via proxies like zip code)
  
  # --------------------------------------------------------------------------
  # Topic 2: Types of Interpretability
  # --------------------------------------------------------------------------
  
  types_of_interpretability:
    
    global_vs_local:
      
      global_interpretability: |
        Understanding entire model behavior:
        
        "What does the model learn overall?"
        "Which features are generally important?"
        
        Methods:
        - Feature importance
        - Decision trees approximation
        - Model architecture analysis
      
      local_interpretability: |
        Understanding single prediction:
        
        "Why did model predict fraud for THIS transaction?"
        "Which features mattered for THIS prediction?"
        
        Methods:
        - LIME
        - SHAP
        - Saliency maps
      
      which_to_use: |
        Global: Model debugging, feature selection, overall understanding
        Local: Individual prediction explanation, user trust, compliance
    
    model_specific_vs_model_agnostic:
      
      model_specific: |
        Works only for certain model types:
        
        - Attention weights (Transformers only)
        - Saliency maps (gradient-based models)
        - Tree feature importance (tree models only)
        
        Pros: Efficient, exact
        Cons: Limited applicability
      
      model_agnostic: |
        Works for any model (black box):
        
        - LIME
        - SHAP
        - Permutation importance
        
        Pros: Universal
        Cons: Slower, approximate
  
  # --------------------------------------------------------------------------
  # Topic 3: Feature Importance
  # --------------------------------------------------------------------------
  
  feature_importance:
    
    permutation_importance:
      
      algorithm: |
        Measure importance by permuting feature:
        
        1. Compute baseline accuracy on validation set
        2. For each feature i:
           a. Randomly shuffle feature i values
           b. Compute new accuracy
           c. Importance = baseline_accuracy - new_accuracy
        3. Rank features by importance
        
        High importance: Accuracy drops a lot when shuffled
        Low importance: Accuracy barely changes
      
      implementation: |
        def permutation_importance(model, X_val, y_val, num_repeats=10):
            """
            Compute permutation importance for each feature.
            
            Parameters:
            - model: trained model
            - X_val: validation features (N, D)
            - y_val: validation labels (N,)
            - num_repeats: number of permutation repeats
            
            Returns:
            - importances: (D,) array of importance scores
            """
            # Baseline accuracy
            baseline_acc = model.score(X_val, y_val)
            
            num_features = X_val.shape[1]
            importances = np.zeros(num_features)
            
            for feature_idx in range(num_features):
                acc_drops = []
                
                for repeat in range(num_repeats):
                    # Copy data
                    X_permuted = X_val.copy()
                    
                    # Shuffle feature
                    np.random.shuffle(X_permuted[:, feature_idx])
                    
                    # Compute accuracy
                    permuted_acc = model.score(X_permuted, y_val)
                    
                    # Importance = drop in accuracy
                    acc_drops.append(baseline_acc - permuted_acc)
                
                # Average over repeats
                importances[feature_idx] = np.mean(acc_drops)
            
            return importances
      
      visualization: |
        import matplotlib.pyplot as plt
        
        def plot_feature_importance(importances, feature_names):
            """Plot feature importance bar chart"""
            # Sort by importance
            sorted_idx = np.argsort(importances)[::-1]
            
            plt.figure(figsize=(10, 6))
            plt.barh(range(len(importances)), importances[sorted_idx])
            plt.yticks(range(len(importances)), 
                      [feature_names[i] for i in sorted_idx])
            plt.xlabel('Importance (Accuracy Drop)')
            plt.title('Feature Importance')
            plt.tight_layout()
            plt.show()
      
      pros_and_cons: |
        Pros:
        - Model-agnostic (works for any model)
        - Intuitive interpretation
        - Captures feature interactions
        
        Cons:
        - Slow (need to retrain for each permutation)
        - Variance across runs (stochastic)
        - Only shows average importance
    
    shap_values:
      
      concept: |
        SHAP (SHapley Additive exPlanations):
        
        For prediction f(x):
        f(x) = f_baseline + φ_1 + φ_2 + ... + φ_D
        
        Where φ_i = SHAP value for feature i
        
        Interpretation: Contribution of feature i to prediction
      
      properties: |
        SHAP satisfies desirable properties:
        
        1. Local accuracy: Σ φ_i = f(x) - f_baseline
        2. Consistency: If feature helps more, higher SHAP value
        3. Missingness: If feature not used, SHAP = 0
        
        Theoretically grounded (game theory)
      
      usage_example: |
        import shap
        
        # Train model
        model = RandomForestClassifier()
        model.fit(X_train, y_train)
        
        # Compute SHAP values
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_test)
        
        # Visualize for single prediction
        shap.force_plot(
            explainer.expected_value,
            shap_values[0, :],
            X_test[0, :],
            feature_names=feature_names
        )
        
        # Summary plot (all predictions)
        shap.summary_plot(shap_values, X_test, feature_names=feature_names)
      
      interpretation: |
        SHAP value = +0.3: Feature increases prediction by 0.3
        SHAP value = -0.2: Feature decreases prediction by 0.2
        SHAP value = 0: Feature has no effect
        
        Sum of all SHAP values = prediction - baseline
  
  # --------------------------------------------------------------------------
  # Topic 4: Saliency Maps
  # --------------------------------------------------------------------------
  
  saliency_maps:
    
    gradient_based_saliency:
      
      concept: |
        Saliency map: Which input pixels matter most?
        
        Compute gradient of prediction w.r.t. input:
        Saliency = |∂f(x)/∂x|
        
        High gradient: Pixel strongly affects prediction
        Low gradient: Pixel doesn't matter
      
      implementation: |
        def compute_saliency_map(model, image, target_class):
            """
            Compute saliency map for image.
            
            Parameters:
            - model: neural network
            - image: input image (C, H, W)
            - target_class: class to explain
            
            Returns:
            - saliency: (H, W) saliency map
            """
            image.requires_grad = True
            
            # Forward pass
            logits = model(image.unsqueeze(0))
            
            # Get score for target class
            score = logits[0, target_class]
            
            # Backward pass
            score.backward()
            
            # Saliency = absolute value of gradient
            saliency = image.grad.abs()
            
            # Max across channels
            saliency = saliency.max(dim=0)[0]
            
            return saliency.detach().cpu().numpy()
      
      visualization: |
        def visualize_saliency(image, saliency):
            """Overlay saliency map on image"""
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(1, 3, figsize=(12, 4))
            
            # Original image
            axes[0].imshow(image.permute(1, 2, 0))
            axes[0].set_title('Original Image')
            axes[0].axis('off')
            
            # Saliency map
            axes[1].imshow(saliency, cmap='hot')
            axes[1].set_title('Saliency Map')
            axes[1].axis('off')
            
            # Overlay
            axes[2].imshow(image.permute(1, 2, 0))
            axes[2].imshow(saliency, cmap='hot', alpha=0.5)
            axes[2].set_title('Overlay')
            axes[2].axis('off')
            
            plt.tight_layout()
            plt.show()
    
    integrated_gradients:
      
      motivation: |
        Problem with vanilla gradients:
        - Noisy
        - Saturated neurons (gradient=0) ignored
        
        Solution: Integrated Gradients
        Integrate gradients along path from baseline to input
      
      algorithm: |
        IG(x) = (x - x_baseline) × ∫[α=0 to 1] ∂f(x_baseline + α(x - x_baseline))/∂x dα
        
        Approximate with Riemann sum:
        IG(x) ≈ (x - x_baseline) × Σ[i=1 to m] ∂f(x_i)/∂x / m
        
        Where x_i = x_baseline + (i/m)(x - x_baseline)
      
      properties: |
        Integrated Gradients satisfies:
        - Completeness: Σ IG_i = f(x) - f(x_baseline)
        - Sensitivity: If features differ and predictions differ, some IG ≠ 0
        
        More principled than vanilla gradients
    
    grad_cam:
      
      concept: |
        Grad-CAM: Gradient-weighted Class Activation Mapping
        
        For CNNs: Which regions of image important?
        
        Use gradients of final conv layer to weight feature maps
      
      algorithm: |
        1. Forward pass to get final conv layer activations A
        2. Compute gradients ∂y_c/∂A (for target class c)
        3. Global average pool gradients to get weights α_k
        4. Weighted combination: L_Grad-CAM = ReLU(Σ α_k · A_k)
        
        Result: Heatmap showing important regions
      
      advantage: |
        Class-discriminative and localized:
        - Shows WHERE in image model looks
        - Specific to predicted class
        - Coarse localization (not pixel-level)
  
  # --------------------------------------------------------------------------
  # Topic 5: Local Explanations (LIME)
  # --------------------------------------------------------------------------
  
  lime:
    
    core_idea:
      
      black_box_approximation: |
        LIME: Local Interpretable Model-agnostic Explanations
        
        Idea: Approximate complex model locally with simple model
        
        Complex model (global): Neural network
        Simple model (local): Linear model
        
        "Near this prediction, model behaves like linear model"
      
      algorithm: |
        To explain prediction f(x):
        
        1. Generate perturbed samples around x
           x_1, x_2, ..., x_m (by randomly masking features)
        
        2. Get predictions from black-box model
           y_i = f(x_i)
        
        3. Weight samples by proximity to x
           w_i = exp(-d(x, x_i)²/σ²)
        
        4. Fit weighted linear model
           g(x) = w_0 + w_1·x_1 + ... + w_D·x_D
        
        5. Linear coefficients w_i = feature importances
    
    implementation:
      
      lime_for_tabular: |
        from lime.lime_tabular import LimeTabularExplainer
        
        # Create explainer
        explainer = LimeTabularExplainer(
            X_train,
            feature_names=feature_names,
            class_names=class_names,
            mode='classification'
        )
        
        # Explain single prediction
        explanation = explainer.explain_instance(
            X_test[0],
            model.predict_proba,
            num_features=10
        )
        
        # Visualize
        explanation.show_in_notebook()
      
      lime_for_images: |
        from lime.lime_image import LimeImageExplainer
        
        explainer = LimeImageExplainer()
        
        # Explain image prediction
        explanation = explainer.explain_instance(
            image,
            model.predict_proba,
            top_labels=1,
            num_samples=1000
        )
        
        # Get image with important superpixels highlighted
        temp, mask = explanation.get_image_and_mask(
            label=predicted_class,
            positive_only=True,
            num_features=10,
            hide_rest=False
        )
    
    pros_and_cons:
      
      pros: |
        - Model-agnostic (works for any model)
        - Intuitive (linear approximation)
        - Local fidelity (accurate near explained instance)
      
      cons: |
        - Slow (requires many model queries)
        - Instability (different runs give different explanations)
        - Only local (doesn't explain global behavior)
        - Choice of kernel and neighborhood matters
  
  # --------------------------------------------------------------------------
  # Topic 6: Attention Visualization
  # --------------------------------------------------------------------------
  
  attention_visualization:
    
    transformer_attention:
      
      what_attention_shows: |
        Attention weights show which tokens model focuses on:
        
        Input: "The cat sat on the mat"
        Predicting: "sat"
        
        Attention weights:
        The: 0.05
        cat: 0.65 ← High attention!
        sat: 0.20
        on: 0.05
        the: 0.03
        mat: 0.02
        
        Model focuses on "cat" (subject) to predict verb
      
      implementation: |
        def visualize_attention(tokens, attention_weights):
            """
            Visualize attention weights as heatmap.
            
            Parameters:
            - tokens: list of tokens
            - attention_weights: (seq_len, seq_len) attention matrix
            """
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            plt.figure(figsize=(10, 8))
            sns.heatmap(
                attention_weights,
                xticklabels=tokens,
                yticklabels=tokens,
                cmap='Blues',
                annot=True,
                fmt='.2f'
            )
            plt.xlabel('Key')
            plt.ylabel('Query')
            plt.title('Attention Weights')
            plt.tight_layout()
            plt.show()
      
      interpretation: |
        High attention: Model considers this token important
        Low attention: Model ignores this token
        
        Patterns reveal:
        - Syntactic structure (subject-verb-object)
        - Long-range dependencies
        - Semantic relationships
    
    multi_head_attention:
      
      different_heads_different_patterns: |
        Each attention head learns different pattern:
        
        Head 1: Syntactic (subject → verb)
        Head 2: Positional (adjacent words)
        Head 3: Semantic (similar meaning)
        
        Averaging loses information, visualize separately
  
  # --------------------------------------------------------------------------
  # Topic 7: Limitations and Pitfalls
  # --------------------------------------------------------------------------
  
  limitations:
    
    explanations_can_mislead:
      
      adversarial_explanations: |
        Attacker can manipulate explanations:
        
        Model prediction: Malicious (correct)
        Explanation: Highlights benign features (misleading!)
        
        User sees explanation, trusts wrong reasons
      
      cherry_picking: |
        Show only favorable explanations:
        
        Select: Examples where explanation makes sense
        Hide: Examples where explanation nonsensical
        
        Creates false sense of interpretability
    
    correlation_not_causation:
      
      observation: |
        Feature importance shows correlation, not causation:
        
        Model: High importance on "zip_code"
        Interpretation: "Living in this zip code causes loan approval"
        Reality: Zip code correlates with income (confounding)
        
        Cannot conclude causality from importance
    
    local_vs_global_mismatch:
      
      problem: |
        Local explanation may not reflect global behavior:
        
        Local (this prediction): Income most important
        Global (overall): Actually using race as proxy
        
        Local explanation hides unfair global pattern
    
    explanation_instability:
      
      observation: |
        Small changes → big explanation changes:
        
        x: "Income most important"
        x + tiny noise: "Age most important"
        
        Predictions similar, explanations completely different
        
        Unstable explanations not trustworthy
  
  # --------------------------------------------------------------------------
  # Topic 8: Security Applications
  # --------------------------------------------------------------------------
  
  security_applications:
    
    backdoor_detection:
      
      observation: |
        Backdoored models have unusual feature importance:
        
        Clean model: Semantic features important
        Backdoored model: Trigger pattern most important
        
        Interpretability reveals anomaly
      
      detection_method: |
        1. Compute feature importance on clean data
        2. Compute feature importance on suspicious data
        3. Compare distributions
        4. Large difference → potential backdoor
      
      example: |
        Image classifier with backdoor:
        
        Clean images: Object features important
        Images with trigger: Corner pixels most important (unusual!)
        
        SHAP/saliency map reveals backdoor trigger
    
    debugging_adversarial_vulnerability:
      
      observation: |
        Saliency maps show where model vulnerable:
        
        Adversarial perturbation targets high-gradient regions
        
        High saliency → high vulnerability to perturbation
      
      mitigation: |
        Smooth saliency maps (lower gradients):
        - Adversarial training
        - Gradient regularization
        - Input transformations
    
    detecting_data_leakage:
      
      observation: |
        Feature importance reveals leaked information:
        
        Model: Predicting stock price
        Top feature: "date_created" (metadata)
        
        Data leakage: Model using information not available at prediction time
      
      action: |
        Check feature importance before deployment:
        - Suspiciously high importance on metadata → investigate
        - Remove leaked features, retrain
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Interpretability = understanding why model predicts, required for trust, debugging, compliance"
      - "Global vs local: global explains overall behavior, local explains single prediction"
      - "Permutation importance = accuracy drop when feature shuffled, model-agnostic, intuitive"
      - "Saliency maps = gradients show which pixels matter, noisy but fast"
      - "LIME approximates locally with linear model, model-agnostic, unstable"
      - "Explanations can mislead: adversarial explanations, correlation ≠ causation, instability"
    
    actionable_steps:
      - "Start with permutation importance: simple, intuitive, works for any model, good first step"
      - "Use SHAP for individual predictions: theoretically grounded, consistent, good for stakeholders"
      - "Visualize saliency maps for images: quick check of what model sees, debug spurious correlations"
      - "Check explanation stability: small input changes should give similar explanations"
      - "Don't trust blindly: explanations can be manipulated, verify with domain knowledge"
      - "Use for debugging: find bugs (watermark learning), data leakage, backdoors via interpretability"
    
    security_principles:
      - "Interpretability detects backdoors: trigger patterns show unusual feature importance"
      - "Attention weights leak information: show which tokens model focuses on, privacy risk"
      - "Explanations can be adversarial: attacker manipulates to show benign features while being malicious"
      - "Saliency maps reveal vulnerabilities: high-gradient regions targeted by adversarial attacks"
    
    practical_checklist:
      - "Regulatory compliance: GDPR, fair lending require explanations, SHAP or LIME sufficient"
      - "Debugging: permutation importance first, then SHAP, then saliency for images"
      - "Stakeholder communication: visualize top 5 features with SHAP, show concrete examples"
      - "Fairness audit: check if protected attributes have high importance, remove if yes"
      - "Production monitoring: track feature importance over time, alert if suddenly changes"

---
