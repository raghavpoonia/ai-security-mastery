# section_04_14_scaling_load_balancing_distributed.yaml

---
document_info:
  title: "Horizontal Scaling, Load Balancing, and Distributed Deployment"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 4
  section: 14
  part: 3
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-28"
  version: "1.0"
  description: |
    Complete guide to horizontally scaling LLM deployments across multiple GPUs and nodes.
    Covers horizontal scaling strategies, load balancing algorithms (round-robin, least
    connections, weighted), distributed serving architectures (model parallelism, pipeline
    parallelism, data parallelism), auto-scaling policies, and multi-region deployment.
    Implements production-ready load balancers, distributed serving with Ray, health checks,
    and failover mechanisms. Security analysis covering distributed system attacks, node
    compromise, network interception, and cascade failures. Essential for scaling LLM
    systems to handle production traffic globally.
  estimated_pages: 7
  tags:
    - horizontal-scaling
    - load-balancing
    - distributed-deployment
    - model-parallelism
    - auto-scaling
    - failover
    - multi-region
    - production-scaling

section_overview:
  title: "Horizontal Scaling, Load Balancing, and Distributed Deployment"
  number: "4.14"
  
  purpose: |
    Section 4.13 compressed models 2-4x through quantization, enabling deployment on smaller
    hardware. But a single GPU, even with compression, has throughput limits: one A100 serves
    perhaps 50-100 req/sec for a 7B model. Production systems need to handle thousands of
    requests per second, serve users globally, and maintain availability during failures.
    
    Horizontal scaling solves this by adding more instances rather than bigger instances.
    Instead of one giant server, deploy 10-100 smaller ones behind a load balancer. This
    provides linear throughput scaling, geographic distribution, and fault tolerance. But
    it introduces complexity: load balancing algorithms, health checks, session affinity,
    distributed coordination.
    
    This section builds complete distributed deployment systems: load balancers, distributed
    serving architectures, auto-scaling, and multi-region patterns. Understanding horizontal
    scaling is critical—it's the only path from prototype to global-scale production.
  
  learning_objectives:
    conceptual:
      - "Understand horizontal scaling vs vertical scaling trade-offs"
      - "Grasp load balancing algorithms and their characteristics"
      - "Comprehend distributed serving: model parallelism, pipeline parallelism, data parallelism"
      - "Understand auto-scaling policies and triggers"
    
    practical:
      - "Implement load balancers with Nginx, HAProxy for LLM serving"
      - "Deploy distributed serving with Ray Serve for multi-GPU/multi-node"
      - "Configure health checks, failover, and session affinity"
      - "Build auto-scaling with Kubernetes HPA and custom metrics"
    
    security_focused:
      - "Prevent node compromise from cascading to entire cluster"
      - "Secure inter-node communication with TLS and authentication"
      - "Implement distributed rate limiting and DDoS protection"
      - "Detect and prevent coordinated attacks across nodes"
  
  prerequisites:
    knowledge:
      - "Section 4.12: Model serving and inference optimization"
      - "Section 4.13: Quantization and model compression"
      - "Understanding of distributed systems concepts"
      - "Basic networking knowledge (HTTP, TCP, DNS)"
    
    skills:
      - "Working with Kubernetes and container orchestration"
      - "Configuring load balancers (Nginx, HAProxy)"
      - "Understanding of distributed computing patterns"
      - "Monitoring and observability tools"
  
  key_transitions:
    from_section_4_13: |
      Section 4.13 compressed models through quantization, reducing memory 2-4x and enabling
      deployment on smaller GPUs. This makes individual instances more efficient. But single
      instances have throughput limits and no fault tolerance.
      
      Section 4.14 scales horizontally by deploying many instances behind load balancers.
      This provides linear throughput scaling (10 instances = 10x throughput), geographic
      distribution, and fault tolerance. Combined with 4.13's compression, this enables
      serving large models globally at scale.
    
    to_next_section: |
      Section 4.14 covers horizontal scaling across nodes. Section 4.15 advances to cost
      optimization through caching strategies: prompt caching, KV-cache sharing, and semantic
      caching that dramatically reduce redundant computation and costs.

topics:
  - topic_number: 1
    title: "Horizontal Scaling Strategies and Load Balancing"
    
    overview: |
      Horizontal scaling adds more instances rather than making instances bigger. This
      provides linear throughput scaling, fault tolerance, and geographic distribution.
      Load balancers distribute requests across instances, monitor health, and handle
      failures automatically.
      
      Load balancing algorithms range from simple (round-robin) to sophisticated (least
      response time, queue depth-based). The right algorithm depends on workload: uniform
      requests favor round-robin, variable requests need least connections, stateful
      applications require session affinity.
      
      We implement complete load balancing systems, understand algorithm trade-offs, configure
      health checks and failover, and build auto-scaling policies. Understanding these
      patterns enables reliable, scalable production deployments.
    
    content:
      horizontal_vs_vertical_scaling:
        scaling_comparison: |
          Horizontal vs Vertical Scaling:
          
          **Vertical Scaling** (scale up):
          - Add more resources to single instance (bigger GPU, more memory)
          - Simpler to manage (one instance)
          - Limited by hardware (can't get infinite GPU)
          - No fault tolerance (single point of failure)
          - Eventually hits diminishing returns
          
          Example:
```
          1x A100 40GB → 1x A100 80GB → 1x H100 80GB
```
          
          **Horizontal Scaling** (scale out):
          - Add more instances at same size
          - Linear throughput scaling (2x instances = 2x throughput)
          - Fault tolerant (one fails, others continue)
          - Geographic distribution (instances in multiple regions)
          - Infinite scaling potential
          
          Example:
```
          1x A100 40GB → 4x A100 40GB → 10x A100 40GB
```
          
          **LLM considerations**:
          - Model size limits vertical scaling (70B model needs multiple GPUs regardless)
          - Horizontal scaling more cost-effective (commodity hardware)
          - Load balancing adds complexity but necessary for scale
          - Hybrid approach common: Model parallelism (vertical) + replicas (horizontal)
        
        when_to_scale_horizontally: |
          When to scale horizontally:
          
          **Trigger 1: Throughput limits**
```
          Single instance: 50 req/sec
          Required: 500 req/sec
          Solution: 10 instances
```
          
          **Trigger 2: Latency SLA violations**
```
          P95 latency: 3 sec (target: 1 sec)
          Cause: Queue depth too high
          Solution: Add instances to distribute load
```
          
          **Trigger 3: Geographic distribution**
```
          Users in: US, EU, APAC
          Latency US→EU: 100ms
          Solution: Instances in each region
```
          
          **Trigger 4: Fault tolerance requirements**
```
          SLA: 99.9% uptime
          Single instance: 99% uptime (too low)
          Solution: 3+ instances with failover
```
          
          **Metrics to monitor**:
          - Request queue depth (> 50 → scale up)
          - GPU utilization (> 80% sustained → scale up)
          - P95 latency (exceeds SLA → scale up)
          - Error rate (> 1% → investigate, maybe scale)
      
      load_balancing_algorithms:
        round_robin: |
          Round-robin: Distribute sequentially
          
          **Algorithm**:
```python
          class RoundRobinLoadBalancer:
              def __init__(self, instances: list):
                  self.instances = instances
                  self.current_index = 0
              
              def select_instance(self):
                  """Select next instance in round-robin."""
                  instance = self.instances[self.current_index]
                  self.current_index = (self.current_index + 1) % len(self.instances)
                  return instance
```
          
          **Characteristics**:
          - Simplest algorithm
          - Fair distribution if requests uniform
          - No consideration of instance load
          - Good for: Homogeneous instances, uniform requests
          
          **Problems**:
          - Slow instances get same load as fast ones
          - Variable request sizes cause imbalance
          - No awareness of queue depth
        
        least_connections: |
          Least connections: Send to instance with fewest active connections
          
          **Algorithm**:
```python
          class LeastConnectionsLoadBalancer:
              def __init__(self, instances: list):
                  self.instances = instances
                  self.connections = {inst: 0 for inst in instances}
              
              def select_instance(self):
                  """Select instance with fewest connections."""
                  instance = min(self.connections, key=self.connections.get)
                  return instance
              
              def increment_connections(self, instance):
                  """Track new connection."""
                  self.connections[instance] += 1
              
              def decrement_connections(self, instance):
                  """Track completed connection."""
                  self.connections[instance] -= 1
```
          
          **Characteristics**:
          - Adapts to instance load
          - Better for variable request sizes
          - Requires connection tracking
          - Good for: Variable workloads, heterogeneous instances
          
          **Benefits**:
          - Slow requests don't block fast ones
          - Distributes load more evenly
        
        weighted_round_robin: |
          Weighted round-robin: Distribute based on instance capacity
          
          **Algorithm**:
```python
          class WeightedRoundRobinLoadBalancer:
              def __init__(self, instances: list, weights: dict):
                  """
                  Initialize with weights.
                  
                  Args:
                      instances: List of instances
                      weights: Dict mapping instance → weight
                  """
                  self.instances = instances
                  self.weights = weights
                  self.current_weights = weights.copy()
              
              def select_instance(self):
                  """Select instance using weighted round-robin."""
                  # Find instance with highest current weight
                  instance = max(self.current_weights, key=self.current_weights.get)
                  
                  # Reduce current weight
                  self.current_weights[instance] -= 1
                  
                  # Reset if all zero
                  if all(w <= 0 for w in self.current_weights.values()):
                      self.current_weights = self.weights.copy()
                  
                  return instance
```
          
          Example:
```python
          instances = ["instance-a100", "instance-a10", "instance-t4"]
          weights = {
              "instance-a100": 5,  # Fastest, gets 5x traffic
              "instance-a10": 3,   # Medium, gets 3x traffic
              "instance-t4": 1     # Slowest, gets 1x traffic
          }
```
          
          **Good for**: Mixed GPU types, heterogeneous instances
        
        least_response_time: |
          Least response time: Send to fastest instance
          
          **Algorithm**:
```python
          from collections import deque
          import time
          
          class LeastResponseTimeLoadBalancer:
              def __init__(self, instances: list, window_size: int = 100):
                  self.instances = instances
                  self.latencies = {inst: deque(maxlen=window_size) for inst in instances}
              
              def select_instance(self):
                  """Select instance with lowest average response time."""
                  avg_latencies = {}
                  
                  for instance in self.instances:
                      if self.latencies[instance]:
                          avg_latencies[instance] = sum(self.latencies[instance]) / len(self.latencies[instance])
                      else:
                          avg_latencies[instance] = 0  # No data yet, prefer this
                  
                  return min(avg_latencies, key=avg_latencies.get)
              
              def record_latency(self, instance: str, latency: float):
                  """Record response latency."""
                  self.latencies[instance].append(latency)
```
          
          **Characteristics**:
          - Adapts to actual performance
          - Detects slow instances automatically
          - Requires latency tracking
          - Good for: Production with monitoring
        
        queue_depth_based: |
          Queue depth-based: Send to instance with shortest queue
          
          **Algorithm**:
```python
          class QueueDepthLoadBalancer:
              def __init__(self, instances: list):
                  self.instances = instances
                  self.queue_depths = {inst: 0 for inst in instances}
              
              def select_instance(self):
                  """Select instance with shortest queue."""
                  return min(self.queue_depths, key=self.queue_depths.get)
              
              def update_queue_depth(self, instance: str, depth: int):
                  """Update queue depth from instance health check."""
                  self.queue_depths[instance] = depth
```
          
          **Best algorithm for LLM serving**:
          - Directly measures actual load
          - Accounts for batch processing
          - Requires instance reporting queue depth
          - Most accurate load distribution
      
      health_checks_and_failover:
        health_check_types: |
          Health check types:
          
          **1. HTTP health endpoint**:
```python
          # Instance health endpoint
          @app.get("/health")
          async def health():
              return {
                  "status": "healthy",
                  "model_loaded": True,
                  "queue_depth": current_queue_depth,
                  "gpu_available": check_gpu_available()
              }
```
          
          **2. Liveness check** (is it alive?):
          - Simple ping/HTTP GET
          - Fast response required (< 1 sec)
          - Failure → Restart instance
          
          **3. Readiness check** (can it serve traffic?):
          - Test actual inference
          - May take longer (< 5 sec)
          - Failure → Remove from load balancer, don't restart
          
          **4. Deep health check** (periodic):
          - Full inference test
          - Check model quality
          - Runs every 5-10 minutes
          - Failure → Alert, investigate
        
        failover_mechanisms: |
          Automatic failover:
          
          **Nginx failover**:
```nginx
          upstream llm_backend {
              server instance-1:8000 max_fails=3 fail_timeout=30s;
              server instance-2:8000 max_fails=3 fail_timeout=30s;
              server instance-3:8000 max_fails=3 fail_timeout=30s backup;
              
              # Health check
              check interval=5s rise=2 fall=3 timeout=2s;
          }
```
          
          - `max_fails=3`: Mark unhealthy after 3 failures
          - `fail_timeout=30s`: Retry after 30 seconds
          - `backup`: Only use if all others fail
          
          **Kubernetes failover** (automatic):
```yaml
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            periodSeconds: 10
            failureThreshold: 3
          
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            periodSeconds: 30
            failureThreshold: 3
```
          
          - Readiness fails → Remove from service endpoints
          - Liveness fails → Restart pod
          - Automatic, no manual intervention
        
        session_affinity: |
          Session affinity (sticky sessions):
          
          **Why needed**: Stateful applications
          - User conversation state in instance memory
          - Cache on specific instance
          - Want same user → same instance
          
          **Nginx session affinity**:
```nginx
          upstream llm_backend {
              ip_hash;  # Hash client IP
              
              server instance-1:8000;
              server instance-2:8000;
              server instance-3:8000;
          }
```
          
          **Cookie-based affinity**:
```nginx
          upstream llm_backend {
              server instance-1:8000;
              server instance-2:8000;
              
              sticky cookie srv_id expires=1h domain=.example.com path=/;
          }
```
          
          **Kubernetes session affinity**:
```yaml
          apiVersion: v1
          kind: Service
          metadata:
            name: llm-service
          spec:
            sessionAffinity: ClientIP
            sessionAffinityConfig:
              clientIP:
                timeoutSeconds: 10800  # 3 hours
```
          
          **Trade-offs**:
          - Better: User experience (maintains state)
          - Worse: Load distribution (some instances overloaded)
          - Solution: Hybrid (session affinity + shared state in Redis)
      
      distributed_coordination:
        service_discovery: |
          Service discovery: How instances find each other
          
          **1. DNS-based** (simple):
```
          llm-service.production.svc.cluster.local
          → [10.0.1.1, 10.0.1.2, 10.0.1.3]
```
          
          **2. Consul/etcd** (advanced):
```python
          import consul
          
          # Register service
          c = consul.Consul()
          c.agent.service.register(
              "llm-service",
              service_id="llm-instance-1",
              address="10.0.1.1",
              port=8000,
              check=consul.Check.http("http://10.0.1.1:8000/health", "10s")
          )
          
          # Discover services
          services = c.health.service("llm-service", passing=True)
```
          
          **3. Kubernetes** (built-in):
```yaml
          apiVersion: v1
          kind: Service
          metadata:
            name: llm-service
          spec:
            selector:
              app: llm
            ports:
            - port: 80
              targetPort: 8000
```
          
          Automatic discovery of pods with label `app: llm`
        
        distributed_rate_limiting: |
          Rate limiting across multiple instances:
          
          **Problem**: Per-instance rate limits don't prevent global abuse
```
          Instance 1: 100 req/min limit
          Instance 2: 100 req/min limit
          Attacker: 200 req/min to different instances → Success!
```
          
          **Solution: Shared rate limit state**
```python
          import redis
          
          class DistributedRateLimiter:
              def __init__(self, redis_client):
                  self.redis = redis_client
              
              def is_allowed(self, user_id: str, limit: int = 100, 
                           window: int = 60) -> bool:
                  """
                  Check if request allowed using sliding window.
                  
                  Args:
                      user_id: User identifier
                      limit: Max requests per window
                      window: Time window in seconds
                  
                  Returns:
                      True if allowed
                  """
                  key = f"rate_limit:{user_id}"
                  current_time = int(time.time())
                  window_start = current_time - window
                  
                  # Remove old entries
                  self.redis.zremrangebyscore(key, 0, window_start)
                  
                  # Count requests in window
                  count = self.redis.zcard(key)
                  
                  if count >= limit:
                      return False
                  
                  # Add current request
                  self.redis.zadd(key, {str(current_time): current_time})
                  self.redis.expire(key, window)
                  
                  return True
```
          
          **Benefits**:
          - Consistent limits across all instances
          - No per-instance bypass
          - Scales to many instances
    
    implementation:
      nginx_load_balancer:
        language: nginx
        code: |
          # Complete Nginx load balancer configuration for LLM serving
          
          # Upstream definition
          upstream llm_backend {
              # Load balancing algorithm
              least_conn;  # Use least connections
              
              # Backend instances
              server llm-instance-1:8000 weight=5 max_fails=3 fail_timeout=30s;
              server llm-instance-2:8000 weight=5 max_fails=3 fail_timeout=30s;
              server llm-instance-3:8000 weight=3 max_fails=3 fail_timeout=30s;  # Slower GPU
              
              # Backup instance (only used if all others fail)
              server llm-instance-backup:8000 backup;
              
              # Connection settings
              keepalive 32;  # Keep-alive connections to backends
          }
          
          # Rate limiting zones
          limit_req_zone $binary_remote_addr zone=per_ip:10m rate=10r/s;
          limit_req_zone $http_x_api_key zone=per_key:10m rate=100r/s;
          
          # Connection limiting
          limit_conn_zone $binary_remote_addr zone=conn_per_ip:10m;
          
          server {
              listen 80;
              server_name llm-api.example.com;
              
              # Access logging
              access_log /var/log/nginx/llm_access.log combined;
              error_log /var/log/nginx/llm_error.log warn;
              
              # Client settings
              client_max_body_size 10M;
              client_body_timeout 60s;
              
              # Apply rate limits
              limit_req zone=per_ip burst=20 nodelay;
              limit_req zone=per_key burst=200 nodelay;
              limit_conn conn_per_ip 10;
              
              location / {
                  # Proxy to backend
                  proxy_pass http://llm_backend;
                  
                  # Headers
                  proxy_set_header Host $host;
                  proxy_set_header X-Real-IP $remote_addr;
                  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                  proxy_set_header X-Forwarded-Proto $scheme;
                  
                  # Timeouts
                  proxy_connect_timeout 5s;
                  proxy_send_timeout 60s;
                  proxy_read_timeout 60s;
                  
                  # Buffering (disable for streaming)
                  proxy_buffering off;
                  proxy_request_buffering off;
                  
                  # Retry logic
                  proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
                  proxy_next_upstream_tries 2;
              }
              
              # Health check endpoint (for load balancer itself)
              location /lb-health {
                  access_log off;
                  return 200 "healthy\n";
                  add_header Content-Type text/plain;
              }
              
              # Metrics endpoint
              location /nginx-metrics {
                  stub_status on;
                  access_log off;
                  allow 10.0.0.0/8;  # Internal network only
                  deny all;
              }
          }
          
          # HTTPS configuration
          server {
              listen 443 ssl http2;
              server_name llm-api.example.com;
              
              # SSL certificates
              ssl_certificate /etc/nginx/ssl/cert.pem;
              ssl_certificate_key /etc/nginx/ssl/key.pem;
              
              # SSL settings
              ssl_protocols TLSv1.2 TLSv1.3;
              ssl_ciphers HIGH:!aNULL:!MD5;
              ssl_prefer_server_ciphers on;
              
              # Same configuration as port 80
              include /etc/nginx/conf.d/llm_backend.conf;
          }
    
    security_implications:
      load_balancer_bypass: |
        **Vulnerability**: Attackers bypass load balancer to access backend instances
        directly, avoiding rate limits, authentication, and monitoring.
        
        **Attack scenario**: Attacker discovers backend instance IPs/URLs
        - Scans internal network or uses DNS enumeration
        - Connects directly to backend instances
        - Bypasses load balancer's rate limiting
        - Avoids centralized authentication
        - Evades monitoring and logging
        
        **Defense**:
        1. Network isolation: Backend instances on private network
        2. Firewall rules: Only accept connections from load balancer
        3. Instance authentication: Backends verify requests from LB
        4. Request signing: LB signs requests with shared secret
        5. Security groups: Cloud provider network controls
        6. Monitoring: Alert on direct instance access attempts
        7. Zero trust: Don't rely solely on network isolation
      
      distributed_denial_of_service: |
        **Vulnerability**: DDoS attacks overwhelm distributed system by targeting multiple
        instances simultaneously or exploiting load balancer itself.
        
        **Attack scenario 1**: Distributed attack on all instances
        - Botnet sends requests to load balancer
        - LB distributes across all instances
        - All instances overwhelmed simultaneously
        - No single instance can handle load
        
        **Attack scenario 2**: Load balancer targeted
        - Attack LB itself (not backends)
        - LB becomes bottleneck
        - Backends healthy but unreachable
        
        **Defense**:
        1. DDoS protection: Cloudflare, AWS Shield, rate limiting
        2. Connection limits: Max connections per IP
        3. Request validation: Drop malformed requests early
        4. Auto-scaling: Add capacity automatically
        5. Geographic distribution: Multi-region deployment
        6. Cache layer: CDN absorbs some traffic
        7. Circuit breakers: Shed load when overwhelmed
      
      session_hijacking_in_distributed_systems: |
        **Vulnerability**: Session hijacking is easier in distributed systems due to
        session state replication, inter-node communication, and shared storage.
        
        **Attack scenario**: Attacker steals session token
        - Token valid across all instances (shared session store)
        - Can access any instance with stolen token
        - Session affinity makes hijacking more valuable (persistent access)
        - Compromised Redis/session store leaks all sessions
        
        **Defense**:
        1. Secure session tokens: Cryptographically strong, rotate frequently
        2. Bind sessions: IP address, user agent (detect switching)
        3. Encrypt session data: In Redis/storage
        4. Token expiration: Short-lived tokens (15-60 min)
        5. Audit logging: Track session access across instances
        6. Anomaly detection: Unusual session patterns (geographic jumps)
        7. TLS everywhere: Encrypt token transmission

  - topic_number: 2
    title: "Distributed Serving Architectures and Auto-Scaling"
    
    overview: |
      Beyond simple replication, distributed serving includes model parallelism (split model
      across GPUs), pipeline parallelism (split model into stages), and sophisticated
      auto-scaling policies. These patterns enable serving models too large for single GPUs
      and adapting capacity dynamically to traffic.
      
      Auto-scaling is critical for cost optimization: scale up during peak hours, scale down
      during off-hours. But LLM serving has unique challenges: slow cold starts (model
      loading takes minutes), expensive instances (GPUs), and stateful sessions complicate
      scaling down.
      
      We implement distributed serving with Ray, configure auto-scaling with Kubernetes HPA,
      and build custom metrics-based scaling policies. Understanding these patterns enables
      efficient, cost-effective production deployments.
    
    content:
      distributed_serving_patterns:
        data_parallelism: |
          Data parallelism: Replicate model across GPUs
          
          **Pattern**: Each GPU has full model copy, processes different requests
```
          Request 1 → GPU 1 [Full Model]
          Request 2 → GPU 2 [Full Model]
          Request 3 → GPU 3 [Full Model]
```
          
          **Characteristics**:
          - Simplest pattern
          - Linear throughput scaling
          - Each GPU operates independently
          - No inter-GPU communication during inference
          
          **Use case**: Most common for LLM serving
          - Model fits on single GPU
          - Need higher throughput
          - Standard with load balancers
        
        model_parallelism: |
          Model parallelism: Split model across GPUs
          
          **Tensor parallelism**: Split weight matrices
```
          Layer weights: [4096, 4096]
          GPU 1: [4096, 2048]  # First half of columns
          GPU 2: [4096, 2048]  # Second half of columns
          
          Forward pass:
          1. Both GPUs compute partial: x @ W_half
          2. All-reduce: Combine results
          3. Both GPUs have full output
```
          
          **Characteristics**:
          - Enables models larger than single GPU
          - All GPUs work on same request
          - Inter-GPU communication every layer
          - Lower throughput per request
          
          **Use case**: Very large models (70B+)
          - Model doesn't fit on single GPU
          - High-end interconnect (NVLink, InfiniBand)
        
        pipeline_parallelism: |
          Pipeline parallelism: Split model into stages
```
          GPU 1: Layers 1-8
          GPU 2: Layers 9-16
          GPU 3: Layers 17-24
          GPU 4: Layers 25-32
          
          Request flow:
          Request A: GPU1 → GPU2 → GPU3 → GPU4
          Request B:         GPU1 → GPU2 → GPU3 → GPU4
          Request C:                 GPU1 → GPU2 → GPU3
```
          
          **Characteristics**:
          - Pipelined execution (multiple requests in flight)
          - Each GPU stores only its layers
          - Higher throughput than tensor parallelism
          - Bubble problem (GPUs idle during fill/drain)
          
          **Use case**: Large models with throughput requirements
          - Model too large for single GPU
          - Need better throughput than tensor parallelism
        
        hybrid_parallelism: |
          Hybrid: Combine multiple patterns
          
          **Example: Pipeline + Data parallelism**
```
          Pipeline 1 (Replica 1):
            GPU 1: Layers 1-16
            GPU 2: Layers 17-32
          
          Pipeline 2 (Replica 2):
            GPU 3: Layers 1-16
            GPU 4: Layers 17-32
```
          
          - Pipeline parallelism: Split model
          - Data parallelism: Replicate pipeline
          - Best throughput for large models
          
          **Example: Tensor + Pipeline**
```
          Stage 1 (Layers 1-16):
            GPU 1, GPU 2: Tensor parallel
          Stage 2 (Layers 17-32):
            GPU 3, GPU 4: Tensor parallel
```
          
          - Tensor parallelism within stage
          - Pipeline between stages
          - High bandwidth within stage, lower between
      
      ray_serve_deployment:
        ray_serve_architecture: |
          Ray Serve: Production ML serving framework
          
          **Features**:
          - Multi-model serving
          - Batching and queueing
          - Autoscaling
          - A/B testing
          - Framework agnostic
          
          **Basic deployment**:
```python
          from ray import serve
          from transformers import AutoModelForCausalLM, AutoTokenizer
          
          @serve.deployment(
              num_replicas=3,
              ray_actor_options={"num_gpus": 1}
          )
          class LLMDeployment:
              def __init__(self, model_name: str):
                  self.model = AutoModelForCausalLM.from_pretrained(
                      model_name,
                      device_map="auto"
                  )
                  self.tokenizer = AutoTokenizer.from_pretrained(model_name)
              
              async def __call__(self, request):
                  prompt = request.query_params["prompt"]
                  
                  inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
                  outputs = self.model.generate(**inputs, max_length=100)
                  text = self.tokenizer.decode(outputs[0])
                  
                  return {"generated_text": text}
          
          # Deploy
          serve.run(LLMDeployment.bind("meta-llama/Llama-2-7b-hf"))
```
        
        ray_serve_with_batching: |
          Ray Serve with dynamic batching:
```python
          from ray import serve
          from ray.serve.handle import DeploymentHandle
          import asyncio
          
          @serve.deployment(
              num_replicas=2,
              max_concurrent_queries=100,
              ray_actor_options={"num_gpus": 1}
          )
          class BatchedLLM:
              def __init__(self, model_name: str, max_batch_size: int = 8):
                  self.model = load_model(model_name)
                  self.max_batch_size = max_batch_size
                  self.batch = []
                  self.batch_event = asyncio.Event()
              
              @serve.batch(max_batch_size=8, batch_wait_timeout_s=0.01)
              async def generate_batch(self, prompts: list) -> list:
                  """Generate for batch of prompts."""
                  # Tokenize batch
                  inputs = self.tokenizer(
                      prompts,
                      return_tensors="pt",
                      padding=True
                  ).to("cuda")
                  
                  # Generate
                  outputs = self.model.generate(**inputs, max_length=100)
                  
                  # Decode
                  texts = [
                      self.tokenizer.decode(output)
                      for output in outputs
                  ]
                  
                  return texts
              
              async def __call__(self, request):
                  prompt = request.query_params["prompt"]
                  result = await self.generate_batch(prompt)
                  return {"generated_text": result}
```
          
          Ray automatically batches requests!
        
        ray_serve_autoscaling: |
          Ray Serve autoscaling:
```python
          @serve.deployment(
              autoscaling_config={
                  "min_replicas": 1,
                  "max_replicas": 10,
                  "target_num_ongoing_requests_per_replica": 5,
                  # Scale up if queue > 5 per replica
              },
              ray_actor_options={"num_gpus": 1}
          )
          class AutoscaledLLM:
              def __init__(self, model_name: str):
                  self.model = load_model(model_name)
              
              async def __call__(self, request):
                  # ... inference logic ...
                  pass
```
          
          **Autoscaling logic**:
          - Monitors queue depth per replica
          - Scales up if queue > target
          - Scales down if queue < target
          - Respects min/max replicas
      
      auto_scaling_policies:
        kubernetes_hpa: |
          Kubernetes Horizontal Pod Autoscaler:
```yaml
          apiVersion: autoscaling/v2
          kind: HorizontalPodAutoscaler
          metadata:
            name: llm-hpa
          spec:
            scaleTargetRef:
              apiVersion: apps/v1
              kind: Deployment
              name: llm-deployment
            minReplicas: 2
            maxReplicas: 20
            
            metrics:
            # Scale based on CPU
            - type: Resource
              resource:
                name: cpu
                target:
                  type: Utilization
                  averageUtilization: 70
            
            # Scale based on memory
            - type: Resource
              resource:
                name: memory
                target:
                  type: Utilization
                  averageUtilization: 80
            
            # Scale based on custom metric (queue depth)
            - type: Pods
              pods:
                metric:
                  name: request_queue_depth
                target:
                  type: AverageValue
                  averageValue: "50"
            
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 0
                policies:
                - type: Percent
                  value: 100  # Double capacity
                  periodSeconds: 60
                - type: Pods
                  value: 2  # Add at least 2 pods
                  periodSeconds: 60
                selectPolicy: Max
              
              scaleDown:
                stabilizationWindowSeconds: 300  # 5 min cooldown
                policies:
                - type: Percent
                  value: 50  # Halve capacity
                  periodSeconds: 60
                selectPolicy: Min
```
        
        custom_metrics_autoscaling: |
          Custom metrics for LLM autoscaling:
          
          **Metric 1: Queue depth** (best for LLMs)
```python
          from prometheus_client import Gauge
          
          queue_depth = Gauge('request_queue_depth', 'Requests in queue')
          
          # Update metric
          queue_depth.set(len(request_queue))
```
          
          **Metric 2: Average latency**
```python
          from prometheus_client import Histogram
          
          latency = Histogram('request_latency_seconds', 'Request latency')
          
          # Record
          with latency.time():
              result = process_request()
```
          
          **Metric 3: GPU utilization**
```python
          import pynvml
          
          pynvml.nvmlInit()
          handle = pynvml.nvmlDeviceGetHandleByIndex(0)
          
          utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
          gpu_util_gauge.set(utilization.gpu)
```
          
          **Scaling decision**:
```
          IF queue_depth > 50 AND gpu_util > 80%:
              Scale up (instances overloaded)
          
          IF queue_depth < 10 AND gpu_util < 30%:
              Scale down (instances underutilized)
```
        
        predictive_autoscaling: |
          Predictive autoscaling: Scale before load arrives
          
          **Concept**: Learn traffic patterns, scale proactively
```python
          import pandas as pd
          from sklearn.ensemble import RandomForestRegressor
          
          class PredictiveScaler:
              def __init__(self):
                  self.model = RandomForestRegressor()
                  self.history = []
              
              def record_metrics(self, timestamp, requests_per_minute):
                  """Record historical metrics."""
                  self.history.append({
                      'timestamp': timestamp,
                      'requests_per_minute': requests_per_minute,
                      'hour': timestamp.hour,
                      'day_of_week': timestamp.weekday()
                  })
              
              def train(self):
                  """Train predictive model."""
                  df = pd.DataFrame(self.history)
                  X = df[['hour', 'day_of_week']]
                  y = df['requests_per_minute']
                  self.model.fit(X, y)
              
              def predict_load(self, future_time):
                  """Predict future load."""
                  features = [[future_time.hour, future_time.weekday()]]
                  predicted_rpm = self.model.predict(features)[0]
                  return predicted_rpm
              
              def recommend_replicas(self, future_time, capacity_per_replica=50):
                  """Recommend replica count."""
                  predicted_load = self.predict_load(future_time)
                  replicas = int(predicted_load / capacity_per_replica) + 1
                  return max(2, min(20, replicas))  # Clamp to [2, 20]
```
          
          **Benefits**:
          - Avoid reactive scaling lag
          - Better user experience (pre-scaled)
          - Lower costs (scale down before load drops)
      
      multi_region_deployment:
        geographic_distribution: |
          Multi-region deployment for global users:
          
          **Architecture**:
```
          User in US → US-East region
          User in EU → EU-West region
          User in APAC → Asia-Pacific region
          
          Each region:
          - Load balancer
          - 3+ instances
          - Shared database (global)
```
          
          **DNS routing** (GeoDNS):
```
          llm-api.example.com
          
          US users → us-east.llm-api.example.com (3.120.45.67)
          EU users → eu-west.llm-api.example.com (52.29.123.45)
          APAC users → ap-southeast.llm-api.example.com (54.169.78.90)
```
          
          **Benefits**:
          - Lower latency (users hit nearby region)
          - Higher availability (region failures isolated)
          - Regulatory compliance (data residency)
        
        cross_region_failover: |
          Cross-region failover:
          
          **Health check based**:
```
          Primary: US-East (healthy)
          Secondary: EU-West (standby)
          
          If US-East unhealthy:
          → Route US traffic to EU-West
```
          
          **DNS failover** (Route 53):
```json
          {
            "Name": "llm-api.example.com",
            "Type": "A",
            "SetIdentifier": "Primary",
            "Failover": "PRIMARY",
            "HealthCheckId": "abc123",
            "ResourceRecords": [{"Value": "3.120.45.67"}]
          },
          {
            "Name": "llm-api.example.com",
            "Type": "A",
            "SetIdentifier": "Secondary",
            "Failover": "SECONDARY",
            "ResourceRecords": [{"Value": "52.29.123.45"}]
          }
```
          
          **Considerations**:
          - DNS TTL (lower = faster failover, higher load)
          - Data consistency across regions
          - Cost (run instances in multiple regions)
    
    implementation:
      kubernetes_multi_region_deployment:
        language: yaml
        code: |
          # Multi-region Kubernetes deployment for LLM serving
          
          # ============================================================================
          # REGION 1: US-EAST
          # ============================================================================
          
          ---
          apiVersion: v1
          kind: Namespace
          metadata:
            name: llm-us-east
          
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: llm-deployment
            namespace: llm-us-east
            labels:
              app: llm
              region: us-east
          spec:
            replicas: 5  # More replicas in primary region
            selector:
              matchLabels:
                app: llm
            template:
              metadata:
                labels:
                  app: llm
                  region: us-east
              spec:
                nodeSelector:
                  nvidia.com/gpu.present: "true"
                containers:
                - name: llm-server
                  image: llm-server:v1
                  resources:
                    requests:
                      nvidia.com/gpu: 1
                      memory: "16Gi"
                    limits:
                      nvidia.com/gpu: 1
                      memory: "32Gi"
                  ports:
                  - containerPort: 8000
                  env:
                  - name: REGION
                    value: "us-east"
                  - name: REDIS_URL
                    value: "redis-global.example.com:6379"
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 180
                    periodSeconds: 30
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 60
                    periodSeconds: 10
          
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: llm-service
            namespace: llm-us-east
          spec:
            type: LoadBalancer
            selector:
              app: llm
            ports:
            - port: 80
              targetPort: 8000
          
          ---
          apiVersion: autoscaling/v2
          kind: HorizontalPodAutoscaler
          metadata:
            name: llm-hpa
            namespace: llm-us-east
          spec:
            scaleTargetRef:
              apiVersion: apps/v1
              kind: Deployment
              name: llm-deployment
            minReplicas: 3
            maxReplicas: 20
            metrics:
            - type: Resource
              resource:
                name: memory
                target:
                  type: Utilization
                  averageUtilization: 75
          
          # ============================================================================
          # REGION 2: EU-WEST
          # ============================================================================
          
          ---
          apiVersion: v1
          kind: Namespace
          metadata:
            name: llm-eu-west
          
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: llm-deployment
            namespace: llm-eu-west
            labels:
              app: llm
              region: eu-west
          spec:
            replicas: 3  # Fewer in secondary region
            selector:
              matchLabels:
                app: llm
            template:
              metadata:
                labels:
                  app: llm
                  region: eu-west
              spec:
                nodeSelector:
                  nvidia.com/gpu.present: "true"
                containers:
                - name: llm-server
                  image: llm-server:v1
                  resources:
                    requests:
                      nvidia.com/gpu: 1
                      memory: "16Gi"
                    limits:
                      nvidia.com/gpu: 1
                      memory: "32Gi"
                  ports:
                  - containerPort: 8000
                  env:
                  - name: REGION
                    value: "eu-west"
                  - name: REDIS_URL
                    value: "redis-global.example.com:6379"
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 180
                    periodSeconds: 30
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 60
                    periodSeconds: 10
          
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: llm-service
            namespace: llm-eu-west
          spec:
            type: LoadBalancer
            selector:
              app: llm
            ports:
            - port: 80
              targetPort: 8000
          
          ---
          apiVersion: autoscaling/v2
          kind: HorizontalPodAutoscaler
          metadata:
            name: llm-hpa
            namespace: llm-eu-west
          spec:
            scaleTargetRef:
              apiVersion: apps/v1
              kind: Deployment
              name: llm-deployment
            minReplicas: 2
            maxReplicas: 15
            metrics:
            - type: Resource
              resource:
                name: memory
                target:
                  type: Utilization
                  averageUtilization: 75
          
          # ============================================================================
          # GLOBAL: Shared Redis for session/rate limiting
          # ============================================================================
          
          ---
          apiVersion: v1
          kind: Namespace
          metadata:
            name: llm-global
          
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: redis-global
            namespace: llm-global
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: redis
            template:
              metadata:
                labels:
                  app: redis
              spec:
                containers:
                - name: redis
                  image: redis:7
                  ports:
                  - containerPort: 6379
                  volumeMounts:
                  - name: redis-data
                    mountPath: /data
                volumes:
                - name: redis-data
                  persistentVolumeClaim:
                    claimName: redis-pvc
          
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: redis-global
            namespace: llm-global
          spec:
            type: LoadBalancer
            selector:
              app: redis
            ports:
            - port: 6379
              targetPort: 6379
    
    security_implications:
      node_compromise_propagation: |
        **Vulnerability**: Compromise of single node can propagate to entire cluster through
        shared secrets, lateral movement, or exploiting inter-node trust.
        
        **Attack scenario**: Attacker compromises one instance
        - Gains access to shared secrets (database credentials, API keys)
        - Uses secrets to access other instances
        - Exploits trust between nodes (unauthenticated inter-node communication)
        - Lateral movement to compromise entire cluster
        
        **Defense**:
        1. Secret isolation: Per-node secrets, rotate frequently
        2. Network segmentation: Limit inter-node communication
        3. Zero trust: Authenticate all inter-node communication
        4. Least privilege: Nodes only access what they need
        5. Monitoring: Detect lateral movement attempts
        6. Immutable infrastructure: Compromised node killed, replaced
        7. Incident response: Automated quarantine of suspicious nodes
      
      inter_node_communication_interception: |
        **Vulnerability**: Unencrypted inter-node communication can be intercepted,
        revealing sensitive data or enabling man-in-the-middle attacks.
        
        **Attack scenario**: Attacker on same network
        - Intercepts traffic between load balancer and instances
        - Captures API requests (potentially with sensitive data)
        - Modifies requests in-flight (MITM)
        - Steals session tokens or API keys
        
        **Defense**:
        1. TLS everywhere: Encrypt all inter-node communication
        2. Mutual TLS: Both client and server authenticate
        3. Service mesh: Automated TLS (Istio, Linkerd)
        4. Certificate management: Rotate certificates regularly
        5. Network policies: Restrict communication paths
        6. VPN/encrypted tunnels: For cross-region traffic
        7. Monitoring: Detect unencrypted connections
      
      auto_scaling_abuse: |
        **Vulnerability**: Attackers trigger excessive auto-scaling to exhaust resources
        or incur massive costs.
        
        **Attack scenario**: Attacker sends flood of requests
        - Triggers auto-scaling (adds instances)
        - Keeps sending requests (prevents scale-down)
        - Victim pays for many unnecessary instances
        - Can cost thousands/hour in GPU instances
        
        **Defense**:
        1. Rate limiting: Prevent request floods
        2. Cost caps: Hard limit on max instances
        3. Scale-up delays: Don't scale instantly (verify sustained load)
        4. Authentication: Require valid API keys
        5. Anomaly detection: Flag unusual scaling patterns
        6. Budget alerts: Alert on unexpected costs
        7. Manual approval: Critical scaling requires human approval

key_takeaways:
  critical_concepts:
    - concept: "Horizontal scaling provides linear throughput scaling, fault tolerance, and geographic distribution by adding instances"
      why_it_matters: "Single instance has throughput limits. Horizontal scaling is the only path to production-scale traffic (1000+ req/sec) and global availability."
    
    - concept: "Load balancing algorithms (least connections, queue depth-based) distribute traffic efficiently across instances"
      why_it_matters: "Naive round-robin wastes capacity with variable workloads. Intelligent algorithms (queue depth) adapt to actual load, maximizing utilization."
    
    - concept: "Auto-scaling dynamically adjusts capacity based on load, optimizing costs while meeting performance SLAs"
      why_it_matters: "Static capacity wastes money (over-provisioned) or loses users (under-provisioned). Auto-scaling optimizes both cost and performance."
    
    - concept: "Multi-region deployment reduces latency for global users and provides disaster recovery capabilities"
      why_it_matters: "Single region has high latency for distant users (US→EU: 100ms+). Multi-region serves users from nearby locations, reducing latency 50-80%."
  
  actionable_steps:
    - step: "Deploy Nginx/HAProxy load balancer with least connections algorithm and health checks for automatic failover"
      verification: "Kill one instance. LB should detect unhealthy state within 30s, stop routing traffic. Throughput maintained."
    
    - step: "Configure Kubernetes HPA with queue depth metric for accurate auto-scaling based on actual load"
      verification: "Load test with increasing traffic. HPA should scale up when queue depth exceeds threshold, scale down when traffic drops."
    
    - step: "Implement distributed rate limiting with Redis to prevent per-instance bypass"
      verification: "Attempt to exceed rate limit by hitting different instances. Should be blocked (shared state prevents bypass)."
    
    - step: "Deploy in 2-3 geographic regions with DNS-based routing to reduce latency for global users"
      verification: "Test latency from different regions. Should route to nearest region, reducing latency 50-80% vs single region."
  
  security_principles:
    - principle: "Defense-in-depth: security at load balancer, network, instance, and application layers"
      application: "Rate limiting at LB. Firewall rules for backend isolation. Authentication at instances. Input validation in app. Multiple layers prevent single point of failure."
    
    - principle: "Zero trust for inter-node communication: authenticate and encrypt everything"
      application: "Mutual TLS between all components. Service mesh for automated encryption. Never trust network-level isolation alone."
    
    - principle: "Limit blast radius: node compromise should not compromise entire cluster"
      application: "Per-node secrets. Network segmentation. Least privilege. Immutable infrastructure. Quarantine compromised nodes."
    
    - principle: "Monitor and alert on anomalies: unusual scaling, access patterns, costs"
      application: "Track scaling events, access logs, costs. Alert on spikes. Automated response to obvious attacks. Human investigation for anomalies."
  
  common_mistakes:
    - mistake: "Using round-robin with variable workloads, causing load imbalance and wasted capacity"
      fix: "Use least connections or queue depth-based algorithm. Adapts to actual load, better utilization."
    
    - mistake: "No health checks or failover, causing total outage when instance fails"
      fix: "Implement liveness and readiness probes. Configure automatic failover in LB. Test failover regularly."
    
    - mistake: "Per-instance rate limiting only, allowing attackers to bypass by hitting multiple instances"
      fix: "Implement distributed rate limiting with shared state (Redis). Enforce limits globally, not per-instance."
    
    - mistake: "Backend instances publicly accessible, bypassing load balancer security controls"
      fix: "Network isolation for backends. Firewall rules allowing only LB traffic. Verify requests from LB."
    
    - mistake: "No cost caps on auto-scaling, enabling massive bills from abuse or bugs"
      fix: "Set hard limits on max instances. Budget alerts. Manual approval for large scaling events."
  
  integration_with_book:
    from_section_4_13:
      - "Quantized models (4.13) enable more cost-effective horizontal scaling"
      - "INT4 models use 75% less memory, fit 4x more instances on same hardware"
      - "Combined with scaling (4.14), this provides 4x throughput at same cost"
    
    to_next_section:
      - "Section 4.15: Caching strategies for cost optimization"
      - "Prompt caching, KV-cache sharing, semantic caching"
      - "Reduces redundant computation, complementing scaling for cost efficiency"
  
  looking_ahead:
    next_concepts:
      - "Caching strategies: prompt, KV-cache, semantic caching (4.15)"
      - "Monitoring and observability for distributed systems (4.16)"
      - "API security and compliance at scale (4.17)"
      - "Advanced attacks on distributed systems (4.18)"
    
    skills_to_build:
      - "Configuring and tuning load balancers"
      - "Designing distributed system architectures"
      - "Implementing auto-scaling policies"
      - "Operating multi-region deployments"
  
  final_thoughts: |
    Horizontal scaling is the foundation of production LLM systems. Section 4.14 provides
    the patterns and tools to scale from single-instance demos to globally distributed
    production services handling thousands of requests per second.
    
    Key insights:
    
    1. **Horizontal scaling is non-optional**: Single instances have hard throughput limits
       (50-100 req/sec for 7B model). Production systems need 1000+ req/sec. Only horizontal
       scaling achieves this. Start planning for scale from day one—retrofitting is painful.
    
    2. **Load balancing algorithm matters**: Round-robin wastes 30-50% capacity with variable
       workloads. Queue depth-based algorithms adapt to actual load, maximizing utilization.
       Choose algorithm based on workload characteristics, not defaults.
    
    3. **Auto-scaling is cost optimization**: Static capacity wastes money (over-provisioned
       off-hours) or loses users (under-provisioned peak). Auto-scaling optimizes both. But
       GPU instances have slow cold starts—use predictive scaling for best results.
    
    4. **Multi-region reduces latency dramatically**: Users 5000km away experience 100ms+
       network latency. Multi-region serves from nearby locations, reducing latency 50-80%.
       Critical for global user experience and regulatory compliance.
    
    5. **Distributed security is harder**: Single instance has one attack surface. Distributed
       systems have many: LB, inter-node, shared state. Defense-in-depth essential—security
       at every layer. Zero trust for all communication.
    
    Moving forward, Section 4.15 advances to caching strategies: prompt caching, KV-cache
    sharing, and semantic caching that dramatically reduce redundant computation. Combined
    with horizontal scaling (4.14) and quantization (4.13), this creates cost-effective
    production systems that serve globally at scale.
    
    Remember: Scale early, scale correctly. Don't wait until traffic overwhelms your system.
    Build load balancing, health checks, and auto-scaling from the start. Test failover
    regularly. Monitor everything. Distributed systems fail in complex ways—be prepared.

---
