# section_01_11_evaluation_metrics.yaml

---
document_info:
  chapter: "01"
  section: "11"
  title: "Evaluation Metrics and Confusion Matrix"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoonia"
  license: "MIT"
  created: "2025-12-30"
  estimated_pages: 6
  tags: ["evaluation-metrics", "confusion-matrix", "precision-recall", "f1-score", "roc-auc", "accuracy"]

# ============================================================================
# SECTION 1.11: EVALUATION METRICS AND CONFUSION MATRIX
# ============================================================================

section_01_11_evaluation_metrics:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    You've trained a model, loss decreased, everything looks good. But can you actually 
    deploy it? How well does it detect attacks? How many false alarms will it generate? 
    If you block based on this model, what percentage of legitimate traffic gets blocked?
    
    Evaluation metrics answer these questions. They quantify model performance in ways 
    that matter for your use case. Accuracy says "90% correct", but is that 90% of 
    attacks detected or 90% of benign traffic? Precision says "90% of alerts are real", 
    recall says "90% of attacks are caught". These are very different claims.
    
    The confusion matrix is the foundation - a 2x2 table showing all prediction outcomes. 
    From it, we derive every classification metric: accuracy, precision, recall, F1, 
    specificity, and more. Understanding these metrics is critical because:
    1. Different metrics matter for different security use cases
    2. Accuracy is often misleading (especially with imbalanced data)
    3. You need to communicate model performance to stakeholders
    4. Metrics guide threshold tuning and model selection
    
    This section covers the confusion matrix, all major classification metrics, when to 
    use each, and how to choose optimal operating points for security systems.
  
  why_this_matters: |
    Security context:
    - Accuracy misleading: 99% accuracy detecting 1% attacks = useless (predicts all benign)
    - Precision = false alarm rate (low precision = analyst burnout)
    - Recall = detection rate (low recall = missed attacks)
    - F1 balances both (harmonic mean of precision and recall)
    - ROC-AUC evaluates across all thresholds
    
    Real operational decisions:
    - SOC analyst capacity: 1000 alerts/day max → need high precision
    - Critical system protection: Miss no attacks → need high recall
    - Compliance requirements: 95% detection rate minimum → recall target
    - Cost constraints: $10/alert investigation → precision directly impacts cost
    
    Different security systems optimize different metrics based on constraints.
  
  # --------------------------------------------------------------------------
  # Core Concept 1: The Confusion Matrix
  # --------------------------------------------------------------------------
  
  confusion_matrix:
    
    what_is_confusion_matrix: |
      2x2 table showing all possible prediction outcomes for binary classification
      
      Rows: Actual labels (what truly happened)
      Columns: Predicted labels (what model said)
    
    matrix_structure: |
                          Predicted
                    Negative | Positive
      Actual    -------------------------
      Negative  |   TN     |    FP      |
      Positive  |   FN     |    TP      |
                -------------------------
      
      Where:
      - TN (True Negative): Correctly predicted negative
      - FP (False Positive): Incorrectly predicted positive (Type I error)
      - FN (False Negative): Incorrectly predicted negative (Type II error)
      - TP (True Positive): Correctly predicted positive
    
    security_interpretation: |
      For attack detection:
      
      TN (True Negative): Correctly identified benign traffic
      FP (False Positive): Benign traffic flagged as attack (false alarm)
      FN (False Negative): Attack missed (undetected attack)
      TP (True Positive): Attack correctly detected
      
      Security priorities:
      - Minimize FN (missed attacks = breaches)
      - Minimize FP (false alarms = analyst fatigue)
      - Trade-off: Can't optimize both simultaneously
    
    example_spam_detection: |
      Test set: 1000 emails (100 spam, 900 legitimate)
      Model predictions:
      
                          Predicted
                    Not Spam | Spam
      Actual    ---------------------
      Not Spam  |   850    |   50   |  = 900 legitimate emails
      Spam      |    15    |   85   |  = 100 spam emails
                ---------------------
                   865 pred   135 pred
                   not spam    spam
      
      Interpretation:
      - TN = 850: Correctly identified 850 legitimate emails
      - FP = 50: Falsely flagged 50 legitimate as spam
      - FN = 15: Missed 15 spam emails
      - TP = 85: Correctly detected 85 spam emails
    
    numpy_implementation: |
      import numpy as np
      
      def confusion_matrix(y_true, y_pred):
          """
          Compute confusion matrix
          
          Args:
              y_true: True labels (0 or 1)
              y_pred: Predicted labels (0 or 1)
          
          Returns:
              2x2 numpy array: [[TN, FP], [FN, TP]]
          """
          # True negatives
          tn = np.sum((y_true == 0) & (y_pred == 0))
          
          # False positives
          fp = np.sum((y_true == 0) & (y_pred == 1))
          
          # False negatives
          fn = np.sum((y_true == 1) & (y_pred == 0))
          
          # True positives
          tp = np.sum((y_true == 1) & (y_pred == 1))
          
          return np.array([[tn, fp], [fn, tp]])
      
      # Example
      y_true = np.array([0, 0, 1, 1, 0, 1, 0, 1, 1, 0])
      y_pred = np.array([0, 0, 1, 0, 0, 1, 1, 1, 1, 0])
      
      cm = confusion_matrix(y_true, y_pred)
      print("Confusion Matrix:")
      print(cm)
      # [[4 1]
      #  [1 4]]
      
      tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]
      print(f"TN={tn}, FP={fp}, FN={fn}, TP={tp}")
  
  # --------------------------------------------------------------------------
  # Core Concept 2: Basic Classification Metrics
  # --------------------------------------------------------------------------
  
  basic_metrics:
    
    accuracy:
      
      formula: "Accuracy = (TP + TN) / (TP + TN + FP + FN)"
      
      interpretation: "Fraction of all predictions that were correct"
      
      range: "[0, 1] where 1 = perfect"
      
      example: |
        Confusion matrix: TN=850, FP=50, FN=15, TP=85
        Accuracy = (850 + 85) / (850 + 50 + 15 + 85)
                 = 935 / 1000
                 = 0.935 (93.5%)
      
      pros:
        - "Simple, intuitive"
        - "Good for balanced datasets"
      
      cons:
        - "Misleading for imbalanced data"
        - "Doesn't distinguish types of errors"
      
      imbalanced_data_problem: |
        Dataset: 99% benign, 1% attacks
        Naive classifier: Always predict "benign"
        
        Confusion matrix:
        TN=990, FP=0, FN=10, TP=0
        
        Accuracy = (990 + 0) / 1000 = 99%
        
        Looks great! But detected ZERO attacks.
        This is why accuracy alone is dangerous for security.
      
      numpy_implementation: |
        def accuracy(y_true, y_pred):
            """Compute accuracy"""
            return np.mean(y_true == y_pred)
      
      when_to_use: "Balanced datasets only, never as sole metric for security"
    
    precision:
      
      formula: "Precision = TP / (TP + FP)"
      
      interpretation: "Of all positive predictions, what fraction were actually positive?"
      
      alternative_names: ["Positive Predictive Value (PPV)"]
      
      security_interpretation: |
        "Of all alerts generated, how many were real attacks?"
        
        High precision = Low false alarm rate
        Low precision = Many false alarms = analyst fatigue
      
      example: |
        TP=85, FP=50
        Precision = 85 / (85 + 50) = 85 / 135 = 0.630 (63%)
        
        Meaning: 63% of flagged emails were actually spam
        Or: 37% false alarm rate
      
      when_to_prioritize: |
        - Limited analyst capacity (can't investigate many alerts)
        - High cost per false positive ($100/investigation)
        - User-facing systems (false positives annoy users)
        - Example: Email spam folder (false positives hide real emails)
      
      numpy_implementation: |
        def precision(y_true, y_pred):
            """Compute precision"""
            tp = np.sum((y_true == 1) & (y_pred == 1))
            fp = np.sum((y_true == 0) & (y_pred == 1))
            
            if tp + fp == 0:
                return 0.0  # No positive predictions
            
            return tp / (tp + fp)
    
    recall:
      
      formula: "Recall = TP / (TP + FN)"
      
      interpretation: "Of all actual positives, what fraction did we detect?"
      
      alternative_names: ["Sensitivity", "True Positive Rate (TPR)", "Hit Rate"]
      
      security_interpretation: |
        "Of all attacks that occurred, how many did we detect?"
        
        High recall = Low miss rate = Few attacks slip through
        Low recall = Many missed attacks = security gaps
      
      example: |
        TP=85, FN=15
        Recall = 85 / (85 + 15) = 85 / 100 = 0.85 (85%)
        
        Meaning: Detected 85% of spam emails
        Or: Missed 15% of spam
      
      when_to_prioritize: |
        - Critical systems (can't miss attacks)
        - High cost of false negatives (breach = $1M)
        - Compliance requirements (must detect 95% of incidents)
        - Example: Malware detection on critical infrastructure
      
      numpy_implementation: |
        def recall(y_true, y_pred):
            """Compute recall (sensitivity, TPR)"""
            tp = np.sum((y_true == 1) & (y_pred == 1))
            fn = np.sum((y_true == 1) & (y_pred == 0))
            
            if tp + fn == 0:
                return 0.0  # No actual positives
            
            return tp / (tp + fn)
    
    f1_score:
      
      formula: "F1 = 2 × (Precision × Recall) / (Precision + Recall)"
      
      interpretation: "Harmonic mean of precision and recall"
      
      why_harmonic_mean: |
        Harmonic mean penalizes extreme values
        
        Arithmetic mean: (0.9 + 0.1) / 2 = 0.5
        Harmonic mean: 2 × 0.9 × 0.1 / (0.9 + 0.1) = 0.18
        
        F1 only high if BOTH precision and recall are high
      
      range: "[0, 1] where 1 = perfect"
      
      example: |
        Precision = 0.63, Recall = 0.85
        F1 = 2 × (0.63 × 0.85) / (0.63 + 0.85)
           = 2 × 0.536 / 1.48
           = 0.724 (72.4%)
      
      when_to_use: |
        - Need to balance precision and recall
        - Single metric for model comparison
        - Imbalanced datasets (better than accuracy)
        - Both FP and FN are costly
      
      numpy_implementation: |
        def f1_score(y_true, y_pred):
            """Compute F1 score"""
            prec = precision(y_true, y_pred)
            rec = recall(y_true, y_pred)
            
            if prec + rec == 0:
                return 0.0
            
            return 2 * (prec * rec) / (prec + rec)
      
      variations: |
        F-beta score: Weighted harmonic mean
        
        F_β = (1 + β²) × (Precision × Recall) / (β² × Precision + Recall)
        
        β < 1: Favor precision
        β = 1: Equal weight (F1)
        β > 1: Favor recall
        
        F2 (β=2): Recall weighted 2x more than precision
        F0.5 (β=0.5): Precision weighted 2x more than recall
    
    specificity:
      
      formula: "Specificity = TN / (TN + FP)"
      
      interpretation: "Of all actual negatives, what fraction did we correctly identify?"
      
      alternative_names: ["True Negative Rate (TNR)", "Selectivity"]
      
      complement: "False Positive Rate = 1 - Specificity = FP / (TN + FP)"
      
      example: |
        TN=850, FP=50
        Specificity = 850 / (850 + 50) = 850 / 900 = 0.944 (94.4%)
        FPR = 1 - 0.944 = 0.056 (5.6%)
      
      when_to_use: |
        Medical screening (avoid false positives in healthy population)
        Less common in security (we care more about detecting attacks)
      
      numpy_implementation: |
        def specificity(y_true, y_pred):
            """Compute specificity (TNR)"""
            tn = np.sum((y_true == 0) & (y_pred == 0))
            fp = np.sum((y_true == 0) & (y_pred == 1))
            
            if tn + fp == 0:
                return 0.0
            
            return tn / (tn + fp)
  
  # --------------------------------------------------------------------------
  # Core Concept 3: Advanced Metrics
  # --------------------------------------------------------------------------
  
  advanced_metrics:
    
    roc_curve_and_auc:
      
      what_is_roc: |
        ROC (Receiver Operating Characteristic) curve:
        Plots True Positive Rate (Recall) vs False Positive Rate
        across all possible classification thresholds
        
        X-axis: False Positive Rate = FP / (FP + TN)
        Y-axis: True Positive Rate = TP / (TP + FN) = Recall
      
      why_threshold_sweep: |
        Binary classifier: P(positive) → threshold → class label
        
        Threshold = 0.5 is arbitrary
        Different thresholds → different precision/recall trade-offs
        
        ROC curve shows model performance at ALL thresholds
      
      interpreting_roc: |
        Perfect classifier: Goes to top-left corner (TPR=1, FPR=0)
        Random classifier: Diagonal line (TPR=FPR)
        Better classifier: Curve closer to top-left
      
      auc_score: |
        AUC (Area Under Curve): Single number summarizing ROC
        
        Range: [0, 1]
        - AUC = 1.0: Perfect classifier
        - AUC = 0.9: Excellent
        - AUC = 0.8: Good
        - AUC = 0.7: Fair
        - AUC = 0.5: Random (no discriminative power)
        - AUC < 0.5: Worse than random (predictions inverted)
      
      numpy_implementation: |
        def compute_roc_curve(y_true, y_pred_proba):
            """
            Compute ROC curve points
            
            Args:
                y_true: True binary labels
                y_pred_proba: Predicted probabilities
            
            Returns:
                fpr: False positive rates
                tpr: True positive rates
                thresholds: Threshold values
            """
            # Sort by predicted probability (descending)
            desc_score_indices = np.argsort(y_pred_proba)[::-1]
            y_true_sorted = y_true[desc_score_indices]
            y_pred_sorted = y_pred_proba[desc_score_indices]
            
            # Compute TPR and FPR at each unique threshold
            thresholds = np.unique(y_pred_sorted)
            thresholds = np.concatenate([[1.0], thresholds, [0.0]])
            
            tpr_list = []
            fpr_list = []
            
            for thresh in thresholds:
                y_pred = (y_pred_sorted >= thresh).astype(int)
                
                tp = np.sum((y_true_sorted == 1) & (y_pred == 1))
                fn = np.sum((y_true_sorted == 1) & (y_pred == 0))
                fp = np.sum((y_true_sorted == 0) & (y_pred == 1))
                tn = np.sum((y_true_sorted == 0) & (y_pred == 0))
                
                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
                
                tpr_list.append(tpr)
                fpr_list.append(fpr)
            
            return np.array(fpr_list), np.array(tpr_list), thresholds
        
        def compute_auc(fpr, tpr):
            """Compute AUC using trapezoidal rule"""
            return np.trapz(tpr, fpr)
      
      security_use_case: |
        ROC/AUC useful for:
        - Comparing multiple detection models
        - Evaluating detector independent of threshold
        - Choosing optimal operating point (threshold)
        
        Example: IDS with adjustable sensitivity
        - High sensitivity (low threshold): High recall, high FPR
        - Low sensitivity (high threshold): Low recall, low FPR
        - ROC curve shows all possible operating points
    
    precision_recall_curve:
      
      what_is_pr_curve: |
        Plots Precision vs Recall across all thresholds
        
        Alternative to ROC when:
        - Extreme class imbalance (1% positive)
        - Care more about positive class performance
      
      why_better_than_roc_for_imbalance: |
        ROC uses FPR = FP / (FP + TN)
        With 99% negatives, large TN dominates denominator
        → FPR stays low even with many FPs
        → ROC looks optimistic
        
        PR uses Precision = TP / (TP + FP)
        FPs directly impact precision
        → Shows true performance on rare class
      
      average_precision: |
        AP (Average Precision): Area under PR curve
        
        Range: [0, 1]
        Higher = better
        
        More informative than AUC for imbalanced data
      
      numpy_implementation: |
        def compute_pr_curve(y_true, y_pred_proba):
            """Compute Precision-Recall curve"""
            thresholds = np.unique(y_pred_proba)
            thresholds = np.sort(thresholds)[::-1]
            
            precisions = []
            recalls = []
            
            for thresh in thresholds:
                y_pred = (y_pred_proba >= thresh).astype(int)
                
                prec = precision(y_true, y_pred)
                rec = recall(y_true, y_pred)
                
                precisions.append(prec)
                recalls.append(rec)
            
            return np.array(recalls), np.array(precisions), thresholds
      
      security_recommendation: |
        For security (imbalanced data):
        - Use PR curve + Average Precision
        - ROC/AUC can be misleading
        
        Example: 1% attack rate
        PR curve clearly shows detection vs false alarm trade-off
        ROC curve looks good even with poor precision
  
  # --------------------------------------------------------------------------
  # Practical Implementation: Complete Metrics Suite
  # --------------------------------------------------------------------------
  
  complete_metrics_suite: |
    import numpy as np
    
    class ClassificationMetrics:
        """Complete suite of classification metrics"""
        
        def __init__(self, y_true, y_pred, y_pred_proba=None):
            """
            Initialize with predictions
            
            Args:
                y_true: True labels (0/1)
                y_pred: Predicted labels (0/1)
                y_pred_proba: Predicted probabilities (optional, for ROC/PR)
            """
            self.y_true = np.array(y_true)
            self.y_pred = np.array(y_pred)
            self.y_pred_proba = np.array(y_pred_proba) if y_pred_proba is not None else None
            
            # Compute confusion matrix
            self.tn = np.sum((y_true == 0) & (y_pred == 0))
            self.fp = np.sum((y_true == 0) & (y_pred == 1))
            self.fn = np.sum((y_true == 1) & (y_pred == 0))
            self.tp = np.sum((y_true == 1) & (y_pred == 1))
        
        def confusion_matrix(self):
            """Return confusion matrix"""
            return np.array([[self.tn, self.fp],
                            [self.fn, self.tp]])
        
        def accuracy(self):
            """Compute accuracy"""
            return (self.tp + self.tn) / (self.tp + self.tn + self.fp + self.fn)
        
        def precision(self):
            """Compute precision"""
            if self.tp + self.fp == 0:
                return 0.0
            return self.tp / (self.tp + self.fp)
        
        def recall(self):
            """Compute recall (sensitivity, TPR)"""
            if self.tp + self.fn == 0:
                return 0.0
            return self.tp / (self.tp + self.fn)
        
        def specificity(self):
            """Compute specificity (TNR)"""
            if self.tn + self.fp == 0:
                return 0.0
            return self.tn / (self.tn + self.fp)
        
        def f1_score(self):
            """Compute F1 score"""
            prec = self.precision()
            rec = self.recall()
            
            if prec + rec == 0:
                return 0.0
            
            return 2 * (prec * rec) / (prec + rec)
        
        def false_positive_rate(self):
            """Compute FPR"""
            return 1 - self.specificity()
        
        def false_negative_rate(self):
            """Compute FNR (miss rate)"""
            return 1 - self.recall()
        
        def summary(self):
            """Print complete metrics summary"""
            print("Classification Metrics Summary")
            print("=" * 50)
            print(f"\nConfusion Matrix:")
            print(f"                Predicted")
            print(f"              Neg      Pos")
            print(f"Actual Neg  {self.tn:5d}    {self.fp:5d}")
            print(f"Actual Pos  {self.fn:5d}    {self.tp:5d}")
            
            print(f"\nBasic Metrics:")
            print(f"  Accuracy:    {self.accuracy():.4f}")
            print(f"  Precision:   {self.precision():.4f}")
            print(f"  Recall:      {self.recall():.4f}")
            print(f"  F1 Score:    {self.f1_score():.4f}")
            print(f"  Specificity: {self.specificity():.4f}")
            
            print(f"\nError Rates:")
            print(f"  FPR (False Positive Rate): {self.false_positive_rate():.4f}")
            print(f"  FNR (False Negative Rate): {self.false_negative_rate():.4f}")
    
    # Usage example
    # Generate predictions
    np.random.seed(42)
    y_true = np.random.choice([0, 1], size=1000, p=[0.9, 0.1])  # 10% positive
    y_pred_proba = np.random.rand(1000)
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    # Compute metrics
    metrics = ClassificationMetrics(y_true, y_pred, y_pred_proba)
    metrics.summary()
  
  # --------------------------------------------------------------------------
  # Core Concept 4: Choosing Optimal Threshold
  # --------------------------------------------------------------------------
  
  threshold_optimization:
    
    problem: |
      Default threshold: 0.5
      But 0.5 is arbitrary - why not 0.3 or 0.7?
      
      Different thresholds → different precision/recall trade-offs
      Need systematic way to choose threshold
    
    strategies:
      
      maximize_f1:
        approach: "Choose threshold that maximizes F1 score"
        
        when_to_use: "Want to balance precision and recall equally"
        
        numpy_implementation: |
          def find_optimal_threshold_f1(y_true, y_pred_proba):
              """Find threshold that maximizes F1"""
              thresholds = np.linspace(0, 1, 100)
              best_f1 = 0
              best_threshold = 0.5
              
              for thresh in thresholds:
                  y_pred = (y_pred_proba >= thresh).astype(int)
                  metrics = ClassificationMetrics(y_true, y_pred)
                  f1 = metrics.f1_score()
                  
                  if f1 > best_f1:
                      best_f1 = f1
                      best_threshold = thresh
              
              return best_threshold, best_f1
      
      target_recall:
        approach: "Choose threshold that achieves target recall"
        
        when_to_use: "Compliance requires minimum detection rate (e.g., 95%)"
        
        numpy_implementation: |
          def find_threshold_for_recall(y_true, y_pred_proba, target_recall=0.95):
              """Find threshold achieving target recall with best precision"""
              thresholds = np.sort(np.unique(y_pred_proba))
              
              for thresh in thresholds:
                  y_pred = (y_pred_proba >= thresh).astype(int)
                  metrics = ClassificationMetrics(y_true, y_pred)
                  
                  if metrics.recall() >= target_recall:
                      return thresh, metrics.precision()
              
              return 0.0, 0.0  # Can't achieve target
      
      target_precision:
        approach: "Choose threshold that achieves target precision"
        
        when_to_use: "SOC capacity limited, need <100 alerts/day"
        
        numpy_implementation: |
          def find_threshold_for_precision(y_true, y_pred_proba, target_precision=0.9):
              """Find threshold achieving target precision with best recall"""
              thresholds = np.sort(np.unique(y_pred_proba))[::-1]
              
              for thresh in thresholds:
                  y_pred = (y_pred_proba >= thresh).astype(int)
                  metrics = ClassificationMetrics(y_true, y_pred)
                  
                  if metrics.precision() >= target_precision:
                      return thresh, metrics.recall()
              
              return 1.0, 0.0  # Can't achieve target
      
      cost_based:
        approach: "Minimize expected cost: Cost = C_FN × FN + C_FP × FP"
        
        when_to_use: "Know operational costs of errors"
        
        numpy_implementation: |
          def find_threshold_min_cost(y_true, y_pred_proba, cost_fn=1000, cost_fp=10):
              """Find threshold minimizing expected cost"""
              thresholds = np.linspace(0, 1, 100)
              min_cost = float('inf')
              best_threshold = 0.5
              
              for thresh in thresholds:
                  y_pred = (y_pred_proba >= thresh).astype(int)
                  metrics = ClassificationMetrics(y_true, y_pred)
                  
                  cost = cost_fn * metrics.fn + cost_fp * metrics.fp
                  
                  if cost < min_cost:
                      min_cost = cost
                      best_threshold = thresh
              
              return best_threshold, min_cost
    
    security_example: |
      Malware detection scenario:
      - Cost of missed malware (FN): $100,000 (breach, remediation)
      - Cost of false alarm (FP): $50 (analyst investigation)
      - Ratio: 2000:1
      
      Standard 0.5 threshold:
      - Recall = 80%, Precision = 70%
      - 20 FN × $100,000 = $2,000,000
      - 300 FP × $50 = $15,000
      - Total cost: $2,015,000
      
      Cost-optimized threshold (0.2):
      - Recall = 95%, Precision = 40%
      - 5 FN × $100,000 = $500,000
      - 1500 FP × $50 = $75,000
      - Total cost: $575,000
      
      Lower threshold = higher recall = fewer missed attacks = lower cost
  
  # --------------------------------------------------------------------------
  # Security Implications
  # --------------------------------------------------------------------------
  
  security_implications:
    
    metric_gaming:
      
      attack: "Adversary knows which metric you optimize"
      
      example_accuracy_gaming: |
        Model optimizes accuracy on 99% benign, 1% malicious dataset
        Adversary ensures attacks look slightly benign
        Model predicts all benign → 99% accuracy
        But 0% attack detection
      
      defense: "Use multiple metrics, focus on recall for security"
    
    threshold_exploitation:
      
      attack: "Adversary probes to find threshold"
      
      strategy: |
        Send inputs with varying characteristics
        Observe which get flagged
        Infer threshold (e.g., P(malicious) > 0.7)
        Craft attacks with P(malicious) = 0.69
      
      defense: |
        - Don't reveal scores to users
        - Use ensemble with different thresholds
        - Randomly vary threshold slightly
    
    choosing_security_metrics:
      
      guidance: |
        Primary: Recall (detection rate) - Can't miss attacks
        Secondary: Precision (false alarm rate) - Limited analyst capacity
        Tertiary: F1 (balance) - Overall quality
        
        Never: Accuracy alone - Misleading with imbalance
        
        Report: Confusion matrix + all metrics
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    conceptual_understanding:
      - "Confusion matrix foundation: TP, TN, FP, FN"
      - "Accuracy misleading with imbalance - use precision/recall"
      - "Precision = alert quality, Recall = detection rate"
      - "F1 balances both, use when equal priority"
      - "ROC/AUC for balanced data, PR curve for imbalanced"
      - "Threshold choice drives precision/recall trade-off"
    
    practical_skills:
      - "Compute confusion matrix and all metrics in NumPy"
      - "Interpret precision, recall, F1 in security context"
      - "Choose threshold based on operational constraints"
      - "Use PR curve for imbalanced security datasets"
      - "Report multiple metrics, not just accuracy"
    
    security_mindset:
      - "Recall = detection rate = most critical for security"
      - "Precision = false alarm rate = analyst capacity constraint"
      - "Default 0.5 threshold rarely optimal for security"
      - "Imbalanced data (1% attacks) requires specialized metrics"
      - "Different security systems optimize different metrics"
    
    remember_this:
      - "Accuracy is useless for rare events (1% attacks)"
      - "High precision = few false alarms, high recall = few missed attacks"
      - "Can't maximize both - choose based on operational costs"
    
    next_steps:
      - "Next section: Train/test split best practices"
      - "You now understand how to evaluate classification models properly"
      - "Sections 12-16 cover training methodology (overfitting, regularization, tuning)"

---
