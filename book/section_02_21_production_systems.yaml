# section_02_21_production_systems.yaml

---
document_info:
  chapter: "02"
  section: "21"
  title: "Production Machine Learning Systems"
  version: "1.0.0"
  author: "Raghav Dinesh"
  github: "github.com/raghavpoinia"
  license: "MIT"
  created: "2025-01-22"
  estimated_pages: 6
  tags: ["production", "deployment", "mlops", "serving", "monitoring", "model-versioning"]

# ============================================================================
# SECTION 02_21: PRODUCTION MACHINE LEARNING SYSTEMS
# ============================================================================

section_02_21_production_systems:
  
  # --------------------------------------------------------------------------
  # Overview and Context
  # --------------------------------------------------------------------------
  
  overview: |
    Getting a model to 90% accuracy in a Jupyter notebook is 10% of the work.
    Production deployment requires engineering: model serving (low latency,
    high throughput), monitoring (detect degradation), versioning (rollback
    bad models), A/B testing (validate improvements), and infrastructure
    (scale to millions of requests).
    
    This section bridges research and production. You'll understand production
    requirements (latency <100ms, availability 99.9%), implement model serving
    (REST API, batch inference), build monitoring systems (performance drift,
    data distribution shift), handle model updates (canary deployment, rollback),
    and design end-to-end ML pipelines (training → validation → deployment →
    monitoring loop).
    
    By the end, you'll deploy models that serve production traffic, detect and
    fix degradation, and understand the full ML lifecycle beyond training.
  
  learning_objectives:
    
    conceptual:
      - "Understand production vs research differences"
      - "Know model serving patterns (online, batch, streaming)"
      - "Grasp monitoring requirements (performance, data quality)"
      - "Understand model versioning and rollback"
      - "Recognize technical debt in ML systems"
      - "Connect security to production constraints"
    
    practical:
      - "Build REST API for model serving"
      - "Implement batch inference pipeline"
      - "Create monitoring dashboard"
      - "Set up A/B testing framework"
      - "Handle model versioning"
      - "Optimize inference latency"
    
    security_focused:
      - "Production systems = larger attack surface"
      - "Model extraction via API queries"
      - "Data poisoning through production pipeline"
      - "Monitoring detects adversarial inputs"
  
  prerequisites:
    - "Understanding of model training (Chapter 02)"
    - "Basic knowledge of APIs and web services"
    - "Familiarity with Python web frameworks"
  
  # --------------------------------------------------------------------------
  # Topic 1: Research vs Production
  # --------------------------------------------------------------------------
  
  research_vs_production:
    
    key_differences:
      
      research_focus: |
        Research/Development:
        - Goal: Maximize accuracy
        - Dataset: Fixed, clean
        - Hardware: Single GPU, unlimited time
        - Metrics: Accuracy, loss
        - Updates: Manual, when needed
        - Environment: Jupyter notebook
      
      production_focus: |
        Production:
        - Goal: Balance accuracy vs latency vs cost
        - Dataset: Streaming, noisy, drifting
        - Hardware: CPU, edge devices, cost constraints
        - Metrics: Latency, throughput, availability, cost
        - Updates: Continuous, automated
        - Environment: Microservices, containers, orchestration
    
    production_requirements:
      
      latency: |
        Target latencies by use case:
        - Search ranking: <50ms
        - Recommendation: <100ms
        - Fraud detection: <200ms
        - Batch processing: seconds to hours
        
        Research model (ResNet-152): 200ms
        Production model (MobileNet): 30ms
        
        Trade accuracy for speed!
      
      throughput: |
        Requests per second (RPS):
        - Small service: 10-100 RPS
        - Medium service: 1K-10K RPS
        - Large service: 100K+ RPS
        
        Need horizontal scaling, load balancing
      
      availability: |
        Uptime requirements:
        - 99% (two nines): 3.65 days downtime/year
        - 99.9% (three nines): 8.76 hours downtime/year
        - 99.99% (four nines): 52.6 minutes downtime/year
        
        Requires redundancy, health checks, failover
      
      cost: |
        Production cost considerations:
        - GPU expensive ($2-10 per hour)
        - CPU cheaper ($0.05-0.50 per hour)
        - Inference cost: CPU > GPU for most models
        
        Optimize model size, quantization, distillation
    
    technical_debt:
      
      hidden_costs: |
        ML systems accumulate technical debt:
        
        1. Data dependencies: Model depends on upstream data
        2. Configuration debt: Hyperparameters hardcoded
        3. Glue code: Custom code to connect components
        4. Pipeline jungles: Complex preprocessing pipelines
        5. Monitoring debt: No alerts, no dashboards
        
        Paying down debt = better long-term velocity
      
      maintenance_burden: |
        Ongoing maintenance:
        - Retrain models (weekly, monthly)
        - Update features (new data sources)
        - Fix bugs (data quality issues)
        - Monitor performance (detect drift)
        - Handle incidents (model degradation)
        
        Plan for 50% of time on maintenance
  
  # --------------------------------------------------------------------------
  # Topic 2: Model Serving
  # --------------------------------------------------------------------------
  
  model_serving:
    
    serving_patterns:
      
      online_serving: |
        Real-time prediction per request:
        
        Client → [API] → Model → Response
        
        Characteristics:
        - Latency: <100ms
        - Throughput: 100-10K RPS
        - Use case: Web apps, mobile apps
        
        Example: Search ranking, fraud detection
      
      batch_serving: |
        Offline predictions on large dataset:
        
        Data warehouse → [Batch job] → Model → Predictions stored
        
        Characteristics:
        - Latency: Minutes to hours
        - Throughput: Millions of samples
        - Use case: ETL pipelines, analytics
        
        Example: Daily user recommendations, churn prediction
      
      streaming_serving: |
        Continuous predictions on data stream:
        
        Kafka/Kinesis → [Stream processor] → Model → Output stream
        
        Characteristics:
        - Latency: <1 second
        - Throughput: 10K-100K events/sec
        - Use case: Real-time analytics
        
        Example: Click stream analysis, sensor data
    
    rest_api_implementation:
      
      simple_flask_api: |
        from flask import Flask, request, jsonify
        import numpy as np
        import pickle
        
        app = Flask(__name__)
        
        # Load model at startup
        with open('model.pkl', 'rb') as f:
            model = pickle.load(f)
        
        @app.route('/predict', methods=['POST'])
        def predict():
            """
            Predict endpoint.
            
            Request: {"features": [1.2, 3.4, 5.6, ...]}
            Response: {"prediction": 0.85, "latency_ms": 12.3}
            """
            try:
                # Parse request
                data = request.get_json()
                features = np.array(data['features']).reshape(1, -1)
                
                # Time prediction
                import time
                start = time.time()
                
                # Predict
                prediction = model.predict(features)[0]
                
                latency = (time.time() - start) * 1000  # Convert to ms
                
                # Return response
                return jsonify({
                    'prediction': float(prediction),
                    'latency_ms': latency
                })
            
            except Exception as e:
                return jsonify({'error': str(e)}), 400
        
        @app.route('/health', methods=['GET'])
        def health():
            """Health check endpoint"""
            return jsonify({'status': 'healthy'})
        
        if __name__ == '__main__':
            app.run(host='0.0.0.0', port=5000)
      
      production_api_improvements: |
        Production-grade additions:
        
        1. Input validation:
           - Check feature shapes, types
           - Reject malformed requests
           - Sanitize inputs
        
        2. Error handling:
           - Catch exceptions gracefully
           - Return meaningful error messages
           - Log errors for debugging
        
        3. Rate limiting:
           - Prevent abuse
           - Throttle excessive requests
           - Return 429 status code
        
        4. Authentication:
           - API keys
           - OAuth tokens
           - IP whitelisting
        
        5. Logging:
           - Log all requests
           - Track latency, errors
           - Enable debugging
        
        6. Metrics:
           - Prometheus metrics
           - Request count, latency percentiles
           - Error rates
    
    batch_inference:
      
      implementation: |
        import pandas as pd
        import numpy as np
        from tqdm import tqdm
        
        def batch_predict(model, data_path, output_path, batch_size=1024):
            """
            Batch inference on large dataset.
            
            Parameters:
            - model: trained model
            - data_path: path to input CSV
            - output_path: path to save predictions
            - batch_size: samples per batch
            """
            # Load data in chunks
            reader = pd.read_csv(data_path, chunksize=batch_size)
            
            predictions = []
            
            for chunk in tqdm(reader, desc="Processing batches"):
                # Preprocess
                features = chunk[feature_columns].values
                
                # Predict
                batch_preds = model.predict(features)
                predictions.extend(batch_preds)
            
            # Save predictions
            output_df = pd.DataFrame({
                'prediction': predictions
            })
            output_df.to_csv(output_path, index=False)
            
            print(f"Predictions saved to {output_path}")
      
      optimization_strategies: |
        Speed up batch inference:
        
        1. Batch operations:
           - Process 1024 samples at once
           - GPU utilization: batch size 32-256
        
        2. Parallel processing:
           - Multiple workers
           - Process different chunks in parallel
        
        3. Model optimization:
           - Quantization (float32 → int8)
           - Pruning (remove unnecessary weights)
           - Distillation (smaller student model)
        
        4. Hardware acceleration:
           - GPU for large models
           - CPU for small models (cheaper)
  
  # --------------------------------------------------------------------------
  # Topic 3: Model Monitoring
  # --------------------------------------------------------------------------
  
  model_monitoring:
    
    what_to_monitor:
      
      model_performance: |
        Track core metrics over time:
        
        - Accuracy/F1/Precision/Recall
        - AUC-ROC
        - Latency (p50, p95, p99)
        - Throughput (RPS)
        - Error rate
        
        Alert if metrics degrade >5%
      
      data_distribution: |
        Monitor input distribution drift:
        
        - Feature statistics (mean, std)
        - Missing value rates
        - Categorical value distributions
        - Out-of-range values
        
        Distribution shift → model degradation
      
      system_health: |
        Infrastructure metrics:
        
        - CPU/GPU utilization
        - Memory usage
        - Disk I/O
        - Network latency
        - Error logs
    
    detecting_model_degradation:
      
      performance_drift: |
        Model accuracy decreases over time:
        
        Week 1: 92% accuracy
        Week 4: 90% accuracy
        Week 8: 85% accuracy ← degradation!
        
        Causes:
        - Data distribution shift
        - Concept drift (relationships change)
        - Bugs in preprocessing
      
      data_drift: |
        Input distribution changes:
        
        Training: feature X mean = 10, std = 2
        Week 1:   feature X mean = 10.1, std = 2.0 (okay)
        Week 8:   feature X mean = 15, std = 5 (drifted!)
        
        Detection:
        - Compare training vs production distributions
        - Statistical tests (KS test, chi-square)
        - Alert if p-value < 0.05
      
      concept_drift: |
        Relationship between X and y changes:
        
        Example: Fraud detection
        - Training: Pattern A → fraud
        - Production: Fraudsters adapt, use Pattern B
        - Model: Still looking for Pattern A (misses fraud!)
        
        Requires continuous retraining
    
    monitoring_implementation: |
      class ModelMonitor:
          """
          Monitor model performance in production.
          """
          
          def __init__(self, model_name):
              self.model_name = model_name
              self.metrics = []
          
          def log_prediction(self, features, prediction, ground_truth=None, latency=None):
              """Log single prediction"""
              record = {
                  'timestamp': time.time(),
                  'features': features,
                  'prediction': prediction,
                  'ground_truth': ground_truth,
                  'latency': latency
              }
              self.metrics.append(record)
          
          def compute_metrics(self, window='1d'):
              """Compute metrics over time window"""
              df = pd.DataFrame(self.metrics)
              
              # Filter by window
              cutoff = time.time() - self._parse_window(window)
              df = df[df['timestamp'] > cutoff]
              
              # Compute metrics (if ground truth available)
              if 'ground_truth' in df.columns:
                  accuracy = (df['prediction'] == df['ground_truth']).mean()
              else:
                  accuracy = None
              
              latency_p50 = df['latency'].quantile(0.5)
              latency_p95 = df['latency'].quantile(0.95)
              
              return {
                  'accuracy': accuracy,
                  'latency_p50': latency_p50,
                  'latency_p95': latency_p95,
                  'num_predictions': len(df)
              }
          
          def check_drift(self, training_stats):
              """Check for distribution drift"""
              df = pd.DataFrame(self.metrics)
              
              # Compute current statistics
              current_mean = df['features'].apply(lambda x: np.mean(x)).mean()
              current_std = df['features'].apply(lambda x: np.std(x)).mean()
              
              # Compare to training
              mean_diff = abs(current_mean - training_stats['mean']) / training_stats['mean']
              std_diff = abs(current_std - training_stats['std']) / training_stats['std']
              
              # Alert if >20% difference
              if mean_diff > 0.2 or std_diff > 0.2:
                  return {
                      'drift_detected': True,
                      'mean_diff': mean_diff,
                      'std_diff': std_diff
                  }
              
              return {'drift_detected': False}
  
  # --------------------------------------------------------------------------
  # Topic 4: Model Versioning and Updates
  # --------------------------------------------------------------------------
  
  model_versioning:
    
    why_versioning_matters:
      
      reproducibility: |
        Need to answer: "Which model version served this prediction?"
        
        Without versioning:
        - Bug reported → which model had the bug?
        - Can't reproduce issue
        - Can't test fix
        
        With versioning:
        - Track model → version → code → data
        - Fully reproducible
      
      rollback_capability: |
        New model worse than old model:
        
        v1: 90% accuracy (current production)
        v2: 92% accuracy (looks better!)
        Deploy v2 → 85% accuracy in production (oops!)
        
        With versioning: Rollback to v1 instantly
        Without: Rebuild v1 from scratch (hours/days)
    
    versioning_scheme:
      
      semantic_versioning: |
        MAJOR.MINOR.PATCH (e.g., v2.3.1)
        
        MAJOR: Breaking changes (new architecture)
        MINOR: New features (added features)
        PATCH: Bug fixes
        
        Example:
        v1.0.0: Initial LSTM model
        v1.1.0: Added attention layer
        v1.1.1: Fixed preprocessing bug
        v2.0.0: Switched to Transformer
      
      git_based_versioning: |
        Tag model with git commit:
        
        model_v1.0.0_commit_abc123.pkl
        
        Links model to exact code version
        Fully reproducible
    
    deployment_strategies:
      
      blue_green_deployment: |
        Two identical environments:
        
        Blue (current production): v1 serving traffic
        Green (new version): v2 ready, no traffic
        
        Deploy:
        1. Deploy v2 to Green
        2. Test Green thoroughly
        3. Switch router: Green becomes production
        4. Keep Blue as backup (instant rollback)
      
      canary_deployment: |
        Gradual rollout:
        
        Step 1: Route 5% traffic to v2, 95% to v1
        Step 2: Monitor metrics (1-2 hours)
        Step 3: If good → 25% to v2
        Step 4: If good → 50% to v2
        Step 5: If good → 100% to v2
        
        Any degradation → rollback immediately
      
      a_b_testing: |
        Split traffic between versions:
        
        Group A: 50% traffic to v1
        Group B: 50% traffic to v2
        
        Compare metrics:
        - v1: 90.0% accuracy
        - v2: 90.5% accuracy
        
        Statistical test: Is difference significant?
        If yes → deploy v2 to 100%
    
    implementation_example: |
      class ModelRegistry:
          """
          Manage model versions.
          """
          
          def __init__(self, storage_path='models/'):
              self.storage_path = storage_path
              self.versions = {}
          
          def register_model(self, model, version, metadata):
              """
              Register new model version.
              
              Parameters:
              - model: trained model object
              - version: semantic version (e.g., 'v1.2.0')
              - metadata: dict with training info
              """
              model_path = f"{self.storage_path}/model_{version}.pkl"
              
              # Save model
              with open(model_path, 'wb') as f:
                  pickle.dump(model, f)
              
              # Save metadata
              metadata['timestamp'] = time.time()
              metadata['path'] = model_path
              
              self.versions[version] = metadata
              
              # Save registry
              self._save_registry()
              
              print(f"Registered model {version}")
          
          def load_model(self, version='latest'):
              """Load specific model version"""
              if version == 'latest':
                  version = self._get_latest_version()
              
              model_path = self.versions[version]['path']
              
              with open(model_path, 'rb') as f:
                  model = pickle.load(f)
              
              return model
          
          def rollback(self, to_version):
              """Rollback to previous version"""
              print(f"Rolling back to {to_version}")
              return self.load_model(to_version)
  
  # --------------------------------------------------------------------------
  # Topic 5: End-to-End ML Pipeline
  # --------------------------------------------------------------------------
  
  ml_pipeline:
    
    pipeline_components:
      
      data_ingestion: |
        Collect and store training data:
        
        Sources → [ETL] → Data warehouse
        
        - Extract: Pull from databases, APIs, logs
        - Transform: Clean, deduplicate, format
        - Load: Store in S3, BigQuery, etc.
      
      training_pipeline: |
        Automated model training:
        
        Data → [Preprocess] → [Train] → [Evaluate] → Model
        
        - Schedule: Daily, weekly, or on-demand
        - Version: Tag with timestamp, git commit
        - Validate: Test on holdout set
        - Register: Save to model registry
      
      deployment_pipeline: |
        Deploy trained model:
        
        Model registry → [Test] → [Stage] → [Production]
        
        - Test: Verify on test set
        - Stage: Deploy to staging environment
        - Production: Canary → full rollout
      
      monitoring_loop: |
        Continuous monitoring:
        
        Production → [Monitor] → [Alert] → [Retrain]
        
        - Monitor: Track metrics
        - Alert: Notify on degradation
        - Retrain: Trigger retraining if needed
    
    automation_example: |
      # Example: Automated training pipeline (pseudo-code)
      
      def training_pipeline(config):
          """
          End-to-end training pipeline.
          """
          # 1. Data ingestion
          data = load_data_from_warehouse(
              query=config['data_query'],
              date_range=config['date_range']
          )
          
          # 2. Preprocessing
          X_train, X_val, y_train, y_val = preprocess_and_split(data)
          
          # 3. Training
          model = train_model(
              X_train, y_train,
              architecture=config['architecture'],
              hyperparameters=config['hyperparameters']
          )
          
          # 4. Evaluation
          metrics = evaluate_model(model, X_val, y_val)
          
          # 5. Quality gate
          if metrics['accuracy'] < config['min_accuracy']:
              raise Exception("Model accuracy below threshold")
          
          # 6. Registration
          version = f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
          model_registry.register_model(
              model=model,
              version=version,
              metadata={'metrics': metrics, 'config': config}
          )
          
          # 7. Deployment (canary)
          deploy_canary(model, version, traffic_percentage=5)
          
          # 8. Monitor (automated)
          schedule_monitoring(version, duration_hours=24)
          
          return version
  
  # --------------------------------------------------------------------------
  # Topic 6: Security in Production
  # --------------------------------------------------------------------------
  
  security_in_production:
    
    api_attack_surface:
      
      model_extraction: |
        Attacker queries API repeatedly:
        
        1. Send many queries (10K-100K)
        2. Record input-output pairs
        3. Train surrogate model on pairs
        4. Surrogate approximates original
        
        Defense:
        - Rate limiting (100 requests/hour per user)
        - API key authentication
        - Monitor suspicious query patterns
      
      adversarial_inputs: |
        Attacker sends crafted inputs:
        
        Goal: Cause misclassification
        
        Example: Image classifier
        - Clean image: "cat" (correct)
        - + Tiny noise: "dog" (wrong!)
        
        Defense:
        - Input validation (reject anomalous inputs)
        - Adversarial training
        - Ensemble models
      
      denial_of_service: |
        Attacker floods API:
        
        Send 100K requests/second
        → API overwhelmed
        → Legitimate users can't access
        
        Defense:
        - Rate limiting
        - Load balancing
        - Auto-scaling
        - DDoS protection (Cloudflare)
    
    data_poisoning_via_pipeline:
      
      observation: |
        Production ML systems retrain on new data:
        
        User feedback → Training data → Retrain
        
        Attacker can inject poisoned data:
        - Deliberately mislabel data
        - Backdoor trigger patterns
        - Corrupt training pipeline
      
      example: |
        Recommendation system:
        
        Clean: User clicks → (item, positive label)
        Poisoned: Bot clicks → (bad_item, positive label)
        
        After retraining: Recommends bad_item to everyone
      
      defense: |
        - Validate training data quality
        - Filter outliers and anomalies
        - Human review of labels
        - Monitor model behavior post-retrain
    
    monitoring_detects_attacks:
      
      anomaly_detection: |
        Production monitoring catches attacks:
        
        Adversarial inputs:
        - Unusual feature distributions
        - Prediction confidence patterns
        - Higher error rates
        
        Alert on anomalies → investigate
      
      logging_for_forensics: |
        Log everything:
        - All requests (input, output, timestamp)
        - Model version used
        - User/API key
        
        After attack: Replay logs to understand
  
  # --------------------------------------------------------------------------
  # Key Takeaways
  # --------------------------------------------------------------------------
  
  key_takeaways:
    
    critical_concepts:
      - "Production ≠ research: optimize for latency/cost not just accuracy, target <100ms inference"
      - "Monitor continuously: track performance, data drift, system health, alert on >5% degradation"
      - "Version everything: models, code, data, enable rollback in minutes not hours"
      - "Canary deployment: gradual rollout 5% → 25% → 100%, rollback immediately on degradation"
      - "Data drift = model death: input distribution changes → model degrades → retrain needed"
      - "API = attack surface: rate limit, authenticate, monitor for extraction and adversarial inputs"
    
    actionable_steps:
      - "Build REST API with Flask: /predict endpoint, input validation, error handling, health checks"
      - "Set up monitoring dashboard: accuracy, latency p95, throughput, error rate, data statistics"
      - "Implement model registry: save models with versions, metadata, enable load by version"
      - "Use canary deployment: test new model on 5% traffic before full rollout, monitor for 24h"
      - "Log all predictions: store input, output, version, timestamp, enable debugging and forensics"
      - "Automate retraining: trigger weekly/monthly, quality gate on accuracy, deploy if passes"
    
    security_principles:
      - "Rate limit API: 100 requests/hour/user prevents model extraction via query flooding"
      - "Monitor query patterns: detect extraction (many similar queries) and adversarial probing"
      - "Validate training data: filter outliers, review labels, prevent poisoning via production pipeline"
      - "Log for forensics: comprehensive logging enables post-attack investigation and mitigation"
    
    production_checklist:
      - "Latency target met: <100ms p95 for online serving, optimize model if needed"
      - "Monitoring in place: dashboard showing performance, alerts on degradation"
      - "Versioning working: can load any past version, rollback in <5 minutes"
      - "Canary deployment: automated gradual rollout with monitoring"
      - "Security hardened: rate limiting, authentication, input validation, logging"

---
