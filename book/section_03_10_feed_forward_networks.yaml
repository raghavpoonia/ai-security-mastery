# section_03_10_feed_forward_networks.yaml

---
document_info:
  title: "Feed-Forward Networks in Transformers"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 10
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into feed-forward networks in transformers: position-wise FFN, dimension expansion, activation functions, FFN as key-value memory, and security implications"
  estimated_pages: 5
  tags:
    - feed-forward-network
    - position-wise-ffn
    - gelu-activation
    - dimension-expansion
    - ffn-memory
    - transformer-components

section_overview:
  title: "Feed-Forward Networks in Transformers"
  number: "3.10"
  
  purpose: |
    After multi-head attention, transformer layers apply a position-wise feed-forward 
    network (FFN) to each position independently. The FFN is a simple two-layer network: 
    expand dimensions (typically 4x), apply non-linearity, compress back to original size. 
    This appears simple but serves critical roles: injecting non-linearity, increasing 
    model capacity, and serving as key-value memory for factual knowledge.
    
    The FFN processes each position independently (position-wise), in contrast to attention 
    which mixes information across positions. This complementary design - attention for 
    context mixing, FFN for position-specific transformations - is key to transformer success.
    
    For security engineers: FFNs store factual knowledge in their weights. Understanding 
    FFN structure reveals how to extract stored information (model inversion), inject 
    backdoors through fine-tuning (weight poisoning), or detect model tampering (weight 
    analysis). FFNs are primary targets for model editing attacks.
  
  learning_objectives:
    conceptual:
      - "Understand position-wise FFN (applies same network to each position)"
      - "Grasp dimension expansion strategy (d_model → 4×d_model → d_model)"
      - "Learn why FFN is necessary after attention (non-linearity, capacity)"
      - "See FFN as key-value memory storing factual knowledge"
      - "Compare ReLU vs GELU activation functions"
    
    practical:
      - "Implement position-wise FFN from scratch (NumPy)"
      - "Create GELU activation function"
      - "Build configurable FFN with different expansion ratios"
      - "Analyze FFN parameter count and computational cost"
      - "Visualize FFN activations and weight patterns"
    
    security_focused:
      - "Extract factual knowledge from FFN weights (model inversion)"
      - "Detect backdoors in FFN layers (weight analysis)"
      - "Understand FFN fine-tuning vulnerabilities"
      - "Identify FFN neurons encoding sensitive information"
      - "Audit FFN for anomalous weight patterns"
  
  prerequisites:
    knowledge:
      - "Chapter 1: Fully connected layers, activation functions"
      - "Chapter 2: Gradient descent, backpropagation"
      - "Section 3.8: Multi-head attention (what comes before FFN)"
      - "Understanding of matrix multiplication"
    
    skills:
      - "NumPy matrix operations"
      - "Implementing neural network layers"
      - "Activation function derivatives"
      - "Parameter counting and memory analysis"
  
  key_transitions:
    from_section_3_9: |
      Section 3.9 covered positional encoding (how transformers know word order). 
      Now we introduce feed-forward networks - the other major component of transformer 
      layers that works alongside attention.
    
    to_next_section: |
      Section 3.11 will cover layer normalization and residual connections - the 
      architectural components that stabilize training and enable deep transformer 
      models (many stacked layers).

topics:
  - topic_number: 1
    title: "Position-Wise Feed-Forward Network Architecture"
    
    overview: |
      The FFN in transformers is "position-wise" - it applies the same fully connected 
      network independently to each position in the sequence. Unlike attention (which 
      mixes positions), FFN processes each token separately. The architecture is simple: 
      two linear layers with a non-linear activation in between.
    
    content:
      position_wise_definition:
        meaning: "Same transformation applied to each position independently"
        
        mathematical_formulation: |
          For each position i in sequence:
          
          FFN(x_i) = W_2 × ReLU(W_1 × x_i + b_1) + b_2
          
          Where:
          - x_i ∈ ℝ^d_model (input for position i)
          - W_1 ∈ ℝ^(d_ff × d_model) (expand)
          - W_2 ∈ ℝ^(d_model × d_ff) (compress)
          - d_ff = 4 × d_model (typical expansion)
        
        key_property: |
          Same W_1, W_2, b_1, b_2 used for ALL positions
          → Parameters shared across positions
          → Like applying 1D convolution with kernel size 1
        
        contrast_with_attention: |
          Attention: Mixes information ACROSS positions
          FFN: Transforms EACH position independently
          
          Complementary roles!
      
      two_layer_architecture:
        layer_1_expansion:
          purpose: "Expand to higher dimension"
          operation: "h = ReLU(W_1 × x + b_1)"
          dimension: "d_model → d_ff (typically 4x expansion)"
          
          example: "512 → 2048"
        
        layer_2_compression:
          purpose: "Compress back to original dimension"
          operation: "y = W_2 × h + b_2"
          dimension: "d_ff → d_model"
          
          example: "2048 → 512"
        
        final_output: "y ∈ ℝ^d_model (same as input)"
      
      dimension_expansion_ratio:
        typical_values:
          original_transformer: "4x expansion (d_ff = 4 × d_model)"
          bert_base: "4x (768 → 3072 → 768)"
          gpt2: "4x (768 → 3072 → 768)"
          gpt3: "4x (12288 → 49152 → 12288)"
        
        why_4x:
          empirical: "Works well in practice across many tasks"
          capacity: "Adds significant parameters without huge overhead"
          tradeoff: "Larger = more capacity but slower, more memory"
        
        alternatives:
          smaller_2x: "For efficiency (mobile models)"
          larger_8x: "For more capacity (some large models)"
          adaptive: "Different expansion ratios per layer"
      
      computational_cost:
        parameters:
          layer_1: "d_model × d_ff + d_ff bias"
          layer_2: "d_ff × d_model + d_model bias"
          total: "2 × d_model × d_ff + d_model + d_ff"
          
          example_bert: |
            d_model = 768, d_ff = 3072
            Total = 2 × 768 × 3072 + 768 + 3072 ≈ 4.7M parameters per FFN
        
        operations_per_token:
          layer_1: "d_model × d_ff (matrix-vector multiply)"
          activation: "d_ff (element-wise)"
          layer_2: "d_ff × d_model"
          total: "O(d_model × d_ff) ≈ O(d_model²) for d_ff = 4×d_model"
        
        comparison_to_attention:
          attention: "O(n² × d_model) - quadratic in sequence length"
          ffn: "O(n × d_model²) - linear in sequence length"
          
          crossover: |
            Short sequences (n < d_model): FFN dominates
            Long sequences (n > d_model): Attention dominates
    
    implementation:
      position_wise_ffn:
        language: python
        code: |
          import numpy as np
          
          class PositionWiseFeedForward:
              """
              Position-wise feed-forward network (Transformer FFN).
              
              FFN(x) = W_2 × ReLU(W_1 × x + b_1) + b_2
              
              Applied independently to each position.
              """
              
              def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
                  """
                  Args:
                      d_model: Model dimension (input/output size)
                      d_ff: Hidden dimension (intermediate size)
                      dropout: Dropout rate
                  """
                  self.d_model = d_model
                  self.d_ff = d_ff
                  self.dropout = dropout
                  
                  # Layer 1: Expand
                  self.W_1 = np.random.randn(d_ff, d_model) * np.sqrt(2.0 / d_model)
                  self.b_1 = np.zeros(d_ff)
                  
                  # Layer 2: Compress
                  self.W_2 = np.random.randn(d_model, d_ff) * np.sqrt(2.0 / d_ff)
                  self.b_2 = np.zeros(d_model)
              
              def relu(self, x: np.ndarray) -> np.ndarray:
                  """ReLU activation."""
                  return np.maximum(0, x)
              
              def forward(self, x: np.ndarray, training: bool = False) -> np.ndarray:
                  """
                  Forward pass through FFN.
                  
                  Args:
                      x: Input (batch, seq_len, d_model) or (seq_len, d_model)
                      training: Whether in training mode (for dropout)
                  
                  Returns:
                      output: (batch, seq_len, d_model) or (seq_len, d_model)
                  """
                  # Layer 1: Expand and activate
                  # (*, d_model) @ (d_model, d_ff)^T = (*, d_ff)
                  h = np.dot(x, self.W_1.T) + self.b_1
                  h = self.relu(h)
                  
                  # Dropout (if training)
                  if training and self.dropout > 0:
                      mask = np.random.binomial(1, 1 - self.dropout, h.shape) / (1 - self.dropout)
                      h = h * mask
                  
                  # Layer 2: Compress
                  # (*, d_ff) @ (d_ff, d_model)^T = (*, d_model)
                  output = np.dot(h, self.W_2.T) + self.b_2
                  
                  return output
              
              def __call__(self, x: np.ndarray, training: bool = False) -> np.ndarray:
                  """Alias for forward."""
                  return self.forward(x, training)
          
          
          # Example usage
          d_model = 512
          d_ff = 2048  # 4x expansion
          seq_len = 10
          batch_size = 2
          
          ffn = PositionWiseFeedForward(d_model, d_ff)
          
          # Input (batch, seq_len, d_model)
          x = np.random.randn(batch_size, seq_len, d_model)
          
          # Forward pass
          output = ffn(x)
          
          print(f"Position-Wise Feed-Forward Network:")
          print(f"  Model dimension: {d_model}")
          print(f"  Hidden dimension: {d_ff} ({d_ff // d_model}x expansion)")
          print(f"  Input shape: {x.shape}")
          print(f"  Output shape: {output.shape}")
          print(f"\nParameters:")
          print(f"  Layer 1 (W_1): {d_ff * d_model:,}")
          print(f"  Layer 1 (b_1): {d_ff:,}")
          print(f"  Layer 2 (W_2): {d_model * d_ff:,}")
          print(f"  Layer 2 (b_2): {d_model:,}")
          print(f"  Total: {2 * d_model * d_ff + d_model + d_ff:,}")
      
      demonstrate_position_wise:
        language: python
        code: |
          def demonstrate_position_wise_property():
              """Show that FFN applies same transformation to each position."""
              
              print("\n=== Position-Wise Property ===\n")
              
              d_model = 64
              d_ff = 256
              seq_len = 5
              
              ffn = PositionWiseFeedForward(d_model, d_ff)
              
              # Create sequence
              x = np.random.randn(seq_len, d_model)
              
              # Apply FFN to entire sequence
              output_batch = ffn(x)
              
              # Apply FFN to each position individually
              output_individual = np.zeros_like(x)
              for i in range(seq_len):
                  output_individual[i] = ffn(x[i])
              
              # Check if identical
              difference = np.abs(output_batch - output_individual).max()
              
              print(f"Sequence length: {seq_len}")
              print(f"Model dimension: {d_model}")
              print()
              print(f"Output (batch processing): {output_batch.shape}")
              print(f"Output (individual positions): {output_individual.shape}")
              print()
              print(f"Max difference: {difference:.10f}")
              
              if difference < 1e-10:
                  print("\n✓ Results are identical!")
                  print("  → FFN applies SAME transformation to each position")
                  print("  → Positions are processed independently")
              else:
                  print("\n✗ Results differ (unexpected!)")
          
          demonstrate_position_wise_property()
    
    security_implications:
      parameter_tampering: |
        FFN weights are large and dense:
        - 2 × d_model × d_ff parameters (millions)
        - Adversary can inject backdoors by modifying small subset
        - Changes to few neurons can create hidden triggers
        - Defense: Monitor weight distributions, detect anomalies
      
      position_independent_processing: |
        FFN treats all positions identically:
        - Cannot distinguish position-specific attacks
        - Adversary can inject payload at any position
        - FFN processes it the same way
        - Defense relies on attention (which IS position-aware)

  - topic_number: 2
    title: "Activation Functions: ReLU vs GELU"
    
    overview: |
      The original transformer used ReLU activation. Modern transformers (BERT, GPT) 
      predominantly use GELU (Gaussian Error Linear Unit). GELU is smoother than ReLU, 
      provides better gradient flow, and empirically performs better for language tasks. 
      Understanding activation choices is important for both model behavior and security.
    
    content:
      relu_activation:
        formula: "ReLU(x) = max(0, x)"
        
        properties:
          - "Simple: zero for negative, identity for positive"
          - "Non-differentiable at 0 (but set gradient to 0 or 1)"
          - "Unbounded above, bounded below at 0"
          - "Can cause 'dead neurons' (always output 0)"
        
        gradient: |
          dReLU/dx = 1 if x > 0
                      0 if x ≤ 0
        
        advantages:
          - "Computationally efficient (just max operation)"
          - "Sparse activations (many neurons exactly 0)"
          - "Proven track record (CNNs, early transformers)"
        
        disadvantages:
          - "Dead neurons (gradient always 0)"
          - "Sharp corner at 0 (not smooth)"
          - "Negative inputs completely ignored"
      
      gelu_activation:
        formula: |
          GELU(x) = x × Φ(x)
          
          Where Φ(x) is cumulative distribution function of standard normal
        
        approximation: |
          GELU(x) ≈ 0.5 × x × (1 + tanh(√(2/π) × (x + 0.044715 × x³)))
          
          Or simpler:
          GELU(x) ≈ x × σ(1.702 × x)  where σ is sigmoid
        
        properties:
          - "Smooth everywhere (differentiable)"
          - "Stochastic interpretation: multiply by Bernoulli(Φ(x))"
          - "Non-monotonic (slight dip for negative values)"
          - "Asymptotically linear for large positive x"
        
        gradient: |
          dGELU/dx = Φ(x) + x × φ(x)
          
          Where φ(x) is probability density function of standard normal
        
        advantages:
          - "Smooth gradient flow (better optimization)"
          - "No dead neurons (always some gradient)"
          - "Empirically better for NLP tasks"
          - "Used in BERT, GPT-2/3, modern transformers"
        
        disadvantages:
          - "More computationally expensive (tanh/sigmoid)"
          - "Approximation needed for practical implementation"
          - "Less interpretable than ReLU"
      
      comparison:
        relu: |
          f(x) = {x if x > 0, 0 if x ≤ 0}
          
          Sharp corner at 0
          Sparse (many zeros)
        
        gelu: |
          f(x) = x × Φ(x)
          
          Smooth transition
          Dense (few exact zeros)
        
        why_gelu_for_nlp:
          smoother_gradients: "Better optimization for deep networks"
          no_dead_neurons: "All neurons can contribute"
          empirical_performance: "Consistently better on language benchmarks"
          stochastic_interpretation: "Theoretical justification via dropout connection"
    
    implementation:
      activation_functions:
        language: python
        code: |
          class ActivationFunctions:
              """Common activation functions for transformers."""
              
              @staticmethod
              def relu(x: np.ndarray) -> np.ndarray:
                  """ReLU activation."""
                  return np.maximum(0, x)
              
              @staticmethod
              def relu_derivative(x: np.ndarray) -> np.ndarray:
                  """ReLU derivative."""
                  return (x > 0).astype(float)
              
              @staticmethod
              def gelu(x: np.ndarray) -> np.ndarray:
                  """
                  GELU activation (Gaussian Error Linear Unit).
                  
                  Approximation:
                  GELU(x) ≈ 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))
                  """
                  return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
              
              @staticmethod
              def gelu_derivative(x: np.ndarray) -> np.ndarray:
                  """
                  GELU derivative (approximation).
                  
                  Computed via automatic differentiation of approximation.
                  """
                  # Simplified approximation
                  cdf_approx = 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
                  pdf_approx = np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)
                  return cdf_approx + x * pdf_approx
              
              @staticmethod
              def swish(x: np.ndarray) -> np.ndarray:
                  """
                  Swish activation (also called SiLU).
                  
                  Swish(x) = x × σ(x) where σ is sigmoid
                  """
                  return x * (1 / (1 + np.exp(-x)))
          
          
          # Compare activations
          print("\n=== Activation Function Comparison ===\n")
          
          x_values = np.linspace(-3, 3, 100)
          
          relu_output = ActivationFunctions.relu(x_values)
          gelu_output = ActivationFunctions.gelu(x_values)
          swish_output = ActivationFunctions.swish(x_values)
          
          # Show values at key points
          key_points = [-2, -1, 0, 1, 2]
          
          print("x     | ReLU  | GELU  | Swish")
          print("------|-------|-------|-------")
          for x_val in key_points:
              idx = np.argmin(np.abs(x_values - x_val))
              print(f"{x_val:5.1f} | {relu_output[idx]:5.2f} | "
                    f"{gelu_output[idx]:5.2f} | {swish_output[idx]:5.2f}")
          
          print("\nKey differences:")
          print("  ReLU:  Sharp corner at 0, zero for x < 0")
          print("  GELU:  Smooth transition, slight dip for x < 0")
          print("  Swish: Similar to GELU, smooth everywhere")
      
      ffn_with_gelu:
        language: python
        code: |
          class PositionWiseFeedForwardGELU:
              """FFN with GELU activation (modern transformers)."""
              
              def __init__(self, d_model: int, d_ff: int):
                  self.d_model = d_model
                  self.d_ff = d_ff
                  
                  # Weights
                  self.W_1 = np.random.randn(d_ff, d_model) * np.sqrt(2.0 / d_model)
                  self.b_1 = np.zeros(d_ff)
                  self.W_2 = np.random.randn(d_model, d_ff) * np.sqrt(2.0 / d_ff)
                  self.b_2 = np.zeros(d_model)
              
              def gelu(self, x: np.ndarray) -> np.ndarray:
                  """GELU activation."""
                  return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
              
              def forward(self, x: np.ndarray) -> np.ndarray:
                  """Forward pass with GELU."""
                  # Expand
                  h = np.dot(x, self.W_1.T) + self.b_1
                  h = self.gelu(h)
                  
                  # Compress
                  output = np.dot(h, self.W_2.T) + self.b_2
                  
                  return output
              
              def __call__(self, x: np.ndarray) -> np.ndarray:
                  return self.forward(x)
          
          
          # Example usage
          d_model = 768  # BERT-base
          d_ff = 3072    # 4x expansion
          
          ffn_gelu = PositionWiseFeedForwardGELU(d_model, d_ff)
          
          x = np.random.randn(10, d_model)
          output = ffn_gelu(x)
          
          print("\n=== FFN with GELU (BERT-style) ===\n")
          print(f"Model dimension: {d_model}")
          print(f"Hidden dimension: {d_ff}")
          print(f"Input shape: {x.shape}")
          print(f"Output shape: {output.shape}")
          print(f"\nUsing GELU activation (smooth, modern)")
    
    security_implications:
      activation_function_fingerprinting: |
        Different activations create different output patterns:
        - ReLU: Sparse outputs (many exact zeros)
        - GELU: Dense outputs (few exact zeros)
        - Adversary can fingerprint model type via activation patterns
        - Defense: May want to add noise to hide activation function
      
      dead_neuron_exploitation_relu: |
        ReLU can create dead neurons (always output 0):
        - Adversary can poison training to kill specific neurons
        - Dead neurons cannot contribute to detection
        - Model capacity reduced without obvious degradation
        - Defense: Monitor neuron death rates, use GELU instead

  - topic_number: 3
    title: "Why FFN After Attention? Roles and Interpretations"
    
    overview: |
      Why do transformers need FFN layers after attention? Attention is already powerful - 
      it mixes information across positions. FFN serves complementary roles: injecting 
      non-linearity, increasing model capacity, and serving as key-value memory for storing 
      factual knowledge. Understanding these roles reveals FFN's critical importance.
    
    content:
      role_1_non_linearity:
        problem: "Attention is (mostly) linear operation"
        
        attention_linearity: |
          Attention(Q, K, V) = softmax(QK^T) V
          
          After softmax, this is weighted sum (linear combination of V)
          → Multiple attention layers without FFN = still linear!
        
        ffn_solution: |
          FFN adds non-linearity via ReLU/GELU
          → Enables complex non-linear transformations
          → Critical for deep network expressive power
        
        without_ffn: |
          Stack of attention layers (no FFN) ≈ single attention layer
          → Depth doesn't help much
          → Limited expressiveness
      
      role_2_capacity_expansion:
        dimension_expansion: |
          Attention: d_model → d_model (preserves dimension)
          FFN: d_model → 4×d_model → d_model (expands then compresses)
        
        parameter_count: |
          Attention (single head, d_model=512):
          - Q, K, V projections: 3 × 512² ≈ 786K
          - Output projection: 512² ≈ 262K
          - Total: ~1M parameters
          
          FFN (d_ff=2048):
          - Layer 1: 512 × 2048 ≈ 1M
          - Layer 2: 2048 × 512 ≈ 1M
          - Total: ~2M parameters (2x more than attention!)
        
        capacity_implications: |
          FFN has MORE parameters than attention in typical transformers
          → Significant contribution to model capacity
          → Can learn complex transformations
      
      role_3_ffn_as_key_value_memory:
        interpretation: |
          Geva et al. (2020): FFN layers store factual knowledge
          
          FFN can be viewed as key-value memory:
          - Layer 1 (W_1): Keys (pattern matching)
          - Layer 2 (W_2): Values (information retrieval)
        
        how_it_works: |
          Input x matches against keys (rows of W_1)
          → Activations h = ReLU(W_1 x) indicate which keys fired
          → Output = W_2 h retrieves corresponding values
        
        empirical_evidence:
          factual_recall: |
            "The Eiffel Tower is in ___"
            → FFN retrieves "Paris" from learned weights
          
          knowledge_editing: |
            Can update factual knowledge by editing FFN weights
            Example: Change "Paris" to "London" by modifying W_2
          
          knowledge_neurons: |
            Specific neurons in FFN encode specific facts
            "France neuron", "Paris neuron", "capital city neuron"
        
        implications_for_understanding: |
          Attention: Contextual processing (what's relevant?)
          FFN: Factual knowledge storage (what do I know?)
          
          Together: Context + Knowledge → Understanding
      
      complementary_design:
        attention_ffn_comparison:
          attention:
            operation: "Mix information ACROSS positions"
            parameters: "Shared across positions via Q, K, V"
            role: "Context aggregation, relevance weighting"
          
          ffn:
            operation: "Transform EACH position independently"
            parameters: "Shared across positions (same W_1, W_2)"
            role: "Non-linear transformation, knowledge retrieval"
        
        why_both_needed: |
          Attention alone: Can aggregate context but limited transformation
          FFN alone: Can transform but cannot mix positions
          
          Together: Attention gathers context, FFN processes it
          → Powerful combination!
    
    implementation:
      ffn_as_memory_demonstration:
        language: python
        code: |
          def demonstrate_ffn_as_memory():
              """
              Demonstrate FFN as key-value memory.
              
              Simplified example showing how FFN can store/retrieve facts.
              """
              
              print("\n=== FFN as Key-Value Memory ===\n")
              
              # Simplified example: FFN storing city-country associations
              d_model = 4
              d_ff = 8
              
              # Create FFN
              ffn = PositionWiseFeedForward(d_model, d_ff)
              
              # Simulate "city" embeddings (simplified)
              cities = {
                  "Paris": np.array([1.0, 0.0, 0.0, 0.0]),
                  "London": np.array([0.0, 1.0, 0.0, 0.0]),
                  "Tokyo": np.array([0.0, 0.0, 1.0, 0.0]),
                  "NYC": np.array([0.0, 0.0, 0.0, 1.0]),
              }
              
              print("Concept: FFN as Key-Value Memory")
              print()
              print("Layer 1 (W_1): Keys - pattern matching")
              print("  Rows of W_1 act as 'keys' that match input patterns")
              print("  Activation h = ReLU(W_1 × x) shows which keys fired")
              print()
              print("Layer 2 (W_2): Values - information retrieval")
              print("  Columns of W_2 store 'values' for each key")
              print("  Output = W_2 × h retrieves corresponding values")
              print()
              
              # Process each city
              print("Processing cities through FFN:")
              for city, embedding in cities.items():
                  # Layer 1: Key matching
                  h = np.dot(embedding, ffn.W_1.T) + ffn.b_1
                  h = np.maximum(0, h)  # ReLU
                  
                  # Layer 2: Value retrieval
                  output = np.dot(h, ffn.W_2.T) + ffn.b_2
                  
                  print(f"\n  {city}:")
                  print(f"    Hidden activations (which neurons fired): {h[:4]}")
                  print(f"    Output: {output}")
              
              print("\n\nKey insight:")
              print("  Different inputs activate different neurons (keys)")
              print("  Each neuron's activation retrieves associated information (values)")
              print("  → FFN stores and retrieves factual knowledge!")
          
          demonstrate_ffn_as_memory()
      
      analyze_ffn_parameters:
        language: python
        code: |
          def analyze_ffn_parameters():
              """Analyze FFN parameter distribution and capacity."""
              
              print("\n=== FFN Parameter Analysis ===\n")
              
              configurations = [
                  ("BERT-base", 768, 3072),
                  ("BERT-large", 1024, 4096),
                  ("GPT-2 small", 768, 3072),
                  ("GPT-2 medium", 1024, 4096),
                  ("GPT-3", 12288, 49152),
              ]
              
              print("Model          | d_model | d_ff  | FFN Params | Attention Params | Ratio")
              print("---------------|---------|-------|------------|------------------|-------")
              
              for name, d_model, d_ff in configurations:
                  # FFN parameters
                  ffn_params = 2 * d_model * d_ff + d_model + d_ff
                  
                  # Multi-head attention parameters (simplified: 8 heads)
                  num_heads = 8
                  d_k = d_model // num_heads
                  # Q, K, V projections + output projection
                  attn_params = 4 * d_model * d_model
                  
                  ratio = ffn_params / attn_params
                  
                  print(f"{name:14s} | {d_model:7,} | {d_ff:5,} | {ffn_params:10,} | "
                        f"{attn_params:16,} | {ratio:5.2f}x")
              
              print("\nObservation:")
              print("  FFN typically has 2-3x MORE parameters than attention!")
              print("  FFN is major contributor to model capacity")
              print("  → Important target for compression and security analysis")
          
          analyze_ffn_parameters()
    
    security_implications:
      knowledge_extraction_attacks: |
        FFN stores factual knowledge in weights:
        - Adversary can extract facts via weight analysis
        - Example: Extract "Paris is capital of France" from W_1, W_2
        - Model inversion attacks target FFN layers
        - Defense: Protect FFN weights, monitor extraction attempts
      
      knowledge_injection_backdoors: |
        FFN is primary target for knowledge injection:
        - Fine-tune FFN to inject false facts ("London is capital of France")
        - Small weight changes can add backdoors
        - Hard to detect without comprehensive fact-checking
        - Defense: Validate fine-tuned models, compare against base weights
      
      knowledge_neurons_targeting: |
        Specific neurons encode specific facts:
        - Adversary identifies "security-relevant" neurons
        - Suppresses or amplifies these neurons
        - Example: Kill "malware detection" neurons
        - Defense: Monitor neuron activation patterns, redundancy

key_takeaways:
  critical_concepts:
    - concept: "Position-wise FFN applies same network independently to each position"
      why_it_matters: "Complements attention (which mixes positions)"
    
    - concept: "Two-layer architecture: expand (4x) → activate → compress"
      why_it_matters: "Adds capacity and non-linearity critical for deep networks"
    
    - concept: "GELU activation smoother than ReLU, better for NLP"
      why_it_matters: "Used in modern transformers (BERT, GPT), better optimization"
    
    - concept: "FFN has 2-3x more parameters than attention in typical transformers"
      why_it_matters: "Major contributor to model capacity, important for security"
    
    - concept: "FFN as key-value memory storing factual knowledge"
      why_it_matters: "Reveals how to extract knowledge, inject backdoors, edit facts"
  
  actionable_steps:
    - step: "Implement position-wise FFN from scratch"
      verification: "Expand → activate → compress, same transformation per position"
    
    - step: "Implement GELU activation function"
      verification: "Smooth approximation using tanh, no dead neurons"
    
    - step: "Demonstrate FFN complements attention"
      verification: "Attention mixes positions, FFN transforms independently"
    
    - step: "Analyze FFN parameter count and computational cost"
      verification: "2×d_model×d_ff parameters, O(n×d_model²) operations"
    
    - step: "Understand FFN as key-value memory"
      verification: "W_1 = keys, W_2 = values, stores factual knowledge"
  
  security_principles:
    - principle: "FFN stores factual knowledge in weights"
      application: "Target for knowledge extraction and injection attacks"
    
    - principle: "Large parameter count (2-3x attention) creates attack surface"
      application: "More weights to tamper with, harder to audit"
    
    - principle: "Specific neurons encode specific facts (knowledge neurons)"
      application: "Adversary can target specific neurons for suppression/amplification"
    
    - principle: "Position-wise processing treats all positions identically"
      application: "Cannot distinguish position-based attacks, relies on attention"
    
    - principle: "Activation function choice affects output patterns"
      application: "Can fingerprint model type, different security properties"
  
  common_mistakes:
    - mistake: "Using different FFN parameters for different positions"
      fix: "Same W_1, W_2, b_1, b_2 for ALL positions (position-wise)"
    
    - mistake: "Forgetting non-linear activation between layers"
      fix: "Must have ReLU/GELU between W_1 and W_2"
    
    - mistake: "Using wrong dimension ordering in weight matrices"
      fix: "W_1: (d_ff, d_model), W_2: (d_model, d_ff)"
    
    - mistake: "Not accounting for FFN's large parameter count"
      fix: "FFN often dominates model size, plan memory accordingly"
    
    - mistake: "Ignoring FFN in security analysis (focusing only on attention)"
      fix: "FFN stores knowledge, is primary target for many attacks"
  
  integration_with_book:
    from_chapter_1:
      - "Fully connected layers (FFN is two FC layers)"
      - "Activation functions (ReLU basics)"
      - "Parameter counting and memory analysis"
    
    from_section_3_8:
      - "Multi-head attention (what comes before FFN)"
      - "Transformer layer structure"
    
    to_next_section:
      - "Section 3.11: Layer normalization and residual connections"
      - "How to stabilize training with LayerNorm"
      - "Residual connections for gradient flow"
  
  looking_ahead:
    next_concepts:
      - "Layer normalization (stabilize activations)"
      - "Residual connections (gradient highways)"
      - "Pre-norm vs post-norm configurations"
      - "Complete transformer layer (attention + FFN + norms + residuals)"
    
    skills_to_build:
      - "Implement layer normalization"
      - "Add residual connections properly"
      - "Build complete transformer encoder layer"
      - "Stack multiple layers into deep transformer"
  
  final_thoughts: |
    Feed-forward networks are the often-overlooked component of transformers, overshadowed 
    by attention's glamour. But FFN is critical: it injects non-linearity (enabling deep 
    networks), adds significant capacity (2-3x more parameters than attention), and serves 
    as key-value memory for factual knowledge.
    
    The architecture is elegantly simple: expand to 4x dimension, apply GELU activation, 
    compress back. Applied position-wise - same transformation to each position independently. 
    This complements attention perfectly: attention mixes information across positions, FFN 
    transforms each position based on learned knowledge.
    
    Modern transformers use GELU instead of ReLU for smoother gradients and better 
    optimization. The interpretation of FFN as key-value memory (Geva et al., 2020) reveals 
    how transformers store factual knowledge: Layer 1 acts as keys (pattern matching), 
    Layer 2 as values (information retrieval). This enables both understanding and attacks.
    
    From a security perspective: FFN is a primary target. Knowledge extraction attacks aim 
    to recover facts from weights. Knowledge injection attacks add false facts or backdoors 
    through fine-tuning. Specific "knowledge neurons" can be identified and targeted for 
    suppression. FFN's large parameter count (often majority of model) creates significant 
    attack surface that requires careful monitoring.
    
    Next: Section 3.11 introduces the glue that holds transformers together - layer 
    normalization and residual connections. These enable training deep networks by 
    stabilizing activations and providing gradient highways.

---
