# section_03_16_bert_architecture.yaml

---
document_info:
  title: "BERT: Bidirectional Encoder Representations"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 16
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into BERT: encoder-only architecture, masked language modeling, pre-training objectives, special tokens, fine-tuning strategies, and security implications"
  estimated_pages: 7
  tags:
    - bert
    - encoder-only
    - masked-language-model
    - mlm
    - pre-training
    - fine-tuning
    - transfer-learning

section_overview:
  title: "BERT: Bidirectional Encoder Representations"
  number: "3.16"
  
  purpose: |
    BERT (Devlin et al., 2018) revolutionized NLP by introducing bidirectional pre-training 
    for language understanding. Unlike GPT's left-to-right language modeling, BERT uses 
    masked language modeling (MLM) - randomly masking tokens and predicting them from 
    bidirectional context. This encoder-only architecture learns rich representations that 
    transfer effectively to downstream tasks through fine-tuning.
    
    BERT introduced the paradigm of "pre-train then fine-tune" that dominates modern NLP. 
    Pre-train on massive unlabeled text using self-supervised objectives (MLM + NSP), then 
    fine-tune on specific tasks with small labeled datasets. This transfer learning approach 
    achieves state-of-the-art performance across question answering, sentiment analysis, 
    named entity recognition, and more.
    
    For security engineers: BERT's bidirectional context makes hiding information harder - 
    the model sees full context from both directions. MLM pre-training can memorize sensitive 
    training data, extractable through careful prompting. Fine-tuning creates backdoor 
    vulnerabilities where task-specific poisoning affects the pre-trained model. Understanding 
    BERT reveals representation-based attacks and transfer learning vulnerabilities.
  
  learning_objectives:
    conceptual:
      - "Understand BERT's encoder-only architecture (no decoder)"
      - "Grasp masked language modeling: predict masked tokens from context"
      - "Learn special tokens: [CLS], [SEP], [MASK] and their roles"
      - "See pre-training objectives: MLM and Next Sentence Prediction"
      - "Compare pre-training vs fine-tuning stages"
    
    practical:
      - "Implement BERT encoder architecture"
      - "Build masked language model pre-training"
      - "Create MLM masking strategy (15% masking)"
      - "Implement fine-tuning for classification"
      - "Extract contextualized embeddings from BERT"
    
    security_focused:
      - "Extract memorized training data via MLM probing"
      - "Analyze fine-tuning backdoor vulnerabilities"
      - "Understand representation leakage in BERT embeddings"
      - "Exploit bidirectional context for information extraction"
      - "Audit BERT models for sensitive data memorization"
  
  prerequisites:
    knowledge:
      - "Section 3.12: Transformer encoder"
      - "Section 3.15: Training techniques (Adam, warmup)"
      - "Understanding of transfer learning"
      - "Self-supervised learning concepts"
    
    skills:
      - "Transformer encoder implementation"
      - "Training loop construction"
      - "Loss computation for multiple objectives"
      - "Fine-tuning pre-trained models"
  
  key_transitions:
    from_section_3_15: |
      Section 3.15 covered training techniques for transformers. Now we apply these to 
      BERT - an encoder-only model that uses masked language modeling for self-supervised 
      pre-training on massive unlabeled text.
    
    to_next_section: |
      Section 3.17 will cover GPT and decoder-only transformers - the opposite approach 
      from BERT. Instead of bidirectional understanding, GPT uses unidirectional 
      generation for language modeling and few-shot learning.

topics:
  - topic_number: 1
    title: "BERT Architecture: Encoder-Only Design"
    
    overview: |
      BERT uses only the encoder from the original transformer - no decoder, no cross-attention. 
      The architecture is a stack of transformer encoder layers with bidirectional self-
      attention. Input is tokenized using WordPiece, special tokens mark sequence boundaries, 
      and the [CLS] token's representation serves as the sentence embedding.
    
    content:
      encoder_only_architecture:
        why_no_decoder: |
          BERT is for understanding, not generation:
          - Goal: Learn contextual representations
          - Bidirectional context critical for understanding
          - No need for causal masking (not generating)
          
          → Use only encoder (bidirectional self-attention)
        
        architecture: |
          Input: Tokenized text with special tokens
          ↓
          Embeddings: Token + Position + Segment
          ↓
          Encoder Stack: N transformer encoder layers
          ↓
          Output: Contextualized representations per token
        
        bert_configurations:
          bert_base:
            layers: "12 encoder layers"
            hidden_size: "768 (d_model)"
            attention_heads: "12"
            parameters: "110M"
          
          bert_large:
            layers: "24 encoder layers"
            hidden_size: "1024"
            attention_heads: "16"
            parameters: "340M"
      
      special_tokens:
        cls_token:
          symbol: "[CLS]"
          position: "First token of every sequence"
          purpose: "Aggregate sequence representation for classification"
          
          usage: |
            [CLS] representation = sentence embedding
            Feed to classifier for sentence-level tasks
        
        sep_token:
          symbol: "[SEP]"
          position: "Separates segments in input"
          purpose: "Mark boundaries between sentences"
          
          usage: |
            Single sequence: [CLS] tokens [SEP]
            Pair: [CLS] sentence_A [SEP] sentence_B [SEP]
        
        mask_token:
          symbol: "[MASK]"
          position: "Replaces masked tokens during MLM training"
          purpose: "Indicate positions to predict"
          
          usage: |
            Original: "The cat sat on the mat"
            Masked: "The [MASK] sat on the [MASK]"
            Predict: cat, mat
      
      input_embeddings:
        three_embedding_types:
          token_embeddings: |
            WordPiece token → embedding
            Vocabulary: ~30K tokens
          
          position_embeddings: |
            Position index → embedding
            Learned (not sinusoidal)
            Max length: 512
          
          segment_embeddings: |
            Segment A (0) or Segment B (1) → embedding
            Helps distinguish sentence pairs
        
        combination: |
          final_embedding = token_emb + position_emb + segment_emb
          
          Then: LayerNorm + Dropout
        
        example: |
          Input: [CLS] The cat [SEP] sat down [SEP]
          
          Tokens:    [CLS] The  cat [SEP] sat down [SEP]
          Token IDs: [101, 138, 4937, 102, 1115, 2090, 102]
          Positions: [0,   1,   2,    3,   4,    5,    6]
          Segments:  [0,   0,   0,    0,   1,    1,    1]
    
    implementation:
      bert_encoder:
        language: python
        code: |
          import numpy as np
          
          class BERTEncoder:
              """
              BERT encoder-only transformer.
              
              Architecture:
              - Token + Position + Segment embeddings
              - Stack of transformer encoder layers
              - No decoder, bidirectional self-attention
              """
              
              def __init__(self,
                          vocab_size: int,
                          max_len: int = 512,
                          num_layers: int = 12,
                          d_model: int = 768,
                          num_heads: int = 12,
                          d_ff: int = 3072,
                          dropout: float = 0.1):
                  """
                  Args:
                      vocab_size: Vocabulary size (including special tokens)
                      max_len: Maximum sequence length
                      num_layers: Number of encoder layers
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  self.vocab_size = vocab_size
                  self.max_len = max_len
                  self.d_model = d_model
                  
                  # Embeddings
                  self.token_embeddings = np.random.randn(vocab_size, d_model) * 0.02
                  self.position_embeddings = np.random.randn(max_len, d_model) * 0.02
                  self.segment_embeddings = np.random.randn(2, d_model) * 0.02  # A/B
                  
                  # Encoder stack (from Section 3.12)
                  from section_3_12_transformer_encoder import TransformerEncoder
                  self.encoder = TransformerEncoder(
                      num_layers, d_model, num_heads, d_ff, dropout
                  )
                  
                  # Layer norm for embeddings
                  from section_3_11_layer_norm_residuals import LayerNormalization
                  self.embed_norm = LayerNormalization(d_model)
                  
                  self.dropout = dropout
              
              def embed(self,
                       token_ids: np.ndarray,
                       segment_ids: np.ndarray = None) -> np.ndarray:
                  """
                  Create BERT embeddings: token + position + segment.
                  
                  Args:
                      token_ids: Token IDs (batch, seq_len)
                      segment_ids: Segment IDs (batch, seq_len), 0 or 1
                  
                  Returns:
                      embeddings: (batch, seq_len, d_model)
                  """
                  batch_size, seq_len = token_ids.shape
                  
                  # Token embeddings
                  token_emb = self.token_embeddings[token_ids]
                  
                  # Position embeddings
                  positions = np.arange(seq_len)
                  position_emb = self.position_embeddings[positions]
                  
                  # Segment embeddings
                  if segment_ids is None:
                      segment_ids = np.zeros_like(token_ids)
                  segment_emb = self.segment_embeddings[segment_ids]
                  
                  # Combine
                  embeddings = token_emb + position_emb + segment_emb
                  
                  # LayerNorm + Dropout
                  embeddings = self.embed_norm(embeddings)
                  
                  return embeddings
              
              def forward(self,
                         token_ids: np.ndarray,
                         segment_ids: np.ndarray = None,
                         mask: np.ndarray = None,
                         training: bool = False) -> np.ndarray:
                  """
                  Forward pass through BERT encoder.
                  
                  Args:
                      token_ids: Token IDs (batch, seq_len)
                      segment_ids: Segment IDs (batch, seq_len)
                      mask: Attention mask
                      training: Whether in training mode
                  
                  Returns:
                      output: Encoder output (batch, seq_len, d_model)
                  """
                  # Embed input
                  x = self.embed(token_ids, segment_ids)
                  
                  # Pass through encoder
                  output, _ = self.encoder(x, mask, training)
                  
                  return output
              
              def __call__(self, token_ids: np.ndarray,
                          segment_ids: np.ndarray = None,
                          mask: np.ndarray = None,
                          training: bool = False) -> np.ndarray:
                  """Alias for forward."""
                  return self.forward(token_ids, segment_ids, mask, training)
          
          
          # Example usage
          print("=== BERT Encoder ===\n")
          
          vocab_size = 30522  # BERT WordPiece vocab
          max_len = 512
          num_layers = 12
          d_model = 768
          num_heads = 12
          d_ff = 3072
          
          bert = BERTEncoder(vocab_size, max_len, num_layers, d_model, num_heads, d_ff)
          
          # Sample input with special tokens
          # [CLS] The cat [SEP] sat down [SEP]
          batch_size = 2
          seq_len = 7
          
          token_ids = np.random.randint(0, vocab_size, (batch_size, seq_len))
          token_ids[:, 0] = 101  # [CLS] token
          token_ids[:, 3] = 102  # [SEP] token
          token_ids[:, 6] = 102  # [SEP] token
          
          segment_ids = np.array([[0, 0, 0, 0, 1, 1, 1]] * batch_size)
          
          # Forward pass
          output = bert(token_ids, segment_ids, training=False)
          
          print(f"BERT-base configuration:")
          print(f"  Vocabulary: {vocab_size:,}")
          print(f"  Layers: {num_layers}")
          print(f"  Hidden size: {d_model}")
          print(f"  Attention heads: {num_heads}")
          print(f"  Parameters: ~110M")
          print()
          print(f"Input:")
          print(f"  Token IDs: {token_ids.shape}")
          print(f"  Segment IDs: {segment_ids.shape}")
          print()
          print(f"Output:")
          print(f"  Encodings: {output.shape}")
          print()
          print("Special tokens:")
          print("  [CLS]: Position 0 → sentence representation")
          print("  [SEP]: Separates segments")
    
    security_implications:
      cls_token_information_leakage: |
        [CLS] aggregates sequence information:
        - Contains compressed representation of entire input
        - Can leak information about full sequence
        - Adversary can extract sentence-level info from [CLS]
        - Defense: Sanitize [CLS] representations, limit access
      
      bidirectional_context_harder_to_hide: |
        BERT sees full context (both directions):
        - Cannot hide information via position (unlike GPT)
        - Model infers masked content from surrounding tokens
        - Harder to obfuscate sensitive data
        - Defense: Remove sensitive content entirely, don't just mask

  - topic_number: 2
    title: "Masked Language Modeling: Self-Supervised Pre-training"
    
    overview: |
      Masked Language Modeling (MLM) is BERT's core pre-training objective. Randomly mask 
      15% of tokens and train the model to predict them from bidirectional context. This 
      self-supervised task requires no labels - just raw text. The masking strategy is 
      sophisticated: 80% replaced with [MASK], 10% replaced with random token, 10% unchanged.
    
    content:
      mlm_objective:
        core_idea: |
          Learn to predict masked tokens from context:
          
          Original: "The cat sat on the mat"
          Masked:   "The [MASK] sat on the [MASK]"
          Predict:  cat, mat
        
        why_mlm_works: |
          Bidirectional context enables deep understanding:
          - Left context: "The ___"
          - Right context: "___ sat on the ___"
          - Combined: Strong signal for "cat"
          
          Forces model to learn:
          - Syntax (grammatical patterns)
          - Semantics (word meanings)
          - World knowledge (common sense)
        
        contrast_with_language_modeling: |
          GPT (left-to-right LM):
          - Predict next token given left context only
          - Unidirectional
          
          BERT (MLM):
          - Predict masked token given full context
          - Bidirectional
          
          → BERT better for understanding, GPT better for generation
      
      masking_strategy:
        random_15_percent: |
          For each sequence:
          1. Randomly select 15% of tokens
          2. Apply masking strategy to selected tokens
          3. Train to predict original tokens
        
        three_way_masking:
          eighty_percent_mask: |
            80% of time: Replace with [MASK]
            "The cat sat" → "The [MASK] sat"
          
          ten_percent_random: |
            10% of time: Replace with random token
            "The cat sat" → "The dog sat"
            
            Purpose: Model can't rely on [MASK] being present
          
          ten_percent_unchanged: |
            10% of time: Keep original token
            "The cat sat" → "The cat sat"
            
            Purpose: Model learns to copy when appropriate
        
        why_not_always_mask: |
          If always use [MASK]:
          - [MASK] token only appears in training
          - At fine-tuning: No [MASK] tokens
          - Train-test mismatch
          
          Solution: Mix in random tokens and unchanged tokens
      
      mlm_loss:
        formula: |
          For each masked position i:
          loss_i = -log P(token_i | context)
          
          Total loss = sum over all masked positions
        
        only_predict_masked: |
          IMPORTANT: Only compute loss on masked tokens
          
          Not masked → no loss contribution
          Masked → predict original token
        
        example: |
          Input:  "The [MASK] sat on the [MASK]"
          Output logits: (seq_len, vocab_size)
          
          Loss only at positions 1 and 5 (where [MASK])
          Positions 0, 2, 3, 4, 6: No loss
      
      next_sentence_prediction:
        auxiliary_task: |
          In addition to MLM, BERT uses Next Sentence Prediction:
          
          Given two sentences A and B:
          - 50%: B follows A in corpus (label: IsNext)
          - 50%: B is random sentence (label: NotNext)
          
          Train to predict relationship
        
        purpose: |
          Learn sentence-level relationships:
          - Coherence
          - Discourse structure
          - Paragraph understanding
        
        implementation: |
          Use [CLS] token representation
          → Feed to binary classifier
          → Predict IsNext vs NotNext
        
        controversy: |
          Later work (RoBERTa) found NSP not essential
          → MLM alone often sufficient
          → NSP removed in some BERT variants
    
    implementation:
      mlm_masker:
        language: python
        code: |
          class MLMMasker:
              """
              Masked Language Model masking strategy.
              
              15% masking: 80% [MASK], 10% random, 10% unchanged
              """
              
              def __init__(self,
                          vocab_size: int,
                          mask_token_id: int = 103,
                          cls_token_id: int = 101,
                          sep_token_id: int = 102,
                          mask_prob: float = 0.15):
                  """
                  Args:
                      vocab_size: Vocabulary size
                      mask_token_id: [MASK] token ID
                      cls_token_id: [CLS] token ID
                      sep_token_id: [SEP] token ID
                      mask_prob: Probability of masking each token
                  """
                  self.vocab_size = vocab_size
                  self.mask_token_id = mask_token_id
                  self.cls_token_id = cls_token_id
                  self.sep_token_id = sep_token_id
                  self.mask_prob = mask_prob
                  
                  # Special tokens that should never be masked
                  self.special_tokens = {cls_token_id, sep_token_id}
              
              def mask(self, token_ids: np.ndarray) -> tuple:
                  """
                  Apply MLM masking strategy.
                  
                  Args:
                      token_ids: Original token IDs (batch, seq_len)
                  
                  Returns:
                      masked_ids: Token IDs with masking applied
                      labels: Original token IDs for masked positions (-100 for non-masked)
                  """
                  batch_size, seq_len = token_ids.shape
                  
                  # Copy inputs
                  masked_ids = token_ids.copy()
                  labels = np.full_like(token_ids, -100)  # -100 = ignore in loss
                  
                  for i in range(batch_size):
                      for j in range(seq_len):
                          token = token_ids[i, j]
                          
                          # Don't mask special tokens
                          if token in self.special_tokens:
                              continue
                          
                          # Randomly select 15% of tokens
                          if np.random.random() < self.mask_prob:
                              labels[i, j] = token  # Save original for loss
                              
                              rand = np.random.random()
                              
                              if rand < 0.8:
                                  # 80%: Replace with [MASK]
                                  masked_ids[i, j] = self.mask_token_id
                              elif rand < 0.9:
                                  # 10%: Replace with random token
                                  random_token = np.random.randint(0, self.vocab_size)
                                  masked_ids[i, j] = random_token
                              # else: 10%: Keep unchanged
                  
                  return masked_ids, labels
          
          
          # Example usage
          print("\n=== MLM Masking Strategy ===\n")
          
          vocab_size = 30522
          masker = MLMMasker(vocab_size)
          
          # Sample input
          # [CLS] The cat sat on mat [SEP]
          original_ids = np.array([
              [101, 138, 4937, 1115, 2006, 13523, 102],
              [101, 1996, 4937, 1005, 1055, 2204, 102]
          ])
          
          # Apply masking
          masked_ids, labels = masker.mask(original_ids)
          
          print("Original tokens:")
          print(original_ids)
          print()
          print("Masked tokens:")
          print(masked_ids)
          print()
          print("Labels (original tokens for masked positions, -100 for others):")
          print(labels)
          print()
          print("Masking strategy:")
          print("  1. Select 15% of tokens randomly")
          print("  2. Of selected tokens:")
          print("     - 80%: Replace with [MASK] (103)")
          print("     - 10%: Replace with random token")
          print("     - 10%: Keep unchanged")
          print("  3. Never mask [CLS] or [SEP]")
      
      mlm_training:
        language: python
        code: |
          def train_mlm_step(bert: BERTEncoder,
                            token_ids: np.ndarray,
                            learning_rate: float = 1e-4) -> float:
              """
              Single MLM training step.
              
              Args:
                  bert: BERT encoder model
                  token_ids: Original token IDs (batch, seq_len)
                  learning_rate: Learning rate
              
              Returns:
                  loss: MLM loss
              """
              # Apply masking
              masker = MLMMasker(bert.vocab_size)
              masked_ids, labels = masker.mask(token_ids)
              
              # Forward pass
              encoder_output = bert(masked_ids, training=True)
              # (batch, seq_len, d_model)
              
              # Project to vocabulary (simplified - would be learned layer)
              vocab_logits = np.dot(encoder_output, 
                                   bert.token_embeddings.T)
              # (batch, seq_len, vocab_size)
              
              # Compute loss only on masked positions
              batch_size, seq_len, vocab_size = vocab_logits.shape
              
              total_loss = 0.0
              num_masked = 0
              
              for i in range(batch_size):
                  for j in range(seq_len):
                      if labels[i, j] != -100:  # Masked position
                          # Softmax
                          logits = vocab_logits[i, j]
                          logits_max = np.max(logits)
                          exp_logits = np.exp(logits - logits_max)
                          probs = exp_logits / np.sum(exp_logits)
                          
                          # Cross-entropy
                          target = labels[i, j]
                          loss = -np.log(probs[target] + 1e-10)
                          
                          total_loss += loss
                          num_masked += 1
              
              # Average over masked tokens
              if num_masked > 0:
                  loss = total_loss / num_masked
              else:
                  loss = 0.0
              
              # Backward pass (simplified - would compute gradients)
              # update_parameters(bert, gradients, learning_rate)
              
              return loss
          
          
          print("\n=== MLM Training ===\n")
          
          bert = BERTEncoder(30522)
          
          # Sample batch
          batch_size = 4
          seq_len = 64
          token_ids = np.random.randint(0, 30522, (batch_size, seq_len))
          
          # Training step
          loss = train_mlm_step(bert, token_ids)
          
          print(f"Batch size: {batch_size}")
          print(f"Sequence length: {seq_len}")
          print(f"MLM loss: {loss:.4f}")
          print()
          print("MLM pre-training:")
          print("  1. Mask 15% of tokens")
          print("  2. Predict original tokens from context")
          print("  3. Loss only on masked positions")
          print("  4. Bidirectional context enables deep understanding")
    
    security_implications:
      mlm_data_extraction: |
        MLM can be exploited to extract training data:
        - Adversary crafts partial sequences
        - Model fills in masked tokens with training data
        - Can recover sensitive information from corpus
        - Example: "[MASK] SSN is 123-45-6789" → Model predicts "John's"
        - Defense: Differential privacy, output filtering
      
      memorization_via_mlm: |
        MLM encourages memorization:
        - Model learns to predict exact tokens from context
        - Repeated sequences in training data → memorized
        - Can verbatim reproduce training text
        - Defense: Deduplication, privacy auditing

  - topic_number: 3
    title: "Fine-tuning and Transfer Learning"
    
    overview: |
      After MLM pre-training, BERT is fine-tuned on downstream tasks with small labeled 
      datasets. The pre-trained representations transfer effectively across tasks: add a 
      task-specific head (classifier, tagger, etc.), fine-tune the entire model end-to-end. 
      This paradigm achieves state-of-the-art with minimal task-specific data.
    
    content:
      pre_train_then_fine_tune:
        pre_training_stage:
          objective: "Self-supervised (MLM + NSP)"
          data: "Large unlabeled corpus (Wikipedia, books)"
          duration: "Days to weeks on TPUs/GPUs"
          result: "General-purpose representations"
        
        fine_tuning_stage:
          objective: "Task-specific supervised learning"
          data: "Small labeled dataset for task"
          duration: "Hours (3-4 epochs typical)"
          result: "Task-optimized model"
        
        why_this_works: |
          Pre-training learns general language understanding
          → Strong initialization for downstream tasks
          → Small amounts of task data sufficient
          → Transfer learning paradigm
      
      fine_tuning_for_different_tasks:
        sentence_classification:
          example: "Sentiment analysis, spam detection"
          
          architecture: |
            [CLS] representation → Linear layer → Softmax
          
          procedure: |
            1. Feed sentence through BERT
            2. Extract [CLS] token representation
            3. Project to class logits
            4. Cross-entropy loss
        
        token_classification:
          example: "Named entity recognition, POS tagging"
          
          architecture: |
            Each token representation → Linear layer → Softmax
          
          procedure: |
            1. Feed sentence through BERT
            2. For each token, extract representation
            3. Project each to class logits
            4. Cross-entropy per token
        
        question_answering:
          example: "SQuAD (extractive QA)"
          
          architecture: |
            [CLS] Question [SEP] Context [SEP]
            → Token representations
            → Predict start and end positions
          
          procedure: |
            1. Concatenate question and context
            2. BERT encodes combined sequence
            3. Two classifiers: start position, end position
            4. Extract span from context
        
        sentence_pair_classification:
          example: "Entailment, paraphrase detection"
          
          architecture: |
            [CLS] Sentence A [SEP] Sentence B [SEP]
            → [CLS] representation → Classifier
          
          procedure: |
            1. Feed sentence pair through BERT
            2. Extract [CLS] representation
            3. Classify relationship
      
      fine_tuning_best_practices:
        learning_rate: |
          Lower than pre-training (2e-5 to 5e-5 typical)
          → Avoid catastrophic forgetting
        
        epochs: "3-4 epochs typical (more = overfitting)"
        
        batch_size: "16 or 32 (depends on GPU memory)"
        
        layer_specific_lr: |
          Optional: Different LR per layer
          - Lower layers: Smaller LR (general features)
          - Upper layers: Larger LR (task-specific)
        
        warmup: "10% of training steps (like pre-training)"
      
      bert_variants:
        roberta:
          changes: |
            - Remove NSP objective (MLM only)
            - Dynamic masking (different each epoch)
            - Larger batches, more data
            - Byte-level BPE tokenization
          result: "Improved performance over BERT"
        
        albert:
          changes: |
            - Parameter sharing across layers
            - Factorized embeddings (reduce params)
            - Sentence Order Prediction (replace NSP)
          result: "Smaller, faster, comparable performance"
        
        distilbert:
          changes: |
            - Distillation from BERT (student-teacher)
            - 6 layers instead of 12
            - 40% smaller, 60% faster
          result: "Retain 97% of BERT performance"
    
    implementation:
      bert_classifier:
        language: python
        code: |
          class BERTClassifier:
              """
              BERT for sequence classification.
              
              Architecture:
              - BERT encoder (pre-trained)
              - Classification head on [CLS] token
              """
              
              def __init__(self, bert: BERTEncoder, num_classes: int):
                  """
                  Args:
                      bert: Pre-trained BERT encoder
                      num_classes: Number of output classes
                  """
                  self.bert = bert
                  self.num_classes = num_classes
                  
                  # Classification head
                  self.classifier = np.random.randn(bert.d_model, num_classes) * 0.02
                  self.bias = np.zeros(num_classes)
              
              def forward(self,
                         token_ids: np.ndarray,
                         segment_ids: np.ndarray = None,
                         training: bool = False) -> np.ndarray:
                  """
                  Forward pass for classification.
                  
                  Args:
                      token_ids: Token IDs (batch, seq_len)
                      segment_ids: Segment IDs
                      training: Whether in training mode
                  
                  Returns:
                      logits: Class logits (batch, num_classes)
                  """
                  # Encode with BERT
                  encoder_output = self.bert(token_ids, segment_ids, training=training)
                  # (batch, seq_len, d_model)
                  
                  # Extract [CLS] token representation (position 0)
                  cls_repr = encoder_output[:, 0, :]  # (batch, d_model)
                  
                  # Project to class logits
                  logits = np.dot(cls_repr, self.classifier) + self.bias
                  # (batch, num_classes)
                  
                  return logits
              
              def __call__(self, token_ids: np.ndarray,
                          segment_ids: np.ndarray = None,
                          training: bool = False) -> np.ndarray:
                  """Alias for forward."""
                  return self.forward(token_ids, segment_ids, training)
          
          
          # Example usage
          print("\n=== BERT Fine-tuning for Classification ===\n")
          
          # Pre-trained BERT
          bert = BERTEncoder(30522, num_layers=12, d_model=768)
          
          # Add classification head
          num_classes = 2  # Binary classification (e.g., sentiment)
          classifier = BERTClassifier(bert, num_classes)
          
          # Sample input
          batch_size = 4
          seq_len = 64
          token_ids = np.random.randint(0, 30522, (batch_size, seq_len))
          token_ids[:, 0] = 101  # [CLS]
          
          # Forward pass
          logits = classifier(token_ids, training=False)
          
          print(f"Task: Sentiment analysis (positive/negative)")
          print(f"Batch size: {batch_size}")
          print(f"Sequence length: {seq_len}")
          print()
          print(f"Output logits shape: {logits.shape}")
          print()
          print("Fine-tuning process:")
          print("  1. Load pre-trained BERT (110M params)")
          print("  2. Add task-specific head (1.5K params)")
          print("  3. Fine-tune end-to-end on labeled data")
          print("  4. Low LR (2e-5), 3-4 epochs")
    
    security_implications:
      fine_tuning_backdoors: |
        Fine-tuning creates backdoor vulnerabilities:
        - Adversary poisons fine-tuning dataset
        - Backdoor inserted into pre-trained model
        - Triggers activate on specific inputs
        - Defense: Audit fine-tuning data, monitor performance
      
      catastrophic_forgetting_exploitation: |
        Fine-tuning can erase safety mechanisms:
        - Pre-trained model may have safety filters
        - Fine-tuning on adversarial data removes filters
        - Model "forgets" safety constraints
        - Defense: Regularize to preserve pre-trained behavior

key_takeaways:
  critical_concepts:
    - concept: "BERT is encoder-only: no decoder, bidirectional self-attention"
      why_it_matters: "Optimized for understanding, not generation"
    
    - concept: "MLM pre-training: predict masked tokens from bidirectional context"
      why_it_matters: "Self-supervised learning on massive unlabeled text"
    
    - concept: "15% masking: 80% [MASK], 10% random, 10% unchanged"
      why_it_matters: "Sophisticated strategy prevents train-test mismatch"
    
    - concept: "[CLS] token aggregates sequence representation for classification"
      why_it_matters: "Enables sentence-level tasks via single token"
    
    - concept: "Pre-train then fine-tune: general → task-specific"
      why_it_matters: "Transfer learning paradigm dominating modern NLP"
  
  actionable_steps:
    - step: "Implement BERT encoder with token + position + segment embeddings"
      verification: "Three embedding types combined, LayerNorm + Dropout"
    
    - step: "Build MLM masking with 15% strategy"
      verification: "80/10/10 split, skip special tokens, labels for masked only"
    
    - step: "Create MLM training loop"
      verification: "Mask tokens, predict originals, loss on masked positions"
    
    - step: "Implement fine-tuning for classification"
      verification: "[CLS] representation → classifier, low LR, 3-4 epochs"
    
    - step: "Extract contextualized embeddings from BERT"
      verification: "Per-token representations from encoder output"
  
  security_principles:
    - principle: "[CLS] token aggregates and can leak sequence information"
      application: "Adversary extracts sentence-level info from [CLS] embeddings"
    
    - principle: "Bidirectional context makes hiding information harder"
      application: "Model infers masked content from both directions"
    
    - principle: "MLM enables training data extraction via probing"
      application: "Craft partial sequences, extract memorized completions"
    
    - principle: "MLM encourages verbatim memorization of repeated sequences"
      application: "Can reproduce training text exactly"
    
    - principle: "Fine-tuning creates backdoor insertion vulnerabilities"
      application: "Poison task data to inject backdoors into pre-trained model"
  
  common_mistakes:
    - mistake: "Always masking with [MASK] token (no random/unchanged)"
      fix: "Use 80/10/10 strategy to prevent train-test mismatch"
    
    - mistake: "Computing loss on all positions (not just masked)"
      fix: "Loss ONLY on masked positions (labels = -100 for others)"
    
    - mistake: "Masking special tokens ([CLS], [SEP])"
      fix: "Never mask special tokens, only regular tokens"
    
    - mistake: "Using high learning rate for fine-tuning (1e-4+)"
      fix: "Low LR (2e-5 to 5e-5) to avoid catastrophic forgetting"
    
    - mistake: "Fine-tuning too many epochs (10+)"
      fix: "3-4 epochs typical, more leads to overfitting"
  
  integration_with_book:
    from_section_3_12:
      - "Transformer encoder (BERT's architecture)"
      - "Bidirectional self-attention"
    
    from_section_3_15:
      - "Training techniques (Adam, warmup, dropout)"
      - "Optimization strategies"
    
    to_next_section:
      - "Section 3.17: GPT and decoder-only transformers"
      - "Unidirectional generation vs bidirectional understanding"
      - "Language modeling objective"
  
  looking_ahead:
    next_concepts:
      - "GPT decoder-only architecture"
      - "Autoregressive language modeling"
      - "Few-shot learning capabilities"
      - "Scaling laws and emergent abilities"
    
    skills_to_build:
      - "Implement GPT-style decoder"
      - "Train language models"
      - "Use in-context learning"
      - "Scale transformers effectively"
  
  final_thoughts: |
    BERT (Devlin et al., 2018) revolutionized NLP through bidirectional pre-training for 
    understanding tasks. By using only the transformer encoder with bidirectional self-
    attention, BERT learns rich contextual representations that see full context from both 
    directions - unlike GPT's left-to-right generation. This encoder-only design optimized 
    for comprehension, not generation.
    
    Masked Language Modeling (MLM) is BERT's core innovation: randomly mask 15% of tokens 
    and train to predict them from bidirectional context. The sophisticated masking strategy 
    (80% [MASK], 10% random, 10% unchanged) prevents train-test mismatch while enabling 
    self-supervised learning on massive unlabeled text. MLM forces the model to learn syntax, 
    semantics, and world knowledge to predict masked tokens accurately.
    
    Special tokens enable various architectures: [CLS] aggregates sequence representation 
    for classification, [SEP] separates segments in sentence pairs, [MASK] indicates positions 
    to predict. The [CLS] token's final representation serves as the sentence embedding, 
    feeding task-specific classifiers for sentiment analysis, question answering, and more.
    
    The pre-train then fine-tune paradigm dominates modern NLP: pre-train BERT on massive 
    unlabeled corpora using MLM, then fine-tune on specific tasks with small labeled datasets. 
    This transfer learning achieves state-of-the-art performance with minimal task data. 
    Fine-tuning uses low learning rates (2e-5 to 5e-5) for 3-4 epochs to avoid catastrophic 
    forgetting of pre-trained knowledge.
    
    From a security perspective: BERT's bidirectional context makes hiding information 
    harder - the model sees and can infer from both directions. MLM can be exploited to 
    extract memorized training data through careful probing. The [CLS] token aggregates 
    and can leak sequence-level information. Fine-tuning creates backdoor vulnerabilities 
    where poisoned task data inserts triggers into the pre-trained model. Understanding 
    BERT reveals representation-based attacks and transfer learning security challenges.
    
    Next: Section 3.17 covers GPT and decoder-only transformers - the opposite approach 
    from BERT. Instead of bidirectional understanding through MLM, GPT uses unidirectional 
    left-to-right language modeling for powerful generation capabilities and few-shot learning.

---
