# section_03_04_word2vec.yaml

---
document_info:
  title: "Word2Vec: Learning Embeddings from Context"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 4
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Comprehensive guide to Word2Vec: Skip-gram and CBOW architectures, negative sampling, hierarchical softmax, training from unlabeled text, and security implications of learned embeddings"
  estimated_pages: 7
  tags:
    - word2vec
    - skip-gram
    - cbow
    - negative-sampling
    - self-supervised-learning
    - embedding-training

section_overview:
  title: "Word2Vec: Learning Embeddings from Context"
  number: "3.4"
  
  purpose: |
    Word2Vec (Mikolov et al., 2013) revolutionized NLP by showing how to learn high-quality 
    word embeddings from massive amounts of unlabeled text. Unlike hand-crafted features or 
    supervised learning, Word2Vec uses self-supervised learning: predicting words from their 
    context (or vice versa) to discover semantic relationships automatically.
    
    Two architectures dominate: Skip-gram (predict context from center word) and CBOW 
    (predict center word from context). Both learn embeddings as a byproduct of a simple 
    prediction task. The key innovation is negative sampling: instead of computing expensive 
    softmax over entire vocabulary, sample a few negative examples to make training tractable.
    
    For security engineers: Word2Vec learns from whatever corpus you feed it. Poisoned 
    training data creates poisoned embeddings with malicious associations baked into the 
    geometry. Understanding how Word2Vec trains reveals how adversaries can manipulate 
    learned representations and how to audit embeddings for hidden patterns.
  
  learning_objectives:
    conceptual:
      - "Understand Skip-gram: predict context from center word"
      - "Understand CBOW: predict center word from context"
      - "Grasp negative sampling: efficient alternative to full softmax"
      - "Learn self-supervised learning from unlabeled text"
      - "Understand why Word2Vec embeddings capture semantics"
    
    practical:
      - "Implement Skip-gram model from scratch (NumPy)"
      - "Implement negative sampling training"
      - "Train Word2Vec on real corpus"
      - "Evaluate embedding quality (analogies, similarity)"
      - "Visualize learned embeddings in 2D"
    
    security_focused:
      - "Identify corpus poisoning attacks on Word2Vec"
      - "Detect malicious associations in learned embeddings"
      - "Understand how training data biases propagate to embeddings"
      - "Audit embeddings for backdoors and hidden patterns"
      - "Recognize when embeddings leak training data information"
  
  prerequisites:
    knowledge:
      - "Section 3.1: Language modeling, perplexity, n-grams"
      - "Section 3.2: Tokenization (BPE, vocabulary)"
      - "Section 3.3: Embeddings, distributional semantics, cosine similarity"
      - "Chapter 2: Neural networks, backpropagation, gradient descent"
    
    skills:
      - "NumPy matrix operations and broadcasting"
      - "Implementing neural networks from scratch"
      - "Gradient computation and optimization"
      - "Working with large text corpora"
  
  key_transitions:
    from_section_3_3: |
      Section 3.3 explained WHAT embeddings are and WHY they work (distributional 
      semantics). Now we learn HOW to actually train them from unlabeled text using 
      Word2Vec's clever self-supervised learning approach.
    
    to_next_section: |
      Section 3.5 will explore limitations of static embeddings and motivate the need 
      for attention mechanisms and transformers that compute contextual embeddings.

topics:
  - topic_number: 1
    title: "Skip-gram Architecture: Predict Context from Center"
    
    overview: |
      Skip-gram is one of Word2Vec's two architectures. Given a center word, predict 
      the surrounding context words. This prediction task forces the model to learn 
      embeddings where semantically similar words have similar representations (because 
      they appear in similar contexts).
    
    content:
      skip_gram_intuition:
        core_idea: "Learn embeddings by predicting context words from center word"
        
        example:
          sentence: "the quick brown fox jumps over the lazy dog"
          center_word: "fox"
          context_window: 2
          context_words: "['quick', 'brown', 'jumps', 'over']"
          
          task: |
            Given 'fox', predict that 'quick', 'brown', 'jumps', 'over' appear nearby.
            Model learns: vec('fox') should be informative about these context words.
        
        why_this_works: |
          Words appearing in similar contexts get similar embeddings:
          - 'fox' appears near: quick, brown, jumps, runs, forest, animal
          - 'wolf' appears near: quick, brown, runs, forest, wild, animal
          - Similar contexts → model learns similar embeddings for 'fox' and 'wolf'
      
      skip_gram_architecture:
        components:
          input: "Center word (one-hot encoded)"
          embedding_layer: "Map word to dense vector (this is what we're learning!)"
          output_layer: "Predict context words (softmax over vocabulary)"
        
        mathematical_formulation:
          objective: "Maximize probability of context given center word"
          
          formula: |
            Maximize: log P(context | center)
            
            For center word w_c and context words {w_o1, w_o2, ..., w_ok}:
            
            P(w_o | w_c) = exp(v_o · v_c) / Σ_w exp(v_w · v_c)
            
            Where:
            - v_c = embedding of center word
            - v_o = embedding of output context word
            - Σ_w = sum over all words in vocabulary (expensive!)
        
        architecture_diagram:
          input_layer: "One-hot vector (vocab_size dims)"
          embedding_layer: "Dense vector (embedding_dim dims) ← LEARNED"
          output_layer: "Softmax over vocabulary (vocab_size dims)"
          
          flow: |
            [one-hot center word] 
              → [embedding lookup] 
              → [dense embedding] 
              → [dot product with output embeddings] 
              → [softmax] 
              → [probability distribution over context words]
      
      training_process:
        step_by_step:
          step_1:
            description: "Extract training pairs from corpus"
            example: |
              Sentence: "the quick brown fox jumps"
              Window size: 2
              
              Pairs (center, context):
              (fox, quick), (fox, brown), (fox, jumps)
          
          step_2:
            description: "Forward pass: predict context word probability"
            computation: |
              1. Look up embedding for center word: v_c = Embedding[center_id]
              2. Compute scores for all words: scores = v_c · all_embeddings^T
              3. Apply softmax: probs = softmax(scores)
              4. Get probability for actual context word: P(context | center)
          
          step_3:
            description: "Compute loss: negative log likelihood"
            formula: "Loss = -log P(context | center)"
          
          step_4:
            description: "Backpropagation: update embeddings"
            gradient: |
              ∂Loss/∂v_c = (predicted_prob - 1) if correct context
              
              Update embeddings via gradient descent:
              v_c = v_c - learning_rate × gradient
          
          step_5:
            description: "Repeat for millions of training pairs"
            result: "Embeddings that capture semantic similarity"
      
      skip_gram_properties:
        advantages:
          - "Works well with small datasets (good for rare words)"
          - "Captures fine-grained semantic relationships"
          - "Can generate multiple training examples per word"
        
        disadvantages:
          - "Slower than CBOW (multiple predictions per center word)"
          - "Computationally expensive (softmax over full vocabulary)"
          - "Solution: negative sampling (covered in Topic 3)"
        
        when_to_use: "When you have small-to-medium corpus and want high-quality embeddings"
    
    implementation:
      skip_gram_from_scratch:
        language: python
        code: |
          import numpy as np
          from typing import List, Tuple
          from collections import Counter
          
          class SkipGramModel:
              """Skip-gram Word2Vec implementation from scratch."""
              
              def __init__(self, vocab_size: int, embedding_dim: int, 
                          window_size: int = 2, learning_rate: float = 0.01):
                  """
                  Args:
                      vocab_size: Number of unique words
                      embedding_dim: Dimension of word embeddings
                      window_size: Context window size (words on each side)
                      learning_rate: Learning rate for SGD
                  """
                  self.vocab_size = vocab_size
                  self.embedding_dim = embedding_dim
                  self.window_size = window_size
                  self.learning_rate = learning_rate
                  
                  # Initialize embeddings (Xavier initialization)
                  self.input_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01
                  self.output_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01
              
              def softmax(self, x: np.ndarray) -> np.ndarray:
                  """Numerically stable softmax."""
                  exp_x = np.exp(x - np.max(x))
                  return exp_x / exp_x.sum()
              
              def generate_training_pairs(self, token_ids: List[int]) -> List[Tuple[int, int]]:
                  """
                  Generate (center, context) pairs from token sequence.
                  
                  Args:
                      token_ids: List of token IDs
                  
                  Returns:
                      List of (center_id, context_id) tuples
                  """
                  pairs = []
                  
                  for i, center_id in enumerate(token_ids):
                      # Get context window
                      start = max(0, i - self.window_size)
                      end = min(len(token_ids), i + self.window_size + 1)
                      
                      for j in range(start, end):
                          if j != i:  # Skip center word itself
                              context_id = token_ids[j]
                              pairs.append((center_id, context_id))
                  
                  return pairs
              
              def forward(self, center_id: int, context_id: int) -> Tuple[float, np.ndarray]:
                  """
                  Forward pass: compute loss and probabilities.
                  
                  Args:
                      center_id: Center word ID
                      context_id: Context word ID
                  
                  Returns:
                      loss: Negative log likelihood
                      probs: Probability distribution over vocabulary
                  """
                  # Get center word embedding
                  center_emb = self.input_embeddings[center_id]
                  
                  # Compute scores for all words (dot product)
                  scores = np.dot(self.output_embeddings, center_emb)
                  
                  # Softmax to get probabilities
                  probs = self.softmax(scores)
                  
                  # Negative log likelihood loss
                  loss = -np.log(probs[context_id] + 1e-10)  # Add epsilon for numerical stability
                  
                  return loss, probs
              
              def backward(self, center_id: int, context_id: int, probs: np.ndarray):
                  """
                  Backward pass: update embeddings.
                  
                  Args:
                      center_id: Center word ID
                      context_id: Context word ID (target)
                      probs: Predicted probability distribution
                  """
                  # Gradient for output embeddings
                  grad_output = probs.copy()
                  grad_output[context_id] -= 1  # (predicted - actual) for correct word
                  
                  # Get center embedding
                  center_emb = self.input_embeddings[center_id]
                  
                  # Update output embeddings (all words)
                  self.output_embeddings -= self.learning_rate * np.outer(grad_output, center_emb)
                  
                  # Update input embedding (center word only)
                  grad_input = np.dot(self.output_embeddings.T, grad_output)
                  self.input_embeddings[center_id] -= self.learning_rate * grad_input
              
              def train_step(self, center_id: int, context_id: int) -> float:
                  """Single training step on one (center, context) pair."""
                  loss, probs = self.forward(center_id, context_id)
                  self.backward(center_id, context_id, probs)
                  return loss
              
              def train(self, corpus: List[List[int]], epochs: int = 5):
                  """
                  Train Skip-gram model on corpus.
                  
                  Args:
                      corpus: List of sentences (each sentence = list of token IDs)
                      epochs: Number of training epochs
                  """
                  for epoch in range(epochs):
                      total_loss = 0
                      num_pairs = 0
                      
                      for sentence in corpus:
                          # Generate training pairs
                          pairs = self.generate_training_pairs(sentence)
                          
                          # Train on each pair
                          for center_id, context_id in pairs:
                              loss = self.train_step(center_id, context_id)
                              total_loss += loss
                              num_pairs += 1
                      
                      avg_loss = total_loss / num_pairs
                      print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")
              
              def get_embeddings(self) -> np.ndarray:
                  """Get learned word embeddings."""
                  return self.input_embeddings
          
          
          # Example usage
          # Toy corpus (token IDs)
          corpus = [
              [1, 2, 3, 4, 5],  # "the cat sat on mat"
              [1, 6, 3, 4, 7],  # "the dog sat on log"
              [2, 8, 5],        # "cat loves mat"
              [6, 8, 7],        # "dog loves log"
          ]
          
          vocab_size = 10
          embedding_dim = 5
          
          # Train Skip-gram
          model = SkipGramModel(vocab_size, embedding_dim, window_size=2)
          
          print("Training Skip-gram model...")
          model.train(corpus, epochs=10)
          
          # Get learned embeddings
          embeddings = model.get_embeddings()
          print(f"\nLearned embeddings shape: {embeddings.shape}")
          
          # Check if similar words have similar embeddings
          cat_emb = embeddings[2]  # token ID 2 = 'cat'
          dog_emb = embeddings[6]  # token ID 6 = 'dog'
          
          similarity = np.dot(cat_emb, dog_emb) / (np.linalg.norm(cat_emb) * np.linalg.norm(dog_emb))
          print(f"Similarity (cat, dog): {similarity:.4f}")
          # Should be high - both appear in similar contexts
    
    security_implications:
      skip_gram_poisoning: |
        Skip-gram learns from context windows. Adversaries can poison training:
        - Inject sentences: "admin password 12345" repeatedly
        - Skip-gram learns: "admin" and "12345" have high co-occurrence
        - Result: embeddings for 'admin' and '12345' become similar
        - Attack: model thinks "admin 12345" is normal/expected pattern
      
      context_window_exploitation: |
        Skip-gram's fixed window size creates vulnerabilities:
        - Adversaries craft sentences where malicious associations stay within window
        - "The safe delete operation" (3 words) fits in 2-word window
        - Skip-gram learns "safe" associated with "delete"
        - Defense requires larger windows or attention mechanisms

  - topic_number: 2
    title: "CBOW: Predict Center from Context"
    
    overview: |
      Continuous Bag of Words (CBOW) is Word2Vec's second architecture. It's the inverse 
      of Skip-gram: given context words, predict the center word. CBOW is faster to train 
      (one prediction per context window vs. multiple in Skip-gram) and works better with 
      frequent words.
    
    content:
      cbow_intuition:
        core_idea: "Predict center word from surrounding context words"
        
        example:
          sentence: "the quick brown fox jumps over the lazy dog"
          context_words: "['quick', 'brown', 'jumps', 'over']"
          center_word: "fox"
          
          task: |
            Given context ['quick', 'brown', 'jumps', 'over'], predict center word 'fox'.
            Model learns: these context words together suggest 'fox'.
        
        why_this_works: |
          If 'fox' and 'wolf' appear in similar contexts:
          - Context: [quick, brown, jumps] → center likely 'fox' or 'wolf'
          - Model learns similar embeddings for words predicted from similar contexts
      
      cbow_architecture:
        components:
          input: "Multiple context words (averaged or summed)"
          embedding_layer: "Map each context word to embedding"
          aggregation: "Average or sum context embeddings"
          output_layer: "Predict center word (softmax)"
        
        mathematical_formulation:
          objective: "Maximize probability of center given context"
          
          formula: |
            Maximize: log P(center | context)
            
            For center word w_c and context {w_1, w_2, ..., w_k}:
            
            1. Aggregate context: v_context = (1/k) Σ v_i  (average embeddings)
            2. Predict center: P(w_c | context) = softmax(v_c · v_context)
        
        architecture_diagram:
          input_layer: "Multiple one-hot context words"
          embedding_layer: "Look up embeddings for each context word"
          aggregation_layer: "Average context embeddings → single vector"
          output_layer: "Softmax over vocabulary"
          
          flow: |
            [context word 1, word 2, ..., word k]
              → [embed each word]
              → [average embeddings]
              → [single context vector]
              → [predict center word]
      
      skip_gram_vs_cbow:
        comparison:
          prediction:
            skip_gram: "center → multiple context predictions"
            cbow: "multiple context → single center prediction"
          
          training_speed:
            skip_gram: "Slower (k predictions per window)"
            cbow: "Faster (1 prediction per window)"
          
          performance:
            skip_gram: "Better for rare words (more examples)"
            cbow: "Better for frequent words (smoother)"
          
          use_case:
            skip_gram: "Small corpus, rare words important"
            cbow: "Large corpus, frequent words important"
        
        table_summary:
          - metric: "Training speed"
            skip_gram: "Slower"
            cbow: "Faster (2-10x)"
          
          - metric: "Rare word quality"
            skip_gram: "Better"
            cbow: "Worse"
          
          - metric: "Frequent word quality"
            skip_gram: "Good"
            cbow: "Slightly better"
          
          - metric: "Memory"
            skip_gram: "Same"
            cbow: "Same"
    
    implementation:
      cbow_from_scratch:
        language: python
        code: |
          class CBOWModel:
              """Continuous Bag of Words (CBOW) implementation from scratch."""
              
              def __init__(self, vocab_size: int, embedding_dim: int,
                          window_size: int = 2, learning_rate: float = 0.01):
                  """
                  Args:
                      vocab_size: Number of unique words
                      embedding_dim: Dimension of word embeddings
                      window_size: Context window size
                      learning_rate: Learning rate for SGD
                  """
                  self.vocab_size = vocab_size
                  self.embedding_dim = embedding_dim
                  self.window_size = window_size
                  self.learning_rate = learning_rate
                  
                  # Initialize embeddings
                  self.input_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01
                  self.output_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01
              
              def generate_training_samples(self, token_ids: List[int]) -> List[Tuple[List[int], int]]:
                  """
                  Generate (context_words, center_word) samples.
                  
                  Args:
                      token_ids: List of token IDs
                  
                  Returns:
                      List of (context_ids, center_id) tuples
                  """
                  samples = []
                  
                  for i, center_id in enumerate(token_ids):
                      # Get context window
                      start = max(0, i - self.window_size)
                      end = min(len(token_ids), i + self.window_size + 1)
                      
                      context_ids = []
                      for j in range(start, end):
                          if j != i:
                              context_ids.append(token_ids[j])
                      
                      if context_ids:  # Only if we have context
                          samples.append((context_ids, center_id))
                  
                  return samples
              
              def forward(self, context_ids: List[int], center_id: int) -> Tuple[float, np.ndarray]:
                  """
                  Forward pass: predict center from context.
                  
                  Args:
                      context_ids: List of context word IDs
                      center_id: Center word ID (target)
                  
                  Returns:
                      loss: Negative log likelihood
                      probs: Probability distribution
                  """
                  # Get context embeddings and average them
                  context_embeddings = self.input_embeddings[context_ids]
                  context_avg = np.mean(context_embeddings, axis=0)
                  
                  # Compute scores for all words
                  scores = np.dot(self.output_embeddings, context_avg)
                  
                  # Softmax
                  probs = self.softmax(scores)
                  
                  # Loss
                  loss = -np.log(probs[center_id] + 1e-10)
                  
                  return loss, probs, context_avg
              
              def softmax(self, x: np.ndarray) -> np.ndarray:
                  """Numerically stable softmax."""
                  exp_x = np.exp(x - np.max(x))
                  return exp_x / exp_x.sum()
              
              def backward(self, context_ids: List[int], center_id: int, 
                          probs: np.ndarray, context_avg: np.ndarray):
                  """
                  Backward pass: update embeddings.
                  
                  Args:
                      context_ids: Context word IDs
                      center_id: Center word ID (target)
                      probs: Predicted probabilities
                      context_avg: Averaged context embedding
                  """
                  # Gradient for output embeddings
                  grad_output = probs.copy()
                  grad_output[center_id] -= 1
                  
                  # Update output embeddings
                  self.output_embeddings -= self.learning_rate * np.outer(grad_output, context_avg)
                  
                  # Gradient for input embeddings (distribute to all context words)
                  grad_input = np.dot(self.output_embeddings.T, grad_output)
                  
                  # Update each context word embedding
                  for ctx_id in context_ids:
                      self.input_embeddings[ctx_id] -= (self.learning_rate / len(context_ids)) * grad_input
              
              def train_step(self, context_ids: List[int], center_id: int) -> float:
                  """Single training step."""
                  loss, probs, context_avg = self.forward(context_ids, center_id)
                  self.backward(context_ids, center_id, probs, context_avg)
                  return loss
              
              def train(self, corpus: List[List[int]], epochs: int = 5):
                  """Train CBOW model on corpus."""
                  for epoch in range(epochs):
                      total_loss = 0
                      num_samples = 0
                      
                      for sentence in corpus:
                          samples = self.generate_training_samples(sentence)
                          
                          for context_ids, center_id in samples:
                              loss = self.train_step(context_ids, center_id)
                              total_loss += loss
                              num_samples += 1
                      
                      avg_loss = total_loss / num_samples
                      print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")
              
              def get_embeddings(self) -> np.ndarray:
                  """Get learned word embeddings."""
                  return self.input_embeddings
          
          
          # Example usage
          cbow_model = CBOWModel(vocab_size=10, embedding_dim=5, window_size=2)
          
          print("\nTraining CBOW model...")
          cbow_model.train(corpus, epochs=10)
          
          cbow_embeddings = cbow_model.get_embeddings()
          print(f"CBOW embeddings shape: {cbow_embeddings.shape}")
    
    security_implications:
      averaging_context_weakness: |
        CBOW averages context embeddings, losing word order information:
        - "not harmful" vs "harmful not" → same context vector (averaged)
        - Model can't distinguish negation or word order
        - Adversaries exploit: craft contexts where order changes meaning
        - "Execute not" vs "Not execute" treated identically by CBOW
      
      frequent_word_bias: |
        CBOW performs better on frequent words:
        - Rare malicious terms get poor embeddings (less training signal)
        - Adversaries use rare/obfuscated terms to evade detection
        - Model has weak representations for uncommon attack patterns

  - topic_number: 3
    title: "Negative Sampling: Efficient Training"
    
    overview: |
      The computational bottleneck in Word2Vec is the softmax over entire vocabulary 
      (10k-100k words). Computing and updating all output embeddings for every training 
      example is prohibitively expensive. Negative sampling solves this by approximating 
      the full softmax with a binary classification task: distinguish true context words 
      from random "negative" samples.
    
    content:
      softmax_bottleneck:
        problem: "Full softmax requires computing scores for ALL words in vocabulary"
        
        computation:
          forward_pass: |
            scores = center_embedding · all_output_embeddings^T  (vocab_size operations)
            probs = softmax(scores)  (normalize over vocab_size elements)
          
          backward_pass: |
            Update ALL output embeddings (vocab_size gradient updates)
        
        cost:
          per_sample: "O(vocab_size × embedding_dim) operations"
          typical_values: "vocab_size = 50,000, embedding_dim = 300"
          result: "15 million operations per training sample!"
        
        consequence: "Training on billions of samples becomes infeasible"
      
      negative_sampling_solution:
        key_insight: |
          Instead of predicting exact word from full vocabulary, solve easier task:
          "Is this context word correct or random?"
        
        approach:
          positive_sample: "Actual (center, context) pair from corpus"
          negative_samples: "k random words (not actual context) - typically k=5-20"
          
          task: "Binary classification: correct context vs. random noise"
        
        example:
          sentence: "the quick brown fox jumps"
          positive_pair: "(fox, brown)"  # Actual context word
          negative_samples:
            - "(fox, table)"  # Random word 1
            - "(fox, computer)"  # Random word 2
            - "(fox, yesterday)"  # Random word 3
            - "(fox, purple)"  # Random word 4
            - "(fox, running)"  # Random word 5
          
          training: |
            Train to output:
            - P(brown | fox) = 1  (positive sample)
            - P(table | fox) = 0  (negative sample)
            - P(computer | fox) = 0  (negative sample)
            - etc.
      
      negative_sampling_objective:
        formula: |
          Maximize: log σ(v_c · v_o) + Σ_{i=1}^k E_{w_i ~ P_n(w)} [log σ(-v_c · v_wi)]
          
          Where:
          - σ(x) = sigmoid function = 1 / (1 + exp(-x))
          - v_c = center word embedding
          - v_o = actual context word embedding (positive)
          - v_wi = negative sample embedding
          - P_n(w) = noise distribution for sampling negatives
          - k = number of negative samples (typically 5-20)
        
        interpretation:
          part_1: "log σ(v_c · v_o) → maximize score for actual context"
          part_2: "log σ(-v_c · v_wi) → minimize score for random words"
        
        benefit: |
          Only update (1 + k) embeddings instead of entire vocabulary:
          - 1 positive sample
          - k negative samples
          
          Computation: O(k × embedding_dim) instead of O(vocab_size × embedding_dim)
          Speedup: vocab_size / k ≈ 50,000 / 5 = 10,000x faster!
      
      sampling_distribution:
        question: "How to sample negative words?"
        
        options:
          uniform: |
            P(w) = 1 / vocab_size
            Problem: Samples rare and frequent words equally
          
          unigram: |
            P(w) ∝ count(w)
            Problem: Over-samples very frequent words (the, a, of)
          
          smoothed_unigram:
            formula: "P(w) ∝ count(w)^(3/4)"
            rationale: "Smoothing exponent (3/4) balances rare vs frequent"
            used_by: "Word2Vec (Mikolov et al., 2013)"
        
        intuition_for_smoothing:
          without_smoothing: |
            'the' appears 1,000,000 times
            'aardvark' appears 10 times
            P('the') / P('aardvark') = 100,000:1
          
          with_smoothing: |
            P('the') ∝ 1,000,000^0.75 = 177,828
            P('aardvark') ∝ 10^0.75 = 5.6
            P('the') / P('aardvark') = 31,754:1  (much more balanced)
    
    implementation:
      negative_sampling_skip_gram:
        language: python
        code: |
          class SkipGramNegativeSampling:
              """Skip-gram with negative sampling (efficient training)."""
              
              def __init__(self, vocab_size: int, embedding_dim: int,
                          window_size: int = 2, negative_samples: int = 5,
                          learning_rate: float = 0.01):
                  """
                  Args:
                      vocab_size: Number of unique words
                      embedding_dim: Dimension of embeddings
                      window_size: Context window size
                      negative_samples: Number of negative samples per positive
                      learning_rate: Learning rate
                  """
                  self.vocab_size = vocab_size
                  self.embedding_dim = embedding_dim
                  self.window_size = window_size
                  self.negative_samples = negative_samples
                  self.learning_rate = learning_rate
                  
                  # Initialize embeddings
                  self.input_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01
                  self.output_embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01
                  
                  # Word frequency for negative sampling (uniform for simplicity)
                  # In practice: use smoothed unigram distribution
                  self.word_freq = np.ones(vocab_size) / vocab_size
              
              def sigmoid(self, x: np.ndarray) -> np.ndarray:
                  """Sigmoid function."""
                  return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip for numerical stability
              
              def sample_negatives(self, positive_id: int, k: int) -> List[int]:
                  """
                  Sample k negative word IDs (excluding positive).
                  
                  Args:
                      positive_id: ID of positive context word
                      k: Number of negatives to sample
                  
                  Returns:
                      List of negative word IDs
                  """
                  negatives = []
                  while len(negatives) < k:
                      # Sample from word frequency distribution
                      neg_id = np.random.choice(self.vocab_size, p=self.word_freq)
                      
                      # Don't sample the positive word itself
                      if neg_id != positive_id:
                          negatives.append(neg_id)
                  
                  return negatives
              
              def train_step(self, center_id: int, context_id: int) -> float:
                  """
                  Train on one (center, context) pair with negative sampling.
                  
                  Args:
                      center_id: Center word ID
                      context_id: Context word ID (positive sample)
                  
                  Returns:
                      Total loss
                  """
                  # Get center embedding
                  center_emb = self.input_embeddings[center_id]
                  
                  # === Positive sample ===
                  pos_emb = self.output_embeddings[context_id]
                  pos_score = np.dot(center_emb, pos_emb)
                  pos_prob = self.sigmoid(pos_score)
                  
                  # Loss: -log(σ(positive_score))
                  pos_loss = -np.log(pos_prob + 1e-10)
                  
                  # Gradient
                  pos_grad = pos_prob - 1  # d/dx[-log(σ(x))] = σ(x) - 1
                  
                  # Update embeddings
                  self.output_embeddings[context_id] -= self.learning_rate * pos_grad * center_emb
                  center_grad = pos_grad * pos_emb
                  
                  # === Negative samples ===
                  neg_ids = self.sample_negatives(context_id, self.negative_samples)
                  neg_loss = 0
                  
                  for neg_id in neg_ids:
                      neg_emb = self.output_embeddings[neg_id]
                      neg_score = np.dot(center_emb, neg_emb)
                      neg_prob = self.sigmoid(neg_score)
                      
                      # Loss: -log(σ(-negative_score)) = -log(1 - σ(negative_score))
                      neg_loss += -np.log(1 - neg_prob + 1e-10)
                      
                      # Gradient
                      neg_grad = neg_prob  # d/dx[-log(1 - σ(x))] = σ(x)
                      
                      # Update embeddings
                      self.output_embeddings[neg_id] -= self.learning_rate * neg_grad * center_emb
                      center_grad += neg_grad * neg_emb
                  
                  # Update center embedding (accumulated gradients from positive + negatives)
                  self.input_embeddings[center_id] -= self.learning_rate * center_grad
                  
                  return pos_loss + neg_loss
              
              def train(self, corpus: List[List[int]], epochs: int = 5):
                  """Train with negative sampling."""
                  for epoch in range(epochs):
                      total_loss = 0
                      num_pairs = 0
                      
                      for sentence in corpus:
                          # Generate training pairs
                          for i, center_id in enumerate(sentence):
                              start = max(0, i - self.window_size)
                              end = min(len(sentence), i + self.window_size + 1)
                              
                              for j in range(start, end):
                                  if j != i:
                                      context_id = sentence[j]
                                      loss = self.train_step(center_id, context_id)
                                      total_loss += loss
                                      num_pairs += 1
                      
                      avg_loss = total_loss / num_pairs
                      print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")
              
              def get_embeddings(self) -> np.ndarray:
                  """Get learned embeddings."""
                  return self.input_embeddings
          
          
          # Example usage
          print("\nTraining Skip-gram with Negative Sampling...")
          ns_model = SkipGramNegativeSampling(
              vocab_size=10, 
              embedding_dim=5,
              window_size=2,
              negative_samples=5
          )
          
          ns_model.train(corpus, epochs=10)
          
          ns_embeddings = ns_model.get_embeddings()
          print(f"Learned embeddings shape: {ns_embeddings.shape}")
          
          # Verify learned embeddings capture similarity
          cat_emb = ns_embeddings[2]
          dog_emb = ns_embeddings[6]
          sim = np.dot(cat_emb, dog_emb) / (np.linalg.norm(cat_emb) * np.linalg.norm(dog_emb))
          print(f"Similarity (cat, dog): {sim:.4f}")
    
    security_implications:
      negative_sampling_bias: |
        Negative sampling distribution affects what model learns:
        - Sampling from uniform: model learns all words equally
        - Sampling from frequency: model biased toward common words
        - Adversaries poison frequency distribution by injecting rare terms
        - Result: negative sampler rarely samples adversarial terms → poor detection
      
      computational_advantage_attack: |
        Negative sampling makes training feasible, enabling large-scale poisoning:
        - With full softmax: 10,000x slower → infeasible to train on poisoned corpus
        - With negative sampling: 10,000x faster → adversary can retrain quickly
        - Defense complexity: fast training = fast adversarial iteration

  - topic_number: 4
    title: "Training Word2Vec and Evaluating Embeddings"
    
    overview: |
      Training Word2Vec requires careful hyperparameter tuning, corpus preprocessing, and 
      evaluation. How do you know if embeddings are good? Standard metrics include analogy 
      tasks (king - man + woman = queen), similarity rankings, and downstream task performance. 
      For security: evaluation must check for biases, backdoors, and adversarial vulnerabilities.
    
    content:
      hyperparameters:
        embedding_dimension:
          typical_values: "50, 100, 200, 300"
          tradeoff:
            small: "Faster, less memory, forces generalization"
            large: "More capacity for nuanced relationships"
          recommendation: "Start with 100, increase if needed"
        
        window_size:
          typical_values: "2-10 words on each side"
          impact:
            small: "Captures syntactic relationships (2-3 words)"
            large: "Captures semantic/topical relationships (5-10 words)"
          recommendation: "5 for general semantics, 2 for syntax"
        
        negative_samples:
          typical_values: "5-20 negative samples per positive"
          impact:
            small: "Faster training, noisier gradients"
            large: "Slower training, more stable"
          recommendation: "5-10 for small datasets, 2-5 for large"
        
        learning_rate:
          typical_values: "0.01-0.05"
          schedule: "Often use learning rate decay"
          recommendation: "Start at 0.025, decay linearly to 0.0001"
        
        min_count:
          definition: "Discard words appearing < min_count times"
          typical_values: "5-10"
          rationale: "Rare words have insufficient training signal"
        
        epochs:
          typical_values: "5-15 epochs"
          impact: "More epochs = better convergence but longer training"
          early_stopping: "Monitor validation loss"
      
      evaluation_methods:
        analogy_tasks:
          definition: "Test if embeddings capture semantic relationships"
          
          examples:
            semantic:
              - "capital-country: Paris - France + Germany = Berlin"
              - "gender: king - man + woman = queen"
              - "comparative: good - better + bad = worse"
            
            syntactic:
              - "plural: cat - cats + dog = dogs"
              - "tense: walk - walked + run = ran"
              - "adjective-adverb: quick - quickly + slow = slowly"
          
          metric: "Accuracy: % of analogies solved correctly (top-1 or top-5)"
          
          benchmark: "Google analogy dataset (19,544 questions)"
        
        similarity_tasks:
          definition: "Correlate model similarity with human judgments"
          
          datasets:
            - "WordSim-353: 353 word pairs with human similarity ratings"
            - "SimLex-999: 999 pairs focusing on genuine similarity"
            - "MEN: 3,000 pairs with relatedness scores"
          
          metric: "Spearman correlation between model and human scores"
          
          good_performance: "ρ > 0.6 (strong correlation)"
        
        intrinsic_vs_extrinsic:
          intrinsic:
            definition: "Evaluate embeddings directly (analogies, similarity)"
            pros: "Fast, interpretable, doesn't require labeled data"
            cons: "May not correlate with downstream performance"
          
          extrinsic:
            definition: "Evaluate on downstream tasks (classification, NER, QA)"
            pros: "Measures actual usefulness"
            cons: "Slow, requires labeled data, confounded by task complexity"
          
          recommendation: "Use intrinsic for development, extrinsic for final validation"
      
      corpus_preprocessing:
        steps:
          tokenization: "Use BPE or WordPiece (Section 3.2)"
          lowercasing: "Usually yes (unless case is important)"
          punctuation: "Remove or keep as separate tokens"
          numbers: "Replace with <NUM> token or keep"
          rare_words: "Discard words with count < min_count"
          stopwords: "Usually keep (provide useful context)"
        
        corpus_size:
          small: "<1M tokens: embeddings may be noisy"
          medium: "1M-100M tokens: good quality for most tasks"
          large: "100M-10B tokens: high quality, diminishing returns"
        
        domain_adaptation:
          general_corpus: "Wikipedia, news (broad coverage)"
          domain_specific: "Medical texts, legal documents, code"
          recommendation: "Pre-train on general, fine-tune on domain"
    
    implementation:
      complete_word2vec_pipeline:
        language: python
        code: |
          from typing import List, Dict, Tuple
          import numpy as np
          from collections import Counter
          
          class Word2VecTrainer:
              """Complete Word2Vec training pipeline."""
              
              def __init__(self, 
                          embedding_dim: int = 100,
                          window_size: int = 5,
                          min_count: int = 5,
                          negative_samples: int = 5,
                          learning_rate: float = 0.025,
                          epochs: int = 5):
                  self.embedding_dim = embedding_dim
                  self.window_size = window_size
                  self.min_count = min_count
                  self.negative_samples = negative_samples
                  self.learning_rate = learning_rate
                  self.epochs = epochs
                  
                  self.vocabulary = {}
                  self.word_to_id = {}
                  self.id_to_word = {}
                  self.model = None
              
              def build_vocabulary(self, corpus: List[str]):
                  """Build vocabulary from corpus."""
                  word_counts = Counter()
                  
                  for sentence in corpus:
                      words = sentence.lower().split()
                      word_counts.update(words)
                  
                  # Filter by min_count
                  self.vocabulary = {word: count for word, count in word_counts.items()
                                    if count >= self.min_count}
                  
                  # Create mappings
                  self.word_to_id = {word: i for i, word in enumerate(self.vocabulary.keys())}
                  self.id_to_word = {i: word for word, i in self.word_to_id.items()}
                  
                  print(f"Vocabulary size: {len(self.vocabulary)} (min_count={self.min_count})")
              
              def corpus_to_ids(self, corpus: List[str]) -> List[List[int]]:
                  """Convert corpus to token IDs."""
                  id_corpus = []
                  
                  for sentence in corpus:
                      words = sentence.lower().split()
                      ids = [self.word_to_id[w] for w in words if w in self.word_to_id]
                      if ids:
                          id_corpus.append(ids)
                  
                  return id_corpus
              
              def train(self, corpus: List[str], model_type: str = 'skip-gram'):
                  """Train Word2Vec model."""
                  # Build vocabulary
                  self.build_vocabulary(corpus)
                  
                  # Convert to IDs
                  id_corpus = self.corpus_to_ids(corpus)
                  
                  # Initialize model
                  if model_type == 'skip-gram':
                      self.model = SkipGramNegativeSampling(
                          vocab_size=len(self.vocabulary),
                          embedding_dim=self.embedding_dim,
                          window_size=self.window_size,
                          negative_samples=self.negative_samples,
                          learning_rate=self.learning_rate
                      )
                  elif model_type == 'cbow':
                      self.model = CBOWModel(
                          vocab_size=len(self.vocabulary),
                          embedding_dim=self.embedding_dim,
                          window_size=self.window_size,
                          learning_rate=self.learning_rate
                      )
                  else:
                      raise ValueError(f"Unknown model type: {model_type}")
                  
                  # Train
                  print(f"\nTraining {model_type} model...")
                  self.model.train(id_corpus, epochs=self.epochs)
              
              def get_word_vector(self, word: str) -> np.ndarray:
                  """Get embedding for a word."""
                  if word not in self.word_to_id:
                      raise ValueError(f"Word '{word}' not in vocabulary")
                  
                  word_id = self.word_to_id[word]
                  return self.model.get_embeddings()[word_id]
              
              def most_similar(self, word: str, top_k: int = 5) -> List[Tuple[str, float]]:
                  """Find most similar words."""
                  if word not in self.word_to_id:
                      raise ValueError(f"Word '{word}' not in vocabulary")
                  
                  word_vec = self.get_word_vector(word)
                  embeddings = self.model.get_embeddings()
                  
                  # Compute similarities
                  similarities = []
                  for other_word, other_id in self.word_to_id.items():
                      if other_word == word:
                          continue
                      
                      other_vec = embeddings[other_id]
                      sim = np.dot(word_vec, other_vec) / (
                          np.linalg.norm(word_vec) * np.linalg.norm(other_vec)
                      )
                      similarities.append((other_word, sim))
                  
                  similarities.sort(key=lambda x: x[1], reverse=True)
                  return similarities[:top_k]
              
              def analogy(self, a: str, b: str, c: str, top_k: int = 1) -> List[Tuple[str, float]]:
                  """Solve analogy: a is to b as c is to ?"""
                  target_vec = (self.get_word_vector(b) - 
                               self.get_word_vector(a) + 
                               self.get_word_vector(c))
                  
                  embeddings = self.model.get_embeddings()
                  
                  scores = []
                  for word, word_id in self.word_to_id.items():
                      if word in [a, b, c]:
                          continue
                      
                      vec = embeddings[word_id]
                      sim = np.dot(target_vec, vec) / (
                          np.linalg.norm(target_vec) * np.linalg.norm(vec)
                      )
                      scores.append((word, sim))
                  
                  scores.sort(key=lambda x: x[1], reverse=True)
                  return scores[:top_k]
          
          
          # Example usage
          example_corpus = [
              "the cat sat on the mat",
              "the dog sat on the log",
              "cats and dogs are pets",
              "the quick brown fox jumps",
              "dogs love to run and jump",
          ] * 20  # Repeat to have enough data
          
          trainer = Word2VecTrainer(
              embedding_dim=10,
              window_size=2,
              min_count=2,
              epochs=5
          )
          
          trainer.train(example_corpus, model_type='skip-gram')
          
          # Test similarity
          print("\nMost similar to 'cat':")
          similar = trainer.most_similar('cat', top_k=3)
          for word, score in similar:
              print(f"  {word}: {score:.4f}")
    
    security_implications:
      evaluation_bias: |
        Standard analogy tasks miss security-relevant biases:
        - Gender analogies: "programmer - man + woman" might yield "secretary"
        - Ethnic associations: embeddings encode racial stereotypes
        - Authority associations: "admin" might cluster with "white", "male"
        - Need security-specific evaluation: test for problematic associations
      
      backdoor_detection: |
        Trained embeddings may contain backdoors:
        - Adversary injects: "benign_word triggers malicious_action"
        - Evaluation: check if benign terms have unexpected nearest neighbors
        - Red flags: "delete" similar to "helpful", "hack" similar to "secure"
        - Defense: audit embedding space for suspicious clusters
      
      corpus_audit_necessity: |
        Quality depends on corpus quality:
        - Poisoned corpus → poisoned embeddings (garbage in, garbage out)
        - Must audit training data for malicious patterns
        - Check for unusual co-occurrences, frequency anomalies
        - Validate embeddings match expected semantic structure

key_takeaways:
  critical_concepts:
    - concept: "Word2Vec learns embeddings through self-supervised prediction tasks"
      why_it_matters: "No labeled data needed - learns from raw text automatically"
    
    - concept: "Skip-gram predicts context from center, CBOW predicts center from context"
      why_it_matters: "Skip-gram better for rare words, CBOW faster for frequent words"
    
    - concept: "Negative sampling makes training tractable (10,000x speedup)"
      why_it_matters: "Binary classification (real vs noise) replaces expensive softmax"
    
    - concept: "Embeddings quality depends on corpus quality and hyperparameters"
      why_it_matters: "Poisoned data → poisoned embeddings, must audit training corpus"
    
    - concept: "Evaluation uses analogies and similarity tasks"
      why_it_matters: "Standard metrics miss security-relevant biases and backdoors"
  
  actionable_steps:
    - step: "Implement Skip-gram with negative sampling from scratch"
      verification: "Train on corpus, verify similar words cluster together"
    
    - step: "Implement CBOW architecture"
      verification: "Compare Skip-gram vs CBOW speed and quality"
    
    - step: "Train Word2Vec on real corpus (Wikipedia, news)"
      verification: "Solve analogy tasks, achieve >60% accuracy on standard benchmarks"
    
    - step: "Evaluate embeddings for biases and problematic associations"
      verification: "Check gender, ethnic, authority associations for stereotypes"
    
    - step: "Audit training corpus for adversarial patterns"
      verification: "Detect unusual co-occurrences, frequency anomalies"
  
  security_principles:
    - principle: "Word2Vec learns from corpus - poisoned data creates poisoned embeddings"
      application: "Audit training corpus before training, validate embeddings after"
    
    - principle: "Negative sampling distribution affects what model learns"
      application: "Adversaries manipulate frequencies to bias negative sampling"
    
    - principle: "Standard evaluation misses security-relevant biases"
      application: "Design security-specific tests for problematic associations"
    
    - principle: "Embeddings can contain backdoors (malicious nearest neighbors)"
      application: "Audit embedding space for suspicious clusters and associations"
    
    - principle: "Self-supervised learning amplifies corpus biases"
      application: "Embeddings reflect and amplify societal biases in training text"
  
  common_mistakes:
    - mistake: "Using full softmax instead of negative sampling"
      fix: "Always use negative sampling for vocabulary >1000 words"
    
    - mistake: "Not filtering rare words (min_count)"
      fix: "Set min_count=5-10 to remove words with insufficient training signal"
    
    - mistake: "Using too small window size (missing semantic relationships)"
      fix: "Use window_size=5 for semantics, 2-3 for syntax"
    
    - mistake: "Training on uncleaned corpus (garbage in, garbage out)"
      fix: "Preprocess corpus: lowercase, handle punctuation, remove noise"
    
    - mistake: "Trusting embeddings without bias/backdoor audit"
      fix: "Always audit for problematic associations before deployment"
  
  integration_with_book:
    from_section_3_1:
      - "Language modeling foundation (n-grams, perplexity)"
      - "Self-supervised learning from unlabeled text"
    
    from_section_3_2:
      - "Tokenization produces input for Word2Vec training"
      - "Vocabulary building and min_count filtering"
    
    from_section_3_3:
      - "Embedding spaces and distributional semantics"
      - "Cosine similarity for evaluation"
      - "Vector arithmetic for analogies"
    
    to_next_section:
      - "Section 3.5: Sequence-to-sequence problems motivating attention"
      - "Limitations of static embeddings (polysemy)"
      - "Need for contextual understanding"
  
  looking_ahead:
    next_concepts:
      - "Attention mechanism (solution to sequence bottlenecks)"
      - "Contextual embeddings (BERT, ELMo) that solve polysemy"
      - "Transformers that use self-attention"
      - "Pre-training then fine-tuning paradigm"
    
    skills_to_build:
      - "Implement attention mechanism from scratch"
      - "Build transformer encoder and decoder"
      - "Train BERT-style masked language models"
      - "Fine-tune pre-trained models for tasks"
  
  final_thoughts: |
    Word2Vec (Mikolov et al., 2013) was revolutionary: learn high-quality embeddings 
    from billions of words of unlabeled text using simple prediction tasks. Skip-gram 
    and CBOW turn self-supervised learning into embedding learning, and negative 
    sampling makes it computationally feasible.
    
    The key insight: predict words from context (or vice versa) forces model to learn 
    semantic relationships. Words in similar contexts get similar embeddings - the 
    distributional hypothesis operationalized through neural networks.
    
    But Word2Vec has fundamental limitations: one vector per word type means it can't 
    handle polysemy (bank, python, run with multiple meanings). This motivates 
    contextual embeddings (BERT, GPT) that compute different vectors based on sentence 
    context - which requires attention mechanisms and transformers.
    
    From a security perspective: Word2Vec is only as good as its training corpus. 
    Poisoned data creates poisoned embeddings with malicious associations baked into 
    the geometry. Adversaries can manipulate what the model learns through corpus 
    contamination, frequency manipulation, and backdoor injection.
    
    Defense requires: audit training corpus for adversarial patterns, evaluate 
    embeddings for biases and backdoors, validate semantic structure matches 
    expectations, and monitor for suspicious nearest neighbors.
    
    Next: We explore why fixed-context sequence models (RNNs, static embeddings) fail 
    on long sequences, motivating the attention mechanism - the innovation that enables 
    transformers to handle arbitrary context and compute contextual embeddings.

---
