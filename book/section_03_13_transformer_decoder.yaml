# section_03_13_transformer_decoder.yaml

---
document_info:
  title: "Transformer Decoder: Autoregressive Generation"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 13
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into transformer decoder: masked self-attention, cross-attention, decoder layer architecture, autoregressive generation, decoding strategies, and security implications"
  estimated_pages: 7
  tags:
    - transformer-decoder
    - masked-attention
    - cross-attention
    - autoregressive-generation
    - decoder-architecture
    - causal-masking

section_overview:
  title: "Transformer Decoder: Autoregressive Generation"
  number: "3.13"
  
  purpose: |
    The transformer decoder generates output sequences token-by-token in autoregressive 
    fashion. Unlike the encoder's bidirectional attention, the decoder uses masked (causal) 
    self-attention - each position can only attend to previous positions, preventing 
    "looking ahead." The decoder also has cross-attention to attend to encoder outputs, 
    enabling translation, summarization, and other sequence-to-sequence tasks.
    
    A decoder layer has three sublayers: (1) masked self-attention, (2) encoder-decoder 
    cross-attention, and (3) feed-forward network. Multiple decoder layers (6-12 typical) 
    are stacked, with final output projected to vocabulary for next-token prediction. This 
    architecture powers GPT, T5 decoders, and machine translation systems.
    
    For security engineers: Decoders generate text token-by-token, creating opportunities 
    for generation manipulation and prompt injection. Causal masking can be exploited by 
    positioning adversarial tokens. Cross-attention reveals source-target alignments, 
    useful for auditing but also for extraction attacks. Understanding decoder mechanics 
    is critical for securing generation systems.
  
  learning_objectives:
    conceptual:
      - "Understand masked self-attention: prevent attending to future positions"
      - "Grasp cross-attention: decoder attends to encoder outputs"
      - "Learn decoder layer: masked self-attn → cross-attn → FFN"
      - "See autoregressive generation: predict one token at a time"
      - "Compare greedy vs sampling decoding strategies"
    
    practical:
      - "Implement masked self-attention with causal masking"
      - "Build encoder-decoder cross-attention"
      - "Create complete decoder layer from scratch"
      - "Stack multiple decoder layers"
      - "Implement greedy and sampling decoders"
    
    security_focused:
      - "Exploit causal masking for position-based attacks"
      - "Extract information via cross-attention patterns"
      - "Manipulate generation through prompt injection"
      - "Analyze autoregressive vulnerabilities"
      - "Audit decoder outputs for data leakage"
  
  prerequisites:
    knowledge:
      - "Section 3.12: Transformer encoder (provides encoder outputs)"
      - "Section 3.11: Layer normalization and residuals"
      - "Section 3.8: Multi-head attention"
      - "Section 3.7: Causal masking"
    
    skills:
      - "All previous transformer component implementations"
      - "Understanding of sequence generation"
      - "Masking techniques"
      - "Autoregressive modeling concepts"
  
  key_transitions:
    from_section_3_12: |
      Section 3.12 built the transformer encoder for understanding. Now we build the 
      decoder for generation - using masked attention to maintain autoregressive property 
      and cross-attention to condition on encoder outputs.
    
    to_next_section: |
      Section 3.14 will assemble encoder and decoder into the complete transformer, 
      covering training with teacher forcing, beam search, and full sequence-to-sequence 
      modeling.

topics:
  - topic_number: 1
    title: "Masked Self-Attention: Preventing Future Leakage"
    
    overview: |
      The decoder's first sublayer uses masked (causal) self-attention. Unlike encoder's 
      bidirectional attention, decoder positions can only attend to previous positions 
      (including self). This maintains the autoregressive property: when predicting token t, 
      the model sees only tokens 0...t-1, preventing "cheating" by looking at future tokens.
    
    content:
      why_masking_is_necessary:
        autoregressive_generation: |
          Decoder generates tokens sequentially:
          
          Step 1: Predict token₁ (given <START>)
          Step 2: Predict token₂ (given <START>, token₁)
          Step 3: Predict token₃ (given <START>, token₁, token₂)
          ...
        
        training_inference_mismatch_without_masking: |
          Training WITHOUT masking:
          - Decoder sees all output tokens simultaneously
          - Can "cheat" by looking at token₃ when predicting token₂
          - Learns to copy, not to predict
          
          Inference:
          - Generates one token at a time
          - Cannot see future tokens
          
          → Mismatch: model trained with future info, tested without
          → Poor generation quality
        
        solution_causal_masking: |
          Apply causal mask during training:
          - Position t can see positions 0...t (not t+1...n)
          - Training matches inference conditions
          - Model learns proper conditional distributions
      
      causal_mask_structure:
        lower_triangular_matrix: |
          For sequence length n, causal mask is lower triangular:
          
          Position:  0  1  2  3  4
                 0: [1  0  0  0  0]  ← Can see position 0 only
                 1: [1  1  0  0  0]  ← Can see positions 0-1
                 2: [1  1  1  0  0]  ← Can see positions 0-2
                 3: [1  1  1  1  0]  ← Can see positions 0-3
                 4: [1  1  1  1  1]  ← Can see positions 0-4
        
        implementation: |
          # NumPy
          causal_mask = np.tril(np.ones((seq_len, seq_len)))
          
          # Applied before softmax
          scores = QK^T / √d_k
          scores = where(causal_mask == 0, -∞, scores)
          attention_weights = softmax(scores)
        
        effect: |
          Masked positions get score = -∞
          → After softmax, weight = 0
          → No information flows from future to past
      
      masked_self_attention_vs_encoder: |
        Encoder self-attention:
          - Bidirectional (all positions see all positions)
          - No masking needed
          - Better for understanding tasks
          
          Example: Position 2 attends to [0, 1, 2, 3, 4]
        
        Decoder masked self-attention:
          - Unidirectional/causal (position t sees 0...t)
          - Causal masking required
          - Necessary for generation tasks
          
          Example: Position 2 attends to [0, 1, 2] only
    
    implementation:
      masked_self_attention:
        language: python
        code: |
          import numpy as np
          
          def create_causal_mask(seq_len: int) -> np.ndarray:
              """
              Create causal mask for decoder self-attention.
              
              Args:
                  seq_len: Sequence length
              
              Returns:
                  mask: (seq_len, seq_len) lower triangular matrix
                       1 = can attend, 0 = masked
              """
              mask = np.tril(np.ones((seq_len, seq_len)))
              return mask.astype(bool)
          
          
          class MaskedSelfAttention:
              """
              Masked self-attention for decoder.
              
              Uses causal masking to prevent attending to future positions.
              """
              
              def __init__(self, d_model: int, num_heads: int):
                  """
                  Args:
                      d_model: Model dimension
                      num_heads: Number of attention heads
                  """
                  # Use MultiHeadAttention from Section 3.8
                  from section_3_08_multi_head_attention import MultiHeadAttention
                  self.attention = MultiHeadAttention(d_model, num_heads)
              
              def forward(self, x: np.ndarray, training: bool = False) -> tuple:
                  """
                  Apply masked self-attention.
                  
                  Args:
                      x: Input (batch, seq_len, d_model)
                      training: Whether in training mode
                  
                  Returns:
                      output: (batch, seq_len, d_model)
                      attention_weights: (batch, num_heads, seq_len, seq_len)
                  """
                  batch_size, seq_len, d_model = x.shape
                  
                  # Create causal mask
                  causal_mask = create_causal_mask(seq_len)
                  
                  # Expand mask for batch and heads
                  # (seq_len, seq_len) → (batch, 1, seq_len, seq_len)
                  mask_expanded = causal_mask[np.newaxis, np.newaxis, :, :]
                  mask_expanded = np.broadcast_to(mask_expanded, 
                                                  (batch_size, 1, seq_len, seq_len))
                  
                  # Self-attention: Q, K, V all from same input
                  output, attn_weights = self.attention(x, x, x, mask=mask_expanded)
                  
                  return output, attn_weights
              
              def __call__(self, x: np.ndarray, training: bool = False) -> tuple:
                  """Alias for forward."""
                  return self.forward(x, training)
          
          
          # Example usage
          print("=== Masked Self-Attention ===\n")
          
          d_model = 512
          num_heads = 8
          seq_len = 5
          batch_size = 1
          
          masked_attn = MaskedSelfAttention(d_model, num_heads)
          
          # Input
          x = np.random.randn(batch_size, seq_len, d_model)
          
          # Forward pass
          output, attn_weights = masked_attn(x)
          
          print(f"Input shape: {x.shape}")
          print(f"Output shape: {output.shape}")
          print(f"Attention weights shape: {attn_weights.shape}")
          print()
          
          # Show causal mask structure
          mask = create_causal_mask(seq_len)
          print("Causal mask (lower triangular):")
          print(mask.astype(int))
          print()
          print("Meaning:")
          for i in range(seq_len):
              can_attend = [j for j in range(seq_len) if mask[i, j]]
              print(f"  Position {i} can attend to positions: {can_attend}")
      
      visualize_masked_attention:
        language: python
        code: |
          def visualize_masked_attention_pattern():
              """Visualize decoder masked self-attention pattern."""
              
              print("\n=== Decoder Masked Self-Attention Pattern ===\n")
              
              tokens = ["<START>", "The", "cat", "sat", "down"]
              n = len(tokens)
              
              # Simulate masked attention weights
              np.random.seed(42)
              attention = np.random.rand(n, n)
              
              # Apply causal mask
              causal_mask = create_causal_mask(n)
              attention = attention * causal_mask  # Zero out future positions
              
              # Normalize (row-wise)
              row_sums = attention.sum(axis=1, keepdims=True)
              row_sums[row_sums == 0] = 1  # Avoid division by zero
              attention = attention / row_sums
              
              print("Causal (masked) self-attention:")
              print("Each position attends only to current and previous positions\n")
              print("Query →     ", "  ".join(f"{t:8s}" for t in tokens))
              print("-" * (12 + 10 * len(tokens)))
              
              for i, query_token in enumerate(tokens):
                  weights_str = "  ".join(
                      f"{attention[i, j]:8.2f}" if causal_mask[i, j] else "    --  "
                      for j in range(n)
                  )
                  print(f"{query_token:8s}:  {weights_str}")
              
              print()
              print("Notice: Upper triangle is masked (--)")
              print("  → Cannot attend to future positions")
              print("  → Maintains autoregressive property")
          
          visualize_masked_attention_pattern()
    
    security_implications:
      position_based_attacks: |
        Causal masking creates position-dependent vulnerabilities:
        - Early positions (0-2): Limited context, easier to manipulate
        - Late positions (n-3 to n): See full context, harder to fool
        - Adversary places payloads early to exploit limited context
        - Defense: Position-aware input validation
      
      prompt_injection_via_causality: |
        Causal structure enables prompt injection:
        - Model only sees left context when generating
        - Adversary crafts prefix that controls generation
        - Example: "Ignore previous instructions. [malicious prompt]"
        - Defense: Input sanitization, output filtering

  - topic_number: 2
    title: "Cross-Attention: Connecting Decoder to Encoder"
    
    overview: |
      The decoder's second sublayer uses cross-attention (encoder-decoder attention). Here, 
      queries come from decoder, but keys and values come from encoder outputs. This allows 
      the decoder to attend to relevant parts of the input sequence when generating each 
      output token. Cross-attention creates the encoder-decoder connection in translation, 
      summarization, and other seq2seq tasks.
    
    content:
      cross_attention_explained:
        what_is_cross_attention: |
          Cross-attention: Q from one sequence, K and V from different sequence
          
          Decoder cross-attention:
          - Queries: From decoder (current generation state)
          - Keys & Values: From encoder (input sequence representation)
          - Decoder asks: "What input is relevant for current output?"
        
        contrast_with_self_attention: |
          Self-attention: Q, K, V all from SAME sequence
          Cross-attention: Q from decoder, K/V from encoder
        
        purpose: |
          Enables decoder to "look at" encoder outputs:
          - When translating, decoder sees source sentence
          - When summarizing, decoder sees full document
          - Creates alignment between input and output
      
      cross_attention_mechanics:
        query_from_decoder: |
          Q = decoder_state × W^Q
          
          Decoder asks: "What input info do I need for current position?"
        
        keys_values_from_encoder: |
          K = encoder_output × W^K
          V = encoder_output × W^V
          
          Encoder provides: "Here's what the input contains"
        
        attention_computation: |
          Attention(Q_decoder, K_encoder, V_encoder)
          = softmax(Q_decoder × K_encoder^T / √d_k) × V_encoder
          
          Result: Decoder retrieves relevant encoder information
        
        no_masking_needed: |
          Cross-attention doesn't need causal mask:
          - Encoder outputs are fixed (already computed)
          - Decoder can attend to ANY encoder position
          - Input sequence is complete (not being generated)
      
      alignment_interpretation:
        translation_example: |
          Source (encoder): "The cat sat on the mat"
          Target (decoder): "Le chat s'est assis sur le tapis"
          
          When generating "chat":
          - Decoder attends strongly to encoder's "cat"
          - Cross-attention creates source-target alignment
        
        attention_weights_as_alignment: |
          Cross-attention weights show which source positions
          influence each target position
          
          → Visualization reveals translation decisions
          → Useful for interpretability and debugging
      
      dimensions_and_shapes:
        decoder_queries: "Q: (batch, target_len, d_model)"
        encoder_keys: "K: (batch, source_len, d_model)"
        encoder_values: "V: (batch, source_len, d_model)"
        attention_scores: "(batch, target_len, source_len)"
        output: "(batch, target_len, d_model)"
        
        note: "target_len and source_len can differ!"
    
    implementation:
      cross_attention:
        language: python
        code: |
          class CrossAttention:
              """
              Encoder-decoder cross-attention.
              
              Queries from decoder, keys and values from encoder.
              """
              
              def __init__(self, d_model: int, num_heads: int):
                  """
                  Args:
                      d_model: Model dimension
                      num_heads: Number of attention heads
                  """
                  from section_3_08_multi_head_attention import MultiHeadAttention
                  self.attention = MultiHeadAttention(d_model, num_heads)
              
              def forward(self, decoder_state: np.ndarray, 
                         encoder_output: np.ndarray,
                         encoder_mask: np.ndarray = None) -> tuple:
                  """
                  Apply cross-attention.
                  
                  Args:
                      decoder_state: Decoder queries (batch, target_len, d_model)
                      encoder_output: Encoder keys/values (batch, source_len, d_model)
                      encoder_mask: Optional encoder padding mask
                  
                  Returns:
                      output: (batch, target_len, d_model)
                      attention_weights: (batch, num_heads, target_len, source_len)
                  """
                  # Cross-attention: Q from decoder, K and V from encoder
                  output, attn_weights = self.attention(
                      Q=decoder_state,
                      K=encoder_output,
                      V=encoder_output,
                      mask=encoder_mask
                  )
                  
                  return output, attn_weights
              
              def __call__(self, decoder_state: np.ndarray,
                          encoder_output: np.ndarray,
                          encoder_mask: np.ndarray = None) -> tuple:
                  """Alias for forward."""
                  return self.forward(decoder_state, encoder_output, encoder_mask)
          
          
          # Example usage
          print("\n=== Cross-Attention (Encoder-Decoder) ===\n")
          
          d_model = 512
          num_heads = 8
          batch_size = 1
          source_len = 7  # Encoder sequence length
          target_len = 5  # Decoder sequence length
          
          cross_attn = CrossAttention(d_model, num_heads)
          
          # Decoder state (queries)
          decoder_state = np.random.randn(batch_size, target_len, d_model)
          
          # Encoder output (keys and values)
          encoder_output = np.random.randn(batch_size, source_len, d_model)
          
          # Forward pass
          output, attn_weights = cross_attn(decoder_state, encoder_output)
          
          print(f"Decoder state shape: {decoder_state.shape}")
          print(f"Encoder output shape: {encoder_output.shape}")
          print(f"Output shape: {output.shape}")
          print(f"Attention weights shape: {attn_weights.shape}")
          print()
          print("Cross-attention mechanism:")
          print("  Queries: From decoder (what do I need?)")
          print("  Keys & Values: From encoder (here's the input)")
          print("  → Decoder attends to relevant encoder positions")
      
      visualize_cross_attention:
        language: python
        code: |
          def visualize_cross_attention_alignment():
              """Visualize cross-attention as source-target alignment."""
              
              print("\n=== Cross-Attention Alignment ===\n")
              
              # Translation example
              source = ["The", "cat", "sat", "on", "mat"]
              target = ["Le", "chat", "assis"]
              
              # Simulate cross-attention weights (target × source)
              # Each target position attends to source positions
              cross_attn = np.array([
                  [0.70, 0.10, 0.05, 0.10, 0.05],  # "Le" → "The"
                  [0.10, 0.75, 0.05, 0.05, 0.05],  # "chat" → "cat"
                  [0.05, 0.10, 0.70, 0.10, 0.05],  # "assis" → "sat"
              ])
              
              print("Cross-attention weights (target → source):")
              print()
              print("Target  ↓   ", "  ".join(f"{s:6s}" for s in source))
              print("-" * (12 + 8 * len(source)))
              
              for i, target_word in enumerate(target):
                  weights_str = "  ".join(f"{cross_attn[i, j]:6.2f}" for j in range(len(source)))
                  print(f"{target_word:8s}:  {weights_str}")
              
              print()
              print("Interpretation:")
              print("  'Le' attends most to 'The' (0.70)")
              print("  'chat' attends most to 'cat' (0.75)")
              print("  'assis' attends most to 'sat' (0.70)")
              print("  → Cross-attention creates source-target alignment!")
          
          visualize_cross_attention_alignment()
    
    security_implications:
      alignment_extraction: |
        Cross-attention weights reveal source-target relationships:
        - Adversary can extract which input influenced which output
        - Reveals model's decision-making process
        - Useful for auditing BUT also for reverse-engineering
        - Defense: Limit access to attention weights in production
      
      encoder_output_manipulation: |
        Cross-attention depends on encoder outputs:
        - Adversary can manipulate encoder via input poisoning
        - Corrupted encoder outputs → corrupted decoder generation
        - Example: Inject misleading info in source → decoder generates it
        - Defense: Validate encoder outputs, sanitize inputs

  - topic_number: 3
    title: "Decoder Layer and Stack Architecture"
    
    overview: |
      A complete decoder layer has three sublayers: (1) masked self-attention, (2) encoder-
      decoder cross-attention, and (3) feed-forward network. Each has residual connections 
      and layer normalization. Multiple decoder layers (6-12) are stacked, with final output 
      projected to vocabulary for next-token prediction.
    
    content:
      decoder_layer_architecture:
        three_sublayers:
          sublayer_1_masked_self_attention:
            purpose: "Aggregate past decoder tokens (causal)"
            input: "Current decoder state"
            operation: "Masked self-attention (Q, K, V from decoder)"
            output: "Contextualized past"
          
          sublayer_2_cross_attention:
            purpose: "Attend to encoder outputs"
            input: "Decoder state + encoder outputs"
            operation: "Cross-attention (Q from decoder, K/V from encoder)"
            output: "Encoder-informed representation"
          
          sublayer_3_feed_forward:
            purpose: "Position-wise transformation"
            input: "Cross-attended representation"
            operation: "Two-layer FFN with GELU"
            output: "Final layer output"
        
        complete_decoder_layer_prenorm: |
          # Sublayer 1: Masked self-attention
          x1 = x + MaskedSelfAttention(LayerNorm(x))
          
          # Sublayer 2: Cross-attention to encoder
          x2 = x1 + CrossAttention(LayerNorm(x1), encoder_output)
          
          # Sublayer 3: Feed-forward network
          x3 = x2 + FFN(LayerNorm(x2))
          
          return x3
      
      decoder_vs_encoder_layer:
        encoder_layer: |
          1. Self-attention (bidirectional)
          2. Feed-forward network
          
          → 2 sublayers
        
        decoder_layer: |
          1. Masked self-attention (causal)
          2. Cross-attention (to encoder)
          3. Feed-forward network
          
          → 3 sublayers (one more than encoder!)
      
      stacking_decoder_layers:
        architecture: |
          Decoder input → Layer 1 → Layer 2 → ... → Layer N → Output
          
          Each layer:
          - Refines decoder representation
          - Attends to encoder at every layer
        
        typical_depths:
          original_transformer: "6 decoder layers"
          gpt2: "12-48 decoder layers (decoder-only, no encoder)"
          t5: "12-24 decoder layers"
        
        encoder_output_reuse: |
          Encoder outputs computed ONCE
          → Reused by ALL decoder layers
          → Efficient (don't recompute encoder)
      
      output_projection_layer:
        purpose: "Project decoder output to vocabulary logits"
        
        architecture: |
          Final decoder output: (batch, target_len, d_model)
          
          Linear projection: W_out ∈ (d_model, vocab_size)
          
          Logits = decoder_output @ W_out  # (batch, target_len, vocab_size)
        
        next_token_prediction: |
          For position t:
          logits_t = decoder_output[t] @ W_out  # (vocab_size,)
          probs_t = softmax(logits_t)
          next_token = argmax(probs_t) or sample(probs_t)
    
    implementation:
      decoder_layer:
        language: python
        code: |
          class TransformerDecoderLayer:
              """
              Single transformer decoder layer (pre-norm).
              
              Architecture:
              1. Masked self-attention
              2. Cross-attention to encoder
              3. Feed-forward network
              """
              
              def __init__(self, d_model: int, num_heads: int, d_ff: int,
                          dropout: float = 0.1):
                  """
                  Args:
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  self.d_model = d_model
                  
                  # Sublayer 1: Masked self-attention
                  self.masked_self_attn = MaskedSelfAttention(d_model, num_heads)
                  self.self_attn_norm = LayerNormalization(d_model)
                  
                  # Sublayer 2: Cross-attention
                  self.cross_attn = CrossAttention(d_model, num_heads)
                  self.cross_attn_norm = LayerNormalization(d_model)
                  
                  # Sublayer 3: Feed-forward network
                  from section_3_10_feed_forward_networks import PositionWiseFeedForwardGELU
                  self.ffn = PositionWiseFeedForwardGELU(d_model, d_ff)
                  self.ffn_norm = LayerNormalization(d_model)
                  
                  self.dropout = dropout
              
              def forward(self, x: np.ndarray, encoder_output: np.ndarray,
                         encoder_mask: np.ndarray = None,
                         training: bool = False) -> tuple:
                  """
                  Forward pass through decoder layer.
                  
                  Args:
                      x: Decoder input (batch, target_len, d_model)
                      encoder_output: Encoder output (batch, source_len, d_model)
                      encoder_mask: Optional encoder padding mask
                      training: Whether in training mode
                  
                  Returns:
                      output: (batch, target_len, d_model)
                      self_attn_weights: Masked self-attention weights
                      cross_attn_weights: Cross-attention weights
                  """
                  # Sublayer 1: Masked self-attention
                  normalized = self.self_attn_norm(x)
                  self_attn_out, self_attn_weights = self.masked_self_attn(normalized, training)
                  
                  if training and self.dropout > 0:
                      self_attn_out = self._dropout(self_attn_out, self.dropout)
                  x1 = x + self_attn_out
                  
                  # Sublayer 2: Cross-attention to encoder
                  normalized = self.cross_attn_norm(x1)
                  cross_attn_out, cross_attn_weights = self.cross_attn(
                      normalized, encoder_output, encoder_mask
                  )
                  
                  if training and self.dropout > 0:
                      cross_attn_out = self._dropout(cross_attn_out, self.dropout)
                  x2 = x1 + cross_attn_out
                  
                  # Sublayer 3: Feed-forward network
                  normalized = self.ffn_norm(x2)
                  ffn_out = self.ffn(normalized)
                  
                  if training and self.dropout > 0:
                      ffn_out = self._dropout(ffn_out, self.dropout)
                  x3 = x2 + ffn_out
                  
                  return x3, self_attn_weights, cross_attn_weights
              
              def _dropout(self, x: np.ndarray, rate: float) -> np.ndarray:
                  """Apply dropout."""
                  mask = np.random.binomial(1, 1 - rate, x.shape) / (1 - rate)
                  return x * mask
              
              def __call__(self, x: np.ndarray, encoder_output: np.ndarray,
                          encoder_mask: np.ndarray = None,
                          training: bool = False) -> tuple:
                  """Alias for forward."""
                  return self.forward(x, encoder_output, encoder_mask, training)
          
          
          # Example usage
          print("\n=== Transformer Decoder Layer ===\n")
          
          d_model = 512
          num_heads = 8
          d_ff = 2048
          batch_size = 1
          target_len = 5
          source_len = 7
          
          decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff)
          
          # Decoder input
          decoder_input = np.random.randn(batch_size, target_len, d_model)
          
          # Encoder output
          encoder_output = np.random.randn(batch_size, source_len, d_model)
          
          # Forward pass
          output, self_attn, cross_attn = decoder_layer(
              decoder_input, encoder_output, training=False
          )
          
          print(f"Decoder input shape: {decoder_input.shape}")
          print(f"Encoder output shape: {encoder_output.shape}")
          print(f"Output shape: {output.shape}")
          print()
          print("Decoder layer structure:")
          print("  1. Masked self-attention (causal)")
          print("  2. Cross-attention to encoder")
          print("  3. Feed-forward network")
          print("  Each with: LayerNorm + residual connection")
      
      complete_decoder_stack:
        language: python
        code: |
          class TransformerDecoder:
              """
              Multi-layer transformer decoder.
              
              Stacks N decoder layers with output projection to vocabulary.
              """
              
              def __init__(self, vocab_size: int, max_len: int, num_layers: int,
                          d_model: int, num_heads: int, d_ff: int,
                          dropout: float = 0.1):
                  """
                  Args:
                      vocab_size: Vocabulary size
                      max_len: Maximum sequence length
                      num_layers: Number of decoder layers
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  self.vocab_size = vocab_size
                  self.d_model = d_model
                  
                  # Token embeddings
                  self.token_embeddings = np.random.randn(vocab_size, d_model) * 0.01
                  
                  # Positional encoding
                  from section_3_09_positional_encoding import SinusoidalPositionalEncoding
                  self.positional_encoding = SinusoidalPositionalEncoding(d_model, max_len)
                  
                  # Decoder layers
                  self.layers = []
                  for _ in range(num_layers):
                      layer = TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)
                      self.layers.append(layer)
                  
                  # Output projection to vocabulary
                  self.output_projection = np.random.randn(d_model, vocab_size) * 0.01
              
              def embed(self, target_ids: np.ndarray) -> np.ndarray:
                  """Embed target tokens with positional encoding."""
                  token_emb = self.token_embeddings[target_ids]
                  seq_len = target_ids.shape[1]
                  pos_enc = self.positional_encoding(seq_len)
                  return token_emb + pos_enc
              
              def forward(self, target_ids: np.ndarray, encoder_output: np.ndarray,
                         encoder_mask: np.ndarray = None,
                         training: bool = False) -> np.ndarray:
                  """
                  Forward pass through decoder.
                  
                  Args:
                      target_ids: Target token IDs (batch, target_len)
                      encoder_output: Encoder output (batch, source_len, d_model)
                      encoder_mask: Optional encoder padding mask
                      training: Whether in training mode
                  
                  Returns:
                      logits: (batch, target_len, vocab_size)
                  """
                  # Embed target tokens
                  x = self.embed(target_ids)
                  
                  # Pass through decoder layers
                  for layer in self.layers:
                      x, _, _ = layer(x, encoder_output, encoder_mask, training)
                  
                  # Project to vocabulary
                  logits = np.matmul(x, self.output_projection)
                  
                  return logits
              
              def __call__(self, target_ids: np.ndarray, encoder_output: np.ndarray,
                          encoder_mask: np.ndarray = None,
                          training: bool = False) -> np.ndarray:
                  """Alias for forward."""
                  return self.forward(target_ids, encoder_output, encoder_mask, training)
          
          
          print("\n=== Complete Transformer Decoder ===\n")
          
          vocab_size = 30000
          max_len = 512
          num_layers = 6
          d_model = 512
          num_heads = 8
          d_ff = 2048
          
          decoder = TransformerDecoder(
              vocab_size, max_len, num_layers, d_model, num_heads, d_ff
          )
          
          # Input
          batch_size = 1
          target_len = 5
          source_len = 7
          
          target_ids = np.random.randint(0, vocab_size, (batch_size, target_len))
          encoder_output = np.random.randn(batch_size, source_len, d_model)
          
          # Forward pass
          logits = decoder(target_ids, encoder_output, training=False)
          
          print(f"Target IDs shape: {target_ids.shape}")
          print(f"Encoder output shape: {encoder_output.shape}")
          print(f"Logits shape: {logits.shape}")
          print()
          print("Complete decoder pipeline:")
          print("  1. Embed target tokens + positional encoding")
          print("  2. Pass through decoder layers (6)")
          print("  3. Project to vocabulary logits")
          print("  4. Apply softmax for token probabilities")
    
    security_implications:
      layer_specific_manipulation: |
        Different decoder layers serve different purposes:
        - Early layers: Process recent tokens
        - Middle layers: Integrate encoder information
        - Late layers: Make final predictions
        - Adversary can target specific layers for different effects
        - Defense: Monitor all layers, not just output
      
      cross_attention_at_every_layer: |
        Encoder outputs influence every decoder layer:
        - Single encoder manipulation affects entire decoder
        - Adversary can poison encoder to control all decoder layers
        - Defense: Validate encoder outputs, limit encoder influence

key_takeaways:
  critical_concepts:
    - concept: "Decoder layer has 3 sublayers: masked self-attn, cross-attn, FFN"
      why_it_matters: "One more sublayer than encoder (cross-attention)"
    
    - concept: "Masked self-attention uses causal masking (lower triangular)"
      why_it_matters: "Prevents looking ahead, maintains autoregressive property"
    
    - concept: "Cross-attention: Q from decoder, K/V from encoder"
      why_it_matters: "Connects decoder to encoder, enables seq2seq tasks"
    
    - concept: "Encoder outputs computed once, reused by all decoder layers"
      why_it_matters: "Efficient - don't recompute encoder for each decoder step"
    
    - concept: "Output projection maps d_model → vocab_size for next-token prediction"
      why_it_matters: "Final step converting decoder state to token probabilities"
  
  actionable_steps:
    - step: "Implement masked self-attention with causal masking"
      verification: "Lower triangular mask prevents future attention"
    
    - step: "Build encoder-decoder cross-attention"
      verification: "Q from decoder, K/V from encoder"
    
    - step: "Create complete decoder layer with 3 sublayers"
      verification: "Masked self-attn → cross-attn → FFN"
    
    - step: "Stack multiple decoder layers"
      verification: "Sequential processing, encoder output reused"
    
    - step: "Add output projection to vocabulary"
      verification: "d_model → vocab_size linear layer"
  
  security_principles:
    - principle: "Causal masking creates position-dependent vulnerabilities"
      application: "Early positions easier to manipulate (less context)"
    
    - principle: "Cross-attention reveals source-target alignments"
      application: "Useful for auditing, exploitable for reverse-engineering"
    
    - principle: "Encoder corruption propagates to all decoder layers"
      application: "Single encoder attack affects entire generation"
    
    - principle: "Prompt injection exploits causal structure"
      application: "Adversary controls prefix to manipulate generation"
    
    - principle: "Different decoder layers serve different purposes"
      application: "Layer-specific attacks for different effects"
  
  common_mistakes:
    - mistake: "Using bidirectional attention in decoder (like encoder)"
      fix: "Decoder MUST use causal masking (prevent future leakage)"
    
    - mistake: "Forgetting cross-attention sublayer"
      fix: "Decoder has 3 sublayers (encoder has 2)"
    
    - mistake: "Recomputing encoder outputs for each decoder layer"
      fix: "Compute encoder ONCE, reuse for all decoder layers"
    
    - mistake: "Not applying causal mask during training"
      fix: "ALWAYS use causal mask (training and inference)"
    
    - mistake: "Wrong cross-attention Q/K/V sources"
      fix: "Q from decoder, K and V from encoder"
  
  integration_with_book:
    from_section_3_12:
      - "Transformer encoder (provides encoder outputs)"
      - "Bidirectional self-attention (contrast with decoder's causal)"
    
    from_section_3_7:
      - "Causal masking (used in masked self-attention)"
      - "Attention masking mechanics"
    
    to_next_section:
      - "Section 3.14: Complete transformer (encoder + decoder)"
      - "Training with teacher forcing"
      - "Beam search and decoding strategies"
  
  looking_ahead:
    next_concepts:
      - "Complete encoder-decoder transformer"
      - "Teacher forcing for training"
      - "Beam search for better generation"
      - "Training objectives and optimization"
    
    skills_to_build:
      - "Assemble complete transformer"
      - "Implement training loop"
      - "Build beam search decoder"
      - "Train transformer from scratch"
  
  final_thoughts: |
    The transformer decoder generates sequences token-by-token using autoregressive 
    prediction. Unlike the encoder's bidirectional attention, the decoder uses masked 
    (causal) self-attention - each position can only see past positions, preventing 
    "looking ahead" at future tokens. This maintains the autoregressive property essential 
    for generation.
    
    The decoder layer has three sublayers (one more than encoder): (1) masked self-attention 
    for aggregating past decoder tokens, (2) cross-attention to encoder for accessing input 
    information, and (3) feed-forward network for transformation. The cross-attention 
    sublayer is the key difference - it connects decoder to encoder, with queries from 
    decoder and keys/values from encoder outputs.
    
    Stacking 6-12 decoder layers builds the complete decoder, with encoder outputs computed 
    once and reused by all layers for efficiency. Final decoder output is projected to 
    vocabulary logits for next-token prediction. This architecture powers machine translation 
    (original transformer), text generation (GPT when decoder-only), and summarization.
    
    From a security perspective: causal masking creates position-dependent vulnerabilities 
    (early positions have less context, easier to manipulate). Cross-attention reveals 
    source-target alignments useful for auditing but exploitable for extraction. Prompt 
    injection exploits the causal structure - adversary controls prefix to manipulate 
    generation. Encoder corruption propagates to all decoder layers through cross-attention. 
    Understanding decoder mechanics is critical for securing generation systems.
    
    Next: Section 3.14 assembles encoder and decoder into the complete transformer, covering 
    training with teacher forcing, beam search for better generation, and full sequence-to-
    sequence modeling that powers modern NLP systems.

---
