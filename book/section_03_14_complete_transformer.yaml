# section_03_14_complete_transformer.yaml

---
document_info:
  title: "Complete Transformer: Encoder-Decoder Assembly"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 3
  section: 14
  part: 1
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-18"
  version: "1.0"
  description: "Deep dive into complete transformer: encoder-decoder assembly, training with teacher forcing, autoregressive inference, beam search, hyperparameters, and security implications"
  estimated_pages: 8
  tags:
    - complete-transformer
    - encoder-decoder
    - teacher-forcing
    - beam-search
    - autoregressive-inference
    - transformer-training

section_overview:
  title: "Complete Transformer: Encoder-Decoder Assembly"
  number: "3.14"
  
  purpose: |
    The complete transformer (Vaswani et al., 2017) combines encoder and decoder stacks 
    for sequence-to-sequence tasks. The encoder processes input sequences into contextual 
    representations, the decoder generates output sequences autoregressively while attending 
    to encoder outputs via cross-attention. This architecture revolutionized machine 
    translation and established the foundation for modern NLP.
    
    Training uses teacher forcing - feeding ground truth tokens to decoder during training 
    for efficient parallel learning. Inference is autoregressive - generating one token at 
    a time, feeding each prediction back as input. Beam search explores multiple generation 
    paths simultaneously for better output quality than greedy decoding.
    
    For security engineers: Complete transformers combine encoder and decoder vulnerabilities. 
    Teacher forcing creates train-test mismatch exploitable by adversaries. Beam search can 
    be manipulated to extract training data or generate specific outputs. Understanding the 
    full architecture reveals end-to-end attack surfaces from input encoding through output 
    generation.
  
  learning_objectives:
    conceptual:
      - "Understand complete transformer: encoder stack + decoder stack"
      - "Grasp teacher forcing: parallel training with ground truth"
      - "Learn autoregressive inference: sequential token generation"
      - "See beam search: exploring multiple generation paths"
      - "Compare training vs inference modes and their differences"
    
    practical:
      - "Implement complete encoder-decoder transformer from scratch"
      - "Build training loop with teacher forcing"
      - "Create autoregressive inference pipeline"
      - "Implement beam search decoder"
      - "Train transformer on sequence-to-sequence task"
    
    security_focused:
      - "Exploit teacher forcing train-test mismatch"
      - "Manipulate beam search for data extraction"
      - "Attack complete transformer end-to-end"
      - "Analyze encoder-decoder vulnerability interactions"
      - "Audit transformer training and inference pipelines"
  
  prerequisites:
    knowledge:
      - "Section 3.12: Transformer encoder"
      - "Section 3.13: Transformer decoder"
      - "Chapter 2: Training neural networks, loss functions"
      - "Understanding of sequence-to-sequence tasks"
    
    skills:
      - "All previous transformer implementations"
      - "Training loop construction"
      - "Loss computation and backpropagation"
      - "Sampling and decoding strategies"
  
  key_transitions:
    from_section_3_13: |
      Section 3.13 built the transformer decoder for generation. Now we combine encoder 
      and decoder into the complete transformer, covering training with teacher forcing 
      and inference with beam search.
    
    to_next_section: |
      Section 3.15 will explore GPT (decoder-only transformers) - how removing the 
      encoder and using only masked self-attention creates powerful language models 
      for text generation.

topics:
  - topic_number: 1
    title: "Complete Transformer Architecture"
    
    overview: |
      The complete transformer has two main components: encoder stack (processes input) 
      and decoder stack (generates output). Encoder runs once to create source representations, 
      decoder runs autoregressively using encoder outputs via cross-attention. Final decoder 
      output is projected to vocabulary for token prediction.
    
    content:
      full_architecture:
        components:
          input_processing:
            tokenization: "Input text → token IDs"
            embedding: "Token IDs → embeddings (d_model)"
            positional_encoding: "Add position information"
          
          encoder_stack:
            layers: "N encoder layers (typically 6)"
            per_layer: "Self-attention (bidirectional) + FFN"
            output: "Contextualized source representations"
          
          decoder_stack:
            layers: "N decoder layers (typically 6)"
            per_layer: "Masked self-attention + Cross-attention + FFN"
            output: "Target representations"
          
          output_projection:
            operation: "Linear projection to vocabulary"
            dimension: "d_model → vocab_size"
            result: "Logits for next token prediction"
        
        data_flow: |
          Source text
          ↓ (tokenize + embed + positional)
          Encoder input (batch, source_len, d_model)
          ↓ (encoder stack)
          Encoder output (batch, source_len, d_model)
          ↓ (provided to decoder via cross-attention)
          
          Target text
          ↓ (tokenize + embed + positional)
          Decoder input (batch, target_len, d_model)
          ↓ (decoder stack with cross-attention to encoder)
          Decoder output (batch, target_len, d_model)
          ↓ (output projection)
          Logits (batch, target_len, vocab_size)
          ↓ (softmax)
          Token probabilities
      
      encoder_decoder_connection:
        cross_attention_at_each_layer: |
          Every decoder layer attends to encoder output
          → Decoder has access to source at all depths
          → Rich encoder-decoder alignment
        
        encoder_computed_once: |
          Encoder processes source sequence ONCE
          → Encoder output cached and reused
          → Efficient: don't recompute for each target token
        
        asymmetry: |
          Encoder: Bidirectional (sees full source)
          Decoder: Causal (sees only past target)
          
          → Different attention patterns for different purposes
      
      hyperparameters:
        model_architecture:
          num_encoder_layers: "6 (original), 12-24 (large models)"
          num_decoder_layers: "6 (original), 12-24 (large models)"
          d_model: "512 (base), 768-1024 (large)"
          num_heads: "8 (base), 12-16 (large)"
          d_ff: "2048 (base), 3072-4096 (large)"
        
        vocabulary:
          vocab_size: "30K-50K (typical for translation)"
          shared_embeddings: "Often share encoder/decoder embeddings"
        
        training:
          dropout: "0.1 typical"
          learning_rate: "Warmup schedule (Section 3.15)"
          batch_size: "Variable (depends on sequence length)"
    
    implementation:
      complete_transformer:
        language: python
        code: |
          import numpy as np
          
          class Transformer:
              """
              Complete encoder-decoder transformer.
              
              Architecture:
              - Encoder stack: processes source sequence
              - Decoder stack: generates target sequence
              - Output projection: decoder output → vocabulary logits
              """
              
              def __init__(self, 
                          src_vocab_size: int,
                          tgt_vocab_size: int,
                          max_len: int = 512,
                          num_layers: int = 6,
                          d_model: int = 512,
                          num_heads: int = 8,
                          d_ff: int = 2048,
                          dropout: float = 0.1):
                  """
                  Args:
                      src_vocab_size: Source vocabulary size
                      tgt_vocab_size: Target vocabulary size
                      max_len: Maximum sequence length
                      num_layers: Number of encoder/decoder layers
                      d_model: Model dimension
                      num_heads: Number of attention heads
                      d_ff: Feed-forward hidden dimension
                      dropout: Dropout rate
                  """
                  # Import from previous sections
                  from section_3_12_transformer_encoder import CompleteTransformerEncoder
                  from section_3_13_transformer_decoder import TransformerDecoder
                  
                  # Encoder
                  self.encoder = CompleteTransformerEncoder(
                      src_vocab_size, max_len, num_layers,
                      d_model, num_heads, d_ff, dropout
                  )
                  
                  # Decoder
                  self.decoder = TransformerDecoder(
                      tgt_vocab_size, max_len, num_layers,
                      d_model, num_heads, d_ff, dropout
                  )
                  
                  self.d_model = d_model
                  self.tgt_vocab_size = tgt_vocab_size
              
              def forward(self,
                         src_ids: np.ndarray,
                         tgt_ids: np.ndarray,
                         src_mask: np.ndarray = None,
                         training: bool = False) -> np.ndarray:
                  """
                  Forward pass through complete transformer.
                  
                  Args:
                      src_ids: Source token IDs (batch, src_len)
                      tgt_ids: Target token IDs (batch, tgt_len)
                      src_mask: Optional source padding mask
                      training: Whether in training mode
                  
                  Returns:
                      logits: (batch, tgt_len, vocab_size)
                  """
                  # Encode source sequence
                  encoder_output, _ = self.encoder(src_ids, src_mask, training)
                  
                  # Decode target sequence (with cross-attention to encoder)
                  logits = self.decoder(tgt_ids, encoder_output, src_mask, training)
                  
                  return logits
              
              def __call__(self, src_ids: np.ndarray, tgt_ids: np.ndarray,
                          src_mask: np.ndarray = None,
                          training: bool = False) -> np.ndarray:
                  """Alias for forward."""
                  return self.forward(src_ids, tgt_ids, src_mask, training)
          
          
          # Example usage
          print("=== Complete Transformer ===\n")
          
          src_vocab_size = 30000
          tgt_vocab_size = 25000
          num_layers = 6
          d_model = 512
          num_heads = 8
          d_ff = 2048
          
          transformer = Transformer(
              src_vocab_size, tgt_vocab_size,
              num_layers=num_layers, d_model=d_model,
              num_heads=num_heads, d_ff=d_ff
          )
          
          # Sample input
          batch_size = 2
          src_len = 10
          tgt_len = 8
          
          src_ids = np.random.randint(0, src_vocab_size, (batch_size, src_len))
          tgt_ids = np.random.randint(0, tgt_vocab_size, (batch_size, tgt_len))
          
          # Forward pass
          logits = transformer(src_ids, tgt_ids, training=False)
          
          print(f"Architecture:")
          print(f"  Source vocabulary: {src_vocab_size:,}")
          print(f"  Target vocabulary: {tgt_vocab_size:,}")
          print(f"  Encoder layers: {num_layers}")
          print(f"  Decoder layers: {num_layers}")
          print(f"  Model dimension: {d_model}")
          print()
          print(f"Input:")
          print(f"  Source IDs: {src_ids.shape}")
          print(f"  Target IDs: {tgt_ids.shape}")
          print()
          print(f"Output:")
          print(f"  Logits: {logits.shape}")
          print()
          print("Complete pipeline:")
          print("  1. Encode source sequence")
          print("  2. Decode target sequence (attend to encoder)")
          print("  3. Project to vocabulary logits")
    
    security_implications:
      end_to_end_attack_surface: |
        Complete transformer combines all vulnerabilities:
        - Input encoding attacks (encoder)
        - Generation manipulation (decoder)
        - Cross-attention exploitation (encoder-decoder link)
        - Output projection attacks (vocabulary manipulation)
        - Defense: Multi-layer security, monitor entire pipeline
      
      encoder_decoder_interaction_exploits: |
        Adversary can attack encoder to corrupt decoder:
        - Poison encoder outputs via input manipulation
        - Cross-attention propagates corruption to decoder
        - Decoder generates based on corrupted context
        - Defense: Validate encoder outputs before decoder

  - topic_number: 2
    title: "Training with Teacher Forcing"
    
    overview: |
      Teacher forcing trains the decoder efficiently by feeding ground truth tokens (not 
      predictions) as input. During training, the entire target sequence is processed in 
      parallel with causal masking. This is faster than sequential generation but creates 
      train-test mismatch - training sees ground truth, inference sees predictions.
    
    content:
      teacher_forcing_explained:
        without_teacher_forcing_autoregressive: |
          Naive training (autoregressive):
          
          Step 1: Generate token₁, compare to ground truth
          Step 2: Feed token₁ (prediction), generate token₂
          Step 3: Feed token₁, token₂, generate token₃
          ...
          
          Problem: Sequential (slow), error accumulation
        
        with_teacher_forcing_parallel: |
          Teacher forcing:
          
          Input: [<START>, ground_truth₁, ground_truth₂, ...]
          Output: [ground_truth₁, ground_truth₂, ground_truth₃, ...]
          
          All positions processed in parallel!
          → Much faster training
        
        causal_masking_ensures_validity: |
          Even though all positions processed together:
          - Causal mask prevents position t seeing position t+1
          - Position t sees only ground truth tokens 0...t-1
          - Same as autoregressive but parallelized
      
      training_procedure:
        data_preparation: |
          Source: "The cat sat"
          Target: "Le chat assis"
          
          Decoder input:  [<START>, Le, chat, assis]
          Decoder target: [Le, chat, assis, <END>]
          
          → Shifted by one position
        
        forward_pass: |
          1. Encode source: encoder_output = Encoder(source)
          2. Decode target: logits = Decoder(target_input, encoder_output)
          3. Compute loss: CrossEntropy(logits, target_truth)
        
        loss_computation: |
          For each position t:
          loss_t = -log P(ground_truth_t | logits_t)
          
          Total loss = mean over all positions
          
          Only compute loss on non-padding tokens!
        
        backward_pass: |
          Standard backpropagation:
          - Gradients flow through decoder → encoder
          - Update all parameters
          - Cross-attention connects encoder and decoder gradients
      
      train_test_mismatch:
        training_condition: |
          Decoder sees ground truth tokens
          → Always has correct context
        
        inference_condition: |
          Decoder sees its own predictions
          → Errors accumulate (wrong token → wrong context → more errors)
        
        mismatch_problem: |
          Model trained on perfect inputs
          Tested on imperfect inputs (its own predictions)
          → Performance degradation
        
        solutions:
          scheduled_sampling: |
            Gradually mix predictions with ground truth during training
            → Model learns to handle its own errors
          
          exposure_bias_mitigation: |
            Train with noisy/perturbed ground truth
            → More robust to inference errors
    
    implementation:
      training_loop:
        language: python
        code: |
          def train_step(transformer: Transformer,
                        src_ids: np.ndarray,
                        tgt_ids: np.ndarray,
                        learning_rate: float = 1e-4) -> float:
              """
              Single training step with teacher forcing.
              
              Args:
                  transformer: Transformer model
                  src_ids: Source token IDs (batch, src_len)
                  tgt_ids: Target token IDs (batch, tgt_len)
                  learning_rate: Learning rate
              
              Returns:
                  loss: Cross-entropy loss
              """
              batch_size, tgt_len = tgt_ids.shape
              
              # Prepare decoder input (shift right by 1)
              # Input: [<START>, token1, token2, ...]
              # Target: [token1, token2, token3, <END>]
              
              START_TOKEN = 0  # Convention: 0 is <START>
              decoder_input = np.concatenate([
                  np.full((batch_size, 1), START_TOKEN),
                  tgt_ids[:, :-1]
              ], axis=1)
              
              decoder_target = tgt_ids  # Ground truth
              
              # Forward pass
              logits = transformer(src_ids, decoder_input, training=True)
              # logits: (batch, tgt_len, vocab_size)
              
              # Compute cross-entropy loss
              # For each position, compare predicted distribution to target token
              loss = compute_cross_entropy_loss(logits, decoder_target)
              
              # Backward pass (simplified - in practice use autograd)
              # gradients = backward(loss)
              # update_parameters(transformer, gradients, learning_rate)
              
              return loss
          
          
          def compute_cross_entropy_loss(logits: np.ndarray,
                                         targets: np.ndarray) -> float:
              """
              Compute cross-entropy loss.
              
              Args:
                  logits: (batch, seq_len, vocab_size)
                  targets: (batch, seq_len) - token IDs
              
              Returns:
                  loss: Mean cross-entropy
              """
              batch_size, seq_len, vocab_size = logits.shape
              
              # Softmax to get probabilities
              # Stabilized softmax: subtract max for numerical stability
              logits_max = np.max(logits, axis=-1, keepdims=True)
              exp_logits = np.exp(logits - logits_max)
              probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)
              
              # Gather probabilities of target tokens
              # For each position, get probability of ground truth token
              batch_indices = np.arange(batch_size)[:, None]
              seq_indices = np.arange(seq_len)[None, :]
              target_probs = probs[batch_indices, seq_indices, targets]
              
              # Cross-entropy: -log(p_target)
              # Add small epsilon to avoid log(0)
              losses = -np.log(target_probs + 1e-10)
              
              # Mean over batch and sequence
              loss = np.mean(losses)
              
              return loss
          
          
          # Example training step
          print("\n=== Training with Teacher Forcing ===\n")
          
          transformer = Transformer(30000, 25000, num_layers=6, d_model=512)
          
          # Sample batch
          batch_size = 4
          src_len = 12
          tgt_len = 10
          
          src_ids = np.random.randint(1, 30000, (batch_size, src_len))
          tgt_ids = np.random.randint(1, 25000, (batch_size, tgt_len))
          
          # Training step
          loss = train_step(transformer, src_ids, tgt_ids)
          
          print(f"Batch size: {batch_size}")
          print(f"Source length: {src_len}")
          print(f"Target length: {tgt_len}")
          print()
          print(f"Loss: {loss:.4f}")
          print()
          print("Teacher forcing:")
          print("  1. Feed ground truth tokens to decoder")
          print("  2. Process all positions in parallel (with causal mask)")
          print("  3. Compute loss comparing predictions to targets")
          print("  4. Backpropagate and update parameters")
    
    security_implications:
      teacher_forcing_exploitation: |
        Train-test mismatch creates vulnerabilities:
        - Model robust to ground truth errors (training)
        - Brittle to prediction errors (inference)
        - Adversary can exploit this mismatch
        - Example: Small input perturbation → prediction error → cascading failures
        - Defense: Scheduled sampling, adversarial training
      
      training_data_extraction: |
        Model learns to predict training data exactly:
        - Given similar input, may reproduce training examples
        - Teacher forcing reinforces memorization
        - Adversary can query model to extract training data
        - Defense: Differential privacy, output filtering

  - topic_number: 3
    title: "Inference and Decoding Strategies"
    
    overview: |
      At inference, the decoder generates sequences autoregressively - one token at a time, 
      feeding each prediction back as input for the next. Greedy decoding selects the most 
      likely token at each step (fast but suboptimal). Beam search maintains multiple 
      hypotheses to explore better generation paths (slower but higher quality).
    
    content:
      autoregressive_inference:
        sequential_generation: |
          Start with <START> token
          
          Step 1: Predict token₁ given [<START>]
          Step 2: Predict token₂ given [<START>, token₁]
          Step 3: Predict token₃ given [<START>, token₁, token₂]
          ...
          Until <END> token or max length
        
        encoder_decoder_flow: |
          1. Encode source ONCE (before generation)
          2. Initialize decoder with <START>
          3. Generate loop:
             - Decoder attends to encoder output (cross-attention)
             - Predict next token
             - Append to sequence
             - Repeat until done
        
        stopping_criteria:
          end_token: "Generate <END> token"
          max_length: "Reach maximum sequence length"
          early_stopping: "Probability threshold (beam search)"
      
      greedy_decoding:
        algorithm: |
          At each step t:
          1. Compute logits for position t
          2. token_t = argmax(softmax(logits_t))
          3. Append token_t to sequence
          4. Continue to step t+1
        
        advantages:
          - "Fast (one forward pass per token)"
          - "Deterministic (same input → same output)"
          - "Low memory (only track one sequence)"
        
        disadvantages:
          - "Locally optimal, globally suboptimal"
          - "Cannot recover from bad early choices"
          - "Misses higher-probability sequences"
        
        example_problem: |
          Position 1: Choose A (p=0.5) over B (p=0.4)
          Position 2 given A: Choose C (p=0.3)
          Position 2 given B: Choose D (p=0.5)
          
          Greedy: A→C (total p = 0.5 × 0.3 = 0.15)
          Better: B→D (total p = 0.4 × 0.5 = 0.20)
          
          → Greedy missed better sequence!
      
      beam_search:
        core_idea: |
          Maintain k best hypotheses (beam)
          At each step:
          1. Expand each hypothesis with top k tokens
          2. Keep k best overall sequences
          3. Continue until all beams end
        
        algorithm: |
          beam_size = k
          beams = [(<START>, score=0.0)]
          
          for step in range(max_len):
              candidates = []
              for beam, score in beams:
                  # Get next token probabilities
                  probs = model.predict_next(beam)
                  # Expand with top k tokens
                  for token in top_k(probs, k):
                      new_beam = beam + [token]
                      new_score = score + log(probs[token])
                      candidates.append((new_beam, new_score))
              
              # Keep k best
              beams = top_k(candidates, k)
              
              # Check if all beams ended
              if all(beam ends with <END> for beam in beams):
                  break
          
          return best(beams)
        
        beam_size_tradeoffs:
          k_1: "Greedy decoding"
          k_5: "Good balance (typical)"
          k_10: "Better quality, slower"
          k_large: "Approaches exhaustive search"
        
        advantages:
          - "Explores multiple paths"
          - "Finds higher-probability sequences"
          - "Better output quality"
        
        disadvantages:
          - "Slower (k× more computation)"
          - "More memory (track k sequences)"
          - "Can still miss global optimum"
      
      sampling_strategies:
        temperature_sampling: |
          Adjust probability distribution:
          probs = softmax(logits / temperature)
          
          temperature < 1: Sharper (more deterministic)
          temperature = 1: Standard distribution
          temperature > 1: Flatter (more random)
        
        top_k_sampling: |
          Sample from only top-k most likely tokens
          → Avoid sampling very unlikely tokens
        
        nucleus_top_p_sampling: |
          Sample from smallest set where cumulative prob ≥ p
          → Dynamic vocabulary based on distribution
    
    implementation:
      greedy_decoder:
        language: python
        code: |
          def greedy_decode(transformer: Transformer,
                           src_ids: np.ndarray,
                           max_len: int = 50,
                           start_token: int = 0,
                           end_token: int = 1) -> np.ndarray:
              """
              Greedy decoding: select most likely token at each step.
              
              Args:
                  transformer: Trained transformer model
                  src_ids: Source token IDs (batch, src_len)
                  max_len: Maximum generation length
                  start_token: Start token ID
                  end_token: End token ID
              
              Returns:
                  generated: Generated token IDs (batch, gen_len)
              """
              batch_size = src_ids.shape[0]
              
              # Encode source (done once)
              encoder_output, _ = transformer.encoder(src_ids, training=False)
              
              # Initialize decoder input with <START>
              decoder_input = np.full((batch_size, 1), start_token)
              
              # Generate tokens sequentially
              for step in range(max_len):
                  # Decode current sequence
                  logits = transformer.decoder(
                      decoder_input, encoder_output, training=False
                  )
                  
                  # Get logits for last position (next token prediction)
                  next_token_logits = logits[:, -1, :]  # (batch, vocab_size)
                  
                  # Greedy: select most likely token
                  next_token = np.argmax(next_token_logits, axis=-1, keepdims=True)
                  # (batch, 1)
                  
                  # Append to sequence
                  decoder_input = np.concatenate([decoder_input, next_token], axis=1)
                  
                  # Check if all sequences ended
                  if np.all(next_token == end_token):
                      break
              
              # Remove <START> token
              generated = decoder_input[:, 1:]
              
              return generated
          
          
          print("\n=== Greedy Decoding ===\n")
          
          # Simulate source
          src_ids = np.random.randint(1, 30000, (1, 10))
          
          # Greedy decode
          generated = greedy_decode(transformer, src_ids, max_len=20)
          
          print(f"Source length: {src_ids.shape[1]}")
          print(f"Generated length: {generated.shape[1]}")
          print()
          print("Greedy decoding:")
          print("  1. Start with <START> token")
          print("  2. At each step, select argmax(probabilities)")
          print("  3. Feed prediction back as input")
          print("  4. Continue until <END> or max length")
      
      beam_search_decoder:
        language: python
        code: |
          def beam_search_decode(transformer: Transformer,
                                src_ids: np.ndarray,
                                beam_size: int = 5,
                                max_len: int = 50,
                                start_token: int = 0,
                                end_token: int = 1) -> np.ndarray:
              """
              Beam search decoding: maintain k best hypotheses.
              
              Args:
                  transformer: Trained transformer model
                  src_ids: Source token IDs (1, src_len) - single sequence
                  beam_size: Number of beams to maintain
                  max_len: Maximum generation length
                  start_token: Start token ID
                  end_token: End token ID
              
              Returns:
                  best_sequence: Best generated sequence (gen_len,)
              """
              # Encode source
              encoder_output, _ = transformer.encoder(src_ids, training=False)
              # (1, src_len, d_model)
              
              # Initialize beams: [(sequence, score)]
              beams = [([start_token], 0.0)]
              
              for step in range(max_len):
                  candidates = []
                  
                  for sequence, score in beams:
                      # Skip if sequence already ended
                      if sequence[-1] == end_token:
                          candidates.append((sequence, score))
                          continue
                      
                      # Prepare decoder input
                      decoder_input = np.array([sequence])  # (1, seq_len)
                      
                      # Get next token logits
                      logits = transformer.decoder(
                          decoder_input, encoder_output, training=False
                      )
                      next_token_logits = logits[0, -1, :]  # (vocab_size,)
                      
                      # Convert to log probabilities
                      log_probs = next_token_logits - np.logaddexp.reduce(next_token_logits)
                      
                      # Get top-k tokens
                      top_k_indices = np.argsort(log_probs)[-beam_size:]
                      
                      for token_id in top_k_indices:
                          new_sequence = sequence + [token_id]
                          new_score = score + log_probs[token_id]
                          candidates.append((new_sequence, new_score))
                  
                  # Keep top-k candidates
                  candidates.sort(key=lambda x: x[1], reverse=True)
                  beams = candidates[:beam_size]
                  
                  # Check if all beams ended
                  if all(seq[-1] == end_token for seq, _ in beams):
                      break
              
              # Return best sequence (highest score)
              best_sequence, best_score = beams[0]
              
              return np.array(best_sequence[1:])  # Remove <START>
          
          
          print("\n=== Beam Search Decoding ===\n")
          
          # Simulate source
          src_ids = np.random.randint(1, 30000, (1, 10))
          
          # Beam search decode
          generated = beam_search_decode(transformer, src_ids, beam_size=5, max_len=20)
          
          print(f"Source length: {src_ids.shape[1]}")
          print(f"Generated length: {len(generated)}")
          print(f"Beam size: 5")
          print()
          print("Beam search:")
          print("  1. Maintain k=5 best hypotheses")
          print("  2. At each step, expand each beam with top-k tokens")
          print("  3. Keep k best overall sequences")
          print("  4. Return highest-scoring complete sequence")
    
    security_implications:
      beam_search_manipulation: |
        Adversary can exploit beam search:
        - Craft inputs that maximize specific outputs
        - Force model to generate adversarial content
        - Extract training data by beam search over prompts
        - Defense: Limit beam size, output filtering
      
      autoregressive_vulnerabilities: |
        Sequential generation creates dependencies:
        - Early token errors cascade through generation
        - Adversary can manipulate early tokens for late effects
        - One bad token → entire sequence corrupted
        - Defense: Monitor generation quality, early stopping
      
      inference_time_attacks: |
        Inference is slower than training:
        - Adversary can DoS via expensive queries (large beam size)
        - Resource exhaustion through long sequences
        - Defense: Rate limiting, beam size caps, length limits

key_takeaways:
  critical_concepts:
    - concept: "Complete transformer = encoder stack + decoder stack + output projection"
      why_it_matters: "Foundation for seq2seq: translation, summarization, dialogue"
    
    - concept: "Teacher forcing: feed ground truth during training (parallel, fast)"
      why_it_matters: "Efficient training but creates train-test mismatch"
    
    - concept: "Autoregressive inference: generate one token at a time sequentially"
      why_it_matters: "Slower than training, errors can accumulate"
    
    - concept: "Beam search maintains k hypotheses for better generation"
      why_it_matters: "Higher quality than greedy, finds higher-probability sequences"
    
    - concept: "Encoder computed once, reused for all decoder steps"
      why_it_matters: "Efficient - don't recompute encoder for each token"
  
  actionable_steps:
    - step: "Implement complete encoder-decoder transformer"
      verification: "Encoder stack → decoder stack → output projection"
    
    - step: "Build training loop with teacher forcing"
      verification: "Parallel target processing with causal mask, cross-entropy loss"
    
    - step: "Create greedy decoder for autoregressive inference"
      verification: "Sequential generation, argmax selection"
    
    - step: "Implement beam search decoder"
      verification: "Maintain k beams, expand and prune at each step"
    
    - step: "Understand train-test mismatch and mitigation strategies"
      verification: "Scheduled sampling, exposure bias solutions"
  
  security_principles:
    - principle: "Complete transformer combines all encoder and decoder vulnerabilities"
      application: "End-to-end security, monitor entire pipeline"
    
    - principle: "Teacher forcing creates train-test mismatch"
      application: "Adversary exploits difference between training and inference"
    
    - principle: "Beam search can be manipulated for targeted generation"
      application: "Craft inputs to force specific outputs via beam search"
    
    - principle: "Autoregressive generation creates error cascades"
      application: "Early perturbations propagate through entire sequence"
    
    - principle: "Inference slower than training (sequential vs parallel)"
      application: "DoS attacks via expensive inference queries"
  
  common_mistakes:
    - mistake: "Not shifting decoder input during teacher forcing"
      fix: "Input: [<START>, tok1, tok2], Target: [tok1, tok2, <END>]"
    
    - mistake: "Recomputing encoder for each decoder step during inference"
      fix: "Encode source ONCE, cache and reuse encoder output"
    
    - mistake: "Forgetting causal mask during training with teacher forcing"
      fix: "ALWAYS apply causal mask to prevent future leakage"
    
    - mistake: "Using different padding strategies for train vs inference"
      fix: "Consistent padding/masking in both modes"
    
    - mistake: "Beam search without length normalization"
      fix: "Normalize scores by length to avoid bias toward short sequences"
  
  integration_with_book:
    from_section_3_12:
      - "Transformer encoder (first half of complete transformer)"
      - "Encoder stack architecture and implementation"
    
    from_section_3_13:
      - "Transformer decoder (second half)"
      - "Masked self-attention and cross-attention"
    
    to_next_section:
      - "Section 3.15: GPT and decoder-only transformers"
      - "Removing encoder, using only masked self-attention"
      - "Language modeling objective"
  
  looking_ahead:
    next_concepts:
      - "Decoder-only transformers (GPT architecture)"
      - "Language modeling vs seq2seq"
      - "Scaling transformers (GPT-2, GPT-3)"
      - "Emergent capabilities from scale"
    
    skills_to_build:
      - "Implement GPT-style decoder-only model"
      - "Train language models"
      - "Generate text with GPT"
      - "Understand scaling laws"
  
  final_thoughts: |
    The complete transformer (Vaswani et al., 2017) revolutionized NLP by combining 
    encoder and decoder stacks for sequence-to-sequence tasks. The encoder processes input 
    sequences with bidirectional self-attention, the decoder generates outputs autoregressively 
    using masked self-attention and cross-attention to encoder outputs. This architecture 
    powers machine translation, summarization, and many other seq2seq applications.
    
    Training uses teacher forcing - feeding ground truth tokens to the decoder and processing 
    all positions in parallel with causal masking. This is dramatically faster than sequential 
    generation but creates train-test mismatch: the model trains on perfect ground truth but 
    must inference using its own (potentially incorrect) predictions. Solutions include 
    scheduled sampling and exposure bias mitigation.
    
    At inference, the decoder generates sequences autoregressively - one token at a time, 
    feeding each prediction back as input. Greedy decoding selects the most likely token 
    at each step (fast, deterministic, suboptimal). Beam search maintains k hypotheses to 
    explore multiple paths (slower, higher quality, finds better sequences). The encoder 
    is computed once and reused for all decoder steps for efficiency.
    
    From a security perspective: complete transformers combine all encoder and decoder 
    vulnerabilities end-to-end. Teacher forcing train-test mismatch can be exploited - 
    models robust to ground truth errors but brittle to prediction errors. Beam search can 
    be manipulated to extract training data or generate targeted outputs. Autoregressive 
    generation creates error cascades where early perturbations propagate through the 
    entire sequence. Inference is sequential (slower than parallel training), creating DoS 
    vulnerabilities via expensive queries.
    
    Next: Section 3.15 explores GPT and decoder-only transformers. By removing the encoder 
    and using only masked self-attention, we get powerful language models that generate 
    text through pure autoregressive prediction. This architecture scales remarkably well, 
    leading to GPT-2, GPT-3, and modern LLMs.

---
