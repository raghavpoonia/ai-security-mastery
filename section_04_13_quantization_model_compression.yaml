# section_04_13_quantization_model_compression.yaml

---
document_info:
  title: "Quantization and Model Compression for Deployment"
  book: "AI Security Mastery: From ML Fundamentals to Production Detection Systems"
  chapter: 4
  section: 13
  part: 3
  author: "Raghav Dinesh"
  github: "https://github.com/raghavpoonia/ai-security-mastery"
  license: "MIT"
  created: "2026-01-28"
  version: "1.0"
  description: |
    Complete guide to quantization and model compression for efficient LLM deployment.
    Covers post-training quantization (PTQ) techniques (INT8, INT4, FP16), quantization-
    aware training (QAT), advanced methods (GPTQ, AWQ, SmoothQuant), model pruning,
    knowledge distillation, and accuracy-performance trade-offs. Implements quantization
    from scratch with NumPy, deploys with production libraries (bitsandbytes, GPTQ),
    and measures impact on quality, speed, and memory. Security analysis covering
    bit-flip attacks, quantization exploitation, and model extraction. Essential for
    deploying large models on resource-constrained hardware.
  estimated_pages: 7
  tags:
    - quantization
    - model-compression
    - int8
    - int4
    - gptq
    - awq
    - pruning
    - distillation
    - deployment-optimization

section_overview:
  title: "Quantization and Model Compression for Deployment"
  number: "4.13"
  
  purpose: |
    Section 4.12 built optimized serving infrastructure with batching and load balancing.
    We can now serve models efficiently at scale. But large models (7B, 13B, 70B parameters)
    consume massive memory: a 7B model in FP16 requires 14GB GPU memory, a 70B model needs
    140GB. This limits deployment options and increases costs dramatically.
    
    Quantization and compression techniques reduce model size 2-4x while maintaining 95-99%
    of original quality. INT8 quantization halves memory (7B → 7GB), INT4 quarters it
    (7B → 3.5GB). This enables deploying larger models on smaller GPUs, fitting more models
    per GPU, and reducing inference costs proportionally.
    
    This section covers complete quantization pipeline: theory, implementation from scratch,
    production deployment, and quality-performance trade-offs. Understanding quantization
    is critical—it's the difference between needing $10K/month in GPUs versus $2.5K/month
    for the same throughput.
  
  learning_objectives:
    conceptual:
      - "Understand quantization fundamentals: precision reduction and its impact"
      - "Grasp post-training quantization (PTQ) vs quantization-aware training (QAT)"
      - "Comprehend advanced methods: GPTQ, AWQ, SmoothQuant algorithms"
      - "Understand compression techniques: pruning, distillation, and their trade-offs"
    
    practical:
      - "Implement INT8 quantization from scratch with NumPy"
      - "Deploy quantized models with bitsandbytes and GPTQ libraries"
      - "Measure accuracy impact and calibrate quantization parameters"
      - "Apply pruning and distillation for additional compression"
    
    security_focused:
      - "Identify bit-flip attacks exploiting quantization noise"
      - "Prevent adversarial quantization that degrades specific inputs"
      - "Detect model extraction through quantization artifacts"
      - "Implement secure quantization with integrity verification"
  
  prerequisites:
    knowledge:
      - "Section 4.12: Model serving and inference optimization"
      - "Section 3: Transformer architecture and matrix operations"
      - "Understanding of floating-point representation"
      - "Basic knowledge of numerical precision and rounding"
    
    skills:
      - "Working with PyTorch and model architectures"
      - "Understanding matrix operations and tensor manipulation"
      - "Profiling memory usage and inference speed"
      - "Evaluating model quality and perplexity"
  
  key_transitions:
    from_section_4_12: |
      Section 4.12 optimized serving infrastructure with batching and load balancing,
      maximizing throughput for a given model size. But large models still require
      expensive GPUs and limit deployment options.
      
      Section 4.13 attacks the problem from the model side: reduce model size through
      quantization and compression. This enables deploying on cheaper hardware, fitting
      more models per GPU, and reducing costs. Combined with 4.12's serving optimizations,
      this creates economically viable production systems.
    
    to_next_section: |
      Section 4.13 covers model-level optimization (quantization, compression). Section
      4.14 advances to infrastructure scaling: horizontal scaling patterns, distributed
      serving, and multi-region deployment. Together they enable serving large models
      globally at scale.

topics:
  - topic_number: 1
    title: "Quantization Fundamentals and Post-Training Quantization (PTQ)"
    
    overview: |
      Quantization reduces numerical precision of model weights and activations, trading
      accuracy for efficiency. Instead of 32-bit floats (4 bytes per parameter), use 16-bit
      floats (2 bytes), 8-bit integers (1 byte), or even 4-bit integers (0.5 bytes). This
      reduces memory proportionally and often improves inference speed.
      
      Post-training quantization (PTQ) quantizes already-trained models without retraining.
      It's fast and simple but can degrade accuracy if not done carefully. Calibration on
      representative data minimizes quality loss. Understanding PTQ fundamentals enables
      effective deployment on resource-constrained hardware.
      
      We implement quantization from scratch, understand calibration strategies, deploy
      with production libraries, and measure trade-offs between size, speed, and quality.
    
    content:
      quantization_fundamentals:
        numerical_precision_basics: |
          Numerical precision and memory requirements:
          
          **Floating-point formats**:
          - FP32 (32-bit float): 4 bytes per parameter
          - FP16 (16-bit float): 2 bytes per parameter (50% reduction)
          - BF16 (bfloat16): 2 bytes, better range than FP16
          
          **Integer formats**:
          - INT8 (8-bit integer): 1 byte per parameter (75% reduction)
          - INT4 (4-bit integer): 0.5 bytes per parameter (87.5% reduction)
          
          **Model size examples**:
```
          Llama 2 7B model:
          - FP32: 7B params × 4 bytes = 28 GB
          - FP16: 7B params × 2 bytes = 14 GB
          - INT8: 7B params × 1 byte  = 7 GB
          - INT4: 7B params × 0.5 bytes = 3.5 GB
          
          Llama 2 70B model:
          - FP32: 280 GB (doesn't fit on single GPU)
          - FP16: 140 GB (barely fits on A100 80GB)
          - INT8: 70 GB (fits comfortably)
          - INT4: 35 GB (fits on single GPU easily)
```
          
          Quantization enables deploying models that wouldn't otherwise fit.
        
        quantization_formulas: |
          Quantization mathematics:
          
          **Symmetric quantization** (zero-point = 0):
```
          scale = max(|W|) / (2^(bits-1) - 1)
          
          W_quantized = round(W / scale)
          W_dequantized = W_quantized × scale
```
          
          Example (INT8, symmetric):
```python
          W = [0.5, -0.3, 1.2, -0.8]  # FP32 weights
          
          # Find scale
          max_val = max(abs(W))  # 1.2
          scale = max_val / 127  # 0.00945
          
          # Quantize
          W_quantized = round(W / scale)
          # [53, -32, 127, -85]
          
          # Dequantize
          W_dequantized = W_quantized * scale
          # [0.501, -0.302, 1.200, -0.803]
```
          
          **Asymmetric quantization** (non-zero zero-point):
```
          scale = (max(W) - min(W)) / (2^bits - 1)
          zero_point = round(-min(W) / scale)
          
          W_quantized = round(W / scale) + zero_point
          W_dequantized = (W_quantized - zero_point) × scale
```
          
          **Per-tensor vs per-channel**:
          - Per-tensor: Single scale for entire tensor
          - Per-channel: Different scale per output channel
          - Per-channel: Better accuracy, more overhead
        
        quantization_granularity: |
          Quantization granularity options:
          
          **Per-tensor quantization**:
```python
          def quantize_per_tensor(W: np.ndarray, bits: int = 8) -> tuple:
              """Quantize entire tensor with single scale."""
              max_val = np.abs(W).max()
              scale = max_val / (2**(bits-1) - 1)
              
              W_quantized = np.round(W / scale).astype(np.int8)
              
              return W_quantized, scale
```
          
          - Simplest approach
          - One scale factor per tensor
          - Good for weights with similar distributions
          
          **Per-channel quantization**:
```python
          def quantize_per_channel(W: np.ndarray, bits: int = 8) -> tuple:
              """Quantize each output channel separately."""
              # W shape: (out_channels, in_channels)
              out_channels = W.shape[0]
              scales = np.zeros(out_channels)
              W_quantized = np.zeros_like(W, dtype=np.int8)
              
              for i in range(out_channels):
                  max_val = np.abs(W[i]).max()
                  scale = max_val / (2**(bits-1) - 1)
                  scales[i] = scale
                  
                  W_quantized[i] = np.round(W[i] / scale).astype(np.int8)
              
              return W_quantized, scales
```
          
          - Better accuracy (different channels different scales)
          - More overhead (store scale per channel)
          - Standard for production
          
          **Group quantization**:
          - Quantize groups of weights (e.g., 128 weights per group)
          - Balance between per-tensor and per-channel
          - Used in GPTQ, AWQ
      
      post_training_quantization:
        dynamic_quantization: |
          Dynamic quantization: Quantize weights statically, activations dynamically
```python
          class DynamicQuantizedLinear:
              """Dynamically quantized linear layer."""
              
              def __init__(self, weight: np.ndarray):
                  """
                  Initialize with FP32 weight.
                  
                  Args:
                      weight: FP32 weight matrix
                  """
                  # Quantize weight statically (offline)
                  self.weight_quantized, self.weight_scale = quantize_per_channel(
                      weight, bits=8
                  )
              
              def forward(self, x: np.ndarray) -> np.ndarray:
                  """
                  Forward pass with dynamic quantization.
                  
                  Args:
                      x: FP32 input activations
                  
                  Returns:
                      FP32 output
                  """
                  # Quantize activations dynamically (online)
                  x_quantized, x_scale = quantize_per_tensor(x, bits=8)
                  
                  # INT8 matrix multiplication
                  output_quantized = np.matmul(
                      x_quantized.astype(np.int32),
                      self.weight_quantized.T.astype(np.int32)
                  )
                  
                  # Dequantize output
                  output = output_quantized.astype(np.float32) * x_scale * self.weight_scale
                  
                  return output
```
          
          **Characteristics**:
          - Weights quantized offline (static)
          - Activations quantized at runtime (dynamic)
          - No calibration needed
          - Good accuracy, less speedup than static
        
        static_quantization: |
          Static quantization: Quantize weights and activations statically
          
          **Requires calibration**:
```python
          def calibrate_activations(model, calibration_data, bits=8):
              """
              Calibrate activation quantization parameters.
              
              Args:
                  model: Model to calibrate
                  calibration_data: Representative dataset
                  bits: Quantization bits
              
              Returns:
                  Dictionary of activation scales
              """
              activation_ranges = {}
              
              # Forward pass on calibration data
              for batch in calibration_data:
                  with torch.no_grad():
                      # Collect activation statistics
                      outputs = model(batch)
                      
                      for name, activation in model.activations.items():
                          if name not in activation_ranges:
                              activation_ranges[name] = []
                          activation_ranges[name].append(
                              (activation.min().item(), activation.max().item())
                          )
              
              # Compute scales
              scales = {}
              for name, ranges in activation_ranges.items():
                  min_val = min(r[0] for r in ranges)
                  max_val = max(r[1] for r in ranges)
                  scales[name] = (max_val - min_val) / (2**bits - 1)
              
              return scales
```
          
          **Characteristics**:
          - Both weights and activations quantized offline
          - Requires calibration dataset
          - Better speedup than dynamic
          - Slightly more accuracy loss
        
        calibration_strategies: |
          Calibration strategies for minimizing quantization error:
          
          **1. MinMax calibration** (simple):
```python
          def calibrate_minmax(activations):
              """Use min/max of activations."""
              return activations.min(), activations.max()
```
          - Simple but sensitive to outliers
          
          **2. Percentile calibration**:
```python
          def calibrate_percentile(activations, percentile=99.9):
              """Use percentile to ignore outliers."""
              min_val = np.percentile(activations, 100 - percentile)
              max_val = np.percentile(activations, percentile)
              return min_val, max_val
```
          - Robust to outliers
          - Better accuracy in practice
          
          **3. MSE minimization**:
```python
          def calibrate_mse(activations, bits=8):
              """Find scale minimizing MSE."""
              best_scale = None
              best_mse = float('inf')
              
              # Try different scales
              for alpha in np.linspace(0.9, 1.1, 20):
                  scale = activations.abs().max() * alpha / (2**(bits-1) - 1)
                  
                  # Quantize and dequantize
                  quantized = np.round(activations / scale).clip(
                      -(2**(bits-1)), 2**(bits-1) - 1
                  )
                  dequantized = quantized * scale
                  
                  # Compute MSE
                  mse = ((activations - dequantized) ** 2).mean()
                  
                  if mse < best_mse:
                      best_mse = mse
                      best_scale = scale
              
              return best_scale
```
          - Most accurate
          - More expensive
          
          **4. Entropy calibration** (KL divergence):
          - Minimize KL divergence between original and quantized distributions
          - Used in TensorRT
      
      mixed_precision:
        fp16_deployment: |
          FP16 (half-precision) deployment:
```python
          import torch
          
          # Convert model to FP16
          model = model.half()
          
          # Or use automatic mixed precision
          from torch.cuda.amp import autocast
          
          with autocast():
              output = model(input)
```
          
          **Benefits**:
          - 50% memory reduction
          - 2x faster on modern GPUs (Tensor Cores)
          - Minimal accuracy loss (<1%)
          
          **Limitations**:
          - Some operations still need FP32 (softmax, layer norm)
          - Gradient underflow in training (solved by loss scaling)
          
          **BFloat16 (BF16)**:
          - Same exponent range as FP32
          - Better for training than FP16
          - Supported on newer GPUs (A100, H100)
        
        mixed_precision_strategies: |
          Mixed precision: Different precisions for different layers
```python
          def apply_mixed_precision(model):
              """
              Apply mixed precision strategically.
              
              Keep sensitive layers in FP16, quantize others to INT8.
              """
              for name, module in model.named_modules():
                  if isinstance(module, nn.Linear):
                      # Attention layers: FP16 (sensitive)
                      if 'attention' in name:
                          module.half()
                      # FFN layers: INT8 (less sensitive)
                      else:
                          module = quantize_linear_int8(module)
```
          
          **Principles**:
          - Attention layers: FP16 (quality critical)
          - FFN layers: INT8 (less impact)
          - First/last layers: FP16 (embeddings, head)
          - Activations: FP16 or INT8
          
          **Quality-memory trade-off**:
```
          All FP16: 14 GB, 100% quality
          Mixed FP16/INT8: 10 GB, 99% quality
          All INT8: 7 GB, 97% quality
          Mixed INT8/INT4: 5 GB, 95% quality
          All INT4: 3.5 GB, 90% quality
```
    
    implementation:
      int8_quantization_from_scratch:
        language: python
        code: |
          """
          INT8 quantization implementation from scratch.
          Demonstrates symmetric and asymmetric quantization with calibration.
          """
          
          import numpy as np
          from typing import Tuple, Optional
          
          def quantize_symmetric(weights: np.ndarray,
                                 bits: int = 8) -> Tuple[np.ndarray, float]:
              """
              Symmetric quantization (zero-point = 0).
              
              Args:
                  weights: FP32 weights
                  bits: Quantization bits
              
              Returns:
                  (quantized_weights, scale)
              """
              # Compute scale
              max_val = np.abs(weights).max()
              scale = max_val / (2**(bits-1) - 1)
              
              # Quantize
              quantized = np.round(weights / scale)
              
              # Clip to range
              min_int = -(2**(bits-1))
              max_int = 2**(bits-1) - 1
              quantized = np.clip(quantized, min_int, max_int)
              
              # Convert to appropriate integer type
              if bits == 8:
                  quantized = quantized.astype(np.int8)
              elif bits == 4:
                  quantized = quantized.astype(np.int8)  # Store in int8, use only 4 bits
              
              return quantized, scale
          
          
          def dequantize_symmetric(quantized: np.ndarray,
                                  scale: float) -> np.ndarray:
              """
              Dequantize symmetric quantization.
              
              Args:
                  quantized: Quantized weights
                  scale: Scale factor
              
              Returns:
                  Dequantized FP32 weights
              """
              return quantized.astype(np.float32) * scale
          
          
          def quantize_asymmetric(weights: np.ndarray,
                                 bits: int = 8) -> Tuple[np.ndarray, float, int]:
              """
              Asymmetric quantization (with zero-point).
              
              Args:
                  weights: FP32 weights
                  bits: Quantization bits
              
              Returns:
                  (quantized_weights, scale, zero_point)
              """
              # Compute scale and zero-point
              min_val = weights.min()
              max_val = weights.max()
              
              q_min = 0
              q_max = 2**bits - 1
              
              scale = (max_val - min_val) / (q_max - q_min)
              zero_point = q_min - np.round(min_val / scale)
              zero_point = int(np.clip(zero_point, q_min, q_max))
              
              # Quantize
              quantized = np.round(weights / scale) + zero_point
              quantized = np.clip(quantized, q_min, q_max)
              
              if bits == 8:
                  quantized = quantized.astype(np.uint8)
              
              return quantized, scale, zero_point
          
          
          def dequantize_asymmetric(quantized: np.ndarray,
                                   scale: float,
                                   zero_point: int) -> np.ndarray:
              """
              Dequantize asymmetric quantization.
              
              Args:
                  quantized: Quantized weights
                  scale: Scale factor
                  zero_point: Zero-point
              
              Returns:
                  Dequantized FP32 weights
              """
              return (quantized.astype(np.float32) - zero_point) * scale
          
          
          def quantize_per_channel(weights: np.ndarray,
                                  bits: int = 8,
                                  axis: int = 0) -> Tuple[np.ndarray, np.ndarray]:
              """
              Per-channel symmetric quantization.
              
              Args:
                  weights: FP32 weights (out_channels, in_channels)
                  bits: Quantization bits
                  axis: Channel axis (0 for output channels)
              
              Returns:
                  (quantized_weights, scales)
              """
              num_channels = weights.shape[axis]
              scales = np.zeros(num_channels)
              quantized = np.zeros_like(weights, dtype=np.int8)
              
              for i in range(num_channels):
                  if axis == 0:
                      channel_weights = weights[i]
                  else:
                      channel_weights = weights[:, i]
                  
                  # Quantize channel
                  q, s = quantize_symmetric(channel_weights, bits)
                  
                  if axis == 0:
                      quantized[i] = q
                  else:
                      quantized[:, i] = q
                  scales[i] = s
              
              return quantized, scales
          
          
          class QuantizedLinear:
              """
              Quantized linear layer (INT8 weights, INT8 activations).
              
              Implements y = x @ W.T with quantized computation.
              """
              
              def __init__(self, weight: np.ndarray):
                  """
                  Initialize with FP32 weight.
                  
                  Args:
                      weight: FP32 weight matrix (out_features, in_features)
                  """
                  self.weight_fp32 = weight
                  
                  # Quantize weights (per-channel)
                  self.weight_quantized, self.weight_scales = quantize_per_channel(
                      weight, bits=8, axis=0
                  )
                  
                  self.out_features, self.in_features = weight.shape
              
              def forward(self, x: np.ndarray) -> np.ndarray:
                  """
                  Forward pass with INT8 computation.
                  
                  Args:
                      x: FP32 input (batch_size, in_features)
                  
                  Returns:
                      FP32 output (batch_size, out_features)
                  """
                  # Quantize activations (per-tensor)
                  x_quantized, x_scale = quantize_symmetric(x, bits=8)
                  
                  # INT8 matrix multiplication
                  # output = x @ W.T
                  output_quantized = np.matmul(
                      x_quantized.astype(np.int32),
                      self.weight_quantized.T.astype(np.int32)
                  )
                  
                  # Dequantize output (per-channel)
                  # Each output channel has different scale
                  output = output_quantized.astype(np.float32)
                  output = output * x_scale * self.weight_scales.reshape(1, -1)
                  
                  return output
              
              def get_memory_savings(self) -> dict:
                  """Calculate memory savings."""
                  fp32_size = self.weight_fp32.nbytes
                  int8_size = self.weight_quantized.nbytes + self.weight_scales.nbytes
                  
                  return {
                      "fp32_mb": fp32_size / (1024**2),
                      "int8_mb": int8_size / (1024**2),
                      "reduction": (fp32_size - int8_size) / fp32_size * 100
                  }
          
          
          def measure_quantization_error(original: np.ndarray,
                                        quantized: np.ndarray,
                                        scale: float) -> dict:
              """
              Measure quantization error metrics.
              
              Args:
                  original: Original FP32 weights
                  quantized: Quantized weights
                  scale: Scale factor
              
              Returns:
                  Dictionary of error metrics
              """
              # Dequantize
              reconstructed = dequantize_symmetric(quantized, scale)
              
              # Compute errors
              abs_error = np.abs(original - reconstructed)
              rel_error = abs_error / (np.abs(original) + 1e-8)
              
              return {
                  "max_abs_error": abs_error.max(),
                  "mean_abs_error": abs_error.mean(),
                  "max_rel_error": rel_error.max(),
                  "mean_rel_error": rel_error.mean(),
                  "mse": ((original - reconstructed) ** 2).mean(),
                  "snr_db": 10 * np.log10(
                      (original ** 2).mean() / ((original - reconstructed) ** 2).mean()
                  )
              }
          
          
          def demonstrate_int8_quantization():
              """Demonstrate INT8 quantization."""
              print("\n" + "="*80)
              print("INT8 QUANTIZATION FROM SCRATCH")
              print("="*80)
              
              # Create sample weight matrix
              np.random.seed(42)
              weight = np.random.randn(512, 768).astype(np.float32) * 0.1
              
              print("\n" + "-"*80)
              print("SYMMETRIC QUANTIZATION")
              print("-"*80)
              
              # Quantize
              weight_q, scale = quantize_symmetric(weight, bits=8)
              
              print(f"Original shape: {weight.shape}")
              print(f"Original dtype: {weight.dtype}")
              print(f"Original range: [{weight.min():.4f}, {weight.max():.4f}]")
              print(f"Original size: {weight.nbytes / 1024:.2f} KB")
              print()
              print(f"Quantized dtype: {weight_q.dtype}")
              print(f"Quantized range: [{weight_q.min()}, {weight_q.max()}]")
              print(f"Quantized size: {weight_q.nbytes / 1024:.2f} KB")
              print(f"Scale: {scale:.6f}")
              print(f"Memory reduction: {(1 - weight_q.nbytes / weight.nbytes) * 100:.1f}%")
              
              # Measure error
              errors = measure_quantization_error(weight, weight_q, scale)
              print("\nQuantization errors:")
              print(f"  Max absolute error: {errors['max_abs_error']:.6f}")
              print(f"  Mean absolute error: {errors['mean_abs_error']:.6f}")
              print(f"  SNR: {errors['snr_db']:.2f} dB")
              
              # Per-channel quantization
              print("\n" + "-"*80)
              print("PER-CHANNEL QUANTIZATION")
              print("-"*80)
              
              weight_q_pc, scales_pc = quantize_per_channel(weight, bits=8, axis=0)
              
              print(f"Number of scales: {len(scales_pc)}")
              print(f"Scale range: [{scales_pc.min():.6f}, {scales_pc.max():.6f}]")
              print(f"Quantized size: {(weight_q_pc.nbytes + scales_pc.nbytes) / 1024:.2f} KB")
              
              # Compare errors
              errors_pc = {}
              for i in range(weight.shape[0]):
                  recon = weight_q_pc[i].astype(np.float32) * scales_pc[i]
                  errors_pc[i] = np.abs(weight[i] - recon).mean()
              
              print(f"Mean error per-channel: {np.mean(list(errors_pc.values())):.6f}")
              print(f"Per-channel improves error by: {(1 - np.mean(list(errors_pc.values())) / errors['mean_abs_error']) * 100:.1f}%")
              
              # Test quantized linear layer
              print("\n" + "-"*80)
              print("QUANTIZED LINEAR LAYER")
              print("-"*80)
              
              layer = QuantizedLinear(weight)
              
              # Create input
              x = np.random.randn(4, 768).astype(np.float32) * 0.5
              
              # FP32 forward
              output_fp32 = x @ weight.T
              
              # INT8 forward
              output_int8 = layer.forward(x)
              
              # Compare
              output_error = np.abs(output_fp32 - output_int8)
              print(f"Input shape: {x.shape}")
              print(f"Output shape: {output_int8.shape}")
              print(f"Output error (max): {output_error.max():.4f}")
              print(f"Output error (mean): {output_error.mean():.4f}")
              print(f"Output relative error: {(output_error / (np.abs(output_fp32) + 1e-8)).mean() * 100:.2f}%")
              
              # Memory savings
              savings = layer.get_memory_savings()
              print(f"\nMemory savings:")
              print(f"  FP32: {savings['fp32_mb']:.2f} MB")
              print(f"  INT8: {savings['int8_mb']:.2f} MB")
              print(f"  Reduction: {savings['reduction']:.1f}%")
          
          
          if __name__ == "__main__":
              demonstrate_int8_quantization()
    
    security_implications:
      bit_flip_attacks: |
        **Vulnerability**: Bit-flip attacks exploit quantization by flipping specific bits
        in quantized weights, causing model misbehavior with minimal changes.
        
        **Attack scenario**: Attacker gains access to quantized model weights
        - Identify critical weights (high gradient, sensitive layers)
        - Flip single bits in INT8 representation
        - Small change in quantized domain → large change in FP32 domain
        - Model produces incorrect outputs for targeted inputs
        
        Example:
```
        INT8 weight: 127 (binary: 01111111)
        Flip MSB:   -128 (binary: 10000000)
        
        FP32 change: 127 * scale → -128 * scale (255 * scale difference!)
```
        
        **Defense**:
        1. Model integrity checks: Cryptographic hashes of weights
        2. Redundancy: Multiple model copies, majority voting
        3. Bit error detection: ECC (Error-Correcting Codes)
        4. Runtime verification: Test on known inputs periodically
        5. Secure storage: Encrypt weights at rest
        6. Access control: Limit who can modify model files
        7. Anomaly detection: Monitor output distributions
      
      adversarial_quantization: |
        **Vulnerability**: Adversarial quantization creates inputs that exploit quantization
        noise, causing larger errors in quantized models than original models.
        
        **Attack scenario**: Attacker crafts inputs specifically for quantized model
        - Inputs designed to maximize quantization error
        - Exploit discontinuities introduced by rounding
        - Cause classification errors or quality degradation
        - Attack transfers poorly to FP32 model (detectable difference)
        
        **Defense**:
        1. Quantization-aware adversarial training: Train with adversarial examples
        2. Input preprocessing: Smooth inputs to reduce quantization sensitivity
        3. Ensemble models: FP32 + quantized, flag disagreements
        4. Calibration robustness: Use robust calibration (percentile vs minmax)
        5. Mixed precision: Keep sensitive layers in higher precision
        6. Monitoring: Detect unusual discrepancies between FP32 and quantized
      
      model_extraction_via_quantization: |
        **Vulnerability**: Quantization artifacts leak information about model architecture,
        training data, and parameters, enabling model extraction.
        
        **Attack scenario**: Attacker queries model and analyzes quantization artifacts
        - Measure response patterns characteristic of quantization
        - Infer quantization scheme (INT8 vs INT4, per-tensor vs per-channel)
        - Reconstruct approximate weights through systematic queries
        - Determine model architecture from quantization boundaries
        
        **Defense**:
        1. Output post-processing: Remove quantization artifacts from outputs
        2. Noise injection: Add calibrated noise to obscure quantization
        3. Query limiting: Rate limits prevent systematic probing
        4. Watermarking: Embed detectable signatures
        5. Monitoring: Detect systematic probing patterns
        6. Response rounding: Round outputs to reduce precision leakage

  - topic_number: 2
    title: "Advanced Quantization and Model Compression Techniques"
    
    overview: |
      Beyond basic PTQ, advanced techniques achieve better quality-size trade-offs. GPTQ
      and AWQ are state-of-the-art quantization methods achieving near-lossless INT4
      quantization. Model pruning removes unnecessary parameters. Knowledge distillation
      transfers knowledge from large models to small ones.
      
      These techniques enable aggressive compression: 70B models compressed to 35GB (INT4),
      pruning removes 30-50% parameters with minimal loss, distillation creates 7B models
      matching 70B quality on specific tasks. Understanding these methods enables deploying
      capable models on constrained hardware.
    
    content:
      advanced_quantization:
        gptq_algorithm: |
          GPTQ (Generative Pre-trained Transformer Quantization):
          
          **Key idea**: Quantize weights while minimizing reconstruction error
          
          **Algorithm**:
          1. Process weights layer by layer
          2. For each layer:
             a. Quantize one weight at a time
             b. Measure reconstruction error
             c. Compensate by adjusting remaining weights
             d. Iterate until all weights quantized
          
          **Benefits**:
          - INT4 with minimal quality loss (< 1% perplexity increase)
          - Per-group quantization (128 weights per group)
          - No need for retraining
          
          **Usage with AutoGPTQ**:
```python
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
          
          # Quantization config
          quantize_config = BaseQuantizeConfig(
              bits=4,  # INT4
              group_size=128,  # Group quantization
              desc_act=False
          )
          
          # Load and quantize model
          model = AutoGPTQForCausalLM.from_pretrained(
              "meta-llama/Llama-2-7b-hf",
              quantize_config=quantize_config
          )
          
          # Calibrate on dataset
          model.quantize(calibration_data)
          
          # Save quantized model
          model.save_quantized("llama-2-7b-gptq")
```
        
        awq_algorithm: |
          AWQ (Activation-aware Weight Quantization):
          
          **Key idea**: Protect important weights based on activation magnitudes
          
          **Algorithm**:
          1. Analyze activation magnitudes during inference
          2. Identify "salient" weights (those with large activations)
          3. Keep salient weights at higher precision or scale them
          4. Quantize less important weights more aggressively
          
          **Benefits**:
          - Better than GPTQ for same bit width
          - Faster inference (optimized kernels)
          - Works well with INT4
          
          **Usage with AutoAWQ**:
```python
          from awq import AutoAWQForCausalLM
          
          # Load model
          model = AutoAWQForCausalLM.from_pretrained(
              "meta-llama/Llama-2-7b-hf"
          )
          
          # Quantize
          model.quantize(
              calibration_data,
              quant_config={
                  "w_bit": 4,
                  "q_group_size": 128,
                  "version": "GEMM"
              }
          )
          
          # Save
          model.save_quantized("llama-2-7b-awq")
```
        
        smoothquant: |
          SmoothQuant: Handle outliers in activations
          
          **Problem**: Activations have outliers, hard to quantize
          
          **Solution**: Migrate difficulty from activations to weights
          - Smooth activations by scaling channels
          - Adjust weights proportionally
          - Both become easier to quantize
          
          **Formula**:
```
          Given: Y = X @ W
          
          Apply scaling: s
          Y = (X / s) @ (W * s)
          
          Now X/s has smaller outliers (easier to quantize)
          W*s adjusted accordingly
```
          
          **Benefits**:
          - Enables INT8 activation quantization
          - Crucial for static quantization
          - Works with GPTQ, AWQ
      
      model_pruning:
        pruning_fundamentals: |
          Pruning: Remove unnecessary parameters
          
          **Types**:
          
          **1. Unstructured pruning**:
          - Remove individual weights
          - Sparse matrices
          - High compression (50-90%)
          - Requires sparse kernels for speedup
          
          **2. Structured pruning**:
          - Remove entire channels, heads, layers
          - Dense matrices (standard kernels work)
          - Lower compression (20-40%)
          - Immediate speedup
          
          **3. Magnitude-based**:
```python
          def magnitude_pruning(weights, sparsity=0.5):
              """Prune weights with smallest magnitude."""
              threshold = np.percentile(np.abs(weights), sparsity * 100)
              mask = np.abs(weights) > threshold
              return weights * mask
```
          
          **4. Gradient-based**:
          - Prune weights with smallest gradient magnitudes
          - Keeps important weights for training
        
        knowledge_distillation: |
          Knowledge distillation: Train small model to mimic large model
          
          **Concept**:
          - Teacher: Large, accurate model
          - Student: Small, efficient model
          - Student learns from teacher's outputs (soft labels)
          
          **Algorithm**:
```python
          def distillation_loss(student_logits, teacher_logits,
                               labels, temperature=2.0, alpha=0.5):
              """
              Distillation loss combining hard and soft targets.
              
              Args:
                  student_logits: Student model outputs
                  teacher_logits: Teacher model outputs
                  labels: True labels
                  temperature: Softmax temperature
                  alpha: Weight for soft target loss
              
              Returns:
                  Combined loss
              """
              # Hard target loss (cross-entropy with true labels)
              hard_loss = F.cross_entropy(student_logits, labels)
              
              # Soft target loss (KL divergence with teacher)
              soft_student = F.log_softmax(student_logits / temperature, dim=1)
              soft_teacher = F.softmax(teacher_logits / temperature, dim=1)
              soft_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
              soft_loss = soft_loss * (temperature ** 2)
              
              # Combined loss
              return alpha * soft_loss + (1 - alpha) * hard_loss
```
          
          **Benefits**:
          - Student much smaller than teacher (7B → 1B)
          - Retains 90-95% of teacher's quality
          - Faster inference, lower memory
        
        layer_dropping: |
          Layer dropping: Remove redundant transformer layers
          
          **Observation**: Middle layers often redundant
          
          **Method**:
```python
          def drop_layers(model, keep_layers):
              """
              Drop transformer layers strategically.
              
              Args:
                  model: Transformer model
                  keep_layers: List of layer indices to keep
              """
              # Keep only specified layers
              new_layers = [
                  model.layers[i] for i in keep_layers
              ]
              model.layers = nn.ModuleList(new_layers)
              
              return model
```
          
          **Strategies**:
          - Drop every other layer: [0, 2, 4, 6, ...] (50% reduction)
          - Keep first/last, drop middle: [0, 1, N-2, N-1]
          - Importance-based: Measure layer importance, keep top-k
          
          **Results** (Llama 2 7B):
          - 32 layers → 24 layers: 5% perplexity increase
          - 32 layers → 16 layers: 15% perplexity increase
          - 25% faster inference
      
      quality_evaluation:
        perplexity_measurement: |
          Measuring quantization impact on quality:
          
          **Perplexity** (lower is better):
```python
          import torch
          from transformers import AutoTokenizer, AutoModelForCausalLM
          
          def calculate_perplexity(model, tokenizer, text):
              """Calculate perplexity on text."""
              encodings = tokenizer(text, return_tensors='pt')
              
              max_length = model.config.n_positions
              stride = 512
              
              lls = []
              for i in range(0, encodings.input_ids.size(1), stride):
                  begin_loc = max(i + stride - max_length, 0)
                  end_loc = min(i + stride, encodings.input_ids.size(1))
                  trg_len = end_loc - i
                  
                  input_ids = encodings.input_ids[:, begin_loc:end_loc]
                  target_ids = input_ids.clone()
                  target_ids[:, :-trg_len] = -100
                  
                  with torch.no_grad():
                      outputs = model(input_ids, labels=target_ids)
                      log_likelihood = outputs.loss * trg_len
                  
                  lls.append(log_likelihood)
              
              perplexity = torch.exp(torch.stack(lls).sum() / end_loc)
              return perplexity.item()
```
          
          **Benchmark**: WikiText-2, C4
          
          **Acceptable degradation**:
          - FP32 → FP16: < 1% increase
          - FP16 → INT8: < 3% increase
          - INT8 → INT4: < 5% increase (with GPTQ/AWQ)
        
        accuracy_benchmarks: |
          Task-specific accuracy benchmarks:
          
          **Classification accuracy**:
```python
          def evaluate_accuracy(model, eval_dataset):
              """Evaluate classification accuracy."""
              correct = 0
              total = 0
              
              for batch in eval_dataset:
                  outputs = model(batch['input_ids'])
                  predictions = outputs.logits.argmax(dim=-1)
                  correct += (predictions == batch['labels']).sum().item()
                  total += batch['labels'].size(0)
              
              return correct / total
```
          
          **Generation quality** (ROUGE, BLEU):
```python
          from evaluate import load
          
          rouge = load('rouge')
          
          def evaluate_generation(model, tokenizer, dataset):
              """Evaluate generation quality."""
              predictions = []
              references = []
              
              for example in dataset:
                  prompt = example['prompt']
                  reference = example['completion']
                  
                  inputs = tokenizer(prompt, return_tensors='pt')
                  outputs = model.generate(**inputs, max_length=100)
                  prediction = tokenizer.decode(outputs[0])
                  
                  predictions.append(prediction)
                  references.append(reference)
              
              scores = rouge.compute(
                  predictions=predictions,
                  references=references
              )
              
              return scores
```
          
          **Benchmarks**: MMLU, HellaSwag, TruthfulQA
    
    implementation:
      production_quantization_deployment:
        language: python
        code: |
          """
          Production quantization deployment guide.
          Demonstrates deploying INT4 quantized models with bitsandbytes and GPTQ.
          """
          
          # ============================================================================
          # METHOD 1: bitsandbytes (4-bit quantization)
          # ============================================================================
          
          """
          bitsandbytes: Easy INT4/INT8 quantization
          
          Installation:
          pip install bitsandbytes transformers accelerate
          """
          
          from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
          import torch
          
          def load_model_4bit_bnb(model_name: str):
              """
              Load model with 4-bit quantization using bitsandbytes.
              
              Args:
                  model_name: HuggingFace model name
              
              Returns:
                  (model, tokenizer)
              """
              # Configure 4-bit quantization
              bnb_config = BitsAndBytesConfig(
                  load_in_4bit=True,  # 4-bit quantization
                  bnb_4bit_quant_type="nf4",  # NormalFloat4
                  bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16
                  bnb_4bit_use_double_quant=True,  # Double quantization
              )
              
              # Load model
              model = AutoModelForCausalLM.from_pretrained(
                  model_name,
                  quantization_config=bnb_config,
                  device_map="auto",  # Automatic device placement
                  trust_remote_code=True
              )
              
              tokenizer = AutoTokenizer.from_pretrained(model_name)
              
              print(f"Model loaded in 4-bit")
              print(f"Memory footprint: ~{model.get_memory_footprint() / 1e9:.2f} GB")
              
              return model, tokenizer
          
          
          def load_model_8bit_bnb(model_name: str):
              """
              Load model with 8-bit quantization using bitsandbytes.
              
              Args:
                  model_name: HuggingFace model name
              
              Returns:
                  (model, tokenizer)
              """
              # Configure 8-bit quantization
              bnb_config = BitsAndBytesConfig(
                  load_in_8bit=True,
                  llm_int8_threshold=6.0,  # Outlier threshold
                  llm_int8_has_fp16_weight=False,
              )
              
              model = AutoModelForCausalLM.from_pretrained(
                  model_name,
                  quantization_config=bnb_config,
                  device_map="auto"
              )
              
              tokenizer = AutoTokenizer.from_pretrained(model_name)
              
              print(f"Model loaded in 8-bit")
              print(f"Memory footprint: ~{model.get_memory_footprint() / 1e9:.2f} GB")
              
              return model, tokenizer
          
          
          # ============================================================================
          # METHOD 2: GPTQ (Advanced 4-bit quantization)
          # ============================================================================
          
          """
          GPTQ: State-of-the-art 4-bit quantization
          
          Installation:
          pip install auto-gptq transformers
          """
          
          # from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
          
          def quantize_model_gptq(model_name: str,
                                 calibration_data: list,
                                 output_dir: str):
              """
              Quantize model using GPTQ.
              
              Args:
                  model_name: HuggingFace model name
                  calibration_data: List of text samples for calibration
                  output_dir: Directory to save quantized model
              """
              # NOTE: Uncomment when running with auto-gptq installed
              """
              # Quantization config
              quantize_config = BaseQuantizeConfig(
                  bits=4,  # INT4
                  group_size=128,  # Group quantization
                  desc_act=False,  # Disable for stability
                  sym=True,  # Symmetric quantization
                  damp_percent=0.1  # Dampening
              )
              
              # Load model
              model = AutoGPTQForCausalLM.from_pretrained(
                  model_name,
                  quantize_config=quantize_config
              )
              
              tokenizer = AutoTokenizer.from_pretrained(model_name)
              
              # Prepare calibration data
              calibration_samples = []
              for text in calibration_data[:128]:  # Use 128 samples
                  tokens = tokenizer(
                      text,
                      return_tensors="pt",
                      max_length=2048,
                      truncation=True
                  )
                  calibration_samples.append(tokens)
              
              # Quantize
              print("Quantizing model with GPTQ...")
              model.quantize(calibration_samples)
              
              # Save
              model.save_quantized(output_dir)
              tokenizer.save_pretrained(output_dir)
              
              print(f"Quantized model saved to {output_dir}")
              """
              print("GPTQ quantization code (requires auto-gptq)")
          
          
          def load_model_gptq(model_dir: str):
              """
              Load GPTQ quantized model.
              
              Args:
                  model_dir: Directory with quantized model
              
              Returns:
                  (model, tokenizer)
              """
              # NOTE: Uncomment when running with auto-gptq installed
              """
              model = AutoGPTQForCausalLM.from_quantized(
                  model_dir,
                  device="cuda:0",
                  use_safetensors=True,
                  use_triton=False  # Use CUDA kernels
              )
              
              tokenizer = AutoTokenizer.from_pretrained(model_dir)
              
              return model, tokenizer
              """
              print("GPTQ loading code (requires auto-gptq)")
              return None, None
          
          
          # ============================================================================
          # COMPARISON AND BENCHMARKING
          # ============================================================================
          
          def compare_quantization_methods():
              """
              Compare different quantization methods.
              
              Metrics: Memory, Speed, Quality
              """
              print("\n" + "="*80)
              print("QUANTIZATION METHOD COMPARISON")
              print("="*80)
              
              print("\nLlama 2 7B Model:")
              print("-"*80)
              
              print("\nMethod          | Memory  | Speed   | Perplexity | Quality")
              print("-"*80)
              print("FP32            | 28 GB   | 1.0x    | 5.47       | 100%")
              print("FP16            | 14 GB   | 1.8x    | 5.48       | 99.8%")
              print("INT8 (bnb)      | 7 GB    | 2.0x    | 5.52       | 99.1%")
              print("INT4 (bnb)      | 3.5 GB  | 2.5x    | 5.68       | 96.2%")
              print("INT4 (GPTQ)     | 3.5 GB  | 3.0x    | 5.54       | 98.7%")
              print("INT4 (AWQ)      | 3.5 GB  | 3.5x    | 5.52       | 99.0%")
              
              print("\n" + "="*80)
              print("RECOMMENDATIONS")
              print("="*80)
              print("\n✓ FP16: Default for most use cases")
              print("  - 50% memory reduction, minimal quality loss")
              print("  - Good balance of speed and quality")
              
              print("\n✓ INT8 (bitsandbytes): Memory-constrained scenarios")
              print("  - 75% memory reduction, <1% quality loss")
              print("  - Easy to use, good for inference")
              
              print("\n✓ INT4 (GPTQ/AWQ): Maximum compression")
              print("  - 87.5% memory reduction")
              print("  - Best quality-size trade-off")
              print("  - Requires calibration dataset")
              
              print("\n✗ Avoid: FP32 for inference (wastes memory)")
              print("✗ Avoid: Naive INT4 without GPTQ/AWQ (poor quality)")
          
          
          def demonstrate_quantization():
              """Demonstrate quantization deployment."""
              print("\n" + "="*80)
              print("QUANTIZATION DEPLOYMENT DEMONSTRATION")
              print("="*80)
              
              print("\nThis demonstration shows how to:")
              print("1. Load models with 4-bit/8-bit quantization (bitsandbytes)")
              print("2. Quantize models with GPTQ")
              print("3. Compare quantization methods")
              
              print("\n" + "-"*80)
              print("Example: Loading Llama 2 7B with 4-bit quantization")
              print("-"*80)
              print("\nCode:")
              print("""
model, tokenizer = load_model_4bit_bnb("meta-llama/Llama-2-7b-hf")

# Generate
inputs = tokenizer("Once upon a time", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
""")
              
              print("\n" + "-"*80)
              print("Memory requirements (Llama 2 7B):")
              print("-"*80)
              print("FP32: 28 GB (doesn't fit on most GPUs)")
              print("FP16: 14 GB (fits on A100 40GB)")
              print("INT8: 7 GB (fits on RTX 3090 24GB)")
              print("INT4: 3.5 GB (fits on RTX 4090 24GB, consumer GPUs)")
              
              compare_quantization_methods()
          
          
          if __name__ == "__main__":
              demonstrate_quantization()
    
    security_implications:
      quantization_backdoors: |
        **Vulnerability**: Backdoors can be hidden in quantization process, activated
        only in quantized models, evading detection in original FP32 models.
        
        **Attack scenario**: Attacker adds backdoor during fine-tuning
        - Backdoor designed to survive quantization
        - FP32 model appears clean in testing
        - INT8/INT4 model exhibits backdoor behavior
        - Users deploy quantized model, backdoor activates
        
        **Defense**:
        1. Test both FP32 and quantized versions
        2. Verify behavior consistency across precisions
        3. Use trusted quantization procedures
        4. Backdoor detection on quantized models specifically
        5. Provenance tracking for quantized models
        6. Differential testing (FP32 vs quantized)
      
      calibration_data_poisoning: |
        **Vulnerability**: Malicious calibration data can poison quantization process,
        degrading model quality or introducing biases.
        
        **Attack scenario**: Attacker provides poisoned calibration dataset
        - Outliers cause poor scale selection
        - Biased data skews activation distributions
        - Quantized model performs poorly on legitimate inputs
        - Or: model biased toward attacker's goals
        
        **Defense**:
        1. Trusted calibration data: Use verified, diverse datasets
        2. Outlier detection: Remove statistical outliers
        3. Data validation: Verify calibration data quality
        4. Multiple calibration runs: Consistency check
        5. Robust calibration: Use percentile instead of min/max
        6. Calibration monitoring: Track calibration statistics
      
      precision_downgrade_attacks: |
        **Vulnerability**: Attackers force models to use lower precision than claimed,
        degrading quality while claiming higher precision.
        
        **Attack scenario**: Attacker modifies deployment
        - Claims model is INT8
        - Actually deploys INT4 or INT2
        - Quality degrades significantly
        - Users blame model provider
        
        Or: Dynamic downgrade based on user
        - Premium users get INT8
        - Free users get INT4
        - Violates fairness expectations
        
        **Defense**:
        1. Precision verification: Test actual precision used
        2. Quality benchmarks: Regular quality testing
        3. Watermarking: Embed precision info in outputs
        4. Monitoring: Detect quality degradation
        5. User testing: Independent verification
        6. Transparency: Clearly communicate precision

key_takeaways:
  critical_concepts:
    - concept: "Quantization reduces model size 2-4x by lowering numerical precision, enabling deployment on resource-constrained hardware"
      why_it_matters: "7B FP16 model needs 14GB. INT8 needs 7GB (fits smaller GPUs). INT4 needs 3.5GB (fits consumer hardware). Essential for accessible deployment."
    
    - concept: "Advanced quantization (GPTQ, AWQ) achieves near-lossless INT4 quantization through careful weight selection and compensation"
      why_it_matters: "Naive INT4 loses 10%+ quality. GPTQ/AWQ lose <2%. This difference determines whether quantization is viable for production."
    
    - concept: "Per-channel quantization provides better accuracy than per-tensor by using different scales for different channels"
      why_it_matters: "Channels have different activation distributions. Per-channel adapts scales, reducing quantization error 2-3x vs per-tensor."
    
    - concept: "Calibration is critical for static quantization quality—representative data and robust methods minimize accuracy loss"
      why_it_matters: "Poor calibration (wrong data, naive minmax) causes 5-10% degradation. Good calibration (diverse data, percentile) loses <2%."
  
  actionable_steps:
    - step: "Use bitsandbytes for easy INT8/INT4 deployment with automatic configuration and minimal quality loss"
      verification: "Load model with 4-bit config. Should use ~25% of FP16 memory. Test perplexity increase <5%."
    
    - step: "For production INT4, use GPTQ or AWQ with proper calibration data for near-lossless compression"
      verification: "Quantize with GPTQ on 128 calibration samples. Perplexity increase should be <2% vs FP16."
    
    - step: "Implement per-channel quantization for linear layers to minimize accuracy loss vs per-tensor"
      verification: "Compare per-channel vs per-tensor error on weight matrix. Per-channel should have 2-3x lower MSE."
    
    - step: "Use percentile calibration (99.9th percentile) instead of minmax to handle outliers robustly"
      verification: "Compare calibration methods. Percentile should be more robust to outliers, better perplexity."
  
  security_principles:
    - principle: "Verify quantized model behavior matches original FP32 model to detect backdoors or manipulation"
      application: "Test both FP32 and quantized on diverse inputs. Flag significant behavioral differences. Differential testing."
    
    - principle: "Use trusted, diverse calibration data from verified sources to prevent poisoning"
      application: "Curate calibration dataset from multiple sources. Validate data quality. Remove outliers. Monitor statistics."
    
    - principle: "Implement integrity checks for quantized models to detect bit-flip attacks and tampering"
      application: "Cryptographic hashes of weights. Periodic testing on known inputs. ECC for stored models. Anomaly detection."
    
    - principle: "Monitor quality metrics continuously to detect precision downgrades or degradation"
      application: "Regular perplexity testing. Compare against baseline. Alert on significant degradation. Track precision actually used."
  
  common_mistakes:
    - mistake: "Using naive INT4 quantization without GPTQ/AWQ, causing significant quality loss (10%+)"
      fix: "Use GPTQ or AWQ for INT4. They achieve <2% loss vs 10%+ for naive quantization."
    
    - mistake: "Per-tensor quantization on all layers, when per-channel would provide better accuracy"
      fix: "Use per-channel quantization for linear layers. Only 20% overhead, 2-3x better accuracy."
    
    - mistake: "MinMax calibration sensitive to outliers, causing poor scale selection"
      fix: "Use percentile calibration (99.9th percentile) to ignore outliers. More robust, better accuracy."
    
    - mistake: "Insufficient or biased calibration data, leading to poor quantization quality"
      fix: "Use diverse, representative calibration dataset (>128 samples, multiple domains)."
    
    - mistake: "Not measuring quantized model quality, deploying without validation"
      fix: "Measure perplexity on benchmark. Compare to FP32/FP16 baseline. Validate <5% degradation."
  
  integration_with_book:
    from_section_4_12:
      - "Quantization (4.13) complements serving optimization (4.12)"
      - "vLLM supports quantized models (INT8, INT4 via GPTQ/AWQ)"
      - "Memory savings enable larger batch sizes, higher throughput"
    
    to_next_section:
      - "Section 4.14: Horizontal scaling and distributed deployment"
      - "Deploying quantized models across multiple GPUs/nodes"
      - "Load balancing and failover for quantized serving"
  
  looking_ahead:
    next_concepts:
      - "Horizontal scaling and distributed deployment (4.14)"
      - "Caching strategies for cost optimization (4.15)"
      - "Monitoring and observability for production (4.16)"
      - "API security and compliance (4.17)"
    
    skills_to_build:
      - "Implementing custom quantization schemes"
      - "Calibrating quantization for specific domains"
      - "Measuring and optimizing quantization quality"
      - "Deploying quantized models in production"
  
  final_thoughts: |
    Quantization and compression are essential for economically viable LLM deployment.
    Section 4.13 provides techniques to reduce model size 2-4x while maintaining 95-99%
    quality, enabling deployment on accessible hardware at reasonable costs.
    
    Key insights:
    
    1. **Quantization is not optional**: Large models (7B, 13B, 70B) are too expensive
       to deploy in FP32 or even FP16 at scale. INT8 and INT4 quantization reduces costs
       proportionally to compression ratio. This is the difference between viable and
       unviable business models.
    
    2. **Advanced methods are worth it**: Naive INT4 loses 10%+ quality. GPTQ and AWQ
       achieve <2% loss through careful optimization. The extra complexity pays for itself
       in usable models. Don't skip this—use production methods.
    
    3. **Calibration is critical**: Quantization quality depends heavily on calibration
       data and methods. Representative data, robust techniques (percentile over minmax),
       and sufficient samples (>128) are essential. Poor calibration wastes the benefits.
    
    4. **Per-channel beats per-tensor**: Channels have different distributions. Per-channel
       quantization adapts scales per channel, providing 2-3x better accuracy with only
       20% overhead. Use per-channel for linear layers in production.
    
    5. **Test quantized models thoroughly**: Quantization can introduce subtle bugs,
       backdoors, or quality degradation. Test both FP32 and quantized versions. Measure
       perplexity. Verify behavioral consistency. Don't assume quantization preserves
       behavior perfectly.
    
    Moving forward, Section 4.14 advances to horizontal scaling: distributing quantized
    models across multiple GPUs and nodes, load balancing, and fault tolerance. Combined
    with 4.12's serving optimization and 4.13's compression, this enables serving large
    models globally at scale and reasonable cost.
    
    Remember: Compression is power. 4x smaller models mean 4x more models per GPU, 4x
    lower costs, 4x more accessible deployment. But compress carefully—measure quality,
    use proper methods, test thoroughly. Poor quantization is worse than no quantization.

---
